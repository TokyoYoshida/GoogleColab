{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playground_dqn_keiba",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TokyoYoshida/GoogleColab/blob/master/playground_dqn_keiba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXxOmD7gDtxX",
        "colab_type": "code",
        "outputId": "df5fd937-20ff-4398-bcf4-f78c44c7fd65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 必要ならば以下のようにディレクトリ移動する\n",
        " "
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB5kzWFnDvLF",
        "colab_type": "code",
        "outputId": "a20383dd-b6f2-4b8c-ab1a-aa8832b25763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/'My Drive'/'★個人専用ファイル'/'開発用・個人サービス開発'/'GoogleColab'/"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/★個人専用ファイル/開発用・個人サービス開発/GoogleColab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tT1SbekEPKY",
        "colab_type": "code",
        "outputId": "c1388488-e7d9-4196-d4f0-93c2c9695519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "%ls data/keiba\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "darby.csv\n",
            "keiba_dat_res_20200528_2355.pkl\n",
            "keiba_dat_res_20200528.pkl\n",
            "keiba_dat_res_20200529_2306_20200531backup.pkl\n",
            "keiba_dat_res_20200529_2306.pkl\n",
            "keiba_dat_res_20200531_1207.pkl\n",
            "keiba_dat_res_20200531_1707.pkl\n",
            "keiba_predict_20200604_1617.pkl\n",
            "result_20200528_2020年時点最新競馬データ.csv\n",
            "調教師一覧.csv\n",
            "調教師一覧.gsheet\n",
            "馬一覧.csv\n",
            "馬一覧.gsheet\n",
            "騎手一覧.csv\n",
            "騎手一覧.gsheet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oeldNv7-4ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdsoSSOZZo9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9aBWsjhDpQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 競馬データの作成"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIWclTGw-4nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ldat = pd.read_pickle(\"./data/keiba/keiba_predict_20200604_1617.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPxKeoI6z0--",
        "colab_type": "code",
        "outputId": "c5c61f45-08e9-4cb5-ae8a-d0ed6604426d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "ldat"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict_lose</th>\n",
              "      <th>predict_win</th>\n",
              "      <th>age</th>\n",
              "      <th>amount</th>\n",
              "      <th>condition</th>\n",
              "      <th>day</th>\n",
              "      <th>diff_arrival</th>\n",
              "      <th>diff_weight</th>\n",
              "      <th>direction</th>\n",
              "      <th>distance</th>\n",
              "      <th>frame_num</th>\n",
              "      <th>full_path</th>\n",
              "      <th>grade</th>\n",
              "      <th>ground</th>\n",
              "      <th>horse_num</th>\n",
              "      <th>jockey</th>\n",
              "      <th>month</th>\n",
              "      <th>name</th>\n",
              "      <th>odds</th>\n",
              "      <th>popularity</th>\n",
              "      <th>prev_age</th>\n",
              "      <th>prev_amount</th>\n",
              "      <th>prev_condition</th>\n",
              "      <th>prev_day</th>\n",
              "      <th>prev_diff_arrival</th>\n",
              "      <th>prev_diff_weight</th>\n",
              "      <th>prev_direction</th>\n",
              "      <th>prev_distance</th>\n",
              "      <th>prev_frame_num</th>\n",
              "      <th>prev_full_path</th>\n",
              "      <th>prev_grade</th>\n",
              "      <th>prev_ground</th>\n",
              "      <th>prev_horse_num</th>\n",
              "      <th>prev_jockey</th>\n",
              "      <th>prev_month</th>\n",
              "      <th>prev_name</th>\n",
              "      <th>prev_odds</th>\n",
              "      <th>prev_popularity</th>\n",
              "      <th>prev_race_index</th>\n",
              "      <th>prev_race_num</th>\n",
              "      <th>prev_rank</th>\n",
              "      <th>prev_return</th>\n",
              "      <th>prev_sex</th>\n",
              "      <th>prev_start_time</th>\n",
              "      <th>prev_time</th>\n",
              "      <th>prev_time_float</th>\n",
              "      <th>prev_trainer</th>\n",
              "      <th>prev_unknown</th>\n",
              "      <th>prev_weather</th>\n",
              "      <th>prev_weight</th>\n",
              "      <th>prev_winner</th>\n",
              "      <th>prev_year</th>\n",
              "      <th>prev_ymd</th>\n",
              "      <th>race_index</th>\n",
              "      <th>race_num</th>\n",
              "      <th>rank</th>\n",
              "      <th>return</th>\n",
              "      <th>sex</th>\n",
              "      <th>start_time</th>\n",
              "      <th>time</th>\n",
              "      <th>time_float</th>\n",
              "      <th>trainer</th>\n",
              "      <th>unknown</th>\n",
              "      <th>weather</th>\n",
              "      <th>weight</th>\n",
              "      <th>winner</th>\n",
              "      <th>year</th>\n",
              "      <th>ymd</th>\n",
              "      <th>prev_high_rank</th>\n",
              "      <th>high_rank</th>\n",
              "      <th>prev_rank_level</th>\n",
              "      <th>rank_level</th>\n",
              "      <th>prev_popularity_level</th>\n",
              "      <th>prev_speed</th>\n",
              "      <th>prev_speed_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>634807</th>\n",
              "      <td>0.854893</td>\n",
              "      <td>0.074793</td>\n",
              "      <td>3</td>\n",
              "      <td>54.0</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>1</td>\n",
              "      <td>html/200903010502.html</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>254</td>\n",
              "      <td>4</td>\n",
              "      <td>8915</td>\n",
              "      <td>34.8</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1600.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>html/200905010603.html</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>254.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8915.0</td>\n",
              "      <td>9.7</td>\n",
              "      <td>4.0</td>\n",
              "      <td>34276.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11:10</td>\n",
              "      <td>1:43.2</td>\n",
              "      <td>103.2</td>\n",
              "      <td>293.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>470.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>20090215.0</td>\n",
              "      <td>44210</td>\n",
              "      <td>2</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>10:20</td>\n",
              "      <td>1:12.7</td>\n",
              "      <td>72.7</td>\n",
              "      <td>147</td>\n",
              "      <td>-</td>\n",
              "      <td>2</td>\n",
              "      <td>480.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2009</td>\n",
              "      <td>20090425</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.503876</td>\n",
              "      <td>15.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558388</th>\n",
              "      <td>0.763956</td>\n",
              "      <td>0.223389</td>\n",
              "      <td>3</td>\n",
              "      <td>54.0</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1700.0</td>\n",
              "      <td>2</td>\n",
              "      <td>html/201302030602.html</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "      <td>8</td>\n",
              "      <td>6151</td>\n",
              "      <td>11.9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1700.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>html/201302030106.html</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6151.0</td>\n",
              "      <td>24.2</td>\n",
              "      <td>8.0</td>\n",
              "      <td>24337.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12:35</td>\n",
              "      <td>1:46.9</td>\n",
              "      <td>106.9</td>\n",
              "      <td>217.0</td>\n",
              "      <td>-</td>\n",
              "      <td>1.0</td>\n",
              "      <td>460.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>20130727.0</td>\n",
              "      <td>38889</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>0</td>\n",
              "      <td>10:20</td>\n",
              "      <td>1:47.8</td>\n",
              "      <td>107.8</td>\n",
              "      <td>217</td>\n",
              "      <td>-</td>\n",
              "      <td>1</td>\n",
              "      <td>454.0</td>\n",
              "      <td>True</td>\n",
              "      <td>2013</td>\n",
              "      <td>20130811</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.902713</td>\n",
              "      <td>15.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559400</th>\n",
              "      <td>0.819242</td>\n",
              "      <td>0.198124</td>\n",
              "      <td>3</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-8.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>6</td>\n",
              "      <td>html/201907040208.html</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>130</td>\n",
              "      <td>12</td>\n",
              "      <td>8722</td>\n",
              "      <td>14.4</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1600.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>html/201908050507.html</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8722.0</td>\n",
              "      <td>62.7</td>\n",
              "      <td>13.0</td>\n",
              "      <td>30194.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13:20</td>\n",
              "      <td>1:35.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>外</td>\n",
              "      <td>0.0</td>\n",
              "      <td>494.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>20191116.0</td>\n",
              "      <td>38961</td>\n",
              "      <td>8</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>13:45</td>\n",
              "      <td>1:24.6</td>\n",
              "      <td>84.6</td>\n",
              "      <td>17</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>486.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2019</td>\n",
              "      <td>20191201</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>16.842105</td>\n",
              "      <td>16.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575366</th>\n",
              "      <td>0.917330</td>\n",
              "      <td>0.076320</td>\n",
              "      <td>3</td>\n",
              "      <td>54.0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1600.0</td>\n",
              "      <td>6</td>\n",
              "      <td>html/201408020310.html</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>45029</td>\n",
              "      <td>59.9</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>html/201408010804.html</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45029.0</td>\n",
              "      <td>15.4</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17293.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11:35</td>\n",
              "      <td>1:27.6</td>\n",
              "      <td>87.6</td>\n",
              "      <td>199.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>452.0</td>\n",
              "      <td>True</td>\n",
              "      <td>2014.0</td>\n",
              "      <td>20140125.0</td>\n",
              "      <td>40071</td>\n",
              "      <td>10</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>15:10</td>\n",
              "      <td>1:39.4</td>\n",
              "      <td>99.4</td>\n",
              "      <td>199</td>\n",
              "      <td>外</td>\n",
              "      <td>1</td>\n",
              "      <td>448.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2014</td>\n",
              "      <td>20140208</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.981735</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>600832</th>\n",
              "      <td>0.765905</td>\n",
              "      <td>0.196395</td>\n",
              "      <td>3</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>7</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1800.0</td>\n",
              "      <td>5</td>\n",
              "      <td>html/201006030202.html</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>29</td>\n",
              "      <td>3</td>\n",
              "      <td>61110</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1600.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>html/201005010805.html</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>61110.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24370.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12:20</td>\n",
              "      <td>1:40.1</td>\n",
              "      <td>100.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>532.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>20100221.0</td>\n",
              "      <td>41853</td>\n",
              "      <td>2</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>10:15</td>\n",
              "      <td>1:59.6</td>\n",
              "      <td>119.6</td>\n",
              "      <td>29</td>\n",
              "      <td>-</td>\n",
              "      <td>1</td>\n",
              "      <td>536.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2010</td>\n",
              "      <td>20100328</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.984016</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534140</th>\n",
              "      <td>0.710303</td>\n",
              "      <td>0.292937</td>\n",
              "      <td>3</td>\n",
              "      <td>54.0</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>5</td>\n",
              "      <td>html/201209020206.html</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>107</td>\n",
              "      <td>3</td>\n",
              "      <td>23576</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>html/201209010611.html</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>23576.0</td>\n",
              "      <td>120.5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>7863.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15:35</td>\n",
              "      <td>1:23.7</td>\n",
              "      <td>83.7</td>\n",
              "      <td>168.0</td>\n",
              "      <td>-</td>\n",
              "      <td>1.0</td>\n",
              "      <td>434.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>20120311.0</td>\n",
              "      <td>37204</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>12:30</td>\n",
              "      <td>1:11.1</td>\n",
              "      <td>71.1</td>\n",
              "      <td>168</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>436.0</td>\n",
              "      <td>True</td>\n",
              "      <td>2012</td>\n",
              "      <td>20120325</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16.726404</td>\n",
              "      <td>16.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608330</th>\n",
              "      <td>0.674699</td>\n",
              "      <td>0.267390</td>\n",
              "      <td>5</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>6</td>\n",
              "      <td>html/200701010809.html</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>60</td>\n",
              "      <td>9</td>\n",
              "      <td>1391</td>\n",
              "      <td>7.4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>57.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>html/200702020409.html</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1391.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>31722.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15:25</td>\n",
              "      <td>2:03.1</td>\n",
              "      <td>123.1</td>\n",
              "      <td>179.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>526.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>20070722.0</td>\n",
              "      <td>42369</td>\n",
              "      <td>9</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>15:25</td>\n",
              "      <td>2:00.8</td>\n",
              "      <td>120.8</td>\n",
              "      <td>179</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>524.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2007</td>\n",
              "      <td>20070902</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.246954</td>\n",
              "      <td>16.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610600</th>\n",
              "      <td>0.846265</td>\n",
              "      <td>0.128427</td>\n",
              "      <td>3</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1800.0</td>\n",
              "      <td>7</td>\n",
              "      <td>html/201006030306.html</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>4</td>\n",
              "      <td>50243</td>\n",
              "      <td>19.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>html/200905050505.html</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>50243.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>26061.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12:10</td>\n",
              "      <td>1:28.1</td>\n",
              "      <td>88.1</td>\n",
              "      <td>33.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>476.0</td>\n",
              "      <td>True</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>20091121.0</td>\n",
              "      <td>42526</td>\n",
              "      <td>6</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>12:50</td>\n",
              "      <td>1:59.1</td>\n",
              "      <td>119.1</td>\n",
              "      <td>33</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>490.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2010</td>\n",
              "      <td>20100403</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.891033</td>\n",
              "      <td>15.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651338</th>\n",
              "      <td>0.866645</td>\n",
              "      <td>0.096126</td>\n",
              "      <td>5</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>16</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>3</td>\n",
              "      <td>html/201509040312.html</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>107</td>\n",
              "      <td>9</td>\n",
              "      <td>4483</td>\n",
              "      <td>43.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>html/201501020606.html</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4483.0</td>\n",
              "      <td>38.4</td>\n",
              "      <td>9.0</td>\n",
              "      <td>23400.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12:40</td>\n",
              "      <td>0:59.4</td>\n",
              "      <td>59.4</td>\n",
              "      <td>269.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>492.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>20150906.0</td>\n",
              "      <td>45364</td>\n",
              "      <td>12</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>16:10</td>\n",
              "      <td>1:12.9</td>\n",
              "      <td>72.9</td>\n",
              "      <td>269</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>482.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2015</td>\n",
              "      <td>20150919</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.835017</td>\n",
              "      <td>16.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610539</th>\n",
              "      <td>0.987555</td>\n",
              "      <td>0.006215</td>\n",
              "      <td>3</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>8</td>\n",
              "      <td>html/201505020804.html</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>85</td>\n",
              "      <td>5</td>\n",
              "      <td>31536</td>\n",
              "      <td>537.5</td>\n",
              "      <td>18.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1150.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>html/201503010601.html</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>31536.0</td>\n",
              "      <td>123.2</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3718.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>09:50</td>\n",
              "      <td>1:12.8</td>\n",
              "      <td>72.8</td>\n",
              "      <td>241.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>20150426.0</td>\n",
              "      <td>42522</td>\n",
              "      <td>4</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>11:35</td>\n",
              "      <td>1:24.5</td>\n",
              "      <td>84.5</td>\n",
              "      <td>241</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>False</td>\n",
              "      <td>2015</td>\n",
              "      <td>20150517</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.796703</td>\n",
              "      <td>15.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>111252 rows × 75 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        predict_lose  predict_win  ...  prev_speed  prev_speed_level\n",
              "634807      0.854893     0.074793  ...   15.503876              15.5\n",
              "558388      0.763956     0.223389  ...   15.902713              15.9\n",
              "559400      0.819242     0.198124  ...   16.842105              16.8\n",
              "575366      0.917330     0.076320  ...   15.981735              16.0\n",
              "600832      0.765905     0.196395  ...   15.984016              16.0\n",
              "...              ...          ...  ...         ...               ...\n",
              "534140      0.710303     0.292937  ...   16.726404              16.7\n",
              "608330      0.674699     0.267390  ...   16.246954              16.2\n",
              "610600      0.846265     0.128427  ...   15.891033              15.9\n",
              "651338      0.866645     0.096126  ...   16.835017              16.8\n",
              "610539      0.987555     0.006215  ...   15.796703              15.8\n",
              "\n",
              "[111252 rows x 75 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGwiRWJ4z1V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlRazfpWyqyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sdat = ldat.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfUTHaPE-4xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = sdat[['frame_num', 'horse_num', 'name', 'sex', 'age', 'amount',\n",
        "#        'jockey', 'odds', 'popularity', 'weight',\n",
        "#        'diff_weight', 'trainer', 'race_num', 'grade', 'ground', 'direction',\n",
        "#         'distance', 'weather', 'condition', 'start_time_float', 'year',\n",
        "#        'month', 'day', 'prev_rank', 'prev_frame_num', 'prev_horse_num',\n",
        "#        'prev_name', 'prev_sex', 'prev_age', 'prev_amount', 'prev_jockey',\n",
        "#        'prev_time_float', 'prev_diff_arrival', 'prev_odds', 'prev_popularity',\n",
        "#        'prev_weight', 'prev_diff_weight', 'prev_trainer', 'prev_race_num',\n",
        "#        'prev_grade', 'prev_ground', 'prev_direction',\n",
        "#        'prev_distance', 'prev_weather', 'prev_condition', 'prev_start_time_float',\n",
        "#        'prev_year', 'prev_month', 'prev_day']].dropna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SW8SNVNSilw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = sdat[['frame_num', 'horse_num', 'sex', 'age', 'amount',\n",
        "#        'odds', 'popularity', 'weight',\n",
        "#        'diff_weight', 'trainer', 'grade', 'ground', 'direction',\n",
        "#         'distance', 'weather', 'condition', 'start_time_float', 'prev_rank', 'prev_frame_num', 'prev_horse_num',\n",
        "#        'prev_sex', 'prev_age', 'prev_amount', 'prev_jockey',\n",
        "#        'prev_time_float', 'prev_diff_arrival', 'prev_odds', 'prev_popularity',\n",
        "#        'prev_weight', 'prev_diff_weight', 'prev_trainer',\n",
        "#        'prev_grade', 'prev_ground', 'prev_direction',\n",
        "#        'prev_distance', 'prev_weather', 'prev_condition', 'prev_start_time_float'\n",
        "#        ]].dropna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L86xkmZGTZ-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = sdat[[\"odds\",\"predict_win\"]].dropna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izs3s8otL3XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = sdat[[\"odds\"]].dropna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT8CcCQR-48e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = sdat[[\"return\"]].dropna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jQ42aW4-5Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 競馬データの作成 終わり"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJdOFuv3-46d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習、テストデータの作成"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2wTrCZbcCdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainlen = int(len(x)/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFK08dQKcCoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainx = x.head(trainlen)\n",
        "trainy = y.head(trainlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW4_Fl4icCt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testx = x.tail(len(x)-trainlen)\n",
        "testy = y.tail(len(x)-trainlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzQJ4Mus-43A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習、テストデータの作成　終わり"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yO24DOecXKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJUwnya_cXRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall -y keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LutfR4VAcXO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e73d75d5-d69b-43c3-9ea6-399ecffc7726"
      },
      "source": [
        "!pip uninstall -y tensorflow"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.14.0:\n",
            "  Successfully uninstalled tensorflow-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qULptQcKFwQ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "f4455904-fbac-4d64-92cb-272a72e3881f"
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "  Using cached https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.29.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.18.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (47.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.1.0)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPH6Z9Cjsf_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqog9TrbFcpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTtTJTMOFssd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwMMRuLyRpqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip uninstall -q keras-rl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0QhjBgmFvtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras-rl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVHdpj8I9ggK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWW3TvU-cy4o",
        "colab_type": "code",
        "outputId": "2ff5a13c-9ba1-4e70-ffc9-857eabeda9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "trainx.min()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odds           1.000000\n",
              "predict_win    0.000001\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL1paZ9XQHQX",
        "colab_type": "code",
        "outputId": "66d3cb36-9fa7-4d3a-cce2-e1fd6b7ad930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip list"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Package                  Version        \n",
            "------------------------ ---------------\n",
            "absl-py                  0.9.0          \n",
            "alabaster                0.7.12         \n",
            "albumentations           0.1.12         \n",
            "altair                   4.1.0          \n",
            "asgiref                  3.2.7          \n",
            "astor                    0.8.1          \n",
            "astropy                  4.0.1.post1    \n",
            "astunparse               1.6.3          \n",
            "atari-py                 0.2.6          \n",
            "atomicwrites             1.4.0          \n",
            "attrs                    19.3.0         \n",
            "audioread                2.1.8          \n",
            "autograd                 1.3            \n",
            "Babel                    2.8.0          \n",
            "backcall                 0.1.0          \n",
            "beautifulsoup4           4.6.3          \n",
            "bleach                   3.1.5          \n",
            "blis                     0.4.1          \n",
            "bokeh                    1.4.0          \n",
            "boto                     2.49.0         \n",
            "boto3                    1.13.19        \n",
            "botocore                 1.16.19        \n",
            "Bottleneck               1.3.2          \n",
            "branca                   0.4.1          \n",
            "bs4                      0.0.1          \n",
            "CacheControl             0.12.6         \n",
            "cachetools               3.1.1          \n",
            "catalogue                1.0.0          \n",
            "certifi                  2020.4.5.1     \n",
            "cffi                     1.14.0         \n",
            "chainer                  6.5.0          \n",
            "chardet                  3.0.4          \n",
            "click                    7.1.2          \n",
            "cloudpickle              1.3.0          \n",
            "cmake                    3.12.0         \n",
            "cmdstanpy                0.4.0          \n",
            "colorlover               0.3.0          \n",
            "community                1.0.0b1        \n",
            "contextlib2              0.5.5          \n",
            "convertdate              2.2.1          \n",
            "coverage                 3.7.1          \n",
            "coveralls                0.5            \n",
            "crcmod                   1.7            \n",
            "cufflinks                0.17.3         \n",
            "cvxopt                   1.2.5          \n",
            "cvxpy                    1.0.31         \n",
            "cycler                   0.10.0         \n",
            "cymem                    2.0.3          \n",
            "Cython                   0.29.19        \n",
            "daft                     0.0.4          \n",
            "dask                     2.12.0         \n",
            "dataclasses              0.7            \n",
            "datascience              0.10.6         \n",
            "decorator                4.4.2          \n",
            "defusedxml               0.6.0          \n",
            "descartes                1.1.0          \n",
            "dill                     0.3.1.1        \n",
            "distributed              1.25.3         \n",
            "Django                   3.0.6          \n",
            "dlib                     19.18.0        \n",
            "docopt                   0.6.2          \n",
            "docutils                 0.15.2         \n",
            "dopamine-rl              1.0.5          \n",
            "earthengine-api          0.1.223        \n",
            "easydict                 1.9            \n",
            "ecos                     2.0.7.post1    \n",
            "editdistance             0.5.3          \n",
            "en-core-web-sm           2.2.5          \n",
            "entrypoints              0.3            \n",
            "ephem                    3.7.7.1        \n",
            "et-xmlfile               1.0.1          \n",
            "fa2                      0.3.5          \n",
            "fancyimpute              0.4.3          \n",
            "fastai                   1.0.61         \n",
            "fastdtw                  0.3.4          \n",
            "fastprogress             0.2.3          \n",
            "fastrlock                0.4            \n",
            "fbprophet                0.6            \n",
            "feather-format           0.4.1          \n",
            "featuretools             0.4.1          \n",
            "filelock                 3.0.12         \n",
            "firebase-admin           4.1.0          \n",
            "fix-yahoo-finance        0.0.22         \n",
            "Flask                    1.1.2          \n",
            "folium                   0.8.3          \n",
            "fsspec                   0.7.4          \n",
            "future                   0.16.0         \n",
            "gast                     0.3.3          \n",
            "GDAL                     2.2.2          \n",
            "gdown                    3.6.4          \n",
            "gensim                   3.6.0          \n",
            "geographiclib            1.50           \n",
            "geopy                    1.17.0         \n",
            "gin-config               0.3.0          \n",
            "glob2                    0.7            \n",
            "google                   2.0.3          \n",
            "google-api-core          1.16.0         \n",
            "google-api-python-client 1.7.12         \n",
            "google-auth              1.7.2          \n",
            "google-auth-httplib2     0.0.3          \n",
            "google-auth-oauthlib     0.4.1          \n",
            "google-cloud-bigquery    1.21.0         \n",
            "google-cloud-core        1.0.3          \n",
            "google-cloud-datastore   1.8.0          \n",
            "google-cloud-firestore   1.7.0          \n",
            "google-cloud-language    1.2.0          \n",
            "google-cloud-storage     1.18.1         \n",
            "google-cloud-translate   1.5.0          \n",
            "google-colab             1.0.0          \n",
            "google-pasta             0.2.0          \n",
            "google-resumable-media   0.4.1          \n",
            "googleapis-common-protos 1.51.0         \n",
            "googledrivedownloader    0.4            \n",
            "graphviz                 0.10.1         \n",
            "grpcio                   1.29.0         \n",
            "gspread                  3.0.1          \n",
            "gspread-dataframe        3.0.7          \n",
            "gym                      0.17.2         \n",
            "h5py                     2.10.0         \n",
            "HeapDict                 1.0.1          \n",
            "holidays                 0.9.12         \n",
            "html5lib                 1.0.1          \n",
            "httpimport               0.5.18         \n",
            "httplib2                 0.17.4         \n",
            "httplib2shim             0.0.3          \n",
            "humanize                 0.5.1          \n",
            "hyperopt                 0.1.2          \n",
            "ideep4py                 2.0.0.post3    \n",
            "idna                     2.9            \n",
            "image                    1.5.32         \n",
            "imageio                  2.4.1          \n",
            "imagesize                1.2.0          \n",
            "imbalanced-learn         0.4.3          \n",
            "imblearn                 0.0            \n",
            "imgaug                   0.2.9          \n",
            "importlib-metadata       1.6.0          \n",
            "imutils                  0.5.3          \n",
            "inflect                  2.1.0          \n",
            "intel-openmp             2020.0.133     \n",
            "intervaltree             2.1.0          \n",
            "ipykernel                4.10.1         \n",
            "ipython                  5.5.0          \n",
            "ipython-genutils         0.2.0          \n",
            "ipython-sql              0.3.9          \n",
            "ipywidgets               7.5.1          \n",
            "itsdangerous             1.1.0          \n",
            "jax                      0.1.68         \n",
            "jaxlib                   0.1.47         \n",
            "jdcal                    1.4.1          \n",
            "jedi                     0.17.0         \n",
            "jieba                    0.42.1         \n",
            "Jinja2                   2.11.2         \n",
            "jmespath                 0.10.0         \n",
            "joblib                   0.15.1         \n",
            "jpeg4py                  0.1.4          \n",
            "jsonschema               2.6.0          \n",
            "jupyter                  1.0.0          \n",
            "jupyter-client           5.3.4          \n",
            "jupyter-console          5.2.0          \n",
            "jupyter-core             4.6.3          \n",
            "kaggle                   1.5.6          \n",
            "kapre                    0.1.3.1        \n",
            "Keras                    2.3.1          \n",
            "Keras-Applications       1.0.8          \n",
            "Keras-Preprocessing      1.1.2          \n",
            "keras-rl                 0.4.2          \n",
            "keras-vis                0.4.1          \n",
            "kiwisolver               1.2.0          \n",
            "knnimpute                0.1.0          \n",
            "librosa                  0.6.3          \n",
            "lightgbm                 2.2.3          \n",
            "llvmlite                 0.31.0         \n",
            "lmdb                     0.98           \n",
            "lucid                    0.3.8          \n",
            "LunarCalendar            0.0.9          \n",
            "lxml                     4.2.6          \n",
            "Markdown                 3.2.2          \n",
            "MarkupSafe               1.1.1          \n",
            "matplotlib               3.2.1          \n",
            "matplotlib-venn          0.11.5         \n",
            "missingno                0.4.2          \n",
            "mistune                  0.8.4          \n",
            "mizani                   0.6.0          \n",
            "mkl                      2019.0         \n",
            "mlxtend                  0.14.0         \n",
            "more-itertools           8.3.0          \n",
            "moviepy                  0.2.3.5        \n",
            "mpmath                   1.1.0          \n",
            "msgpack                  1.0.0          \n",
            "multiprocess             0.70.9         \n",
            "multitasking             0.0.9          \n",
            "murmurhash               1.0.2          \n",
            "music21                  5.5.0          \n",
            "natsort                  5.5.0          \n",
            "nbconvert                5.6.1          \n",
            "nbformat                 5.0.6          \n",
            "networkx                 2.4            \n",
            "nibabel                  3.0.2          \n",
            "nltk                     3.2.5          \n",
            "notebook                 5.2.2          \n",
            "np-utils                 0.5.12.1       \n",
            "numba                    0.48.0         \n",
            "numexpr                  2.7.1          \n",
            "numpy                    1.18.4         \n",
            "nvidia-ml-py3            7.352.0        \n",
            "oauth2client             4.1.3          \n",
            "oauthlib                 3.1.0          \n",
            "okgrade                  0.4.3          \n",
            "opencv-contrib-python    4.1.2.30       \n",
            "opencv-python            4.1.2.30       \n",
            "openpyxl                 2.5.9          \n",
            "opt-einsum               3.2.1          \n",
            "osqp                     0.6.1          \n",
            "packaging                20.4           \n",
            "palettable               3.3.0          \n",
            "pandas                   1.0.4          \n",
            "pandas-datareader        0.8.1          \n",
            "pandas-gbq               0.11.0         \n",
            "pandas-profiling         1.4.1          \n",
            "pandocfilters            1.4.2          \n",
            "parso                    0.7.0          \n",
            "pathlib                  1.0.1          \n",
            "patsy                    0.5.1          \n",
            "pexpect                  4.8.0          \n",
            "pickleshare              0.7.5          \n",
            "Pillow                   7.0.0          \n",
            "pip                      19.3.1         \n",
            "pip-tools                4.5.1          \n",
            "plac                     1.1.3          \n",
            "plotly                   4.4.1          \n",
            "plotnine                 0.6.0          \n",
            "pluggy                   0.7.1          \n",
            "portpicker               1.3.1          \n",
            "prefetch-generator       1.0.1          \n",
            "preshed                  3.0.2          \n",
            "prettytable              0.7.2          \n",
            "progressbar2             3.38.0         \n",
            "prometheus-client        0.8.0          \n",
            "promise                  2.3            \n",
            "prompt-toolkit           1.0.18         \n",
            "protobuf                 3.10.0         \n",
            "psutil                   5.4.8          \n",
            "psycopg2                 2.7.6.1        \n",
            "ptyprocess               0.6.0          \n",
            "py                       1.8.1          \n",
            "pyarrow                  0.14.1         \n",
            "pyasn1                   0.4.8          \n",
            "pyasn1-modules           0.2.8          \n",
            "pycocotools              2.0.0          \n",
            "pycparser                2.20           \n",
            "pydata-google-auth       1.1.0          \n",
            "pydot                    1.3.0          \n",
            "pydot-ng                 2.0.0          \n",
            "pydotplus                2.0.2          \n",
            "PyDrive                  1.3.1          \n",
            "pyemd                    0.5.1          \n",
            "pyglet                   1.5.0          \n",
            "Pygments                 2.1.3          \n",
            "pygobject                3.26.1         \n",
            "pymc3                    3.7            \n",
            "PyMeeus                  0.3.7          \n",
            "pymongo                  3.10.1         \n",
            "pymystem3                0.2.0          \n",
            "PyOpenGL                 3.1.5          \n",
            "pyparsing                2.4.7          \n",
            "pyrsistent               0.16.0         \n",
            "pysndfile                1.3.8          \n",
            "PySocks                  1.7.1          \n",
            "pystan                   2.19.1.1       \n",
            "pytest                   3.6.4          \n",
            "python-apt               1.6.5+ubuntu0.2\n",
            "python-chess             0.23.11        \n",
            "python-dateutil          2.8.1          \n",
            "python-louvain           0.14           \n",
            "python-slugify           4.0.0          \n",
            "python-utils             2.4.0          \n",
            "pytz                     2018.9         \n",
            "PyWavelets               1.1.1          \n",
            "PyYAML                   3.13           \n",
            "pyzmq                    19.0.1         \n",
            "qtconsole                4.7.4          \n",
            "QtPy                     1.9.0          \n",
            "regex                    2019.12.20     \n",
            "requests                 2.23.0         \n",
            "requests-oauthlib        1.3.0          \n",
            "resampy                  0.2.2          \n",
            "retrying                 1.3.3          \n",
            "rpy2                     3.2.7          \n",
            "rsa                      4.0            \n",
            "s3fs                     0.4.2          \n",
            "s3transfer               0.3.3          \n",
            "scikit-image             0.16.2         \n",
            "scikit-learn             0.22.2.post1   \n",
            "scipy                    1.4.1          \n",
            "screen-resolution-extra  0.0.0          \n",
            "scs                      2.1.2          \n",
            "seaborn                  0.10.1         \n",
            "Send2Trash               1.5.0          \n",
            "setuptools               47.1.1         \n",
            "setuptools-git           1.2            \n",
            "Shapely                  1.7.0          \n",
            "simplegeneric            0.8.1          \n",
            "six                      1.12.0         \n",
            "sklearn                  0.0            \n",
            "sklearn-pandas           1.8.0          \n",
            "smart-open               2.0.0          \n",
            "snowballstemmer          2.0.0          \n",
            "sortedcontainers         2.1.0          \n",
            "spacy                    2.2.4          \n",
            "Sphinx                   1.8.5          \n",
            "sphinxcontrib-websupport 1.2.2          \n",
            "SQLAlchemy               1.3.17         \n",
            "sqlparse                 0.3.1          \n",
            "srsly                    1.0.2          \n",
            "statsmodels              0.10.2         \n",
            "sympy                    1.1.1          \n",
            "tables                   3.4.4          \n",
            "tabulate                 0.8.7          \n",
            "tbb                      2020.0.133     \n",
            "tblib                    1.6.0          \n",
            "tensorboard              1.14.0         \n",
            "tensorboard-plugin-wit   1.6.0.post3    \n",
            "tensorboardcolab         0.0.22         \n",
            "tensorflow               1.14.0         \n",
            "tensorflow-addons        0.8.3          \n",
            "tensorflow-datasets      2.1.0          \n",
            "tensorflow-estimator     1.14.0         \n",
            "tensorflow-gcs-config    2.1.8          \n",
            "tensorflow-hub           0.8.0          \n",
            "tensorflow-metadata      0.22.1         \n",
            "tensorflow-privacy       0.2.2          \n",
            "tensorflow-probability   0.10.0         \n",
            "termcolor                1.1.0          \n",
            "terminado                0.8.3          \n",
            "testpath                 0.4.4          \n",
            "text-unidecode           1.3            \n",
            "textblob                 0.15.3         \n",
            "textgenrnn               1.4.1          \n",
            "Theano                   1.0.4          \n",
            "thinc                    7.4.0          \n",
            "tifffile                 2020.5.30      \n",
            "toolz                    0.10.0         \n",
            "torch                    1.5.0+cu101    \n",
            "torchsummary             1.5.1          \n",
            "torchtext                0.3.1          \n",
            "torchvision              0.6.0+cu101    \n",
            "tornado                  4.5.3          \n",
            "tqdm                     4.41.1         \n",
            "traitlets                4.3.3          \n",
            "tweepy                   3.6.0          \n",
            "typeguard                2.7.1          \n",
            "typing                   3.6.6          \n",
            "typing-extensions        3.6.6          \n",
            "tzlocal                  1.5.1          \n",
            "umap-learn               0.4.3          \n",
            "uritemplate              3.0.1          \n",
            "urllib3                  1.24.3         \n",
            "vega-datasets            0.8.0          \n",
            "wasabi                   0.6.0          \n",
            "wcwidth                  0.2.2          \n",
            "webencodings             0.5.1          \n",
            "Werkzeug                 1.0.1          \n",
            "wheel                    0.34.2         \n",
            "widgetsnbextension       3.5.1          \n",
            "wordcloud                1.5.0          \n",
            "wrapt                    1.12.1         \n",
            "xarray                   0.15.1         \n",
            "xgboost                  0.90           \n",
            "xkit                     0.0.0          \n",
            "xlrd                     1.1.0          \n",
            "xlwt                     1.3.0          \n",
            "yellowbrick              0.9.1          \n",
            "zict                     2.0.0          \n",
            "zipp                     3.1.0          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxY0VLwhcy8C",
        "colab_type": "code",
        "outputId": "99d30e42-b171-4571-8930-bdaa55a3c06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(trainx.iloc[1])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.0310000e+02, 2.0886004e-02])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsQH_MC8pD5E",
        "colab_type": "code",
        "outputId": "9dde349e-d79c-4166-e94d-ef1396ee211b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "float(trainy.iloc[0])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT6WwE8OFwwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "\n",
        "\n",
        "class HotterColder(gym.Env):\n",
        "    \"\"\"Hotter Colder\n",
        "    The goal of hotter colder is to guess closer to a randomly selected number\n",
        "    After each step the agent receives an observation of:\n",
        "    0 - No guess yet submitted (only after reset)\n",
        "    1 - Guess is lower than the target\n",
        "    2 - Guess is equal to the target\n",
        "    3 - Guess is higher than the target\n",
        "    The rewards is calculated as:\n",
        "    (min(action, self.number) + self.range) / (max(action, self.number) + self.range)\n",
        "    Ideally an agent will be able to recognise the 'scent' of a higher reward and\n",
        "    increase the rate in which is guesses in that direction until the reward reaches\n",
        "    its maximum\n",
        "    \"\"\"\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "        self.range = 10  # +/- value the randomly select number can be between\n",
        "        self.bounds = 20  # Action space bounds\n",
        "\n",
        "        self.number = 0\n",
        "        self.guess_count = 0\n",
        "        self.guess_max = 200\n",
        "        self.race_num = 0\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(11)\n",
        "        min = np.array(self.x.min())\n",
        "        low = np.concatenate([min])\n",
        "        max = np.array(self.x.max())        \n",
        "        high = np.concatenate([max])\n",
        "        self.observation_space = gym.spaces.Box(low=low, high=high) \n",
        "        \n",
        "        self.seed()\n",
        "        self.reset()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        self.done =  self._is_done(action)\n",
        "\n",
        "        reward = 0\n",
        "        plus = 0\n",
        "        if self.done == False:\n",
        "          if action > 0:\n",
        "            plus = (-1 * action + float(self.y.iloc[self.race_num]*action))\n",
        "            reward = plus\n",
        "            self.amount += plus\n",
        "        else:\n",
        "            reward = self.amount\n",
        "\n",
        "#          print(\"\")\n",
        "\n",
        "#         self.amount += reward\n",
        "\n",
        "        self.guess_count += 1\n",
        "        self.race_num += 1\n",
        "\n",
        "        if self.race_num >= len(self.x):\n",
        "          self.race_num = 0\n",
        "\n",
        "#        print(action, end=\"\")\n",
        "#        pprint((self.x.iloc[self.race_num], reward, self.done, {\"guesses\": self.guess_count}))\n",
        "        return np.array(self.x.iloc[self.race_num]), reward, self.done, {\"guesses\": self.guess_count}\n",
        "\n",
        "    def reset(self):\n",
        "        self.done = False\n",
        "        self.amount = 0\n",
        "        self.guess_count = 0\n",
        "        \n",
        "#        pprint(np.concatenate([[self.amount],np.array(self.x.iloc[self.race_num])]))\n",
        "        return np.array(self.x.iloc[self.race_num])\n",
        "      \n",
        "    def _is_done(self, action):\n",
        "        if self.guess_count >= self.guess_max:\n",
        "#           print(\"Over\")\n",
        "          return True\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqnYw2IWFxJU",
        "colab_type": "code",
        "outputId": "f2fec115-2744-4812-aa99-c0e1027c05dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "env = HotterColder(trainx, trainy)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adFXL4cUx8pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x =  spaces.Box(low=np.array([-200]), high=np.array([200]),                                       dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6kxbgCSyAqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_actions = env.action_space.n\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq3JKmJX4YUt",
        "colab_type": "code",
        "outputId": "09c54a6f-5924-4d43-87e8-66c3cfc00ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nb_actions"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X23sqWZI1fKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# env.action_space.contains(np.array([-2000]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqTWlfL9EYZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gym\n",
        "\n",
        "# # GymのPendulum環境を作成\n",
        "# env = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "# # 取りうる”打ち手”のアクション数と値の定義\n",
        "# nb_actions = 2\n",
        "# ACT_ID_TO_VALUE = {0: [-1], 1: [+1]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu7q1TXbFCVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rl.core import Processor\n",
        "\n",
        "\n",
        "class PendulumProcessor(Processor):\n",
        "\n",
        "    # Duel-DQNの出力と、Gym環境の入力の違いを吸収\n",
        "    def process_action(self, action):\n",
        "        return action\n",
        "\n",
        "    # Gym環境の報酬の出力と、Duel-DQNの報酬の入力との違いを吸収\n",
        "    def process_reward(self, reward):\n",
        "      return reward\n",
        "#         if reward > -0.2:\n",
        "#             return 1\n",
        "#         elif reward > -1.0:\n",
        "#             return 0\n",
        "#         else:\n",
        "#             return 0\n",
        "    def process_observation(self, observation):\n",
        "      return observation\n",
        "\n",
        "processor = PendulumProcessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIhFvu7zFRc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dense(nb_actions, activation=\"linear\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDlJ8WYULUzj",
        "colab_type": "code",
        "outputId": "32c9cbbb-12cf-4e80-adef-60110df601e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(1,) + env.observation_space.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNWefvWLNOE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyzzMQBTHmgl",
        "colab_type": "code",
        "outputId": "9dad1586-b16b-4a13-bc17-4f055be6c1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "\n",
        "# Duel-DQNアルゴリズム関連の幾つかの設定\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = BoltzmannQPolicy()\n",
        "\n",
        "# Duel-DQNのAgentクラスオブジェクトの準備 （上記processorやmodelを元に）\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "               enable_dueling_network=True, dueling_type=\"avg\", target_model_update=1e-2, policy=policy,processor=processor)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=[\"mae\"])\n",
        "print(dqn.model.summary())\n",
        "\n",
        "\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2_input (InputLayer) (None, 1, 2)              0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 12)                204       \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 11)                0         \n",
            "=================================================================\n",
            "Total params: 524\n",
            "Trainable params: 524\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uql1Js50Tb8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import rl.callbacks\n",
        "class EpisodeLogger(rl.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.observations = {}\n",
        "        self.rewards = {}\n",
        "        self.actions = {}\n",
        "\n",
        "    def on_episode_begin(self, episode, logs):\n",
        "        self.observations[episode] = []\n",
        "        self.rewards[episode] = []\n",
        "        self.actions[episode] = []\n",
        "\n",
        "    def on_step_end(self, step, logs):\n",
        "        episode = logs['episode']\n",
        "        self.observations[episode].append(logs['observation'])\n",
        "        self.rewards[episode].append(logs['reward'])\n",
        "        self.actions[episode].append(logs['action'])\n",
        "\n",
        "cb_train = EpisodeLogger()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdeS8JczcdkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.callbacks import EarlyStopping\n",
        "\n",
        "#early_stop = EarlyStopping(patience=69)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkJxlTvZdG6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame({\"dummy\":1},index=[0]).to_csv(\"h5f/save_start.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka3BQCoIVk_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2262a1c6-4371-4242-bc2b-5fcbb5e46c37"
      },
      "source": [
        "# 定義課題環境に対して、アルゴリズムの学習を実行 （必要に応じて適切なCallbackも定義、設定可能）\n",
        "# 上記Processorクラスの適切な設定によって、Agent-環境間の入出力を通して設計課題に対しての学習が進行\n",
        "# dqn.fit(env, nb_steps=50000, visualize=False, callbacks=[early_stop], verbose=2)\n",
        "dqn.fit(env, nb_steps=10000000, visualize=False, callbacks=[cb_train], verbose=2)\n",
        "# 学習後のモデルの重みの出力\n",
        "dqn.save_weights(\"h5f/my_dqn_weights_try_modify_reward.h5f\", overwrite=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 10000000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n",
            "  527223/10000000: episode: 2623, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -500.200, mean reward: -2.489 [-250.100, 57.900], mean action: 2.373 [0.000, 9.000], mean observation: 35.813 [0.000, 522.100], loss: 175.699707, mae: 42.181900, mean_q: -43.632568\n",
            "  527424/10000000: episode: 2624, duration: 1.549s, episode steps: 201, steps per second: 130, episode reward: -119.400, mean reward: -0.594 [-59.700, 93.000], mean action: 2.731 [0.000, 10.000], mean observation: 26.364 [0.001, 589.600], loss: 273.457855, mae: 41.477859, mean_q: -43.162247\n",
            "  527625/10000000: episode: 2625, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: -32.200, mean reward: -0.160 [-16.100, 201.600], mean action: 2.294 [0.000, 9.000], mean observation: 36.947 [0.000, 813.800], loss: 244.066666, mae: 41.776264, mean_q: -43.199444\n",
            "  527826/10000000: episode: 2626, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -730.000, mean reward: -3.632 [-365.000, 30.000], mean action: 2.592 [0.000, 9.000], mean observation: 39.716 [0.000, 632.400], loss: 131.292252, mae: 42.108608, mean_q: -43.585819\n",
            "  528027/10000000: episode: 2627, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: -389.200, mean reward: -1.936 [-194.600, 123.600], mean action: 2.264 [0.000, 9.000], mean observation: 29.934 [0.003, 531.400], loss: 200.018555, mae: 42.168541, mean_q: -43.611412\n",
            "  528228/10000000: episode: 2628, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -169.600, mean reward: -0.844 [-84.800, 143.500], mean action: 2.493 [0.000, 9.000], mean observation: 35.550 [0.002, 519.900], loss: 304.640961, mae: 42.162701, mean_q: -43.577484\n",
            "  528429/10000000: episode: 2629, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -277.200, mean reward: -1.379 [-138.600, 71.000], mean action: 2.353 [0.000, 9.000], mean observation: 35.820 [0.001, 518.800], loss: 251.511230, mae: 41.943478, mean_q: -43.525604\n",
            "  528630/10000000: episode: 2630, duration: 1.613s, episode steps: 201, steps per second: 125, episode reward: -723.400, mean reward: -3.599 [-361.700, 79.200], mean action: 2.687 [0.000, 9.000], mean observation: 39.392 [0.001, 554.400], loss: 175.475220, mae: 42.530338, mean_q: -43.993626\n",
            "  528831/10000000: episode: 2631, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: 320.000, mean reward: 1.592 [-10.000, 285.500], mean action: 1.806 [0.000, 10.000], mean observation: 37.816 [0.001, 627.200], loss: 256.619476, mae: 42.790142, mean_q: -43.909836\n",
            "  529032/10000000: episode: 2632, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: 387.600, mean reward: 1.928 [-9.000, 198.400], mean action: 1.980 [0.000, 9.000], mean observation: 36.357 [0.000, 604.500], loss: 255.715195, mae: 42.442081, mean_q: -43.467937\n",
            "  529233/10000000: episode: 2633, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -590.400, mean reward: -2.937 [-295.200, 16.600], mean action: 1.945 [0.000, 8.000], mean observation: 32.545 [0.001, 500.600], loss: 210.158356, mae: 42.398655, mean_q: -43.565910\n",
            "  529434/10000000: episode: 2634, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -668.400, mean reward: -3.325 [-334.200, 56.800], mean action: 2.582 [0.000, 9.000], mean observation: 28.735 [0.001, 464.600], loss: 178.268845, mae: 41.968098, mean_q: -43.702297\n",
            "  529635/10000000: episode: 2635, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -345.400, mean reward: -1.718 [-172.700, 50.400], mean action: 2.522 [0.000, 10.000], mean observation: 31.191 [0.001, 531.100], loss: 237.456741, mae: 42.070816, mean_q: -43.824055\n",
            "  529836/10000000: episode: 2636, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: 40.600, mean reward: 0.202 [-10.000, 126.400], mean action: 2.284 [0.000, 10.000], mean observation: 34.927 [0.001, 507.400], loss: 180.699509, mae: 42.304852, mean_q: -43.614418\n",
            "  530037/10000000: episode: 2637, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -458.000, mean reward: -2.279 [-229.000, 39.200], mean action: 1.905 [0.000, 9.000], mean observation: 35.147 [0.000, 784.200], loss: 234.707794, mae: 42.349018, mean_q: -43.577118\n",
            "  530238/10000000: episode: 2638, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -308.000, mean reward: -1.532 [-154.000, 109.600], mean action: 2.249 [0.000, 8.000], mean observation: 36.663 [0.000, 722.600], loss: 241.114502, mae: 42.079300, mean_q: -43.608063\n",
            "  530439/10000000: episode: 2639, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: 124.600, mean reward: 0.620 [-9.000, 171.500], mean action: 2.343 [0.000, 9.000], mean observation: 32.427 [0.001, 542.200], loss: 191.403244, mae: 42.015087, mean_q: -43.296825\n",
            "  530640/10000000: episode: 2640, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -93.600, mean reward: -0.466 [-46.800, 175.100], mean action: 2.473 [0.000, 10.000], mean observation: 35.924 [0.001, 448.100], loss: 169.774918, mae: 42.223923, mean_q: -43.590981\n",
            "  530841/10000000: episode: 2641, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -586.200, mean reward: -2.916 [-293.100, 51.300], mean action: 2.219 [0.000, 8.000], mean observation: 30.192 [0.000, 527.300], loss: 257.760925, mae: 42.121487, mean_q: -43.214642\n",
            "  531042/10000000: episode: 2642, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -659.200, mean reward: -3.280 [-329.600, 40.000], mean action: 2.179 [0.000, 9.000], mean observation: 31.121 [0.003, 542.400], loss: 265.118713, mae: 42.119667, mean_q: -43.357296\n",
            "  531243/10000000: episode: 2643, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -557.400, mean reward: -2.773 [-278.700, 23.000], mean action: 1.900 [0.000, 8.000], mean observation: 39.374 [0.000, 694.400], loss: 299.166748, mae: 42.065357, mean_q: -43.215717\n",
            "  531444/10000000: episode: 2644, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -428.400, mean reward: -2.131 [-214.200, 103.600], mean action: 2.388 [0.000, 9.000], mean observation: 34.510 [0.000, 598.300], loss: 302.396637, mae: 42.456848, mean_q: -43.634716\n",
            "  531645/10000000: episode: 2645, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -315.200, mean reward: -1.568 [-157.600, 98.000], mean action: 2.035 [0.000, 10.000], mean observation: 30.337 [0.004, 555.700], loss: 253.842667, mae: 42.432808, mean_q: -43.357185\n",
            "  531846/10000000: episode: 2646, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: 572.200, mean reward: 2.847 [-8.000, 376.800], mean action: 1.891 [0.000, 9.000], mean observation: 35.006 [0.000, 669.900], loss: 293.355438, mae: 42.199150, mean_q: -42.985191\n",
            "  532047/10000000: episode: 2647, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: -138.800, mean reward: -0.691 [-69.400, 76.500], mean action: 1.935 [0.000, 8.000], mean observation: 38.573 [0.001, 662.900], loss: 275.018982, mae: 41.948734, mean_q: -42.910740\n",
            "  532248/10000000: episode: 2648, duration: 1.613s, episode steps: 201, steps per second: 125, episode reward: 45.200, mean reward: 0.225 [-8.000, 79.100], mean action: 2.134 [0.000, 8.000], mean observation: 35.247 [0.002, 482.600], loss: 308.474701, mae: 41.548828, mean_q: -42.725822\n",
            "  532449/10000000: episode: 2649, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: 370.400, mean reward: 1.843 [-9.000, 294.400], mean action: 2.736 [0.000, 9.000], mean observation: 35.839 [0.001, 419.000], loss: 326.993805, mae: 41.366749, mean_q: -43.027771\n",
            "  532650/10000000: episode: 2650, duration: 1.601s, episode steps: 201, steps per second: 126, episode reward: -411.600, mean reward: -2.048 [-205.800, 100.800], mean action: 2.796 [0.000, 9.000], mean observation: 36.198 [0.000, 654.700], loss: 356.076324, mae: 40.843906, mean_q: -42.473984\n",
            "  532851/10000000: episode: 2651, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -463.400, mean reward: -2.305 [-231.700, 58.600], mean action: 2.313 [0.000, 9.000], mean observation: 33.243 [0.000, 706.400], loss: 173.230103, mae: 40.650433, mean_q: -41.931068\n",
            "  533052/10000000: episode: 2652, duration: 1.664s, episode steps: 201, steps per second: 121, episode reward: -452.800, mean reward: -2.253 [-226.400, 68.400], mean action: 2.124 [0.000, 9.000], mean observation: 32.838 [0.001, 480.700], loss: 167.704407, mae: 40.697323, mean_q: -41.960373\n",
            "  533253/10000000: episode: 2653, duration: 1.600s, episode steps: 201, steps per second: 126, episode reward: 175.200, mean reward: 0.872 [-9.000, 327.200], mean action: 2.577 [0.000, 9.000], mean observation: 32.338 [0.001, 568.100], loss: 153.210495, mae: 40.698280, mean_q: -42.242687\n",
            "  533454/10000000: episode: 2654, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -335.800, mean reward: -1.671 [-167.900, 45.200], mean action: 1.945 [0.000, 9.000], mean observation: 29.873 [0.001, 676.900], loss: 257.261688, mae: 40.797935, mean_q: -41.861931\n",
            "  533655/10000000: episode: 2655, duration: 1.605s, episode steps: 201, steps per second: 125, episode reward: -530.800, mean reward: -2.641 [-265.400, 43.800], mean action: 2.323 [0.000, 9.000], mean observation: 37.743 [0.000, 714.300], loss: 270.264801, mae: 40.470661, mean_q: -41.229904\n",
            "  533856/10000000: episode: 2656, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -543.400, mean reward: -2.703 [-271.700, 46.200], mean action: 2.025 [0.000, 9.000], mean observation: 36.674 [0.000, 601.500], loss: 214.841064, mae: 40.044670, mean_q: -40.793854\n",
            "  534057/10000000: episode: 2657, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -80.200, mean reward: -0.399 [-40.100, 80.000], mean action: 2.229 [0.000, 10.000], mean observation: 34.735 [0.000, 582.800], loss: 186.118729, mae: 39.522766, mean_q: -40.329926\n",
            "  534258/10000000: episode: 2658, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -540.800, mean reward: -2.691 [-270.400, 29.400], mean action: 2.418 [0.000, 10.000], mean observation: 33.925 [0.001, 498.800], loss: 145.809067, mae: 39.044724, mean_q: -40.020443\n",
            "  534459/10000000: episode: 2659, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: -160.200, mean reward: -0.797 [-80.100, 81.600], mean action: 2.498 [0.000, 9.000], mean observation: 28.134 [0.002, 508.500], loss: 311.196014, mae: 38.816597, mean_q: -39.582729\n",
            "  534660/10000000: episode: 2660, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: 5.400, mean reward: 0.027 [-9.000, 229.200], mean action: 2.303 [0.000, 9.000], mean observation: 31.069 [0.004, 413.800], loss: 252.044540, mae: 38.443798, mean_q: -39.393482\n",
            "  534861/10000000: episode: 2661, duration: 1.552s, episode steps: 201, steps per second: 129, episode reward: -443.800, mean reward: -2.208 [-221.900, 93.100], mean action: 2.338 [0.000, 8.000], mean observation: 34.607 [0.000, 367.900], loss: 267.130707, mae: 38.713474, mean_q: -39.601551\n",
            "  535062/10000000: episode: 2662, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -586.000, mean reward: -2.915 [-293.000, 56.000], mean action: 2.353 [0.000, 9.000], mean observation: 37.529 [0.001, 630.500], loss: 236.972397, mae: 38.625774, mean_q: -39.364067\n",
            "  535263/10000000: episode: 2663, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -26.600, mean reward: -0.132 [-13.300, 157.200], mean action: 2.418 [0.000, 8.000], mean observation: 31.241 [0.001, 518.600], loss: 216.390549, mae: 37.706707, mean_q: -38.664097\n",
            "  535464/10000000: episode: 2664, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -244.800, mean reward: -1.218 [-122.400, 111.100], mean action: 2.065 [0.000, 9.000], mean observation: 34.995 [0.000, 562.200], loss: 184.672653, mae: 37.927982, mean_q: -38.554813\n",
            "  535665/10000000: episode: 2665, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -246.200, mean reward: -1.225 [-123.100, 47.200], mean action: 2.289 [0.000, 9.000], mean observation: 33.728 [0.002, 637.200], loss: 212.864548, mae: 37.337730, mean_q: -37.921589\n",
            "  535866/10000000: episode: 2666, duration: 1.630s, episode steps: 201, steps per second: 123, episode reward: -603.800, mean reward: -3.004 [-301.900, 83.400], mean action: 2.627 [0.000, 9.000], mean observation: 28.279 [0.005, 381.200], loss: 272.458649, mae: 36.961704, mean_q: -38.002949\n",
            "  536067/10000000: episode: 2667, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: 161.800, mean reward: 0.805 [-9.000, 171.900], mean action: 2.647 [0.000, 9.000], mean observation: 31.511 [0.002, 607.700], loss: 304.934692, mae: 36.836128, mean_q: -37.727146\n",
            "  536268/10000000: episode: 2668, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -446.000, mean reward: -2.219 [-223.000, 71.200], mean action: 2.134 [0.000, 9.000], mean observation: 27.112 [0.001, 497.800], loss: 217.347794, mae: 36.554153, mean_q: -37.652153\n",
            "  536469/10000000: episode: 2669, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -515.600, mean reward: -2.565 [-257.800, 28.000], mean action: 1.826 [0.000, 8.000], mean observation: 38.976 [0.001, 462.800], loss: 290.791779, mae: 37.226360, mean_q: -38.158535\n",
            "  536670/10000000: episode: 2670, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -426.200, mean reward: -2.120 [-213.100, 60.000], mean action: 2.289 [0.000, 9.000], mean observation: 34.774 [0.000, 772.000], loss: 234.381531, mae: 37.038891, mean_q: -37.911396\n",
            "  536871/10000000: episode: 2671, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -190.000, mean reward: -0.945 [-95.000, 104.400], mean action: 2.129 [0.000, 9.000], mean observation: 30.747 [0.001, 558.800], loss: 186.877014, mae: 37.388649, mean_q: -38.214733\n",
            "  537072/10000000: episode: 2672, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -594.800, mean reward: -2.959 [-297.400, 42.000], mean action: 2.771 [0.000, 9.000], mean observation: 30.501 [0.000, 501.100], loss: 216.136307, mae: 38.132069, mean_q: -39.422626\n",
            "  537273/10000000: episode: 2673, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: 863.000, mean reward: 4.294 [-9.000, 473.600], mean action: 2.373 [0.000, 9.000], mean observation: 36.212 [0.001, 645.600], loss: 158.285492, mae: 38.743710, mean_q: -39.892044\n",
            "  537474/10000000: episode: 2674, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -433.400, mean reward: -2.156 [-216.700, 30.400], mean action: 2.114 [0.000, 9.000], mean observation: 31.108 [0.001, 487.300], loss: 195.113190, mae: 38.623634, mean_q: -39.546284\n",
            "  537675/10000000: episode: 2675, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: 161.600, mean reward: 0.804 [-9.000, 108.000], mean action: 2.781 [0.000, 9.000], mean observation: 33.264 [0.000, 555.400], loss: 201.438400, mae: 38.687672, mean_q: -39.584106\n",
            "  537876/10000000: episode: 2676, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -696.200, mean reward: -3.464 [-348.100, 45.900], mean action: 2.483 [0.000, 9.000], mean observation: 35.858 [0.000, 553.100], loss: 245.074661, mae: 38.395512, mean_q: -39.509239\n",
            "  538077/10000000: episode: 2677, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -866.800, mean reward: -4.312 [-433.400, 22.800], mean action: 2.806 [0.000, 9.000], mean observation: 35.023 [0.000, 668.100], loss: 211.460632, mae: 38.279648, mean_q: -39.598568\n",
            "  538278/10000000: episode: 2678, duration: 1.658s, episode steps: 201, steps per second: 121, episode reward: -840.200, mean reward: -4.180 [-420.100, 36.900], mean action: 3.070 [0.000, 9.000], mean observation: 33.032 [0.000, 489.400], loss: 172.771896, mae: 38.463295, mean_q: -39.699986\n",
            "  538479/10000000: episode: 2679, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -265.600, mean reward: -1.321 [-132.800, 79.500], mean action: 2.711 [0.000, 10.000], mean observation: 28.671 [0.002, 513.300], loss: 267.664215, mae: 38.474472, mean_q: -39.575329\n",
            "  538680/10000000: episode: 2680, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -324.400, mean reward: -1.614 [-162.200, 47.400], mean action: 2.383 [0.000, 10.000], mean observation: 33.594 [0.000, 606.600], loss: 245.911194, mae: 38.335049, mean_q: -39.085766\n",
            "  538881/10000000: episode: 2681, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -316.200, mean reward: -1.573 [-158.100, 185.100], mean action: 2.995 [0.000, 9.000], mean observation: 35.967 [0.001, 493.400], loss: 181.865112, mae: 38.157707, mean_q: -39.290432\n",
            "  539082/10000000: episode: 2682, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -491.000, mean reward: -2.443 [-245.500, 31.500], mean action: 2.507 [0.000, 10.000], mean observation: 33.349 [0.002, 578.200], loss: 250.601410, mae: 38.466957, mean_q: -39.218639\n",
            "  539283/10000000: episode: 2683, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -574.200, mean reward: -2.857 [-287.100, 36.900], mean action: 2.403 [0.000, 9.000], mean observation: 31.978 [0.001, 461.100], loss: 181.502213, mae: 38.498886, mean_q: -39.115349\n",
            "  539484/10000000: episode: 2684, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: -489.000, mean reward: -2.433 [-244.500, 40.500], mean action: 2.224 [0.000, 9.000], mean observation: 40.368 [0.000, 527.200], loss: 247.486206, mae: 38.825447, mean_q: -39.425758\n",
            "  539685/10000000: episode: 2685, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -314.200, mean reward: -1.563 [-157.100, 62.000], mean action: 2.114 [0.000, 10.000], mean observation: 30.685 [0.001, 675.300], loss: 130.318604, mae: 38.900887, mean_q: -39.645309\n",
            "  539886/10000000: episode: 2686, duration: 1.709s, episode steps: 201, steps per second: 118, episode reward: -309.200, mean reward: -1.538 [-154.600, 42.700], mean action: 2.090 [0.000, 10.000], mean observation: 31.385 [0.001, 605.800], loss: 276.749878, mae: 38.709721, mean_q: -39.378342\n",
            "  540087/10000000: episode: 2687, duration: 1.814s, episode steps: 201, steps per second: 111, episode reward: 1142.200, mean reward: 5.683 [-9.000, 646.800], mean action: 2.388 [0.000, 9.000], mean observation: 32.440 [0.000, 527.500], loss: 232.413132, mae: 38.495617, mean_q: -39.232056\n",
            "  540288/10000000: episode: 2688, duration: 1.836s, episode steps: 201, steps per second: 109, episode reward: 315.000, mean reward: 1.567 [-9.000, 157.500], mean action: 2.289 [0.000, 9.000], mean observation: 30.623 [0.000, 544.700], loss: 417.101837, mae: 38.324482, mean_q: -38.726776\n",
            "  540489/10000000: episode: 2689, duration: 1.706s, episode steps: 201, steps per second: 118, episode reward: -193.800, mean reward: -0.964 [-96.900, 84.000], mean action: 2.547 [0.000, 9.000], mean observation: 39.586 [0.001, 532.900], loss: 202.872528, mae: 37.681114, mean_q: -38.491867\n",
            "  540690/10000000: episode: 2690, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: 376.400, mean reward: 1.873 [-10.000, 188.200], mean action: 2.179 [0.000, 10.000], mean observation: 34.322 [0.002, 510.800], loss: 225.784470, mae: 37.770821, mean_q: -38.266125\n",
            "  540891/10000000: episode: 2691, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -470.000, mean reward: -2.338 [-235.000, 57.600], mean action: 2.373 [0.000, 10.000], mean observation: 32.296 [0.001, 647.400], loss: 173.909393, mae: 38.062138, mean_q: -38.669502\n",
            "  541092/10000000: episode: 2692, duration: 1.614s, episode steps: 201, steps per second: 125, episode reward: -412.400, mean reward: -2.052 [-206.200, 33.600], mean action: 1.975 [0.000, 9.000], mean observation: 32.345 [0.000, 602.400], loss: 219.497787, mae: 38.388596, mean_q: -38.938171\n",
            "  541293/10000000: episode: 2693, duration: 1.633s, episode steps: 201, steps per second: 123, episode reward: -493.200, mean reward: -2.454 [-246.600, 42.700], mean action: 2.433 [0.000, 9.000], mean observation: 32.390 [0.002, 364.300], loss: 210.461411, mae: 38.391518, mean_q: -39.165329\n",
            "  541494/10000000: episode: 2694, duration: 1.642s, episode steps: 201, steps per second: 122, episode reward: 497.800, mean reward: 2.477 [-9.000, 356.800], mean action: 2.527 [0.000, 9.000], mean observation: 28.443 [0.001, 458.100], loss: 252.198654, mae: 38.592529, mean_q: -39.326557\n",
            "  541695/10000000: episode: 2695, duration: 1.711s, episode steps: 201, steps per second: 118, episode reward: 82.600, mean reward: 0.411 [-9.000, 115.000], mean action: 2.159 [0.000, 9.000], mean observation: 32.645 [0.000, 695.400], loss: 322.616730, mae: 38.937958, mean_q: -39.705803\n",
            "  541896/10000000: episode: 2696, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -136.200, mean reward: -0.678 [-68.100, 157.600], mean action: 2.274 [0.000, 9.000], mean observation: 37.375 [0.000, 660.900], loss: 235.484528, mae: 39.291851, mean_q: -39.949223\n",
            "  542097/10000000: episode: 2697, duration: 1.685s, episode steps: 201, steps per second: 119, episode reward: 624.800, mean reward: 3.108 [-9.000, 318.000], mean action: 2.045 [0.000, 9.000], mean observation: 32.349 [0.001, 400.300], loss: 301.771973, mae: 39.436375, mean_q: -40.324234\n",
            "  542298/10000000: episode: 2698, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: -66.600, mean reward: -0.331 [-33.300, 160.000], mean action: 2.438 [0.000, 9.000], mean observation: 32.590 [0.000, 446.900], loss: 263.106140, mae: 39.488564, mean_q: -40.479080\n",
            "  542499/10000000: episode: 2699, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -247.800, mean reward: -1.233 [-123.900, 133.000], mean action: 2.308 [0.000, 7.000], mean observation: 34.195 [0.001, 654.600], loss: 291.263184, mae: 39.347004, mean_q: -40.232800\n",
            "  542700/10000000: episode: 2700, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: 101.800, mean reward: 0.506 [-9.000, 146.100], mean action: 2.239 [0.000, 9.000], mean observation: 31.294 [0.000, 467.400], loss: 173.337769, mae: 38.887051, mean_q: -39.597630\n",
            "  542901/10000000: episode: 2701, duration: 1.609s, episode steps: 201, steps per second: 125, episode reward: -424.800, mean reward: -2.113 [-212.400, 45.000], mean action: 1.876 [0.000, 9.000], mean observation: 36.721 [0.000, 420.600], loss: 193.697647, mae: 38.943996, mean_q: -39.599056\n",
            "  543102/10000000: episode: 2702, duration: 1.611s, episode steps: 201, steps per second: 125, episode reward: -132.400, mean reward: -0.659 [-66.200, 124.400], mean action: 2.035 [0.000, 9.000], mean observation: 31.737 [0.000, 697.600], loss: 268.030426, mae: 38.680584, mean_q: -39.346001\n",
            "  543303/10000000: episode: 2703, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -739.200, mean reward: -3.678 [-369.600, 35.600], mean action: 2.209 [0.000, 9.000], mean observation: 32.443 [0.000, 443.500], loss: 304.507416, mae: 38.580837, mean_q: -39.542645\n",
            "  543504/10000000: episode: 2704, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -103.400, mean reward: -0.514 [-51.700, 159.200], mean action: 2.124 [0.000, 9.000], mean observation: 33.836 [0.001, 470.800], loss: 153.648468, mae: 38.457718, mean_q: -39.425957\n",
            "  543705/10000000: episode: 2705, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: 273.800, mean reward: 1.362 [-9.000, 252.700], mean action: 2.612 [0.000, 9.000], mean observation: 27.793 [0.002, 400.000], loss: 303.764221, mae: 38.334404, mean_q: -39.407524\n",
            "  543906/10000000: episode: 2706, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -86.400, mean reward: -0.430 [-43.200, 172.800], mean action: 2.254 [0.000, 9.000], mean observation: 34.353 [0.002, 515.900], loss: 310.569916, mae: 38.170078, mean_q: -38.884949\n",
            "  544107/10000000: episode: 2707, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: -168.400, mean reward: -0.838 [-84.200, 84.000], mean action: 2.025 [0.000, 9.000], mean observation: 35.178 [0.002, 467.700], loss: 239.579987, mae: 37.916374, mean_q: -38.807686\n",
            "  544308/10000000: episode: 2708, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -187.600, mean reward: -0.933 [-93.800, 90.300], mean action: 2.547 [0.000, 10.000], mean observation: 27.563 [0.005, 434.600], loss: 152.542923, mae: 37.819561, mean_q: -38.691280\n",
            "  544509/10000000: episode: 2709, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -170.600, mean reward: -0.849 [-85.300, 122.400], mean action: 2.786 [0.000, 9.000], mean observation: 28.314 [0.000, 638.600], loss: 267.289307, mae: 37.567879, mean_q: -38.715469\n",
            "  544710/10000000: episode: 2710, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -376.200, mean reward: -1.872 [-188.100, 103.200], mean action: 2.358 [0.000, 10.000], mean observation: 39.951 [0.000, 584.000], loss: 142.639587, mae: 37.995705, mean_q: -39.104580\n",
            "  544911/10000000: episode: 2711, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -269.800, mean reward: -1.342 [-134.900, 137.200], mean action: 2.368 [0.000, 9.000], mean observation: 29.387 [0.000, 530.500], loss: 194.314301, mae: 38.418453, mean_q: -39.383911\n",
            "  545112/10000000: episode: 2712, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -425.400, mean reward: -2.116 [-212.700, 46.800], mean action: 2.368 [0.000, 9.000], mean observation: 36.369 [0.000, 796.200], loss: 215.074371, mae: 38.211243, mean_q: -39.277542\n",
            "  545313/10000000: episode: 2713, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -470.200, mean reward: -2.339 [-235.100, 34.800], mean action: 2.085 [0.000, 10.000], mean observation: 37.149 [0.000, 588.400], loss: 255.180389, mae: 38.762329, mean_q: -40.001171\n",
            "  545514/10000000: episode: 2714, duration: 1.602s, episode steps: 201, steps per second: 126, episode reward: -224.800, mean reward: -1.118 [-112.400, 224.000], mean action: 2.438 [0.000, 10.000], mean observation: 33.871 [0.000, 507.900], loss: 196.825134, mae: 38.525787, mean_q: -39.533123\n",
            "  545715/10000000: episode: 2715, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: 654.000, mean reward: 3.254 [-9.000, 589.600], mean action: 1.871 [0.000, 9.000], mean observation: 32.613 [0.003, 463.800], loss: 208.941238, mae: 39.171131, mean_q: -40.135567\n",
            "  545916/10000000: episode: 2716, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -6.000, mean reward: -0.030 [-9.000, 111.000], mean action: 1.866 [0.000, 9.000], mean observation: 30.287 [0.002, 545.200], loss: 158.405945, mae: 39.254772, mean_q: -40.117413\n",
            "  546117/10000000: episode: 2717, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -343.600, mean reward: -1.709 [-171.800, 37.500], mean action: 1.652 [0.000, 10.000], mean observation: 28.187 [0.004, 451.800], loss: 240.511292, mae: 38.885918, mean_q: -39.679905\n",
            "  546318/10000000: episode: 2718, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 171.000, mean reward: 0.851 [-9.000, 269.200], mean action: 1.821 [0.000, 9.000], mean observation: 35.806 [0.000, 491.000], loss: 163.547668, mae: 38.882565, mean_q: -39.548035\n",
            "  546519/10000000: episode: 2719, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -518.400, mean reward: -2.579 [-259.200, 30.600], mean action: 2.005 [0.000, 9.000], mean observation: 33.507 [0.001, 481.400], loss: 198.762924, mae: 37.940941, mean_q: -38.923065\n",
            "  546720/10000000: episode: 2720, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -565.800, mean reward: -2.815 [-282.900, 37.600], mean action: 2.174 [0.000, 9.000], mean observation: 31.041 [0.000, 746.900], loss: 179.964355, mae: 37.527180, mean_q: -38.494141\n",
            "  546921/10000000: episode: 2721, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -522.000, mean reward: -2.597 [-261.000, 100.800], mean action: 2.363 [0.000, 9.000], mean observation: 30.442 [0.000, 609.400], loss: 208.276352, mae: 37.347424, mean_q: -38.742477\n",
            "  547122/10000000: episode: 2722, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -412.400, mean reward: -2.052 [-206.200, 57.500], mean action: 2.134 [0.000, 10.000], mean observation: 30.249 [0.001, 583.400], loss: 251.065750, mae: 37.383896, mean_q: -38.456955\n",
            "  547323/10000000: episode: 2723, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -415.800, mean reward: -2.069 [-207.900, 57.200], mean action: 2.358 [0.000, 10.000], mean observation: 32.984 [0.000, 702.100], loss: 208.436935, mae: 37.146381, mean_q: -37.888630\n",
            "  547524/10000000: episode: 2724, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -531.600, mean reward: -2.645 [-265.800, 34.400], mean action: 1.935 [0.000, 7.000], mean observation: 42.541 [0.000, 653.600], loss: 180.464401, mae: 37.309441, mean_q: -37.865307\n",
            "  547725/10000000: episode: 2725, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -406.200, mean reward: -2.021 [-203.100, 81.200], mean action: 2.507 [0.000, 10.000], mean observation: 25.592 [0.000, 509.800], loss: 158.458923, mae: 36.899338, mean_q: -37.447277\n",
            "  547926/10000000: episode: 2726, duration: 1.613s, episode steps: 201, steps per second: 125, episode reward: -211.800, mean reward: -1.054 [-105.900, 63.700], mean action: 1.940 [0.000, 10.000], mean observation: 37.516 [0.002, 630.900], loss: 239.187073, mae: 36.945396, mean_q: -37.501522\n",
            "  548127/10000000: episode: 2727, duration: 1.725s, episode steps: 201, steps per second: 117, episode reward: -154.000, mean reward: -0.766 [-77.000, 105.600], mean action: 2.184 [0.000, 9.000], mean observation: 37.260 [0.001, 571.000], loss: 200.647079, mae: 37.167740, mean_q: -38.097763\n",
            "  548328/10000000: episode: 2728, duration: 1.828s, episode steps: 201, steps per second: 110, episode reward: -480.200, mean reward: -2.389 [-240.100, 55.200], mean action: 1.965 [0.000, 10.000], mean observation: 35.860 [0.001, 591.000], loss: 112.971779, mae: 37.486328, mean_q: -38.236897\n",
            "  548529/10000000: episode: 2729, duration: 1.720s, episode steps: 201, steps per second: 117, episode reward: -550.000, mean reward: -2.736 [-275.000, 28.800], mean action: 1.930 [0.000, 9.000], mean observation: 30.047 [0.001, 532.400], loss: 336.481781, mae: 36.718372, mean_q: -37.477303\n",
            "  548730/10000000: episode: 2730, duration: 1.789s, episode steps: 201, steps per second: 112, episode reward: -6.600, mean reward: -0.033 [-10.000, 82.400], mean action: 1.985 [0.000, 10.000], mean observation: 34.086 [0.002, 595.600], loss: 134.031235, mae: 36.830662, mean_q: -37.779076\n",
            "  548931/10000000: episode: 2731, duration: 1.756s, episode steps: 201, steps per second: 114, episode reward: -354.800, mean reward: -1.765 [-177.400, 24.400], mean action: 1.612 [0.000, 9.000], mean observation: 30.905 [0.000, 530.000], loss: 148.109314, mae: 37.194901, mean_q: -37.559078\n",
            "  549132/10000000: episode: 2732, duration: 1.695s, episode steps: 201, steps per second: 119, episode reward: 378.200, mean reward: 1.882 [-9.000, 371.400], mean action: 2.035 [0.000, 9.000], mean observation: 29.875 [0.002, 504.200], loss: 288.306122, mae: 37.047459, mean_q: -37.785461\n",
            "  549333/10000000: episode: 2733, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -619.000, mean reward: -3.080 [-309.500, 54.400], mean action: 2.846 [0.000, 10.000], mean observation: 33.981 [0.001, 591.100], loss: 185.439926, mae: 37.050308, mean_q: -38.504395\n",
            "  549534/10000000: episode: 2734, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: 507.800, mean reward: 2.526 [-9.000, 293.500], mean action: 2.418 [0.000, 10.000], mean observation: 30.169 [0.003, 545.800], loss: 262.909424, mae: 37.107727, mean_q: -38.021488\n",
            "  549735/10000000: episode: 2735, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -628.600, mean reward: -3.127 [-314.300, 16.100], mean action: 1.950 [0.000, 9.000], mean observation: 38.489 [0.001, 598.700], loss: 171.616028, mae: 37.247978, mean_q: -37.972248\n",
            "  549936/10000000: episode: 2736, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -100.000, mean reward: -0.498 [-50.000, 94.800], mean action: 1.975 [0.000, 10.000], mean observation: 28.909 [0.002, 308.400], loss: 193.711990, mae: 36.935944, mean_q: -37.981499\n",
            "  550137/10000000: episode: 2737, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -259.600, mean reward: -1.292 [-129.800, 81.000], mean action: 2.408 [0.000, 10.000], mean observation: 30.586 [0.000, 479.300], loss: 262.711273, mae: 37.056202, mean_q: -38.040699\n",
            "  550338/10000000: episode: 2738, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: -369.000, mean reward: -1.836 [-184.500, 99.200], mean action: 2.751 [0.000, 10.000], mean observation: 33.785 [0.000, 663.800], loss: 326.187836, mae: 36.844269, mean_q: -37.858128\n",
            "  550539/10000000: episode: 2739, duration: 1.629s, episode steps: 201, steps per second: 123, episode reward: -90.400, mean reward: -0.450 [-45.200, 94.400], mean action: 2.711 [0.000, 10.000], mean observation: 28.381 [0.001, 477.000], loss: 170.667313, mae: 36.606731, mean_q: -38.025101\n",
            "  550740/10000000: episode: 2740, duration: 1.617s, episode steps: 201, steps per second: 124, episode reward: -505.200, mean reward: -2.513 [-252.600, 42.900], mean action: 2.841 [0.000, 10.000], mean observation: 35.075 [0.000, 639.500], loss: 234.298203, mae: 36.730022, mean_q: -37.999985\n",
            "  550941/10000000: episode: 2741, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: -758.000, mean reward: -3.771 [-379.000, 54.800], mean action: 2.577 [0.000, 9.000], mean observation: 31.451 [0.001, 496.800], loss: 250.354401, mae: 36.883472, mean_q: -37.918709\n",
            "  551142/10000000: episode: 2742, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -488.200, mean reward: -2.429 [-244.100, 40.400], mean action: 2.662 [0.000, 10.000], mean observation: 31.563 [0.001, 619.200], loss: 249.784790, mae: 36.500751, mean_q: -37.704350\n",
            "  551343/10000000: episode: 2743, duration: 1.613s, episode steps: 201, steps per second: 125, episode reward: -414.600, mean reward: -2.063 [-207.300, 116.100], mean action: 2.219 [0.000, 10.000], mean observation: 36.833 [0.001, 651.100], loss: 254.554459, mae: 36.706966, mean_q: -37.673420\n",
            "  551544/10000000: episode: 2744, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: -589.200, mean reward: -2.931 [-294.600, 19.600], mean action: 2.174 [0.000, 8.000], mean observation: 37.725 [0.000, 818.100], loss: 161.804886, mae: 37.340836, mean_q: -38.235153\n",
            "  551745/10000000: episode: 2745, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -171.600, mean reward: -0.854 [-85.800, 110.000], mean action: 2.274 [0.000, 10.000], mean observation: 30.854 [0.003, 540.500], loss: 248.623215, mae: 36.923111, mean_q: -37.859123\n",
            "  551946/10000000: episode: 2746, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -605.600, mean reward: -3.013 [-302.800, 58.400], mean action: 2.383 [0.000, 9.000], mean observation: 34.357 [0.001, 565.900], loss: 216.121368, mae: 37.258240, mean_q: -38.091888\n",
            "  552147/10000000: episode: 2747, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -723.600, mean reward: -3.600 [-361.800, 39.300], mean action: 2.572 [0.000, 9.000], mean observation: 33.786 [0.000, 720.900], loss: 199.107895, mae: 37.343590, mean_q: -38.164181\n",
            "  552348/10000000: episode: 2748, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -269.600, mean reward: -1.341 [-134.800, 107.100], mean action: 2.348 [0.000, 9.000], mean observation: 30.343 [0.002, 624.500], loss: 285.235992, mae: 37.447353, mean_q: -38.349960\n",
            "  552549/10000000: episode: 2749, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -348.400, mean reward: -1.733 [-174.200, 126.000], mean action: 2.393 [0.000, 8.000], mean observation: 34.739 [0.002, 503.100], loss: 165.728394, mae: 37.419121, mean_q: -38.165680\n",
            "  552750/10000000: episode: 2750, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -278.800, mean reward: -1.387 [-139.400, 88.000], mean action: 2.443 [0.000, 10.000], mean observation: 27.541 [0.001, 462.800], loss: 208.820618, mae: 37.565224, mean_q: -38.375145\n",
            "  552951/10000000: episode: 2751, duration: 1.612s, episode steps: 201, steps per second: 125, episode reward: -328.400, mean reward: -1.634 [-164.200, 74.800], mean action: 2.234 [0.000, 8.000], mean observation: 34.477 [0.000, 476.600], loss: 390.754089, mae: 37.180431, mean_q: -37.678032\n",
            "  553152/10000000: episode: 2752, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: -405.800, mean reward: -2.019 [-202.900, 26.400], mean action: 2.174 [0.000, 7.000], mean observation: 38.033 [0.001, 642.700], loss: 251.824982, mae: 36.682869, mean_q: -36.978691\n",
            "  553353/10000000: episode: 2753, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -455.200, mean reward: -2.265 [-227.600, 84.600], mean action: 2.090 [0.000, 8.000], mean observation: 37.378 [0.000, 907.100], loss: 147.360031, mae: 36.468224, mean_q: -36.754456\n",
            "  553554/10000000: episode: 2754, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -281.000, mean reward: -1.398 [-140.500, 51.200], mean action: 2.289 [0.000, 8.000], mean observation: 30.146 [0.001, 399.700], loss: 279.913055, mae: 36.460403, mean_q: -36.824635\n",
            "  553755/10000000: episode: 2755, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -506.600, mean reward: -2.520 [-253.300, 66.400], mean action: 2.219 [0.000, 9.000], mean observation: 34.911 [0.001, 507.200], loss: 257.917236, mae: 36.701988, mean_q: -37.059959\n",
            "  553956/10000000: episode: 2756, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -319.400, mean reward: -1.589 [-159.700, 72.100], mean action: 2.264 [0.000, 9.000], mean observation: 30.754 [0.002, 522.800], loss: 228.737579, mae: 36.345009, mean_q: -37.032269\n",
            "  554157/10000000: episode: 2757, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: 14.800, mean reward: 0.074 [-8.000, 164.400], mean action: 2.816 [0.000, 8.000], mean observation: 29.211 [0.000, 623.900], loss: 317.162384, mae: 36.181656, mean_q: -36.997944\n",
            "  554358/10000000: episode: 2758, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: 496.400, mean reward: 2.470 [-9.000, 524.700], mean action: 2.443 [0.000, 9.000], mean observation: 35.007 [0.001, 542.000], loss: 329.824432, mae: 36.382496, mean_q: -36.738064\n",
            "  554559/10000000: episode: 2759, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: -240.800, mean reward: -1.198 [-120.400, 140.800], mean action: 2.910 [0.000, 8.000], mean observation: 32.501 [0.000, 493.100], loss: 166.841141, mae: 36.274002, mean_q: -37.159470\n",
            "  554760/10000000: episode: 2760, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -485.400, mean reward: -2.415 [-242.700, 49.000], mean action: 2.731 [0.000, 10.000], mean observation: 32.702 [0.000, 781.200], loss: 298.184204, mae: 36.679417, mean_q: -37.831905\n",
            "  554961/10000000: episode: 2761, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: 26.400, mean reward: 0.131 [-10.000, 127.400], mean action: 2.592 [0.000, 10.000], mean observation: 30.886 [0.001, 492.700], loss: 312.101471, mae: 36.745682, mean_q: -37.558624\n",
            "  555162/10000000: episode: 2762, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -129.200, mean reward: -0.643 [-64.600, 70.500], mean action: 2.174 [0.000, 10.000], mean observation: 35.739 [0.001, 467.200], loss: 313.618713, mae: 36.858635, mean_q: -37.555668\n",
            "  555363/10000000: episode: 2763, duration: 1.589s, episode steps: 201, steps per second: 127, episode reward: -72.400, mean reward: -0.360 [-36.200, 202.500], mean action: 2.199 [0.000, 10.000], mean observation: 33.182 [0.001, 511.400], loss: 205.781662, mae: 36.479008, mean_q: -36.958729\n",
            "  555564/10000000: episode: 2764, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: -66.600, mean reward: -0.331 [-33.300, 94.400], mean action: 2.259 [0.000, 8.000], mean observation: 35.093 [0.000, 764.600], loss: 265.738525, mae: 36.295624, mean_q: -36.788784\n",
            "  555765/10000000: episode: 2765, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -296.000, mean reward: -1.473 [-148.000, 57.200], mean action: 2.323 [0.000, 9.000], mean observation: 30.697 [0.000, 498.200], loss: 218.406464, mae: 36.068478, mean_q: -36.690742\n",
            "  555966/10000000: episode: 2766, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -580.200, mean reward: -2.887 [-290.100, 40.800], mean action: 2.378 [0.000, 10.000], mean observation: 26.287 [0.001, 516.900], loss: 253.998444, mae: 36.310696, mean_q: -36.986855\n",
            "  556167/10000000: episode: 2767, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -349.600, mean reward: -1.739 [-174.800, 148.400], mean action: 2.294 [0.000, 10.000], mean observation: 34.667 [0.000, 598.900], loss: 240.888611, mae: 36.388046, mean_q: -36.888828\n",
            "  556368/10000000: episode: 2768, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -656.000, mean reward: -3.264 [-328.000, 59.000], mean action: 2.119 [0.000, 9.000], mean observation: 42.167 [0.000, 792.800], loss: 142.303513, mae: 36.233826, mean_q: -36.906048\n",
            "  556569/10000000: episode: 2769, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -668.800, mean reward: -3.327 [-334.400, 48.600], mean action: 2.393 [0.000, 10.000], mean observation: 27.383 [0.001, 420.000], loss: 184.854401, mae: 36.092564, mean_q: -36.771206\n",
            "  556770/10000000: episode: 2770, duration: 1.642s, episode steps: 201, steps per second: 122, episode reward: -152.400, mean reward: -0.758 [-76.200, 84.800], mean action: 2.199 [0.000, 10.000], mean observation: 41.776 [0.000, 746.700], loss: 180.844482, mae: 36.445362, mean_q: -37.398792\n",
            "  556971/10000000: episode: 2771, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -236.800, mean reward: -1.178 [-118.400, 87.600], mean action: 2.055 [0.000, 10.000], mean observation: 30.891 [0.001, 375.100], loss: 255.379044, mae: 36.183609, mean_q: -36.926971\n",
            "  557172/10000000: episode: 2772, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -259.600, mean reward: -1.292 [-129.800, 77.000], mean action: 1.801 [0.000, 10.000], mean observation: 33.259 [0.000, 541.500], loss: 258.468567, mae: 36.567722, mean_q: -36.977512\n",
            "  557373/10000000: episode: 2773, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -193.800, mean reward: -0.964 [-96.900, 42.600], mean action: 1.886 [0.000, 9.000], mean observation: 29.617 [0.001, 519.300], loss: 226.545746, mae: 36.464573, mean_q: -36.877960\n",
            "  557574/10000000: episode: 2774, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -441.200, mean reward: -2.195 [-220.600, 34.200], mean action: 1.721 [0.000, 9.000], mean observation: 35.103 [0.002, 460.100], loss: 241.301163, mae: 36.015930, mean_q: -36.627743\n",
            "  557775/10000000: episode: 2775, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: 1847.000, mean reward: 9.189 [-10.000, 923.500], mean action: 2.249 [0.000, 10.000], mean observation: 35.462 [0.000, 579.300], loss: 284.724243, mae: 35.626247, mean_q: -36.486839\n",
            "  557976/10000000: episode: 2776, duration: 1.625s, episode steps: 201, steps per second: 124, episode reward: -161.800, mean reward: -0.805 [-80.900, 268.800], mean action: 2.612 [0.000, 9.000], mean observation: 31.849 [0.001, 576.200], loss: 194.948715, mae: 35.860783, mean_q: -36.715126\n",
            "  558177/10000000: episode: 2777, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: -93.400, mean reward: -0.465 [-46.700, 71.600], mean action: 2.289 [0.000, 8.000], mean observation: 29.034 [0.001, 550.200], loss: 400.075317, mae: 36.022610, mean_q: -36.719978\n",
            "  558378/10000000: episode: 2778, duration: 1.633s, episode steps: 201, steps per second: 123, episode reward: -496.200, mean reward: -2.469 [-248.100, 98.000], mean action: 2.393 [0.000, 9.000], mean observation: 32.460 [0.001, 548.600], loss: 229.882217, mae: 35.504364, mean_q: -36.156487\n",
            "  558579/10000000: episode: 2779, duration: 1.659s, episode steps: 201, steps per second: 121, episode reward: 104.400, mean reward: 0.519 [-10.000, 138.600], mean action: 2.687 [0.000, 10.000], mean observation: 33.073 [0.001, 494.100], loss: 244.463226, mae: 35.316067, mean_q: -36.284328\n",
            "  558780/10000000: episode: 2780, duration: 1.637s, episode steps: 201, steps per second: 123, episode reward: -504.600, mean reward: -2.510 [-252.300, 97.600], mean action: 2.502 [0.000, 9.000], mean observation: 35.076 [0.000, 525.700], loss: 188.958649, mae: 35.112381, mean_q: -36.033985\n",
            "  558981/10000000: episode: 2781, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -584.600, mean reward: -2.908 [-292.300, 66.500], mean action: 2.587 [0.000, 10.000], mean observation: 43.636 [0.000, 803.400], loss: 270.466125, mae: 34.819004, mean_q: -35.783588\n",
            "  559182/10000000: episode: 2782, duration: 1.625s, episode steps: 201, steps per second: 124, episode reward: -116.600, mean reward: -0.580 [-58.300, 58.800], mean action: 2.602 [0.000, 10.000], mean observation: 27.799 [0.000, 597.200], loss: 216.418106, mae: 35.016640, mean_q: -36.031654\n",
            "  559383/10000000: episode: 2783, duration: 1.627s, episode steps: 201, steps per second: 124, episode reward: -689.600, mean reward: -3.431 [-344.800, 23.400], mean action: 2.468 [0.000, 9.000], mean observation: 29.874 [0.002, 455.800], loss: 176.260254, mae: 35.599621, mean_q: -36.637867\n",
            "  559584/10000000: episode: 2784, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -626.600, mean reward: -3.117 [-313.300, 27.000], mean action: 2.085 [0.000, 9.000], mean observation: 41.593 [0.001, 545.700], loss: 171.129074, mae: 35.832073, mean_q: -36.723911\n",
            "  559785/10000000: episode: 2785, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -384.000, mean reward: -1.910 [-192.000, 91.200], mean action: 1.886 [0.000, 9.000], mean observation: 29.080 [0.000, 581.400], loss: 230.098801, mae: 36.023930, mean_q: -36.662758\n",
            "  559986/10000000: episode: 2786, duration: 1.628s, episode steps: 201, steps per second: 123, episode reward: -324.200, mean reward: -1.613 [-162.100, 115.500], mean action: 2.299 [0.000, 9.000], mean observation: 36.663 [0.000, 622.400], loss: 180.594803, mae: 35.737057, mean_q: -36.564919\n",
            "  560187/10000000: episode: 2787, duration: 1.625s, episode steps: 201, steps per second: 124, episode reward: -674.200, mean reward: -3.354 [-337.100, 23.600], mean action: 2.562 [0.000, 10.000], mean observation: 35.346 [0.001, 430.400], loss: 203.660309, mae: 35.301613, mean_q: -36.120094\n",
            "  560388/10000000: episode: 2788, duration: 1.630s, episode steps: 201, steps per second: 123, episode reward: -360.800, mean reward: -1.795 [-180.400, 65.000], mean action: 2.239 [0.000, 9.000], mean observation: 35.891 [0.000, 669.400], loss: 269.796722, mae: 35.502468, mean_q: -36.548470\n",
            "  560589/10000000: episode: 2789, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -359.000, mean reward: -1.786 [-179.500, 71.200], mean action: 2.383 [0.000, 10.000], mean observation: 34.239 [0.002, 511.500], loss: 186.885727, mae: 35.650364, mean_q: -36.462379\n",
            "  560790/10000000: episode: 2790, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -395.200, mean reward: -1.966 [-197.600, 90.600], mean action: 2.219 [0.000, 8.000], mean observation: 37.907 [0.000, 663.000], loss: 172.550644, mae: 35.866585, mean_q: -36.781776\n",
            "  560991/10000000: episode: 2791, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -648.200, mean reward: -3.225 [-324.100, 45.000], mean action: 2.453 [0.000, 9.000], mean observation: 36.798 [0.000, 637.900], loss: 218.408081, mae: 36.046860, mean_q: -36.954441\n",
            "  561192/10000000: episode: 2792, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -99.000, mean reward: -0.493 [-49.500, 152.800], mean action: 2.055 [0.000, 8.000], mean observation: 33.425 [0.002, 542.900], loss: 188.431015, mae: 36.374363, mean_q: -37.335396\n",
            "  561393/10000000: episode: 2793, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -446.800, mean reward: -2.223 [-223.400, 43.500], mean action: 2.468 [0.000, 9.000], mean observation: 31.918 [0.001, 581.100], loss: 222.763123, mae: 36.112541, mean_q: -37.211742\n",
            "  561594/10000000: episode: 2794, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: -863.800, mean reward: -4.298 [-431.900, 17.600], mean action: 2.433 [0.000, 9.000], mean observation: 33.598 [0.001, 469.000], loss: 186.559814, mae: 35.950314, mean_q: -37.038925\n",
            "  561795/10000000: episode: 2795, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -491.200, mean reward: -2.444 [-245.600, 144.900], mean action: 2.851 [0.000, 9.000], mean observation: 35.332 [0.000, 622.800], loss: 245.231018, mae: 35.868408, mean_q: -36.898891\n",
            "  561996/10000000: episode: 2796, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -797.600, mean reward: -3.968 [-398.800, 55.300], mean action: 2.876 [0.000, 9.000], mean observation: 34.472 [0.001, 446.000], loss: 203.752838, mae: 36.270332, mean_q: -37.504658\n",
            "  562197/10000000: episode: 2797, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -484.200, mean reward: -2.409 [-242.100, 164.800], mean action: 3.090 [0.000, 10.000], mean observation: 32.620 [0.001, 424.400], loss: 251.982666, mae: 36.190289, mean_q: -37.348705\n",
            "  562398/10000000: episode: 2798, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: -667.800, mean reward: -3.322 [-333.900, 128.800], mean action: 2.632 [0.000, 8.000], mean observation: 32.160 [0.000, 749.600], loss: 216.771866, mae: 36.515068, mean_q: -37.603905\n",
            "  562599/10000000: episode: 2799, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: 62.000, mean reward: 0.308 [-9.000, 308.000], mean action: 2.532 [0.000, 9.000], mean observation: 29.985 [0.001, 587.600], loss: 108.392761, mae: 37.046772, mean_q: -37.970173\n",
            "  562800/10000000: episode: 2800, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -22.000, mean reward: -0.109 [-11.000, 202.100], mean action: 2.154 [0.000, 9.000], mean observation: 37.104 [0.002, 516.600], loss: 185.642380, mae: 37.082890, mean_q: -37.885746\n",
            "  563001/10000000: episode: 2801, duration: 1.564s, episode steps: 201, steps per second: 128, episode reward: -383.800, mean reward: -1.909 [-191.900, 70.000], mean action: 2.179 [0.000, 8.000], mean observation: 25.747 [0.001, 415.200], loss: 178.702621, mae: 37.312000, mean_q: -38.389034\n",
            "  563202/10000000: episode: 2802, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: 32.000, mean reward: 0.159 [-9.000, 68.300], mean action: 2.100 [0.000, 10.000], mean observation: 36.446 [0.004, 509.100], loss: 252.697662, mae: 37.313927, mean_q: -38.305847\n",
            "  563403/10000000: episode: 2803, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: -368.800, mean reward: -1.835 [-184.400, 96.800], mean action: 2.144 [0.000, 9.000], mean observation: 28.152 [0.000, 590.900], loss: 135.660004, mae: 37.182991, mean_q: -38.278214\n",
            "  563604/10000000: episode: 2804, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: -489.600, mean reward: -2.436 [-244.800, 58.200], mean action: 2.418 [0.000, 9.000], mean observation: 33.145 [0.003, 566.300], loss: 372.971313, mae: 37.111317, mean_q: -38.156628\n",
            "  563805/10000000: episode: 2805, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -573.800, mean reward: -2.855 [-286.900, 49.500], mean action: 2.259 [0.000, 9.000], mean observation: 26.844 [0.000, 486.700], loss: 258.953430, mae: 36.710186, mean_q: -37.413231\n",
            "  564006/10000000: episode: 2806, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -312.000, mean reward: -1.552 [-156.000, 90.000], mean action: 1.841 [0.000, 9.000], mean observation: 36.527 [0.001, 510.800], loss: 218.876038, mae: 36.700550, mean_q: -37.023331\n",
            "  564207/10000000: episode: 2807, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: 709.600, mean reward: 3.530 [-8.000, 354.800], mean action: 1.871 [0.000, 8.000], mean observation: 32.765 [0.001, 423.300], loss: 261.749115, mae: 36.382114, mean_q: -36.652489\n",
            "  564408/10000000: episode: 2808, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: 62.800, mean reward: 0.312 [-8.000, 153.200], mean action: 2.249 [0.000, 8.000], mean observation: 32.442 [0.000, 637.000], loss: 190.202133, mae: 36.123791, mean_q: -36.691612\n",
            "  564609/10000000: episode: 2809, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -119.000, mean reward: -0.592 [-59.500, 108.600], mean action: 1.627 [0.000, 8.000], mean observation: 32.528 [0.002, 472.000], loss: 214.078598, mae: 36.195404, mean_q: -36.497501\n",
            "  564810/10000000: episode: 2810, duration: 1.617s, episode steps: 201, steps per second: 124, episode reward: 21.800, mean reward: 0.108 [-8.000, 103.800], mean action: 1.935 [0.000, 8.000], mean observation: 30.388 [0.000, 640.400], loss: 174.945099, mae: 36.348934, mean_q: -36.747864\n",
            "  565011/10000000: episode: 2811, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: 60.800, mean reward: 0.302 [-8.000, 140.200], mean action: 1.776 [0.000, 8.000], mean observation: 32.063 [0.001, 512.800], loss: 167.066452, mae: 36.118629, mean_q: -36.551758\n",
            "  565212/10000000: episode: 2812, duration: 1.635s, episode steps: 201, steps per second: 123, episode reward: -92.800, mean reward: -0.462 [-46.400, 70.200], mean action: 2.090 [0.000, 9.000], mean observation: 32.440 [0.001, 558.800], loss: 214.725662, mae: 36.497482, mean_q: -37.150032\n",
            "  565413/10000000: episode: 2813, duration: 1.644s, episode steps: 201, steps per second: 122, episode reward: -239.600, mean reward: -1.192 [-119.800, 84.600], mean action: 2.343 [0.000, 9.000], mean observation: 35.620 [0.000, 539.800], loss: 243.739410, mae: 36.355736, mean_q: -36.962631\n",
            "  565614/10000000: episode: 2814, duration: 1.602s, episode steps: 201, steps per second: 125, episode reward: -633.200, mean reward: -3.150 [-316.600, 43.200], mean action: 2.393 [0.000, 8.000], mean observation: 36.206 [0.001, 584.600], loss: 219.719757, mae: 36.709713, mean_q: -37.367149\n",
            "  565815/10000000: episode: 2815, duration: 1.645s, episode steps: 201, steps per second: 122, episode reward: -215.000, mean reward: -1.070 [-107.500, 173.200], mean action: 2.020 [0.000, 8.000], mean observation: 35.710 [0.000, 455.400], loss: 224.621353, mae: 36.649345, mean_q: -37.104504\n",
            "  566016/10000000: episode: 2816, duration: 1.642s, episode steps: 201, steps per second: 122, episode reward: -375.400, mean reward: -1.868 [-187.700, 32.400], mean action: 1.871 [0.000, 8.000], mean observation: 32.626 [0.001, 510.200], loss: 197.938477, mae: 36.458858, mean_q: -36.757317\n",
            "  566217/10000000: episode: 2817, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -581.000, mean reward: -2.891 [-290.500, 23.200], mean action: 2.174 [0.000, 10.000], mean observation: 31.350 [0.000, 708.200], loss: 164.798279, mae: 36.383831, mean_q: -36.824108\n",
            "  566418/10000000: episode: 2818, duration: 1.632s, episode steps: 201, steps per second: 123, episode reward: 260.000, mean reward: 1.294 [-8.000, 318.900], mean action: 2.144 [0.000, 8.000], mean observation: 34.253 [0.002, 500.200], loss: 184.702927, mae: 36.731209, mean_q: -37.378162\n",
            "  566619/10000000: episode: 2819, duration: 1.652s, episode steps: 201, steps per second: 122, episode reward: -268.800, mean reward: -1.337 [-134.400, 56.100], mean action: 2.214 [0.000, 9.000], mean observation: 38.407 [0.000, 440.500], loss: 161.736206, mae: 36.490986, mean_q: -37.266682\n",
            "  566820/10000000: episode: 2820, duration: 1.627s, episode steps: 201, steps per second: 124, episode reward: 359.000, mean reward: 1.786 [-8.000, 179.500], mean action: 2.179 [0.000, 8.000], mean observation: 34.559 [0.000, 566.800], loss: 216.287720, mae: 36.640625, mean_q: -37.564083\n",
            "  567021/10000000: episode: 2821, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: -405.600, mean reward: -2.018 [-202.800, 37.800], mean action: 2.149 [0.000, 9.000], mean observation: 30.987 [0.001, 571.900], loss: 200.118698, mae: 37.046886, mean_q: -37.825371\n",
            "  567222/10000000: episode: 2822, duration: 2.015s, episode steps: 201, steps per second: 100, episode reward: -502.400, mean reward: -2.500 [-251.200, 59.500], mean action: 2.055 [0.000, 10.000], mean observation: 34.508 [0.000, 494.100], loss: 335.875305, mae: 36.953526, mean_q: -37.587730\n",
            "  567423/10000000: episode: 2823, duration: 2.114s, episode steps: 201, steps per second: 95, episode reward: -271.800, mean reward: -1.352 [-135.900, 51.800], mean action: 1.796 [0.000, 10.000], mean observation: 28.035 [0.002, 465.200], loss: 205.293030, mae: 36.677002, mean_q: -36.930073\n",
            "  567624/10000000: episode: 2824, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 166.000, mean reward: 0.826 [-9.000, 227.000], mean action: 1.587 [0.000, 10.000], mean observation: 30.775 [0.001, 533.900], loss: 212.836517, mae: 36.300392, mean_q: -36.741402\n",
            "  567825/10000000: episode: 2825, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: -135.600, mean reward: -0.675 [-67.800, 212.800], mean action: 2.488 [0.000, 10.000], mean observation: 33.901 [0.000, 798.300], loss: 209.335938, mae: 36.027359, mean_q: -36.748905\n",
            "  568026/10000000: episode: 2826, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -426.800, mean reward: -2.123 [-213.400, 57.600], mean action: 1.866 [0.000, 9.000], mean observation: 30.605 [0.001, 434.300], loss: 177.298981, mae: 36.012192, mean_q: -36.397617\n",
            "  568227/10000000: episode: 2827, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -505.000, mean reward: -2.512 [-252.500, 25.200], mean action: 1.786 [0.000, 9.000], mean observation: 32.346 [0.000, 668.900], loss: 178.105194, mae: 36.283066, mean_q: -36.730190\n",
            "  568428/10000000: episode: 2828, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -211.200, mean reward: -1.051 [-105.600, 56.600], mean action: 1.866 [0.000, 9.000], mean observation: 34.367 [0.000, 506.300], loss: 190.014816, mae: 36.337559, mean_q: -36.829853\n",
            "  568629/10000000: episode: 2829, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -323.400, mean reward: -1.609 [-161.700, 75.900], mean action: 2.234 [0.000, 9.000], mean observation: 29.466 [0.003, 383.900], loss: 216.749130, mae: 36.067226, mean_q: -36.601154\n",
            "  568830/10000000: episode: 2830, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -472.200, mean reward: -2.349 [-236.100, 21.600], mean action: 1.682 [0.000, 10.000], mean observation: 36.420 [0.000, 605.900], loss: 248.599731, mae: 36.272251, mean_q: -36.814350\n",
            "  569031/10000000: episode: 2831, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -420.000, mean reward: -2.090 [-210.000, 26.600], mean action: 1.657 [0.000, 8.000], mean observation: 35.868 [0.000, 650.000], loss: 188.477371, mae: 36.037235, mean_q: -36.618717\n",
            "  569232/10000000: episode: 2832, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: 271.200, mean reward: 1.349 [-8.000, 360.000], mean action: 2.159 [0.000, 8.000], mean observation: 28.303 [0.003, 432.300], loss: 251.914017, mae: 35.711014, mean_q: -36.288586\n",
            "  569433/10000000: episode: 2833, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: 38.400, mean reward: 0.191 [-7.000, 192.900], mean action: 2.149 [0.000, 7.000], mean observation: 35.414 [0.002, 420.100], loss: 212.904739, mae: 35.854546, mean_q: -36.467709\n",
            "  569634/10000000: episode: 2834, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 30.400, mean reward: 0.151 [-9.000, 262.000], mean action: 1.841 [0.000, 9.000], mean observation: 42.103 [0.001, 510.400], loss: 173.795822, mae: 36.107929, mean_q: -36.617500\n",
            "  569835/10000000: episode: 2835, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: 84.600, mean reward: 0.421 [-10.000, 187.700], mean action: 1.896 [0.000, 10.000], mean observation: 31.610 [0.000, 508.900], loss: 204.314682, mae: 36.072796, mean_q: -36.434925\n",
            "  570036/10000000: episode: 2836, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 123.200, mean reward: 0.613 [-7.000, 337.500], mean action: 1.746 [0.000, 7.000], mean observation: 33.121 [0.002, 447.500], loss: 199.182861, mae: 35.855858, mean_q: -36.131817\n",
            "  570237/10000000: episode: 2837, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -300.600, mean reward: -1.496 [-150.300, 58.300], mean action: 2.159 [0.000, 10.000], mean observation: 31.171 [0.001, 618.400], loss: 171.618866, mae: 35.816677, mean_q: -36.167343\n",
            "  570438/10000000: episode: 2838, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: 243.200, mean reward: 1.210 [-8.000, 160.400], mean action: 2.229 [0.000, 8.000], mean observation: 32.301 [0.001, 512.000], loss: 208.168274, mae: 35.828800, mean_q: -36.124664\n",
            "  570639/10000000: episode: 2839, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -369.000, mean reward: -1.836 [-184.500, 64.000], mean action: 2.109 [0.000, 9.000], mean observation: 33.081 [0.000, 586.000], loss: 225.659668, mae: 35.544163, mean_q: -35.616264\n",
            "  570840/10000000: episode: 2840, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -551.200, mean reward: -2.742 [-275.600, 47.600], mean action: 2.124 [0.000, 9.000], mean observation: 32.272 [0.001, 423.700], loss: 194.089294, mae: 35.474411, mean_q: -35.715961\n",
            "  571041/10000000: episode: 2841, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -211.000, mean reward: -1.050 [-105.500, 232.500], mean action: 2.244 [0.000, 7.000], mean observation: 30.305 [0.001, 459.200], loss: 170.737350, mae: 35.417770, mean_q: -35.595062\n",
            "  571242/10000000: episode: 2842, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: -436.600, mean reward: -2.172 [-218.300, 70.400], mean action: 2.284 [0.000, 8.000], mean observation: 31.416 [0.001, 422.300], loss: 197.399368, mae: 35.644012, mean_q: -35.921631\n",
            "  571443/10000000: episode: 2843, duration: 1.611s, episode steps: 201, steps per second: 125, episode reward: -99.400, mean reward: -0.495 [-49.700, 228.800], mean action: 2.303 [0.000, 8.000], mean observation: 33.156 [0.001, 497.500], loss: 192.245728, mae: 36.202782, mean_q: -36.755276\n",
            "  571644/10000000: episode: 2844, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -157.200, mean reward: -0.782 [-78.600, 70.400], mean action: 2.095 [0.000, 10.000], mean observation: 30.815 [0.002, 439.900], loss: 184.672684, mae: 36.180450, mean_q: -36.696739\n",
            "  571845/10000000: episode: 2845, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: -605.600, mean reward: -3.013 [-302.800, 35.200], mean action: 2.483 [0.000, 9.000], mean observation: 30.142 [0.002, 527.400], loss: 236.562546, mae: 36.102821, mean_q: -36.709839\n",
            "  572046/10000000: episode: 2846, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -180.800, mean reward: -0.900 [-90.400, 143.800], mean action: 2.313 [0.000, 8.000], mean observation: 37.323 [0.000, 720.900], loss: 137.748108, mae: 35.813057, mean_q: -36.589333\n",
            "  572247/10000000: episode: 2847, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -408.600, mean reward: -2.033 [-204.300, 38.100], mean action: 2.363 [0.000, 9.000], mean observation: 32.099 [0.001, 500.800], loss: 150.521103, mae: 36.194782, mean_q: -36.993423\n",
            "  572448/10000000: episode: 2848, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: 25.400, mean reward: 0.126 [-8.000, 122.700], mean action: 2.015 [0.000, 8.000], mean observation: 31.436 [0.001, 424.400], loss: 205.998795, mae: 36.363956, mean_q: -36.798420\n",
            "  572649/10000000: episode: 2849, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: 110.200, mean reward: 0.548 [-8.000, 292.400], mean action: 2.010 [0.000, 8.000], mean observation: 32.182 [0.000, 441.700], loss: 129.116486, mae: 36.465336, mean_q: -36.856831\n",
            "  572850/10000000: episode: 2850, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 623.800, mean reward: 3.103 [-8.000, 311.900], mean action: 1.905 [0.000, 8.000], mean observation: 27.998 [0.003, 380.000], loss: 270.786194, mae: 36.508163, mean_q: -36.793564\n",
            "  573051/10000000: episode: 2851, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: -302.600, mean reward: -1.505 [-151.300, 43.600], mean action: 1.786 [0.000, 8.000], mean observation: 34.105 [0.002, 537.100], loss: 193.089859, mae: 36.735935, mean_q: -36.938457\n",
            "  573252/10000000: episode: 2852, duration: 1.661s, episode steps: 201, steps per second: 121, episode reward: -241.800, mean reward: -1.203 [-120.900, 70.400], mean action: 1.866 [0.000, 8.000], mean observation: 34.025 [0.001, 562.800], loss: 239.111206, mae: 36.554638, mean_q: -36.662308\n",
            "  573453/10000000: episode: 2853, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: 540.200, mean reward: 2.688 [-8.000, 435.200], mean action: 2.577 [0.000, 8.000], mean observation: 30.578 [0.001, 530.600], loss: 185.897385, mae: 36.698769, mean_q: -36.857494\n",
            "  573654/10000000: episode: 2854, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -453.200, mean reward: -2.255 [-226.600, 48.600], mean action: 1.756 [0.000, 7.000], mean observation: 41.526 [0.000, 693.300], loss: 178.311722, mae: 36.583912, mean_q: -36.528297\n",
            "  573855/10000000: episode: 2855, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -318.400, mean reward: -1.584 [-159.200, 92.400], mean action: 2.149 [0.000, 9.000], mean observation: 33.028 [0.000, 727.600], loss: 246.108795, mae: 36.526550, mean_q: -36.532417\n",
            "  574056/10000000: episode: 2856, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -62.000, mean reward: -0.308 [-31.000, 62.800], mean action: 2.219 [0.000, 8.000], mean observation: 34.079 [0.002, 453.000], loss: 179.664642, mae: 36.557377, mean_q: -36.647892\n",
            "  574257/10000000: episode: 2857, duration: 1.600s, episode steps: 201, steps per second: 126, episode reward: -358.400, mean reward: -1.783 [-179.200, 51.600], mean action: 2.174 [0.000, 8.000], mean observation: 32.484 [0.001, 606.200], loss: 137.319717, mae: 36.328926, mean_q: -36.504704\n",
            "  574458/10000000: episode: 2858, duration: 1.564s, episode steps: 201, steps per second: 128, episode reward: -442.800, mean reward: -2.203 [-221.400, 66.700], mean action: 2.199 [0.000, 8.000], mean observation: 36.286 [0.003, 593.100], loss: 103.025307, mae: 36.844509, mean_q: -37.153805\n",
            "  574659/10000000: episode: 2859, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -459.200, mean reward: -2.285 [-229.600, 29.600], mean action: 1.935 [0.000, 8.000], mean observation: 30.424 [0.002, 482.900], loss: 149.091827, mae: 36.810143, mean_q: -37.099762\n",
            "  574860/10000000: episode: 2860, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -455.000, mean reward: -2.264 [-227.500, 48.400], mean action: 2.144 [0.000, 8.000], mean observation: 29.088 [0.001, 697.100], loss: 236.324387, mae: 36.442665, mean_q: -36.638103\n",
            "  575061/10000000: episode: 2861, duration: 1.639s, episode steps: 201, steps per second: 123, episode reward: -549.600, mean reward: -2.734 [-274.800, 44.000], mean action: 2.189 [0.000, 9.000], mean observation: 29.370 [0.003, 454.700], loss: 199.440781, mae: 36.442463, mean_q: -36.929203\n",
            "  575262/10000000: episode: 2862, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: -521.000, mean reward: -2.592 [-260.500, 43.200], mean action: 2.348 [0.000, 10.000], mean observation: 30.167 [0.001, 562.800], loss: 178.758942, mae: 36.826698, mean_q: -37.357327\n",
            "  575463/10000000: episode: 2863, duration: 1.612s, episode steps: 201, steps per second: 125, episode reward: -675.400, mean reward: -3.360 [-337.700, 23.800], mean action: 2.154 [0.000, 8.000], mean observation: 36.135 [0.000, 458.900], loss: 139.539124, mae: 36.758694, mean_q: -37.288944\n",
            "  575664/10000000: episode: 2864, duration: 1.677s, episode steps: 201, steps per second: 120, episode reward: 426.600, mean reward: 2.122 [-8.000, 344.400], mean action: 1.846 [0.000, 8.000], mean observation: 30.346 [0.001, 522.200], loss: 162.748154, mae: 37.004116, mean_q: -37.366261\n",
            "  575865/10000000: episode: 2865, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: -183.800, mean reward: -0.914 [-91.900, 82.400], mean action: 2.119 [0.000, 10.000], mean observation: 31.687 [0.000, 527.200], loss: 123.497917, mae: 36.705803, mean_q: -37.230801\n",
            "  576066/10000000: episode: 2866, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: -893.000, mean reward: -4.443 [-446.500, 60.200], mean action: 2.622 [0.000, 9.000], mean observation: 36.364 [0.000, 451.200], loss: 199.327499, mae: 36.899200, mean_q: -37.903542\n",
            "  576267/10000000: episode: 2867, duration: 1.625s, episode steps: 201, steps per second: 124, episode reward: -331.400, mean reward: -1.649 [-165.700, 136.500], mean action: 2.726 [0.000, 9.000], mean observation: 30.800 [0.000, 765.000], loss: 238.751755, mae: 36.649807, mean_q: -37.511288\n",
            "  576468/10000000: episode: 2868, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: 220.200, mean reward: 1.096 [-9.000, 255.000], mean action: 2.582 [0.000, 9.000], mean observation: 34.882 [0.000, 638.500], loss: 141.208374, mae: 36.808758, mean_q: -37.736267\n",
            "  576669/10000000: episode: 2869, duration: 1.616s, episode steps: 201, steps per second: 124, episode reward: 490.200, mean reward: 2.439 [-9.000, 308.800], mean action: 2.090 [0.000, 9.000], mean observation: 32.950 [0.000, 534.900], loss: 196.166672, mae: 36.812328, mean_q: -37.594181\n",
            "  576870/10000000: episode: 2870, duration: 1.625s, episode steps: 201, steps per second: 124, episode reward: -497.200, mean reward: -2.474 [-248.600, 46.400], mean action: 2.055 [0.000, 10.000], mean observation: 39.126 [0.000, 793.800], loss: 155.501007, mae: 37.054886, mean_q: -37.788010\n",
            "  577071/10000000: episode: 2871, duration: 1.646s, episode steps: 201, steps per second: 122, episode reward: -477.400, mean reward: -2.375 [-238.700, 55.200], mean action: 2.308 [0.000, 9.000], mean observation: 33.788 [0.001, 681.600], loss: 278.250031, mae: 36.841869, mean_q: -37.472111\n",
            "  577272/10000000: episode: 2872, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: -727.600, mean reward: -3.620 [-363.800, 17.400], mean action: 2.000 [0.000, 8.000], mean observation: 43.145 [0.000, 701.500], loss: 212.786194, mae: 36.707230, mean_q: -37.154354\n",
            "  577473/10000000: episode: 2873, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: 520.800, mean reward: 2.591 [-9.000, 260.400], mean action: 2.174 [0.000, 9.000], mean observation: 31.870 [0.000, 783.800], loss: 133.842865, mae: 36.874550, mean_q: -37.832863\n",
            "  577674/10000000: episode: 2874, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -345.800, mean reward: -1.720 [-172.900, 41.400], mean action: 2.179 [0.000, 9.000], mean observation: 37.962 [0.000, 642.000], loss: 166.939697, mae: 36.931576, mean_q: -37.819988\n",
            "  577875/10000000: episode: 2875, duration: 1.637s, episode steps: 201, steps per second: 123, episode reward: -607.800, mean reward: -3.024 [-303.900, 31.200], mean action: 2.692 [0.000, 10.000], mean observation: 33.758 [0.002, 515.200], loss: 148.617401, mae: 37.241226, mean_q: -38.155785\n",
            "  578076/10000000: episode: 2876, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: -384.200, mean reward: -1.911 [-192.100, 84.300], mean action: 2.368 [0.000, 10.000], mean observation: 34.795 [0.001, 430.300], loss: 220.513138, mae: 37.229794, mean_q: -37.956532\n",
            "  578277/10000000: episode: 2877, duration: 1.619s, episode steps: 201, steps per second: 124, episode reward: -240.600, mean reward: -1.197 [-120.300, 58.600], mean action: 2.229 [0.000, 9.000], mean observation: 41.037 [0.000, 673.700], loss: 163.905746, mae: 37.228191, mean_q: -38.036308\n",
            "  578478/10000000: episode: 2878, duration: 1.748s, episode steps: 201, steps per second: 115, episode reward: -338.000, mean reward: -1.682 [-169.000, 34.400], mean action: 2.229 [0.000, 10.000], mean observation: 29.008 [0.001, 459.000], loss: 131.944260, mae: 37.080486, mean_q: -37.842495\n",
            "  578679/10000000: episode: 2879, duration: 1.811s, episode steps: 201, steps per second: 111, episode reward: 260.000, mean reward: 1.294 [-10.000, 282.000], mean action: 2.348 [0.000, 10.000], mean observation: 39.225 [0.001, 558.800], loss: 183.644257, mae: 36.791058, mean_q: -37.690601\n",
            "  578880/10000000: episode: 2880, duration: 1.839s, episode steps: 201, steps per second: 109, episode reward: 143.800, mean reward: 0.715 [-9.000, 174.200], mean action: 2.468 [0.000, 9.000], mean observation: 35.367 [0.000, 933.200], loss: 189.373917, mae: 36.907635, mean_q: -37.975445\n",
            "  579081/10000000: episode: 2881, duration: 1.632s, episode steps: 201, steps per second: 123, episode reward: -496.400, mean reward: -2.470 [-248.200, 29.600], mean action: 2.254 [0.000, 9.000], mean observation: 30.550 [0.000, 684.900], loss: 164.132614, mae: 36.875526, mean_q: -37.897556\n",
            "  579282/10000000: episode: 2882, duration: 1.705s, episode steps: 201, steps per second: 118, episode reward: 59.800, mean reward: 0.298 [-9.000, 159.200], mean action: 1.751 [0.000, 9.000], mean observation: 32.852 [0.000, 567.300], loss: 132.336502, mae: 37.252754, mean_q: -37.997372\n",
            "  579483/10000000: episode: 2883, duration: 1.623s, episode steps: 201, steps per second: 124, episode reward: -63.400, mean reward: -0.315 [-31.700, 116.800], mean action: 1.925 [0.000, 8.000], mean observation: 30.673 [0.001, 556.700], loss: 170.007736, mae: 37.408539, mean_q: -37.878994\n",
            "  579684/10000000: episode: 2884, duration: 1.648s, episode steps: 201, steps per second: 122, episode reward: -411.600, mean reward: -2.048 [-205.800, 48.600], mean action: 1.940 [0.000, 9.000], mean observation: 35.793 [0.000, 444.500], loss: 160.305054, mae: 37.011246, mean_q: -37.341606\n",
            "  579885/10000000: episode: 2885, duration: 1.620s, episode steps: 201, steps per second: 124, episode reward: -639.400, mean reward: -3.181 [-319.700, 50.000], mean action: 2.249 [0.000, 9.000], mean observation: 35.747 [0.000, 607.900], loss: 220.997253, mae: 36.973568, mean_q: -37.612045\n",
            "  580086/10000000: episode: 2886, duration: 1.671s, episode steps: 201, steps per second: 120, episode reward: -403.400, mean reward: -2.007 [-201.700, 112.400], mean action: 2.109 [0.000, 9.000], mean observation: 36.146 [0.001, 528.400], loss: 187.650925, mae: 36.900486, mean_q: -37.535870\n",
            "  580287/10000000: episode: 2887, duration: 1.606s, episode steps: 201, steps per second: 125, episode reward: 242.200, mean reward: 1.205 [-9.000, 266.000], mean action: 2.164 [0.000, 9.000], mean observation: 30.885 [0.000, 455.800], loss: 165.492706, mae: 37.267349, mean_q: -38.102386\n",
            "  580488/10000000: episode: 2888, duration: 1.704s, episode steps: 201, steps per second: 118, episode reward: -242.000, mean reward: -1.204 [-121.000, 40.600], mean action: 2.284 [0.000, 8.000], mean observation: 29.336 [0.000, 818.500], loss: 189.240707, mae: 37.273350, mean_q: -38.145081\n",
            "  580689/10000000: episode: 2889, duration: 1.660s, episode steps: 201, steps per second: 121, episode reward: -296.600, mean reward: -1.476 [-148.300, 61.500], mean action: 2.199 [0.000, 10.000], mean observation: 33.758 [0.002, 441.900], loss: 177.925064, mae: 37.348301, mean_q: -38.171379\n",
            "  580890/10000000: episode: 2890, duration: 1.658s, episode steps: 201, steps per second: 121, episode reward: -721.600, mean reward: -3.590 [-360.800, 18.000], mean action: 2.144 [0.000, 9.000], mean observation: 29.963 [0.001, 619.000], loss: 184.369843, mae: 37.122295, mean_q: -37.763351\n",
            "  581091/10000000: episode: 2891, duration: 1.637s, episode steps: 201, steps per second: 123, episode reward: 57.000, mean reward: 0.284 [-8.000, 150.000], mean action: 2.403 [0.000, 8.000], mean observation: 31.675 [0.001, 412.100], loss: 229.773849, mae: 36.834343, mean_q: -37.570683\n",
            "  581292/10000000: episode: 2892, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -183.400, mean reward: -0.912 [-91.700, 251.600], mean action: 2.313 [0.000, 9.000], mean observation: 37.014 [0.002, 402.500], loss: 204.606888, mae: 36.758446, mean_q: -37.534081\n",
            "  581493/10000000: episode: 2893, duration: 1.600s, episode steps: 201, steps per second: 126, episode reward: -330.000, mean reward: -1.642 [-165.000, 40.400], mean action: 1.960 [0.000, 10.000], mean observation: 34.622 [0.000, 501.600], loss: 151.057693, mae: 36.490486, mean_q: -36.937710\n",
            "  581694/10000000: episode: 2894, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -141.400, mean reward: -0.703 [-70.700, 108.000], mean action: 1.502 [0.000, 8.000], mean observation: 35.343 [0.001, 492.600], loss: 253.286423, mae: 35.870583, mean_q: -35.883450\n",
            "  581895/10000000: episode: 2895, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -237.200, mean reward: -1.180 [-118.600, 98.400], mean action: 1.557 [0.000, 8.000], mean observation: 32.313 [0.000, 590.900], loss: 157.142548, mae: 35.280064, mean_q: -35.590527\n",
            "  582096/10000000: episode: 2896, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -71.400, mean reward: -0.355 [-35.700, 86.400], mean action: 1.826 [0.000, 8.000], mean observation: 35.749 [0.000, 747.100], loss: 119.350990, mae: 35.348312, mean_q: -35.727631\n",
            "  582297/10000000: episode: 2897, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -443.800, mean reward: -2.208 [-221.900, 27.100], mean action: 1.652 [0.000, 10.000], mean observation: 31.328 [0.000, 462.000], loss: 185.441605, mae: 35.264038, mean_q: -35.622509\n",
            "  582498/10000000: episode: 2898, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -111.200, mean reward: -0.553 [-55.600, 73.600], mean action: 2.209 [0.000, 10.000], mean observation: 29.152 [0.001, 534.800], loss: 211.151703, mae: 35.140732, mean_q: -35.716625\n",
            "  582699/10000000: episode: 2899, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -368.400, mean reward: -1.833 [-184.200, 57.900], mean action: 2.408 [0.000, 10.000], mean observation: 30.112 [0.000, 580.300], loss: 291.842194, mae: 35.081623, mean_q: -35.723255\n",
            "  582900/10000000: episode: 2900, duration: 1.664s, episode steps: 201, steps per second: 121, episode reward: -175.600, mean reward: -0.874 [-87.800, 52.800], mean action: 2.070 [0.000, 9.000], mean observation: 37.921 [0.000, 522.100], loss: 273.724030, mae: 34.811821, mean_q: -35.045910\n",
            "  583101/10000000: episode: 2901, duration: 1.650s, episode steps: 201, steps per second: 122, episode reward: -37.600, mean reward: -0.187 [-18.800, 93.000], mean action: 2.378 [0.000, 10.000], mean observation: 26.679 [0.000, 769.000], loss: 130.347504, mae: 34.569019, mean_q: -35.160210\n",
            "  583302/10000000: episode: 2902, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -214.000, mean reward: -1.065 [-107.000, 121.600], mean action: 2.338 [0.000, 10.000], mean observation: 37.997 [0.000, 813.800], loss: 225.407364, mae: 34.402634, mean_q: -35.033554\n",
            "  583503/10000000: episode: 2903, duration: 1.616s, episode steps: 201, steps per second: 124, episode reward: -823.400, mean reward: -4.097 [-411.700, 20.000], mean action: 2.448 [0.000, 9.000], mean observation: 33.740 [0.001, 632.400], loss: 142.671829, mae: 34.179070, mean_q: -34.786022\n",
            "  583704/10000000: episode: 2904, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -576.600, mean reward: -2.869 [-288.300, 82.200], mean action: 2.587 [0.000, 8.000], mean observation: 33.490 [0.003, 531.400], loss: 164.076065, mae: 34.149616, mean_q: -34.712753\n",
            "  583905/10000000: episode: 2905, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -537.800, mean reward: -2.676 [-268.900, 40.800], mean action: 2.174 [0.000, 9.000], mean observation: 34.679 [0.001, 519.900], loss: 139.368805, mae: 34.316383, mean_q: -34.739502\n",
            "  584106/10000000: episode: 2906, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -340.800, mean reward: -1.696 [-170.400, 48.800], mean action: 2.229 [0.000, 9.000], mean observation: 39.163 [0.001, 518.800], loss: 141.081879, mae: 34.221767, mean_q: -34.617466\n",
            "  584307/10000000: episode: 2907, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: 305.800, mean reward: 1.521 [-8.000, 152.900], mean action: 2.418 [0.000, 8.000], mean observation: 36.566 [0.001, 627.200], loss: 165.121613, mae: 33.979332, mean_q: -34.460335\n",
            "  584508/10000000: episode: 2908, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: 177.600, mean reward: 0.884 [-9.000, 247.600], mean action: 2.169 [0.000, 10.000], mean observation: 37.723 [0.001, 501.700], loss: 169.358154, mae: 33.862930, mean_q: -34.329681\n",
            "  584709/10000000: episode: 2909, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: -225.800, mean reward: -1.123 [-112.900, 77.200], mean action: 1.980 [0.000, 10.000], mean observation: 33.398 [0.000, 604.500], loss: 203.958954, mae: 33.734882, mean_q: -33.999237\n",
            "  584910/10000000: episode: 2910, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: -298.800, mean reward: -1.487 [-149.400, 71.600], mean action: 1.682 [0.000, 10.000], mean observation: 33.147 [0.001, 500.600], loss: 168.699234, mae: 33.489990, mean_q: -33.520916\n",
            "  585111/10000000: episode: 2911, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -323.000, mean reward: -1.607 [-161.500, 56.800], mean action: 1.766 [0.000, 10.000], mean observation: 30.878 [0.001, 464.600], loss: 176.073502, mae: 33.224499, mean_q: -33.071613\n",
            "  585312/10000000: episode: 2912, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: -426.600, mean reward: -2.122 [-213.300, 27.600], mean action: 1.975 [0.000, 9.000], mean observation: 31.755 [0.001, 531.100], loss: 126.637360, mae: 32.679230, mean_q: -32.841820\n",
            "  585513/10000000: episode: 2913, duration: 1.621s, episode steps: 201, steps per second: 124, episode reward: -169.000, mean reward: -0.841 [-84.500, 94.800], mean action: 1.871 [0.000, 9.000], mean observation: 34.654 [0.002, 439.400], loss: 121.007874, mae: 32.741272, mean_q: -32.927818\n",
            "  585714/10000000: episode: 2914, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -386.600, mean reward: -1.923 [-193.300, 32.700], mean action: 1.726 [0.000, 8.000], mean observation: 35.233 [0.000, 784.200], loss: 93.381699, mae: 32.461845, mean_q: -32.573753\n",
            "  585915/10000000: episode: 2915, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -441.000, mean reward: -2.194 [-220.500, 52.500], mean action: 1.771 [0.000, 8.000], mean observation: 38.621 [0.000, 722.600], loss: 177.995377, mae: 31.819595, mean_q: -31.706039\n",
            "  586116/10000000: episode: 2916, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -554.200, mean reward: -2.757 [-277.100, 34.300], mean action: 1.925 [0.000, 10.000], mean observation: 30.650 [0.001, 542.200], loss: 150.621582, mae: 32.059010, mean_q: -31.954899\n",
            "  586317/10000000: episode: 2917, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -327.200, mean reward: -1.628 [-163.600, 37.200], mean action: 1.532 [0.000, 9.000], mean observation: 37.533 [0.001, 527.300], loss: 108.616585, mae: 32.075047, mean_q: -31.964872\n",
            "  586518/10000000: episode: 2918, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -533.200, mean reward: -2.653 [-266.600, 51.500], mean action: 2.134 [0.000, 10.000], mean observation: 26.884 [0.000, 529.100], loss: 254.312698, mae: 31.902470, mean_q: -31.932501\n",
            "  586719/10000000: episode: 2919, duration: 1.602s, episode steps: 201, steps per second: 125, episode reward: -723.800, mean reward: -3.601 [-361.900, 28.800], mean action: 2.443 [0.000, 10.000], mean observation: 33.016 [0.000, 694.400], loss: 106.103386, mae: 31.993271, mean_q: -32.260105\n",
            "  586920/10000000: episode: 2920, duration: 1.656s, episode steps: 201, steps per second: 121, episode reward: -641.000, mean reward: -3.189 [-320.500, 10.000], mean action: 1.856 [0.000, 10.000], mean observation: 39.157 [0.001, 598.300], loss: 116.400139, mae: 32.345364, mean_q: -32.460537\n",
            "  587121/10000000: episode: 2921, duration: 1.767s, episode steps: 201, steps per second: 114, episode reward: -431.000, mean reward: -2.144 [-215.500, 44.400], mean action: 1.736 [0.000, 9.000], mean observation: 34.761 [0.000, 555.700], loss: 296.173035, mae: 32.468437, mean_q: -32.335712\n",
            "  587322/10000000: episode: 2922, duration: 1.761s, episode steps: 201, steps per second: 114, episode reward: -330.000, mean reward: -1.642 [-165.000, 122.500], mean action: 1.896 [0.000, 10.000], mean observation: 28.116 [0.003, 492.800], loss: 161.549469, mae: 31.927589, mean_q: -31.589390\n",
            "  587523/10000000: episode: 2923, duration: 1.729s, episode steps: 201, steps per second: 116, episode reward: -504.400, mean reward: -2.509 [-252.200, 33.600], mean action: 1.945 [0.000, 8.000], mean observation: 38.100 [0.000, 669.900], loss: 173.724487, mae: 31.555868, mean_q: -31.304697\n",
            "  587724/10000000: episode: 2924, duration: 1.706s, episode steps: 201, steps per second: 118, episode reward: -262.600, mean reward: -1.306 [-131.300, 86.800], mean action: 2.199 [0.000, 10.000], mean observation: 37.683 [0.001, 662.900], loss: 186.964859, mae: 31.976610, mean_q: -31.866196\n",
            "  587925/10000000: episode: 2925, duration: 1.739s, episode steps: 201, steps per second: 116, episode reward: -57.000, mean reward: -0.284 [-28.500, 88.400], mean action: 2.119 [0.000, 10.000], mean observation: 33.881 [0.002, 482.600], loss: 206.607239, mae: 31.940439, mean_q: -31.762035\n",
            "  588126/10000000: episode: 2926, duration: 1.746s, episode steps: 201, steps per second: 115, episode reward: 223.000, mean reward: 1.109 [-8.000, 139.200], mean action: 1.721 [0.000, 8.000], mean observation: 39.848 [0.000, 654.700], loss: 154.699585, mae: 31.618767, mean_q: -31.477253\n",
            "  588327/10000000: episode: 2927, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -347.800, mean reward: -1.730 [-173.900, 108.600], mean action: 2.080 [0.000, 10.000], mean observation: 32.768 [0.000, 706.400], loss: 111.781136, mae: 31.150434, mean_q: -31.215687\n",
            "  588528/10000000: episode: 2928, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -125.400, mean reward: -0.624 [-62.700, 175.800], mean action: 2.055 [0.000, 10.000], mean observation: 32.842 [0.001, 480.700], loss: 171.022003, mae: 30.993713, mean_q: -30.958586\n",
            "  588729/10000000: episode: 2929, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: 22.000, mean reward: 0.109 [-10.000, 148.500], mean action: 1.856 [0.000, 10.000], mean observation: 34.837 [0.001, 422.500], loss: 120.475822, mae: 31.033508, mean_q: -30.810431\n",
            "  588930/10000000: episode: 2930, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -500.000, mean reward: -2.488 [-250.000, 17.600], mean action: 1.607 [0.000, 10.000], mean observation: 31.401 [0.001, 676.900], loss: 199.446259, mae: 30.621433, mean_q: -30.131035\n",
            "  589131/10000000: episode: 2931, duration: 1.616s, episode steps: 201, steps per second: 124, episode reward: -272.000, mean reward: -1.353 [-136.000, 67.800], mean action: 1.891 [0.000, 10.000], mean observation: 27.011 [0.005, 497.200], loss: 217.548935, mae: 30.499727, mean_q: -29.982038\n",
            "  589332/10000000: episode: 2932, duration: 1.611s, episode steps: 201, steps per second: 125, episode reward: 105.400, mean reward: 0.524 [-8.000, 159.200], mean action: 1.831 [0.000, 8.000], mean observation: 43.487 [0.000, 714.300], loss: 183.549744, mae: 30.061367, mean_q: -29.738398\n",
            "  589533/10000000: episode: 2933, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -470.600, mean reward: -2.341 [-235.300, 24.500], mean action: 1.677 [0.000, 10.000], mean observation: 33.595 [0.000, 582.800], loss: 198.586716, mae: 30.581491, mean_q: -30.312435\n",
            "  589734/10000000: episode: 2934, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: 202.200, mean reward: 1.006 [-8.000, 132.400], mean action: 1.935 [0.000, 8.000], mean observation: 34.050 [0.000, 566.700], loss: 134.862579, mae: 30.966105, mean_q: -30.877861\n",
            "  589935/10000000: episode: 2935, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -135.600, mean reward: -0.675 [-67.800, 44.000], mean action: 1.866 [0.000, 10.000], mean observation: 30.305 [0.001, 498.800], loss: 147.389160, mae: 31.229090, mean_q: -30.900978\n",
            "  590136/10000000: episode: 2936, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -128.400, mean reward: -0.639 [-64.200, 39.000], mean action: 1.806 [0.000, 10.000], mean observation: 31.691 [0.002, 508.500], loss: 192.136230, mae: 30.893162, mean_q: -30.562145\n",
            "  590337/10000000: episode: 2937, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: 233.400, mean reward: 1.161 [-10.000, 229.200], mean action: 1.557 [0.000, 10.000], mean observation: 31.888 [0.000, 413.800], loss: 136.197998, mae: 30.278584, mean_q: -29.930897\n",
            "  590538/10000000: episode: 2938, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -634.400, mean reward: -3.156 [-317.200, 18.000], mean action: 1.876 [0.000, 9.000], mean observation: 37.558 [0.000, 451.700], loss: 247.057831, mae: 29.723618, mean_q: -29.545174\n",
            "  590739/10000000: episode: 2939, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -594.400, mean reward: -2.957 [-297.200, 23.200], mean action: 1.925 [0.000, 8.000], mean observation: 33.822 [0.001, 630.500], loss: 178.037735, mae: 29.492723, mean_q: -29.190998\n",
            "  590940/10000000: episode: 2940, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: 198.800, mean reward: 0.989 [-8.000, 132.100], mean action: 1.905 [0.000, 8.000], mean observation: 32.658 [0.001, 527.300], loss: 134.548050, mae: 29.225939, mean_q: -28.958982\n",
            "  591141/10000000: episode: 2941, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 288.800, mean reward: 1.437 [-10.000, 333.300], mean action: 1.791 [0.000, 10.000], mean observation: 34.504 [0.000, 562.200], loss: 160.637283, mae: 29.354414, mean_q: -29.011301\n",
            "  591342/10000000: episode: 2942, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: 151.000, mean reward: 0.751 [-10.000, 269.200], mean action: 2.204 [0.000, 10.000], mean observation: 33.091 [0.002, 637.200], loss: 193.168961, mae: 28.875326, mean_q: -28.729429\n",
            "  591543/10000000: episode: 2943, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -535.400, mean reward: -2.664 [-267.700, 30.000], mean action: 2.060 [0.000, 8.000], mean observation: 26.017 [0.003, 436.400], loss: 159.551620, mae: 29.254499, mean_q: -29.131739\n",
            "  591744/10000000: episode: 2944, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: 381.600, mean reward: 1.899 [-10.000, 192.000], mean action: 2.333 [0.000, 10.000], mean observation: 31.377 [0.001, 607.700], loss: 137.429733, mae: 28.779322, mean_q: -29.155075\n",
            "  591945/10000000: episode: 2945, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -372.600, mean reward: -1.854 [-186.300, 100.200], mean action: 2.080 [0.000, 10.000], mean observation: 32.914 [0.001, 462.800], loss: 229.251617, mae: 29.079569, mean_q: -29.100132\n",
            "  592146/10000000: episode: 2946, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -375.200, mean reward: -1.867 [-187.600, 84.000], mean action: 2.159 [0.000, 10.000], mean observation: 38.409 [0.003, 524.500], loss: 189.122513, mae: 28.796919, mean_q: -28.816271\n",
            "  592347/10000000: episode: 2947, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -365.400, mean reward: -1.818 [-182.700, 52.500], mean action: 2.607 [0.000, 10.000], mean observation: 28.525 [0.000, 772.000], loss: 220.618301, mae: 29.111687, mean_q: -29.292372\n",
            "  592548/10000000: episode: 2948, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -594.200, mean reward: -2.956 [-297.100, 46.900], mean action: 2.821 [0.000, 10.000], mean observation: 32.081 [0.000, 558.800], loss: 140.492676, mae: 29.397543, mean_q: -29.937859\n",
            "  592749/10000000: episode: 2949, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -506.400, mean reward: -2.519 [-253.200, 84.000], mean action: 2.333 [0.000, 10.000], mean observation: 33.899 [0.001, 501.100], loss: 130.426117, mae: 29.816921, mean_q: -30.173285\n",
            "  592950/10000000: episode: 2950, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -567.800, mean reward: -2.825 [-283.900, 37.600], mean action: 2.313 [0.000, 10.000], mean observation: 31.390 [0.001, 645.600], loss: 146.428925, mae: 30.095320, mean_q: -30.195265\n",
            "  593151/10000000: episode: 2951, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -203.600, mean reward: -1.013 [-101.800, 69.300], mean action: 2.756 [0.000, 10.000], mean observation: 33.906 [0.001, 516.000], loss: 89.455635, mae: 29.955914, mean_q: -30.094328\n",
            "  593352/10000000: episode: 2952, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: 669.400, mean reward: 3.330 [-10.000, 334.700], mean action: 2.532 [0.000, 10.000], mean observation: 35.230 [0.000, 555.400], loss: 174.651001, mae: 30.117420, mean_q: -30.535589\n",
            "  593553/10000000: episode: 2953, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -761.200, mean reward: -3.787 [-380.600, 68.800], mean action: 2.816 [0.000, 10.000], mean observation: 33.206 [0.000, 553.100], loss: 112.991646, mae: 30.497780, mean_q: -31.208008\n",
            "  593754/10000000: episode: 2954, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -273.000, mean reward: -1.358 [-136.500, 130.400], mean action: 2.871 [0.000, 10.000], mean observation: 37.385 [0.000, 668.100], loss: 154.437729, mae: 31.206575, mean_q: -31.988132\n",
            "  593955/10000000: episode: 2955, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -892.200, mean reward: -4.439 [-446.100, 15.000], mean action: 2.786 [0.000, 10.000], mean observation: 30.570 [0.000, 483.800], loss: 160.816208, mae: 31.644308, mean_q: -32.200230\n",
            "  594156/10000000: episode: 2956, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: 138.800, mean reward: 0.691 [-10.000, 261.600], mean action: 2.950 [0.000, 10.000], mean observation: 28.247 [0.002, 513.300], loss: 131.755157, mae: 32.253872, mean_q: -32.945053\n",
            "  594357/10000000: episode: 2957, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -290.200, mean reward: -1.444 [-145.100, 85.200], mean action: 2.313 [0.000, 10.000], mean observation: 35.819 [0.000, 606.600], loss: 225.202393, mae: 32.545918, mean_q: -32.925320\n",
            "  594558/10000000: episode: 2958, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -645.400, mean reward: -3.211 [-322.700, 23.400], mean action: 1.965 [0.000, 10.000], mean observation: 38.092 [0.001, 493.400], loss: 162.269226, mae: 32.627045, mean_q: -32.737484\n",
            "  594759/10000000: episode: 2959, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -537.400, mean reward: -2.674 [-268.700, 43.800], mean action: 2.104 [0.000, 10.000], mean observation: 33.270 [0.002, 578.200], loss: 224.957367, mae: 32.502136, mean_q: -32.760239\n",
            "  594960/10000000: episode: 2960, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -361.400, mean reward: -1.798 [-180.700, 42.800], mean action: 1.980 [0.000, 10.000], mean observation: 31.853 [0.001, 478.600], loss: 145.626068, mae: 32.314068, mean_q: -32.442486\n",
            "  595161/10000000: episode: 2961, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -223.400, mean reward: -1.111 [-111.700, 51.000], mean action: 2.090 [0.000, 10.000], mean observation: 36.818 [0.000, 527.200], loss: 160.836563, mae: 31.927418, mean_q: -31.796162\n",
            "  595362/10000000: episode: 2962, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: 180.000, mean reward: 0.896 [-10.000, 132.400], mean action: 2.259 [0.000, 10.000], mean observation: 32.441 [0.001, 675.300], loss: 130.334961, mae: 31.672438, mean_q: -31.559235\n",
            "  595563/10000000: episode: 2963, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -515.800, mean reward: -2.566 [-257.900, 78.400], mean action: 2.294 [0.000, 10.000], mean observation: 30.186 [0.004, 432.400], loss: 219.579819, mae: 31.328226, mean_q: -31.147627\n",
            "  595764/10000000: episode: 2964, duration: 1.632s, episode steps: 201, steps per second: 123, episode reward: 622.200, mean reward: 3.096 [-8.000, 485.100], mean action: 2.144 [0.000, 10.000], mean observation: 31.844 [0.000, 527.500], loss: 176.469406, mae: 31.375534, mean_q: -31.280655\n",
            "  595965/10000000: episode: 2965, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: 666.600, mean reward: 3.316 [-10.000, 333.300], mean action: 2.149 [0.000, 10.000], mean observation: 35.109 [0.000, 544.700], loss: 145.174789, mae: 31.469099, mean_q: -31.327795\n",
            "  596166/10000000: episode: 2966, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -287.200, mean reward: -1.429 [-143.600, 56.000], mean action: 2.119 [0.000, 8.000], mean observation: 36.870 [0.001, 532.900], loss: 178.772095, mae: 31.422739, mean_q: -31.464169\n",
            "  596367/10000000: episode: 2967, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -278.400, mean reward: -1.385 [-139.200, 78.900], mean action: 2.279 [0.000, 10.000], mean observation: 34.454 [0.001, 647.400], loss: 108.410683, mae: 31.501579, mean_q: -31.582294\n",
            "  596568/10000000: episode: 2968, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -458.400, mean reward: -2.281 [-229.200, 108.800], mean action: 2.348 [0.000, 10.000], mean observation: 32.345 [0.001, 602.400], loss: 180.929626, mae: 31.523079, mean_q: -31.634310\n",
            "  596769/10000000: episode: 2969, duration: 1.609s, episode steps: 201, steps per second: 125, episode reward: -535.400, mean reward: -2.664 [-267.700, 38.400], mean action: 1.990 [0.000, 10.000], mean observation: 33.582 [0.000, 453.500], loss: 183.113663, mae: 31.375576, mean_q: -31.591738\n",
            "  596970/10000000: episode: 2970, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -485.600, mean reward: -2.416 [-242.800, 71.700], mean action: 2.527 [0.000, 10.000], mean observation: 29.218 [0.002, 364.300], loss: 104.267197, mae: 31.133930, mean_q: -31.439983\n",
            "  597171/10000000: episode: 2971, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: 526.400, mean reward: 2.619 [-10.000, 263.200], mean action: 2.239 [0.000, 10.000], mean observation: 27.991 [0.001, 498.300], loss: 126.294746, mae: 31.421005, mean_q: -31.707979\n",
            "  597372/10000000: episode: 2972, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: -357.000, mean reward: -1.776 [-178.500, 49.200], mean action: 2.244 [0.000, 10.000], mean observation: 35.555 [0.000, 695.400], loss: 159.863586, mae: 31.476307, mean_q: -31.745359\n",
            "  597573/10000000: episode: 2973, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: 601.400, mean reward: 2.992 [-10.000, 318.000], mean action: 2.403 [0.000, 10.000], mean observation: 34.082 [0.000, 525.900], loss: 173.510773, mae: 31.806675, mean_q: -32.053932\n",
            "  597774/10000000: episode: 2974, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -251.600, mean reward: -1.252 [-125.800, 118.400], mean action: 2.851 [0.000, 10.000], mean observation: 35.004 [0.001, 446.900], loss: 128.356628, mae: 31.955879, mean_q: -32.223022\n",
            "  597975/10000000: episode: 2975, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -140.000, mean reward: -0.697 [-70.000, 240.000], mean action: 2.607 [0.000, 10.000], mean observation: 32.187 [0.000, 386.400], loss: 155.018768, mae: 31.707973, mean_q: -32.017597\n",
            "  598176/10000000: episode: 2976, duration: 1.600s, episode steps: 201, steps per second: 126, episode reward: -515.800, mean reward: -2.566 [-257.900, 92.100], mean action: 3.035 [0.000, 10.000], mean observation: 30.993 [0.001, 654.600], loss: 175.400925, mae: 31.669024, mean_q: -32.299999\n",
            "  598377/10000000: episode: 2977, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: 164.400, mean reward: 0.818 [-10.000, 336.000], mean action: 3.020 [0.000, 10.000], mean observation: 35.852 [0.000, 467.400], loss: 182.656631, mae: 31.719294, mean_q: -32.319126\n",
            "  598578/10000000: episode: 2978, duration: 1.614s, episode steps: 201, steps per second: 125, episode reward: -240.800, mean reward: -1.198 [-120.400, 153.600], mean action: 2.826 [0.000, 10.000], mean observation: 36.385 [0.000, 697.600], loss: 142.678635, mae: 31.832483, mean_q: -32.350704\n",
            "  598779/10000000: episode: 2979, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -382.200, mean reward: -1.901 [-191.100, 124.400], mean action: 2.711 [0.000, 10.000], mean observation: 31.134 [0.000, 499.800], loss: 193.463791, mae: 31.434879, mean_q: -31.855133\n",
            "  598980/10000000: episode: 2980, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: -550.000, mean reward: -2.736 [-275.000, 71.200], mean action: 2.886 [0.000, 10.000], mean observation: 31.027 [0.001, 443.500], loss: 195.311646, mae: 31.004528, mean_q: -31.670864\n",
            "  599181/10000000: episode: 2981, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -319.800, mean reward: -1.591 [-159.900, 150.500], mean action: 2.622 [0.000, 10.000], mean observation: 34.497 [0.001, 470.800], loss: 131.457352, mae: 31.083662, mean_q: -31.777912\n",
            "  599382/10000000: episode: 2982, duration: 1.617s, episode steps: 201, steps per second: 124, episode reward: -44.000, mean reward: -0.219 [-22.000, 232.000], mean action: 2.647 [0.000, 10.000], mean observation: 29.525 [0.002, 427.500], loss: 167.737000, mae: 31.400576, mean_q: -32.245785\n",
            "  599583/10000000: episode: 2983, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -498.400, mean reward: -2.480 [-249.200, 83.200], mean action: 2.662 [0.000, 10.000], mean observation: 32.066 [0.002, 515.900], loss: 167.463867, mae: 31.224905, mean_q: -31.841204\n",
            "  599784/10000000: episode: 2984, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -166.400, mean reward: -0.828 [-83.200, 108.600], mean action: 2.632 [0.000, 10.000], mean observation: 35.383 [0.002, 467.700], loss: 132.651428, mae: 31.484119, mean_q: -32.175503\n",
            "  599985/10000000: episode: 2985, duration: 1.664s, episode steps: 201, steps per second: 121, episode reward: -527.200, mean reward: -2.623 [-263.600, 53.000], mean action: 3.269 [0.000, 10.000], mean observation: 27.954 [0.002, 434.600], loss: 178.706390, mae: 31.903776, mean_q: -32.725071\n",
            "  600186/10000000: episode: 2986, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -583.000, mean reward: -2.900 [-291.500, 61.200], mean action: 2.905 [0.000, 10.000], mean observation: 32.314 [0.000, 638.600], loss: 179.996582, mae: 32.406979, mean_q: -33.323627\n",
            "  600387/10000000: episode: 2987, duration: 1.605s, episode steps: 201, steps per second: 125, episode reward: -340.000, mean reward: -1.692 [-170.000, 103.200], mean action: 2.990 [0.000, 10.000], mean observation: 33.986 [0.000, 407.100], loss: 173.401794, mae: 32.544109, mean_q: -33.615891\n",
            "  600588/10000000: episode: 2988, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -214.200, mean reward: -1.066 [-107.100, 274.400], mean action: 3.378 [0.000, 10.000], mean observation: 34.702 [0.000, 530.500], loss: 128.387589, mae: 32.982777, mean_q: -33.922710\n",
            "  600789/10000000: episode: 2989, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: 97.200, mean reward: 0.484 [-10.000, 104.400], mean action: 2.507 [0.000, 10.000], mean observation: 34.989 [0.000, 796.200], loss: 115.278702, mae: 33.256149, mean_q: -34.218117\n",
            "  600990/10000000: episode: 2990, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -205.400, mean reward: -1.022 [-102.700, 392.000], mean action: 2.846 [0.000, 10.000], mean observation: 36.682 [0.000, 588.400], loss: 128.972595, mae: 32.935783, mean_q: -34.113537\n",
            "  601191/10000000: episode: 2991, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -165.200, mean reward: -0.822 [-82.600, 296.000], mean action: 3.199 [0.000, 10.000], mean observation: 31.498 [0.001, 500.200], loss: 167.209900, mae: 33.217522, mean_q: -34.033684\n",
            "  601392/10000000: episode: 2992, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -530.400, mean reward: -2.639 [-265.200, 147.400], mean action: 3.055 [0.000, 10.000], mean observation: 33.673 [0.003, 438.400], loss: 193.848770, mae: 32.866791, mean_q: -34.048538\n",
            "  601593/10000000: episode: 2993, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -81.200, mean reward: -0.404 [-40.600, 222.000], mean action: 3.478 [0.000, 10.000], mean observation: 28.500 [0.002, 545.200], loss: 182.352219, mae: 33.358074, mean_q: -34.526649\n",
            "  601794/10000000: episode: 2994, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -497.200, mean reward: -2.474 [-248.600, 91.200], mean action: 3.582 [0.000, 10.000], mean observation: 34.248 [0.001, 491.000], loss: 125.068466, mae: 33.227722, mean_q: -33.984802\n",
            "  601995/10000000: episode: 2995, duration: 1.652s, episode steps: 201, steps per second: 122, episode reward: -107.000, mean reward: -0.532 [-53.500, 201.900], mean action: 3.194 [0.000, 10.000], mean observation: 30.836 [0.000, 452.700], loss: 159.406891, mae: 33.005650, mean_q: -33.708126\n",
            "  602196/10000000: episode: 2996, duration: 1.602s, episode steps: 201, steps per second: 125, episode reward: -1118.800, mean reward: -5.566 [-559.400, 36.800], mean action: 3.323 [0.000, 10.000], mean observation: 35.122 [0.002, 481.400], loss: 165.596069, mae: 32.458839, mean_q: -33.398987\n",
            "  602397/10000000: episode: 2997, duration: 1.651s, episode steps: 201, steps per second: 122, episode reward: -816.200, mean reward: -4.061 [-408.100, 102.000], mean action: 3.488 [0.000, 10.000], mean observation: 28.152 [0.000, 746.900], loss: 139.140793, mae: 32.399525, mean_q: -33.553875\n",
            "  602598/10000000: episode: 2998, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -621.000, mean reward: -3.090 [-310.500, 100.800], mean action: 3.214 [0.000, 10.000], mean observation: 32.671 [0.000, 609.400], loss: 197.953171, mae: 32.274433, mean_q: -33.567299\n",
            "  602799/10000000: episode: 2999, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -611.200, mean reward: -3.041 [-305.600, 307.200], mean action: 3.602 [0.000, 10.000], mean observation: 31.152 [0.000, 702.100], loss: 270.001404, mae: 32.233356, mean_q: -33.568951\n",
            "  603000/10000000: episode: 3000, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -743.200, mean reward: -3.698 [-371.600, 81.600], mean action: 3.647 [0.000, 10.000], mean observation: 34.980 [0.000, 472.400], loss: 114.812050, mae: 32.925251, mean_q: -33.965092\n",
            "  603201/10000000: episode: 3001, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: 316.400, mean reward: 1.574 [-10.000, 340.800], mean action: 2.876 [0.000, 10.000], mean observation: 38.236 [0.000, 653.600], loss: 104.388664, mae: 33.261326, mean_q: -33.989223\n",
            "  603402/10000000: episode: 3002, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -25.800, mean reward: -0.128 [-12.900, 55.800], mean action: 2.443 [0.000, 10.000], mean observation: 27.202 [0.000, 509.800], loss: 244.481842, mae: 33.895119, mean_q: -34.248768\n",
            "  603603/10000000: episode: 3003, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: 0.400, mean reward: 0.002 [-10.000, 85.200], mean action: 1.846 [0.000, 10.000], mean observation: 39.108 [0.002, 630.900], loss: 158.840683, mae: 34.302734, mean_q: -34.385845\n",
            "  603804/10000000: episode: 3004, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: 496.000, mean reward: 2.468 [-10.000, 252.900], mean action: 2.204 [0.000, 10.000], mean observation: 33.827 [0.001, 571.000], loss: 187.853485, mae: 34.117477, mean_q: -34.412582\n",
            "  604005/10000000: episode: 3005, duration: 1.606s, episode steps: 201, steps per second: 125, episode reward: -550.000, mean reward: -2.736 [-275.000, 31.600], mean action: 2.438 [0.000, 10.000], mean observation: 36.329 [0.001, 591.000], loss: 182.535706, mae: 33.999969, mean_q: -34.117329\n",
            "  604206/10000000: episode: 3006, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -70.800, mean reward: -0.352 [-35.400, 61.200], mean action: 2.363 [0.000, 10.000], mean observation: 30.946 [0.001, 595.600], loss: 132.734528, mae: 34.176826, mean_q: -34.805759\n",
            "  604407/10000000: episode: 3007, duration: 1.626s, episode steps: 201, steps per second: 124, episode reward: -275.400, mean reward: -1.370 [-137.700, 144.000], mean action: 2.796 [0.000, 10.000], mean observation: 34.379 [0.000, 530.000], loss: 189.073502, mae: 34.341293, mean_q: -35.227116\n",
            "  604608/10000000: episode: 3008, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: 35.000, mean reward: 0.174 [-10.000, 145.200], mean action: 2.915 [0.000, 10.000], mean observation: 31.292 [0.001, 453.300], loss: 251.187973, mae: 34.205719, mean_q: -35.001015\n",
            "  604809/10000000: episode: 3009, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -373.200, mean reward: -1.857 [-186.600, 162.400], mean action: 2.522 [0.000, 10.000], mean observation: 31.412 [0.006, 504.200], loss: 146.870804, mae: 33.953251, mean_q: -34.901638\n",
            "  605010/10000000: episode: 3010, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: -77.000, mean reward: -0.383 [-38.500, 136.000], mean action: 3.199 [0.000, 10.000], mean observation: 31.372 [0.001, 591.100], loss: 155.257599, mae: 33.960365, mean_q: -35.340538\n",
            "  605211/10000000: episode: 3011, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -412.600, mean reward: -2.053 [-206.300, 154.800], mean action: 3.095 [0.000, 10.000], mean observation: 31.878 [0.001, 578.100], loss: 162.515686, mae: 34.412201, mean_q: -35.771835\n",
            "  605412/10000000: episode: 3012, duration: 1.611s, episode steps: 201, steps per second: 125, episode reward: -601.000, mean reward: -2.990 [-300.500, 131.600], mean action: 2.234 [0.000, 10.000], mean observation: 36.214 [0.001, 598.700], loss: 302.019501, mae: 34.415997, mean_q: -34.853893\n",
            "  605613/10000000: episode: 3013, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 241.600, mean reward: 1.202 [-10.000, 120.800], mean action: 1.781 [0.000, 10.000], mean observation: 28.778 [0.003, 308.400], loss: 109.110168, mae: 34.019779, mean_q: -34.058014\n",
            "  605814/10000000: episode: 3014, duration: 1.600s, episode steps: 201, steps per second: 126, episode reward: 295.600, mean reward: 1.471 [-10.000, 246.400], mean action: 1.821 [0.000, 10.000], mean observation: 32.672 [0.000, 540.000], loss: 150.914917, mae: 33.939178, mean_q: -33.961872\n",
            "  606015/10000000: episode: 3015, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: 66.800, mean reward: 0.332 [-10.000, 140.000], mean action: 2.433 [0.000, 10.000], mean observation: 30.297 [0.000, 663.800], loss: 231.728485, mae: 33.827621, mean_q: -33.918339\n",
            "  606216/10000000: episode: 3016, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -617.000, mean reward: -3.070 [-308.500, 64.000], mean action: 2.970 [0.000, 10.000], mean observation: 30.420 [0.002, 477.000], loss: 155.575302, mae: 33.221931, mean_q: -33.744549\n",
            "  606417/10000000: episode: 3017, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -512.800, mean reward: -2.551 [-256.400, 69.600], mean action: 2.791 [0.000, 10.000], mean observation: 34.567 [0.000, 639.500], loss: 136.983826, mae: 33.433002, mean_q: -33.685932\n",
            "  606618/10000000: episode: 3018, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -845.000, mean reward: -4.204 [-422.500, 80.000], mean action: 2.866 [0.000, 10.000], mean observation: 37.147 [0.001, 619.200], loss: 155.732590, mae: 33.114525, mean_q: -33.955265\n",
            "  606819/10000000: episode: 3019, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -621.200, mean reward: -3.091 [-310.600, 70.700], mean action: 2.597 [0.000, 10.000], mean observation: 29.333 [0.001, 651.100], loss: 164.473099, mae: 33.668003, mean_q: -34.115520\n",
            "  607020/10000000: episode: 3020, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -339.000, mean reward: -1.687 [-169.500, 44.800], mean action: 2.597 [0.000, 10.000], mean observation: 38.333 [0.000, 818.100], loss: 211.783829, mae: 33.666367, mean_q: -33.968410\n",
            "  607221/10000000: episode: 3021, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -314.000, mean reward: -1.562 [-157.000, 100.800], mean action: 2.159 [0.000, 10.000], mean observation: 32.575 [0.002, 429.000], loss: 200.438049, mae: 33.386681, mean_q: -33.384655\n",
            "  607422/10000000: episode: 3022, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: -494.600, mean reward: -2.461 [-247.300, 120.800], mean action: 2.871 [0.000, 10.000], mean observation: 33.680 [0.001, 565.900], loss: 154.271255, mae: 32.627327, mean_q: -32.792591\n",
            "  607623/10000000: episode: 3023, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -585.200, mean reward: -2.911 [-292.600, 58.400], mean action: 2.254 [0.000, 10.000], mean observation: 32.971 [0.002, 446.700], loss: 233.790634, mae: 32.286114, mean_q: -32.258305\n",
            "  607824/10000000: episode: 3024, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -822.800, mean reward: -4.094 [-411.400, 39.300], mean action: 2.507 [0.000, 10.000], mean observation: 32.384 [0.000, 720.900], loss: 217.165787, mae: 32.531967, mean_q: -33.089573\n",
            "  608025/10000000: episode: 3025, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -161.200, mean reward: -0.802 [-80.600, 214.200], mean action: 2.672 [0.000, 10.000], mean observation: 31.280 [0.003, 624.500], loss: 136.277710, mae: 32.951904, mean_q: -33.863323\n",
            "  608226/10000000: episode: 3026, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -240.400, mean reward: -1.196 [-120.200, 94.500], mean action: 2.274 [0.000, 10.000], mean observation: 36.196 [0.002, 503.100], loss: 174.443024, mae: 33.247795, mean_q: -33.855343\n",
            "  608427/10000000: episode: 3027, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -597.400, mean reward: -2.972 [-298.700, 57.200], mean action: 2.358 [0.000, 10.000], mean observation: 24.820 [0.001, 462.800], loss: 167.276993, mae: 33.435284, mean_q: -33.769413\n",
            "  608628/10000000: episode: 3028, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -84.800, mean reward: -0.422 [-42.400, 115.300], mean action: 2.318 [0.000, 10.000], mean observation: 38.137 [0.000, 476.600], loss: 183.440292, mae: 32.686565, mean_q: -33.440205\n",
            "  608829/10000000: episode: 3029, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -378.200, mean reward: -1.882 [-189.100, 74.000], mean action: 2.726 [0.000, 10.000], mean observation: 36.567 [0.000, 642.700], loss: 181.803955, mae: 33.280972, mean_q: -34.128059\n",
            "  609030/10000000: episode: 3030, duration: 1.630s, episode steps: 201, steps per second: 123, episode reward: -210.200, mean reward: -1.046 [-105.100, 126.900], mean action: 2.662 [0.000, 10.000], mean observation: 35.306 [0.000, 907.100], loss: 112.901131, mae: 33.698387, mean_q: -34.336411\n",
            "  609231/10000000: episode: 3031, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: -518.400, mean reward: -2.579 [-259.200, 98.500], mean action: 3.065 [0.000, 10.000], mean observation: 31.597 [0.004, 399.700], loss: 180.939865, mae: 33.894020, mean_q: -34.570599\n",
            "  609432/10000000: episode: 3032, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: -572.000, mean reward: -2.846 [-286.000, 170.400], mean action: 3.488 [0.000, 10.000], mean observation: 35.402 [0.001, 522.800], loss: 183.757858, mae: 33.505566, mean_q: -34.188129\n",
            "  609633/10000000: episode: 3033, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: -235.200, mean reward: -1.170 [-117.600, 209.600], mean action: 3.313 [0.000, 10.000], mean observation: 29.195 [0.000, 476.000], loss: 170.453247, mae: 33.132065, mean_q: -33.773468\n",
            "  609834/10000000: episode: 3034, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: 1082.800, mean reward: 5.387 [-10.000, 541.400], mean action: 3.637 [0.000, 10.000], mean observation: 31.182 [0.001, 623.900], loss: 204.892166, mae: 33.304810, mean_q: -33.755985\n",
            "  610035/10000000: episode: 3035, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -574.800, mean reward: -2.860 [-287.400, 64.000], mean action: 2.647 [0.000, 8.000], mean observation: 31.259 [0.002, 542.000], loss: 151.878998, mae: 33.432098, mean_q: -33.966080\n",
            "  610236/10000000: episode: 3036, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: 347.600, mean reward: 1.729 [-10.000, 173.800], mean action: 2.607 [0.000, 10.000], mean observation: 39.180 [0.000, 515.500], loss: 201.892990, mae: 33.692387, mean_q: -34.112499\n",
            "  610437/10000000: episode: 3037, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -75.800, mean reward: -0.377 [-37.900, 116.800], mean action: 3.020 [0.000, 10.000], mean observation: 27.335 [0.000, 781.200], loss: 164.953354, mae: 34.424870, mean_q: -34.975693\n",
            "  610638/10000000: episode: 3038, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -468.800, mean reward: -2.332 [-234.400, 66.800], mean action: 2.493 [0.000, 10.000], mean observation: 32.125 [0.001, 492.700], loss: 151.592957, mae: 34.356964, mean_q: -34.809040\n",
            "  610839/10000000: episode: 3039, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -301.200, mean reward: -1.499 [-150.600, 80.800], mean action: 2.313 [0.000, 10.000], mean observation: 35.706 [0.002, 467.200], loss: 165.921524, mae: 34.211174, mean_q: -34.636902\n",
            "  611040/10000000: episode: 3040, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -19.400, mean reward: -0.097 [-10.000, 94.400], mean action: 2.687 [0.000, 10.000], mean observation: 33.847 [0.000, 511.400], loss: 184.722748, mae: 33.969929, mean_q: -34.264465\n",
            "  611241/10000000: episode: 3041, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -53.200, mean reward: -0.265 [-26.600, 163.800], mean action: 2.826 [0.000, 10.000], mean observation: 34.205 [0.000, 764.600], loss: 162.277924, mae: 33.616039, mean_q: -34.341385\n",
            "  611442/10000000: episode: 3042, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -245.400, mean reward: -1.221 [-122.700, 77.600], mean action: 2.169 [0.000, 10.000], mean observation: 28.609 [0.004, 498.200], loss: 157.609680, mae: 34.009262, mean_q: -34.468605\n",
            "  611643/10000000: episode: 3043, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -581.800, mean reward: -2.895 [-290.900, 37.600], mean action: 2.607 [0.000, 9.000], mean observation: 30.958 [0.000, 598.900], loss: 126.991936, mae: 33.828575, mean_q: -34.586094\n",
            "  611844/10000000: episode: 3044, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -704.200, mean reward: -3.503 [-352.100, 76.200], mean action: 2.886 [0.000, 10.000], mean observation: 34.579 [0.001, 508.700], loss: 202.687302, mae: 34.380051, mean_q: -35.007442\n",
            "  612045/10000000: episode: 3045, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -387.800, mean reward: -1.929 [-193.900, 71.100], mean action: 2.055 [0.000, 10.000], mean observation: 38.689 [0.000, 792.800], loss: 128.924164, mae: 34.502789, mean_q: -34.866524\n",
            "  612246/10000000: episode: 3046, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: -574.800, mean reward: -2.860 [-287.400, 84.800], mean action: 2.522 [0.000, 10.000], mean observation: 31.009 [0.002, 455.000], loss: 232.383575, mae: 34.116241, mean_q: -34.616638\n",
            "  612447/10000000: episode: 3047, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: 18.800, mean reward: 0.094 [-9.000, 78.600], mean action: 2.527 [0.000, 10.000], mean observation: 39.002 [0.000, 746.700], loss: 160.794449, mae: 34.253979, mean_q: -34.749775\n",
            "  612648/10000000: episode: 3048, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: 90.400, mean reward: 0.450 [-10.000, 298.800], mean action: 2.751 [0.000, 10.000], mean observation: 33.414 [0.001, 541.500], loss: 174.568954, mae: 33.956631, mean_q: -34.732559\n",
            "  612849/10000000: episode: 3049, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -648.400, mean reward: -3.226 [-324.200, 43.200], mean action: 3.005 [0.000, 10.000], mean observation: 31.790 [0.000, 516.300], loss: 219.434235, mae: 33.801067, mean_q: -34.663860\n",
            "  613050/10000000: episode: 3050, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -192.000, mean reward: -0.955 [-96.000, 64.000], mean action: 2.965 [0.000, 9.000], mean observation: 31.058 [0.001, 519.300], loss: 129.508972, mae: 33.473534, mean_q: -34.297253\n",
            "  613251/10000000: episode: 3051, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: 1220.200, mean reward: 6.071 [-10.000, 903.200], mean action: 2.791 [0.000, 10.000], mean observation: 35.303 [0.003, 566.800], loss: 186.366272, mae: 33.725964, mean_q: -34.403000\n",
            "  613452/10000000: episode: 3052, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -74.000, mean reward: -0.368 [-37.000, 210.700], mean action: 2.642 [0.000, 10.000], mean observation: 33.489 [0.000, 579.300], loss: 175.350861, mae: 33.586456, mean_q: -34.466373\n",
            "  613653/10000000: episode: 3053, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -269.800, mean reward: -1.342 [-134.900, 268.800], mean action: 2.960 [0.000, 9.000], mean observation: 28.451 [0.001, 576.200], loss: 236.374191, mae: 33.469101, mean_q: -34.396515\n",
            "  613854/10000000: episode: 3054, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -576.200, mean reward: -2.867 [-288.100, 79.200], mean action: 2.995 [0.000, 10.000], mean observation: 32.743 [0.001, 550.200], loss: 160.071075, mae: 33.369553, mean_q: -34.066311\n",
            "  614055/10000000: episode: 3055, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -736.000, mean reward: -3.662 [-368.000, 56.000], mean action: 2.279 [0.000, 10.000], mean observation: 30.412 [0.001, 416.700], loss: 196.345612, mae: 33.777901, mean_q: -34.240582\n",
            "  614256/10000000: episode: 3056, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: 410.200, mean reward: 2.041 [-9.000, 316.800], mean action: 2.647 [0.000, 9.000], mean observation: 34.656 [0.001, 494.100], loss: 145.704819, mae: 33.245857, mean_q: -33.633541\n",
            "  614457/10000000: episode: 3057, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -417.800, mean reward: -2.079 [-208.900, 36.600], mean action: 1.721 [0.000, 10.000], mean observation: 37.856 [0.000, 525.700], loss: 178.044815, mae: 33.545376, mean_q: -33.672890\n",
            "  614658/10000000: episode: 3058, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: 171.200, mean reward: 0.852 [-9.000, 326.200], mean action: 2.224 [0.000, 9.000], mean observation: 37.822 [0.000, 803.400], loss: 255.684921, mae: 33.132420, mean_q: -33.234524\n",
            "  614859/10000000: episode: 3059, duration: 1.540s, episode steps: 201, steps per second: 130, episode reward: -352.800, mean reward: -1.755 [-176.400, 151.200], mean action: 2.731 [0.000, 9.000], mean observation: 31.789 [0.000, 597.200], loss: 189.283890, mae: 32.363194, mean_q: -32.794376\n",
            "  615060/10000000: episode: 3060, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -561.400, mean reward: -2.793 [-280.700, 127.200], mean action: 2.900 [0.000, 8.000], mean observation: 32.004 [0.002, 525.700], loss: 150.531296, mae: 32.505054, mean_q: -33.225067\n",
            "  615261/10000000: episode: 3061, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -534.200, mean reward: -2.658 [-267.100, 82.400], mean action: 2.866 [0.000, 8.000], mean observation: 36.173 [0.001, 545.700], loss: 265.291901, mae: 32.864422, mean_q: -33.822575\n",
            "  615462/10000000: episode: 3062, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -956.000, mean reward: -4.756 [-478.000, 68.400], mean action: 3.383 [0.000, 8.000], mean observation: 32.289 [0.000, 622.400], loss: 200.788681, mae: 33.325581, mean_q: -33.988632\n",
            "  615663/10000000: episode: 3063, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -785.400, mean reward: -3.907 [-392.700, 115.500], mean action: 3.567 [0.000, 10.000], mean observation: 38.654 [0.000, 592.700], loss: 239.939407, mae: 33.303837, mean_q: -34.040150\n",
            "  615864/10000000: episode: 3064, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -748.600, mean reward: -3.724 [-374.300, 95.900], mean action: 3.378 [0.000, 8.000], mean observation: 34.843 [0.000, 669.400], loss: 141.527725, mae: 33.423237, mean_q: -34.411827\n",
            "  616065/10000000: episode: 3065, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -17.200, mean reward: -0.086 [-8.600, 249.900], mean action: 3.627 [0.000, 8.000], mean observation: 33.747 [0.000, 556.200], loss: 206.914215, mae: 33.855907, mean_q: -34.940125\n",
            "  616266/10000000: episode: 3066, duration: 1.602s, episode steps: 201, steps per second: 125, episode reward: -540.400, mean reward: -2.689 [-270.200, 211.400], mean action: 3.488 [0.000, 8.000], mean observation: 35.021 [0.002, 511.500], loss: 195.917374, mae: 34.652622, mean_q: -35.860909\n",
            "  616467/10000000: episode: 3067, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -739.600, mean reward: -3.680 [-369.800, 58.100], mean action: 3.095 [0.000, 9.000], mean observation: 40.984 [0.000, 663.000], loss: 244.578857, mae: 34.785915, mean_q: -35.819885\n",
            "  616668/10000000: episode: 3068, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -755.400, mean reward: -3.758 [-377.700, 97.200], mean action: 3.453 [0.000, 10.000], mean observation: 32.300 [0.000, 637.900], loss: 217.039337, mae: 35.199860, mean_q: -36.563145\n",
            "  616869/10000000: episode: 3069, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -298.400, mean reward: -1.485 [-149.200, 95.500], mean action: 2.726 [0.000, 10.000], mean observation: 34.749 [0.001, 459.000], loss: 183.329239, mae: 35.715389, mean_q: -36.849129\n",
            "  617070/10000000: episode: 3070, duration: 1.673s, episode steps: 201, steps per second: 120, episode reward: -431.000, mean reward: -2.144 [-215.500, 72.600], mean action: 2.512 [0.000, 9.000], mean observation: 32.488 [0.001, 581.100], loss: 145.371994, mae: 35.701805, mean_q: -36.519749\n",
            "  617271/10000000: episode: 3071, duration: 1.839s, episode steps: 201, steps per second: 109, episode reward: -144.800, mean reward: -0.720 [-72.400, 128.800], mean action: 2.617 [0.000, 10.000], mean observation: 31.461 [0.001, 622.800], loss: 143.744507, mae: 35.848709, mean_q: -36.954769\n",
            "  617472/10000000: episode: 3072, duration: 1.777s, episode steps: 201, steps per second: 113, episode reward: -876.800, mean reward: -4.362 [-438.400, 20.600], mean action: 2.642 [0.000, 10.000], mean observation: 34.102 [0.000, 570.400], loss: 219.431061, mae: 36.394119, mean_q: -37.480740\n",
            "  617673/10000000: episode: 3073, duration: 1.723s, episode steps: 201, steps per second: 117, episode reward: -404.800, mean reward: -2.014 [-202.400, 110.700], mean action: 2.687 [0.000, 10.000], mean observation: 35.838 [0.001, 446.000], loss: 161.900146, mae: 36.628529, mean_q: -37.467037\n",
            "  617874/10000000: episode: 3074, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -593.000, mean reward: -2.950 [-296.500, 82.400], mean action: 2.622 [0.000, 9.000], mean observation: 32.682 [0.002, 498.600], loss: 245.440277, mae: 36.417950, mean_q: -37.116188\n",
            "  618075/10000000: episode: 3075, duration: 1.606s, episode steps: 201, steps per second: 125, episode reward: -858.000, mean reward: -4.269 [-429.000, 87.500], mean action: 3.189 [0.000, 9.000], mean observation: 30.411 [0.000, 749.600], loss: 221.768616, mae: 37.086700, mean_q: -38.463631\n",
            "  618276/10000000: episode: 3076, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -154.600, mean reward: -0.769 [-77.300, 238.700], mean action: 2.652 [0.000, 10.000], mean observation: 34.140 [0.001, 587.600], loss: 214.495544, mae: 37.674736, mean_q: -38.891457\n",
            "  618477/10000000: episode: 3077, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: 322.600, mean reward: 1.605 [-10.000, 383.600], mean action: 2.502 [0.000, 10.000], mean observation: 32.776 [0.002, 516.600], loss: 246.789764, mae: 38.090839, mean_q: -39.485332\n",
            "  618678/10000000: episode: 3078, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -638.000, mean reward: -3.174 [-319.000, 84.000], mean action: 3.065 [0.000, 10.000], mean observation: 28.703 [0.001, 415.200], loss: 274.324982, mae: 37.888374, mean_q: -39.395683\n",
            "  618879/10000000: episode: 3079, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: 125.400, mean reward: 0.624 [-10.000, 478.100], mean action: 3.632 [0.000, 10.000], mean observation: 34.680 [0.004, 509.100], loss: 169.963470, mae: 37.917866, mean_q: -39.679096\n",
            "  619080/10000000: episode: 3080, duration: 1.605s, episode steps: 201, steps per second: 125, episode reward: -170.800, mean reward: -0.850 [-85.400, 77.400], mean action: 3.065 [0.000, 10.000], mean observation: 34.103 [0.000, 590.900], loss: 241.232544, mae: 38.563213, mean_q: -40.398476\n",
            "  619281/10000000: episode: 3081, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -656.800, mean reward: -3.268 [-328.400, 87.200], mean action: 2.607 [0.000, 10.000], mean observation: 25.189 [0.000, 359.900], loss: 120.860046, mae: 39.241974, mean_q: -41.047832\n",
            "  619482/10000000: episode: 3082, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -275.400, mean reward: -1.370 [-137.700, 96.600], mean action: 2.249 [0.000, 10.000], mean observation: 33.405 [0.001, 504.300], loss: 147.430603, mae: 39.462376, mean_q: -40.963356\n",
            "  619683/10000000: episode: 3083, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 72.400, mean reward: 0.360 [-10.000, 117.600], mean action: 2.289 [0.000, 10.000], mean observation: 32.238 [0.003, 510.800], loss: 159.144379, mae: 39.427441, mean_q: -40.842575\n",
            "  619884/10000000: episode: 3084, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: 300.800, mean reward: 1.497 [-10.000, 301.200], mean action: 2.388 [0.000, 10.000], mean observation: 32.047 [0.001, 487.500], loss: 255.922958, mae: 38.380363, mean_q: -39.877544\n",
            "  620085/10000000: episode: 3085, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: 362.800, mean reward: 1.805 [-10.000, 207.200], mean action: 2.985 [0.000, 10.000], mean observation: 35.672 [0.000, 637.000], loss: 206.154465, mae: 37.988136, mean_q: -39.785900\n",
            "  620286/10000000: episode: 3086, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -847.400, mean reward: -4.216 [-423.700, 25.600], mean action: 2.791 [0.000, 10.000], mean observation: 31.066 [0.000, 640.400], loss: 213.322495, mae: 38.111286, mean_q: -39.571774\n",
            "  620487/10000000: episode: 3087, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: 961.600, mean reward: 4.784 [-9.000, 480.800], mean action: 3.149 [0.000, 9.000], mean observation: 29.890 [0.001, 413.700], loss: 211.443665, mae: 38.271606, mean_q: -39.524590\n",
            "  620688/10000000: episode: 3088, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -670.000, mean reward: -3.333 [-335.000, 52.200], mean action: 2.766 [0.000, 9.000], mean observation: 32.122 [0.001, 512.800], loss: 254.505249, mae: 38.009670, mean_q: -39.427536\n",
            "  620889/10000000: episode: 3089, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 60.200, mean reward: 0.300 [-9.000, 115.200], mean action: 2.488 [0.000, 10.000], mean observation: 30.775 [0.000, 558.800], loss: 179.194580, mae: 38.205044, mean_q: -39.384850\n",
            "  621090/10000000: episode: 3090, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -55.400, mean reward: -0.276 [-27.700, 169.200], mean action: 2.294 [0.000, 8.000], mean observation: 38.706 [0.001, 539.800], loss: 183.835922, mae: 38.283100, mean_q: -39.283398\n",
            "  621291/10000000: episode: 3091, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -858.200, mean reward: -4.270 [-429.100, 64.800], mean action: 3.353 [0.000, 10.000], mean observation: 34.706 [0.000, 584.600], loss: 245.203629, mae: 38.122650, mean_q: -39.544518\n",
            "  621492/10000000: episode: 3092, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -699.400, mean reward: -3.480 [-349.700, 129.900], mean action: 3.323 [0.000, 10.000], mean observation: 38.437 [0.002, 510.200], loss: 202.422577, mae: 38.001682, mean_q: -39.813072\n",
            "  621693/10000000: episode: 3093, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: -473.600, mean reward: -2.356 [-236.800, 122.000], mean action: 2.866 [0.000, 10.000], mean observation: 28.853 [0.001, 486.800], loss: 187.825439, mae: 38.651569, mean_q: -40.377769\n",
            "  621894/10000000: episode: 3094, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: 283.800, mean reward: 1.412 [-10.000, 425.200], mean action: 3.045 [0.000, 10.000], mean observation: 32.425 [0.000, 708.200], loss: 182.171616, mae: 38.710609, mean_q: -40.733562\n",
            "  622095/10000000: episode: 3095, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -222.600, mean reward: -1.107 [-111.300, 100.500], mean action: 2.851 [0.000, 10.000], mean observation: 38.386 [0.002, 500.200], loss: 148.894028, mae: 39.203403, mean_q: -40.715519\n",
            "  622296/10000000: episode: 3096, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -48.600, mean reward: -0.242 [-24.300, 171.000], mean action: 2.199 [0.000, 10.000], mean observation: 35.391 [0.000, 440.500], loss: 226.741150, mae: 39.049091, mean_q: -40.189888\n",
            "  622497/10000000: episode: 3097, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -159.200, mean reward: -0.792 [-79.600, 88.800], mean action: 2.070 [0.000, 9.000], mean observation: 34.601 [0.000, 566.800], loss: 301.981598, mae: 38.242252, mean_q: -39.467194\n",
            "  622698/10000000: episode: 3098, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -517.800, mean reward: -2.576 [-258.900, 51.000], mean action: 2.637 [0.000, 10.000], mean observation: 29.736 [0.001, 571.900], loss: 182.691666, mae: 37.881142, mean_q: -39.628128\n",
            "  622899/10000000: episode: 3099, duration: 1.540s, episode steps: 201, steps per second: 130, episode reward: -825.200, mean reward: -4.105 [-412.600, 54.900], mean action: 2.488 [0.000, 10.000], mean observation: 32.493 [0.000, 494.100], loss: 225.226196, mae: 38.297348, mean_q: -39.923317\n",
            "  623100/10000000: episode: 3100, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -508.800, mean reward: -2.531 [-254.400, 22.200], mean action: 2.000 [0.000, 10.000], mean observation: 30.693 [0.001, 465.200], loss: 263.526611, mae: 38.358818, mean_q: -39.365479\n",
            "  623301/10000000: episode: 3101, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -366.600, mean reward: -1.824 [-183.300, 76.800], mean action: 2.632 [0.000, 9.000], mean observation: 31.820 [0.000, 798.300], loss: 158.187637, mae: 37.741402, mean_q: -39.291729\n",
            "  623502/10000000: episode: 3102, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -680.400, mean reward: -3.385 [-340.200, 92.400], mean action: 3.020 [0.000, 10.000], mean observation: 33.264 [0.002, 556.200], loss: 238.601944, mae: 37.871082, mean_q: -39.498077\n",
            "  623703/10000000: episode: 3103, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -702.800, mean reward: -3.497 [-351.400, 60.600], mean action: 2.891 [0.000, 9.000], mean observation: 26.382 [0.001, 434.300], loss: 251.474121, mae: 38.153824, mean_q: -39.468826\n",
            "  623904/10000000: episode: 3104, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -559.800, mean reward: -2.785 [-279.900, 55.200], mean action: 2.468 [0.000, 9.000], mean observation: 35.132 [0.000, 668.900], loss: 265.807922, mae: 38.417164, mean_q: -39.347580\n",
            "  624105/10000000: episode: 3105, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: 486.600, mean reward: 2.421 [-9.000, 309.400], mean action: 2.925 [0.000, 10.000], mean observation: 32.922 [0.000, 506.300], loss: 187.625092, mae: 37.889111, mean_q: -39.423088\n",
            "  624306/10000000: episode: 3106, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: -571.000, mean reward: -2.841 [-285.500, 90.000], mean action: 2.697 [0.000, 9.000], mean observation: 32.460 [0.000, 531.100], loss: 184.980316, mae: 38.237076, mean_q: -39.545013\n",
            "  624507/10000000: episode: 3107, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: 189.000, mean reward: 0.940 [-9.000, 360.000], mean action: 2.980 [0.000, 9.000], mean observation: 37.054 [0.000, 650.000], loss: 242.732391, mae: 38.126873, mean_q: -39.752754\n",
            "  624708/10000000: episode: 3108, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: 543.400, mean reward: 2.703 [-9.000, 630.000], mean action: 2.572 [0.000, 9.000], mean observation: 36.276 [0.000, 574.000], loss: 139.266739, mae: 38.237991, mean_q: -39.602974\n",
            "  624909/10000000: episode: 3109, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -667.800, mean reward: -3.322 [-333.900, 141.200], mean action: 3.249 [0.000, 10.000], mean observation: 26.865 [0.002, 420.100], loss: 162.183823, mae: 37.448441, mean_q: -39.151840\n",
            "  625110/10000000: episode: 3110, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: 269.800, mean reward: 1.342 [-9.000, 262.000], mean action: 2.398 [0.000, 9.000], mean observation: 39.439 [0.002, 470.300], loss: 237.328049, mae: 37.506550, mean_q: -38.681747\n",
            "  625311/10000000: episode: 3111, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -580.600, mean reward: -2.889 [-290.300, 73.800], mean action: 3.010 [0.000, 10.000], mean observation: 36.226 [0.001, 510.400], loss: 240.497314, mae: 37.568378, mean_q: -38.752392\n",
            "  625512/10000000: episode: 3112, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: 93.200, mean reward: 0.464 [-8.000, 337.500], mean action: 2.771 [0.000, 8.000], mean observation: 34.529 [0.000, 508.900], loss: 188.744904, mae: 37.383091, mean_q: -38.465485\n",
            "  625713/10000000: episode: 3113, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -756.200, mean reward: -3.762 [-378.100, 31.200], mean action: 2.602 [0.000, 9.000], mean observation: 31.800 [0.002, 472.800], loss: 179.057373, mae: 37.603661, mean_q: -38.517319\n",
            "  625914/10000000: episode: 3114, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -516.200, mean reward: -2.568 [-258.100, 116.600], mean action: 4.144 [0.000, 10.000], mean observation: 28.935 [0.001, 618.400], loss: 304.361359, mae: 36.868301, mean_q: -38.324284\n",
            "  626115/10000000: episode: 3115, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: -158.800, mean reward: -0.790 [-79.400, 277.800], mean action: 3.194 [0.000, 10.000], mean observation: 33.701 [0.001, 512.000], loss: 194.601059, mae: 37.675606, mean_q: -38.606850\n",
            "  626316/10000000: episode: 3116, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: 250.600, mean reward: 1.247 [-10.000, 193.200], mean action: 2.607 [0.000, 10.000], mean observation: 33.695 [0.000, 586.000], loss: 238.785812, mae: 37.425854, mean_q: -38.409992\n",
            "  626517/10000000: episode: 3117, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -716.000, mean reward: -3.562 [-358.000, 47.600], mean action: 2.955 [0.000, 10.000], mean observation: 32.207 [0.001, 431.000], loss: 195.146179, mae: 37.495640, mean_q: -38.413322\n",
            "  626718/10000000: episode: 3118, duration: 1.735s, episode steps: 201, steps per second: 116, episode reward: -6.600, mean reward: -0.033 [-9.000, 83.500], mean action: 2.522 [0.000, 9.000], mean observation: 28.635 [0.001, 459.200], loss: 174.466919, mae: 37.302784, mean_q: -38.082298\n",
            "  626919/10000000: episode: 3119, duration: 1.790s, episode steps: 201, steps per second: 112, episode reward: -1102.600, mean reward: -5.486 [-551.300, 36.000], mean action: 3.542 [0.000, 9.000], mean observation: 34.083 [0.001, 422.300], loss: 180.115982, mae: 37.142982, mean_q: -38.803757\n",
            "  627120/10000000: episode: 3120, duration: 1.704s, episode steps: 201, steps per second: 118, episode reward: -8.600, mean reward: -0.043 [-9.000, 228.800], mean action: 2.761 [0.000, 9.000], mean observation: 33.942 [0.001, 497.500], loss: 184.696640, mae: 37.617065, mean_q: -38.860767\n",
            "  627321/10000000: episode: 3121, duration: 1.771s, episode steps: 201, steps per second: 113, episode reward: -567.400, mean reward: -2.823 [-283.700, 61.600], mean action: 2.925 [0.000, 10.000], mean observation: 28.961 [0.002, 430.700], loss: 250.303604, mae: 37.252140, mean_q: -38.701000\n",
            "  627522/10000000: episode: 3122, duration: 1.775s, episode steps: 201, steps per second: 113, episode reward: -781.000, mean reward: -3.886 [-390.500, 52.800], mean action: 2.582 [0.000, 9.000], mean observation: 30.639 [0.002, 527.400], loss: 229.165436, mae: 37.262489, mean_q: -38.597359\n",
            "  627723/10000000: episode: 3123, duration: 1.714s, episode steps: 201, steps per second: 117, episode reward: 41.600, mean reward: 0.207 [-10.000, 359.500], mean action: 2.965 [0.000, 10.000], mean observation: 36.099 [0.000, 720.900], loss: 195.676620, mae: 37.559090, mean_q: -38.706688\n",
            "  627924/10000000: episode: 3124, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -179.800, mean reward: -0.895 [-89.900, 37.600], mean action: 2.308 [0.000, 10.000], mean observation: 31.181 [0.001, 500.800], loss: 163.484604, mae: 37.157318, mean_q: -37.736382\n",
            "  628125/10000000: episode: 3125, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -235.400, mean reward: -1.171 [-117.700, 63.800], mean action: 2.373 [0.000, 10.000], mean observation: 31.650 [0.001, 424.400], loss: 232.051910, mae: 37.292488, mean_q: -38.126793\n",
            "  628326/10000000: episode: 3126, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: 137.600, mean reward: 0.685 [-9.000, 153.300], mean action: 2.398 [0.000, 9.000], mean observation: 32.478 [0.000, 441.700], loss: 228.483948, mae: 37.908783, mean_q: -38.329533\n",
            "  628527/10000000: episode: 3127, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -18.000, mean reward: -0.090 [-10.000, 118.400], mean action: 2.124 [0.000, 10.000], mean observation: 30.198 [0.002, 387.100], loss: 172.391907, mae: 36.692371, mean_q: -37.617153\n",
            "  628728/10000000: episode: 3128, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: -433.000, mean reward: -2.154 [-216.500, 226.800], mean action: 3.279 [0.000, 10.000], mean observation: 33.408 [0.002, 537.100], loss: 260.011414, mae: 36.017982, mean_q: -37.717289\n",
            "  628929/10000000: episode: 3129, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: 72.600, mean reward: 0.361 [-10.000, 352.800], mean action: 3.532 [0.000, 10.000], mean observation: 35.276 [0.001, 562.800], loss: 295.068207, mae: 36.516048, mean_q: -38.368690\n",
            "  629130/10000000: episode: 3130, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 1138.200, mean reward: 5.663 [-10.000, 761.600], mean action: 4.134 [0.000, 10.000], mean observation: 29.830 [0.000, 693.300], loss: 282.493652, mae: 36.187725, mean_q: -37.830799\n",
            "  629331/10000000: episode: 3131, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -1004.000, mean reward: -4.995 [-502.000, 48.600], mean action: 3.363 [0.000, 10.000], mean observation: 40.944 [0.001, 600.600], loss: 276.922363, mae: 36.014149, mean_q: -37.352169\n",
            "  629532/10000000: episode: 3132, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -820.400, mean reward: -4.082 [-410.200, 109.600], mean action: 3.846 [0.000, 10.000], mean observation: 35.329 [0.000, 727.600], loss: 218.154343, mae: 35.724209, mean_q: -36.892200\n",
            "  629733/10000000: episode: 3133, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: 689.400, mean reward: 3.430 [-9.000, 344.700], mean action: 4.139 [0.000, 10.000], mean observation: 34.218 [0.001, 606.200], loss: 270.668457, mae: 35.876560, mean_q: -37.236286\n",
            "  629934/10000000: episode: 3134, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: -627.400, mean reward: -3.121 [-313.700, 120.400], mean action: 3.194 [0.000, 9.000], mean observation: 31.177 [0.001, 597.900], loss: 146.615219, mae: 36.213760, mean_q: -37.477131\n",
            "  630135/10000000: episode: 3135, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -837.600, mean reward: -4.167 [-418.800, 28.700], mean action: 3.100 [0.000, 9.000], mean observation: 33.730 [0.003, 593.100], loss: 311.554535, mae: 35.948257, mean_q: -37.217739\n",
            "  630336/10000000: episode: 3136, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -866.000, mean reward: -4.308 [-433.000, 53.600], mean action: 2.965 [0.000, 10.000], mean observation: 29.777 [0.002, 435.000], loss: 204.253464, mae: 36.740257, mean_q: -38.101757\n",
            "  630537/10000000: episode: 3137, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -424.600, mean reward: -2.112 [-212.300, 64.800], mean action: 2.920 [0.000, 9.000], mean observation: 30.410 [0.001, 697.100], loss: 192.089935, mae: 36.463589, mean_q: -37.765808\n",
            "  630738/10000000: episode: 3138, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: -375.600, mean reward: -1.869 [-187.800, 84.800], mean action: 2.368 [0.000, 10.000], mean observation: 30.375 [0.001, 562.800], loss: 136.948425, mae: 36.566772, mean_q: -37.557304\n",
            "  630939/10000000: episode: 3139, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -807.400, mean reward: -4.017 [-403.700, 35.400], mean action: 2.522 [0.000, 10.000], mean observation: 31.104 [0.001, 388.800], loss: 268.176880, mae: 36.634743, mean_q: -37.334709\n",
            "  631140/10000000: episode: 3140, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -694.400, mean reward: -3.455 [-347.200, 28.800], mean action: 2.214 [0.000, 10.000], mean observation: 35.262 [0.000, 522.200], loss: 136.965607, mae: 36.599556, mean_q: -37.268452\n",
            "  631341/10000000: episode: 3141, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -362.600, mean reward: -1.804 [-181.300, 86.100], mean action: 2.328 [0.000, 9.000], mean observation: 29.901 [0.001, 521.600], loss: 204.838226, mae: 36.932659, mean_q: -37.979343\n",
            "  631542/10000000: episode: 3142, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: 767.200, mean reward: 3.817 [-10.000, 383.600], mean action: 2.627 [0.000, 10.000], mean observation: 33.325 [0.000, 527.200], loss: 341.921844, mae: 37.241470, mean_q: -38.752178\n",
            "  631743/10000000: episode: 3143, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -815.400, mean reward: -4.057 [-407.700, 51.600], mean action: 3.020 [0.000, 10.000], mean observation: 33.915 [0.002, 451.200], loss: 217.851364, mae: 37.830109, mean_q: -39.307472\n",
            "  631944/10000000: episode: 3144, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -816.800, mean reward: -4.064 [-408.400, 66.500], mean action: 3.045 [0.000, 9.000], mean observation: 32.110 [0.000, 765.000], loss: 143.434189, mae: 37.769852, mean_q: -39.236183\n",
            "  632145/10000000: episode: 3145, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: 306.800, mean reward: 1.526 [-10.000, 169.000], mean action: 3.299 [0.000, 10.000], mean observation: 36.671 [0.002, 439.000], loss: 313.070557, mae: 37.201797, mean_q: -38.753563\n",
            "  632346/10000000: episode: 3146, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -103.600, mean reward: -0.515 [-51.800, 305.200], mean action: 3.209 [0.000, 10.000], mean observation: 34.625 [0.000, 793.800], loss: 280.527252, mae: 37.171879, mean_q: -38.705013\n",
            "  632547/10000000: episode: 3147, duration: 1.617s, episode steps: 201, steps per second: 124, episode reward: -216.200, mean reward: -1.076 [-108.100, 82.800], mean action: 2.642 [0.000, 10.000], mean observation: 35.719 [0.000, 633.800], loss: 240.678329, mae: 37.377819, mean_q: -38.813923\n",
            "  632748/10000000: episode: 3148, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -711.800, mean reward: -3.541 [-355.900, 37.800], mean action: 2.766 [0.000, 9.000], mean observation: 35.750 [0.000, 701.500], loss: 237.144547, mae: 37.324348, mean_q: -38.455463\n",
            "  632949/10000000: episode: 3149, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -560.000, mean reward: -2.786 [-280.000, 62.000], mean action: 2.403 [0.000, 9.000], mean observation: 40.663 [0.002, 542.400], loss: 251.178879, mae: 37.697029, mean_q: -38.970573\n",
            "  633150/10000000: episode: 3150, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: 1210.600, mean reward: 6.023 [-10.000, 605.300], mean action: 2.323 [0.000, 10.000], mean observation: 32.049 [0.000, 783.800], loss: 234.799530, mae: 38.366714, mean_q: -39.329460\n",
            "  633351/10000000: episode: 3151, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -39.600, mean reward: -0.197 [-19.800, 103.500], mean action: 2.343 [0.000, 10.000], mean observation: 37.527 [0.000, 620.700], loss: 237.856110, mae: 38.470097, mean_q: -39.578365\n",
            "  633552/10000000: episode: 3152, duration: 1.611s, episode steps: 201, steps per second: 125, episode reward: -382.600, mean reward: -1.903 [-191.300, 75.600], mean action: 2.413 [0.000, 10.000], mean observation: 36.059 [0.002, 515.200], loss: 253.490326, mae: 39.193184, mean_q: -40.241707\n",
            "  633753/10000000: episode: 3153, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: -366.200, mean reward: -1.822 [-183.100, 136.800], mean action: 2.602 [0.000, 10.000], mean observation: 35.044 [0.000, 673.700], loss: 265.955841, mae: 39.257786, mean_q: -40.777256\n",
            "  633954/10000000: episode: 3154, duration: 1.603s, episode steps: 201, steps per second: 125, episode reward: -772.800, mean reward: -3.845 [-386.400, 33.600], mean action: 2.731 [0.000, 10.000], mean observation: 36.848 [0.002, 446.200], loss: 369.285828, mae: 39.177235, mean_q: -40.834198\n",
            "  634155/10000000: episode: 3155, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: 35.800, mean reward: 0.178 [-10.000, 306.000], mean action: 2.597 [0.000, 10.000], mean observation: 33.960 [0.001, 459.000], loss: 274.592377, mae: 39.436470, mean_q: -41.161392\n",
            "  634356/10000000: episode: 3156, duration: 1.576s, episode steps: 201, steps per second: 127, episode reward: -409.200, mean reward: -2.036 [-204.600, 120.400], mean action: 2.214 [0.000, 10.000], mean observation: 37.327 [0.001, 558.800], loss: 223.851608, mae: 39.829704, mean_q: -41.136173\n",
            "  634557/10000000: episode: 3157, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: 562.000, mean reward: 2.796 [-9.000, 313.000], mean action: 2.692 [0.000, 10.000], mean observation: 32.764 [0.000, 933.200], loss: 190.455231, mae: 39.487488, mean_q: -40.982227\n",
            "  634758/10000000: episode: 3158, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -65.000, mean reward: -0.323 [-32.500, 52.400], mean action: 2.259 [0.000, 10.000], mean observation: 31.781 [0.000, 684.900], loss: 248.119827, mae: 40.101406, mean_q: -41.253109\n",
            "  634959/10000000: episode: 3159, duration: 1.605s, episode steps: 201, steps per second: 125, episode reward: -290.000, mean reward: -1.443 [-145.000, 97.200], mean action: 2.149 [0.000, 10.000], mean observation: 31.541 [0.002, 507.800], loss: 218.107727, mae: 40.747677, mean_q: -41.790264\n",
            "  635160/10000000: episode: 3160, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: -118.200, mean reward: -0.588 [-59.100, 143.100], mean action: 2.214 [0.000, 8.000], mean observation: 33.541 [0.000, 556.700], loss: 236.307312, mae: 40.424809, mean_q: -41.422943\n",
            "  635361/10000000: episode: 3161, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -518.200, mean reward: -2.578 [-259.100, 107.100], mean action: 3.000 [0.000, 10.000], mean observation: 34.934 [0.001, 444.500], loss: 273.505310, mae: 39.674610, mean_q: -41.330349\n",
            "  635562/10000000: episode: 3162, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -454.000, mean reward: -2.259 [-227.000, 150.000], mean action: 2.547 [0.000, 10.000], mean observation: 35.440 [0.000, 607.900], loss: 227.051926, mae: 40.426472, mean_q: -41.630043\n",
            "  635763/10000000: episode: 3163, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: -347.600, mean reward: -1.729 [-173.800, 138.000], mean action: 2.935 [0.000, 10.000], mean observation: 36.226 [0.000, 528.400], loss: 314.835327, mae: 39.955013, mean_q: -41.403496\n",
            "  635964/10000000: episode: 3164, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: 841.000, mean reward: 4.184 [-10.000, 532.000], mean action: 3.358 [0.000, 10.000], mean observation: 29.202 [0.001, 455.800], loss: 209.881226, mae: 39.468468, mean_q: -41.412270\n",
            "  636165/10000000: episode: 3165, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -213.800, mean reward: -1.064 [-106.900, 110.400], mean action: 3.353 [0.000, 10.000], mean observation: 31.613 [0.000, 818.500], loss: 311.070282, mae: 40.224651, mean_q: -42.202785\n",
            "  636366/10000000: episode: 3166, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -616.400, mean reward: -3.067 [-308.200, 123.000], mean action: 2.995 [0.000, 10.000], mean observation: 32.064 [0.002, 441.900], loss: 234.213791, mae: 40.465282, mean_q: -42.243874\n",
            "  636567/10000000: episode: 3167, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -163.600, mean reward: -0.814 [-81.800, 225.000], mean action: 3.040 [0.000, 10.000], mean observation: 32.493 [0.001, 619.000], loss: 213.083023, mae: 40.170593, mean_q: -41.835133\n",
            "  636768/10000000: episode: 3168, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: 50.000, mean reward: 0.249 [-10.000, 323.100], mean action: 2.164 [0.000, 10.000], mean observation: 34.264 [0.001, 412.100], loss: 282.323456, mae: 40.840061, mean_q: -42.079700\n",
            "  636969/10000000: episode: 3169, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -632.800, mean reward: -3.148 [-316.400, 40.200], mean action: 2.254 [0.000, 10.000], mean observation: 33.396 [0.000, 500.900], loss: 298.083405, mae: 40.505161, mean_q: -41.648708\n",
            "  637170/10000000: episode: 3170, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -454.200, mean reward: -2.260 [-227.100, 102.000], mean action: 2.637 [0.000, 8.000], mean observation: 38.287 [0.001, 501.600], loss: 150.111771, mae: 39.664597, mean_q: -41.479759\n",
            "  637371/10000000: episode: 3171, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: 506.800, mean reward: 2.521 [-10.000, 295.200], mean action: 2.493 [0.000, 10.000], mean observation: 31.641 [0.001, 590.900], loss: 258.314789, mae: 40.247387, mean_q: -41.816959\n",
            "  637572/10000000: episode: 3172, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -323.600, mean reward: -1.610 [-161.800, 98.900], mean action: 1.965 [0.000, 9.000], mean observation: 28.994 [0.000, 567.300], loss: 213.259171, mae: 40.815220, mean_q: -41.751637\n",
            "  637773/10000000: episode: 3173, duration: 1.653s, episode steps: 201, steps per second: 122, episode reward: -496.200, mean reward: -2.469 [-248.100, 32.600], mean action: 2.070 [0.000, 9.000], mean observation: 38.677 [0.000, 747.100], loss: 226.354248, mae: 40.385666, mean_q: -41.442757\n",
            "  637974/10000000: episode: 3174, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -120.000, mean reward: -0.597 [-60.000, 172.800], mean action: 2.363 [0.000, 10.000], mean observation: 30.376 [0.000, 534.800], loss: 288.132904, mae: 40.432858, mean_q: -41.797783\n",
            "  638175/10000000: episode: 3175, duration: 1.641s, episode steps: 201, steps per second: 122, episode reward: 141.400, mean reward: 0.703 [-10.000, 220.000], mean action: 2.677 [0.000, 10.000], mean observation: 28.545 [0.000, 437.900], loss: 247.097961, mae: 39.752953, mean_q: -41.314873\n",
            "  638376/10000000: episode: 3176, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -791.800, mean reward: -3.939 [-395.900, 23.800], mean action: 2.736 [0.000, 10.000], mean observation: 34.113 [0.000, 580.300], loss: 249.577896, mae: 39.180054, mean_q: -40.795933\n",
            "  638577/10000000: episode: 3177, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -427.600, mean reward: -2.127 [-213.800, 68.500], mean action: 2.617 [0.000, 10.000], mean observation: 33.257 [0.000, 451.500], loss: 204.441040, mae: 39.955803, mean_q: -41.279194\n",
            "  638778/10000000: episode: 3178, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -382.200, mean reward: -1.901 [-191.100, 114.300], mean action: 2.353 [0.000, 10.000], mean observation: 31.189 [0.000, 813.800], loss: 216.847916, mae: 39.743439, mean_q: -40.838394\n",
            "  638979/10000000: episode: 3179, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -417.400, mean reward: -2.077 [-208.700, 230.400], mean action: 2.806 [0.000, 10.000], mean observation: 36.663 [0.000, 621.900], loss: 277.175049, mae: 38.841244, mean_q: -40.424793\n",
            "  639180/10000000: episode: 3180, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -726.400, mean reward: -3.614 [-363.200, 92.700], mean action: 3.124 [0.000, 9.000], mean observation: 31.962 [0.002, 632.400], loss: 201.361481, mae: 38.325035, mean_q: -39.794174\n",
            "  639381/10000000: episode: 3181, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -393.000, mean reward: -1.955 [-196.500, 83.000], mean action: 2.443 [0.000, 10.000], mean observation: 33.117 [0.003, 531.400], loss: 163.226212, mae: 38.584045, mean_q: -39.852551\n",
            "  639582/10000000: episode: 3182, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: -509.400, mean reward: -2.534 [-254.700, 102.500], mean action: 2.687 [0.000, 10.000], mean observation: 36.401 [0.001, 519.900], loss: 194.684845, mae: 38.758297, mean_q: -40.048889\n",
            "  639783/10000000: episode: 3183, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -371.800, mean reward: -1.850 [-185.900, 84.700], mean action: 2.905 [0.000, 10.000], mean observation: 37.168 [0.001, 447.800], loss: 264.507111, mae: 38.469883, mean_q: -39.925678\n",
            "  639984/10000000: episode: 3184, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -79.400, mean reward: -0.395 [-39.700, 106.500], mean action: 2.617 [0.000, 10.000], mean observation: 38.698 [0.001, 627.200], loss: 282.107025, mae: 38.863743, mean_q: -40.017418\n",
            "  640185/10000000: episode: 3185, duration: 1.612s, episode steps: 201, steps per second: 125, episode reward: 35.600, mean reward: 0.177 [-10.000, 247.600], mean action: 4.109 [0.000, 10.000], mean observation: 39.503 [0.003, 501.700], loss: 342.627991, mae: 38.938747, mean_q: -40.580624\n",
            "  640386/10000000: episode: 3186, duration: 1.635s, episode steps: 201, steps per second: 123, episode reward: 361.200, mean reward: 1.797 [-9.000, 307.800], mean action: 4.050 [0.000, 9.000], mean observation: 30.037 [0.000, 604.500], loss: 244.199768, mae: 38.960995, mean_q: -40.514153\n",
            "  640587/10000000: episode: 3187, duration: 1.609s, episode steps: 201, steps per second: 125, episode reward: 12.000, mean reward: 0.060 [-10.000, 254.700], mean action: 4.035 [0.000, 10.000], mean observation: 31.939 [0.001, 437.600], loss: 365.307831, mae: 38.324638, mean_q: -39.701202\n",
            "  640788/10000000: episode: 3188, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: 113.200, mean reward: 0.563 [-10.000, 247.800], mean action: 3.667 [0.000, 10.000], mean observation: 31.037 [0.001, 464.600], loss: 235.103699, mae: 38.073990, mean_q: -39.624241\n",
            "  640989/10000000: episode: 3189, duration: 1.605s, episode steps: 201, steps per second: 125, episode reward: 240.800, mean reward: 1.198 [-9.000, 120.400], mean action: 2.721 [0.000, 9.000], mean observation: 32.400 [0.001, 531.100], loss: 256.097107, mae: 38.688251, mean_q: -39.847313\n",
            "  641190/10000000: episode: 3190, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: -346.400, mean reward: -1.723 [-173.200, 66.600], mean action: 2.572 [0.000, 10.000], mean observation: 38.676 [0.000, 467.700], loss: 261.427246, mae: 38.315762, mean_q: -39.657887\n",
            "  641391/10000000: episode: 3191, duration: 1.601s, episode steps: 201, steps per second: 126, episode reward: -374.000, mean reward: -1.861 [-187.000, 68.400], mean action: 3.209 [0.000, 10.000], mean observation: 35.693 [0.000, 784.200], loss: 193.816833, mae: 37.938332, mean_q: -39.354473\n",
            "  641592/10000000: episode: 3192, duration: 1.629s, episode steps: 201, steps per second: 123, episode reward: -196.000, mean reward: -0.975 [-98.000, 84.000], mean action: 2.915 [0.000, 10.000], mean observation: 33.189 [0.000, 507.700], loss: 347.297302, mae: 38.329559, mean_q: -39.724438\n",
            "  641793/10000000: episode: 3193, duration: 1.617s, episode steps: 201, steps per second: 124, episode reward: -638.400, mean reward: -3.176 [-319.200, 112.500], mean action: 3.179 [0.000, 10.000], mean observation: 32.451 [0.001, 542.200], loss: 222.016144, mae: 38.426079, mean_q: -39.788734\n",
            "  641994/10000000: episode: 3194, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: 651.200, mean reward: 3.240 [-9.000, 504.000], mean action: 3.816 [0.000, 10.000], mean observation: 34.955 [0.001, 527.300], loss: 303.324188, mae: 38.154610, mean_q: -39.844463\n",
            "  642195/10000000: episode: 3195, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -850.600, mean reward: -4.232 [-425.300, 102.600], mean action: 3.896 [0.000, 10.000], mean observation: 29.206 [0.000, 542.400], loss: 195.990112, mae: 38.453415, mean_q: -40.186222\n",
            "  642396/10000000: episode: 3196, duration: 1.615s, episode steps: 201, steps per second: 124, episode reward: -1052.000, mean reward: -5.234 [-526.000, 32.800], mean action: 3.532 [0.000, 9.000], mean observation: 32.828 [0.000, 694.400], loss: 282.883728, mae: 38.888924, mean_q: -40.547939\n",
            "  642597/10000000: episode: 3197, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -1229.600, mean reward: -6.117 [-614.800, 36.800], mean action: 3.517 [0.000, 10.000], mean observation: 43.282 [0.001, 598.300], loss: 259.076141, mae: 38.761902, mean_q: -40.195625\n",
            "  642798/10000000: episode: 3198, duration: 1.616s, episode steps: 201, steps per second: 124, episode reward: -1127.400, mean reward: -5.609 [-563.700, 69.800], mean action: 3.900 [0.000, 9.000], mean observation: 31.247 [0.000, 555.700], loss: 298.171051, mae: 38.458324, mean_q: -40.110828\n",
            "  642999/10000000: episode: 3199, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -408.400, mean reward: -2.032 [-204.200, 94.200], mean action: 3.259 [0.000, 10.000], mean observation: 29.690 [0.003, 492.800], loss: 218.392105, mae: 38.935482, mean_q: -40.451736\n",
            "  643200/10000000: episode: 3200, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -1083.400, mean reward: -5.390 [-541.700, 12.100], mean action: 3.040 [0.000, 10.000], mean observation: 39.820 [0.000, 669.900], loss: 331.287048, mae: 39.418415, mean_q: -40.943939\n",
            "  643401/10000000: episode: 3201, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -277.400, mean reward: -1.380 [-138.700, 195.300], mean action: 3.294 [0.000, 10.000], mean observation: 34.161 [0.002, 547.800], loss: 257.344604, mae: 39.071461, mean_q: -40.611431\n",
            "  643602/10000000: episode: 3202, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: 369.200, mean reward: 1.837 [-10.000, 333.900], mean action: 3.174 [0.000, 10.000], mean observation: 35.504 [0.002, 482.600], loss: 330.928955, mae: 39.323338, mean_q: -40.557713\n",
            "  643803/10000000: episode: 3203, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -526.600, mean reward: -2.620 [-263.300, 129.600], mean action: 3.075 [0.000, 10.000], mean observation: 36.778 [0.000, 654.700], loss: 274.678162, mae: 39.380482, mean_q: -40.898521\n",
            "  644004/10000000: episode: 3204, duration: 1.615s, episode steps: 201, steps per second: 124, episode reward: 261.600, mean reward: 1.301 [-10.000, 527.400], mean action: 3.458 [0.000, 10.000], mean observation: 36.032 [0.000, 706.400], loss: 214.025421, mae: 39.779327, mean_q: -41.116974\n",
            "  644205/10000000: episode: 3205, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -694.600, mean reward: -3.456 [-347.300, 54.900], mean action: 2.826 [0.000, 10.000], mean observation: 30.519 [0.001, 480.700], loss: 195.388870, mae: 39.861763, mean_q: -40.928234\n",
            "  644406/10000000: episode: 3206, duration: 2.204s, episode steps: 201, steps per second: 91, episode reward: -403.600, mean reward: -2.008 [-201.800, 49.500], mean action: 2.090 [0.000, 10.000], mean observation: 34.779 [0.001, 444.500], loss: 202.914413, mae: 39.412331, mean_q: -40.143383\n",
            "  644607/10000000: episode: 3207, duration: 1.892s, episode steps: 201, steps per second: 106, episode reward: -703.400, mean reward: -3.500 [-351.700, 35.100], mean action: 2.915 [0.000, 10.000], mean observation: 29.370 [0.001, 676.900], loss: 314.101440, mae: 38.857327, mean_q: -39.939507\n",
            "  644808/10000000: episode: 3208, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -991.200, mean reward: -4.931 [-495.600, 152.100], mean action: 5.259 [0.000, 10.000], mean observation: 33.220 [0.001, 714.300], loss: 356.160919, mae: 38.311966, mean_q: -39.066719\n",
            "  645009/10000000: episode: 3209, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -66.800, mean reward: -0.332 [-33.400, 358.200], mean action: 4.920 [0.000, 9.000], mean observation: 43.142 [0.000, 601.500], loss: 449.395752, mae: 37.730206, mean_q: -38.469955\n",
            "  645210/10000000: episode: 3210, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -1063.400, mean reward: -5.291 [-531.700, 96.600], mean action: 4.328 [0.000, 9.000], mean observation: 30.940 [0.000, 582.800], loss: 271.996063, mae: 36.678356, mean_q: -37.553463\n",
            "  645411/10000000: episode: 3211, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: 612.200, mean reward: 3.046 [-9.000, 306.100], mean action: 3.731 [0.000, 10.000], mean observation: 33.002 [0.000, 566.700], loss: 318.878387, mae: 36.608543, mean_q: -37.551079\n",
            "  645612/10000000: episode: 3212, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: 126.600, mean reward: 0.630 [-9.000, 111.600], mean action: 3.289 [0.000, 9.000], mean observation: 30.182 [0.001, 498.800], loss: 367.927551, mae: 36.707180, mean_q: -37.578384\n",
            "  645813/10000000: episode: 3213, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -210.800, mean reward: -1.049 [-105.400, 229.200], mean action: 3.040 [0.000, 10.000], mean observation: 32.327 [0.002, 508.500], loss: 269.143066, mae: 36.562710, mean_q: -37.537643\n",
            "  646014/10000000: episode: 3214, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: 454.200, mean reward: 2.260 [-10.000, 279.300], mean action: 2.995 [0.000, 10.000], mean observation: 33.495 [0.000, 413.800], loss: 262.507538, mae: 36.560356, mean_q: -37.852253\n",
            "  646215/10000000: episode: 3215, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -874.600, mean reward: -4.351 [-437.300, 60.300], mean action: 3.144 [0.000, 9.000], mean observation: 38.221 [0.000, 630.500], loss: 257.265076, mae: 36.712826, mean_q: -38.224873\n",
            "  646416/10000000: episode: 3216, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -5.600, mean reward: -0.028 [-9.000, 480.800], mean action: 3.577 [0.000, 9.000], mean observation: 30.732 [0.001, 518.600], loss: 348.035278, mae: 36.923851, mean_q: -38.670147\n",
            "  646617/10000000: episode: 3217, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: 2234.200, mean reward: 11.115 [-9.000, 1188.900], mean action: 2.881 [0.000, 9.000], mean observation: 35.336 [0.000, 562.200], loss: 281.405762, mae: 37.474171, mean_q: -38.853569\n",
            "  646818/10000000: episode: 3218, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -459.600, mean reward: -2.287 [-229.800, 109.900], mean action: 2.269 [0.000, 9.000], mean observation: 30.833 [0.002, 468.200], loss: 292.686310, mae: 37.704700, mean_q: -38.351158\n",
            "  647019/10000000: episode: 3219, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -478.000, mean reward: -2.378 [-239.000, 47.200], mean action: 2.308 [0.000, 9.000], mean observation: 34.569 [0.002, 637.200], loss: 358.344604, mae: 36.910740, mean_q: -37.845486\n",
            "  647220/10000000: episode: 3220, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -50.600, mean reward: -0.252 [-25.300, 375.300], mean action: 3.204 [0.000, 10.000], mean observation: 27.865 [0.003, 515.700], loss: 391.197723, mae: 36.912136, mean_q: -38.102562\n",
            "  647421/10000000: episode: 3221, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -230.000, mean reward: -1.144 [-115.000, 109.200], mean action: 3.776 [0.000, 10.000], mean observation: 29.652 [0.001, 607.700], loss: 397.341766, mae: 36.423210, mean_q: -37.503906\n",
            "  647622/10000000: episode: 3222, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -1210.400, mean reward: -6.022 [-605.200, 115.500], mean action: 4.806 [0.000, 10.000], mean observation: 34.214 [0.001, 462.800], loss: 353.325165, mae: 35.841923, mean_q: -37.006191\n",
            "  647823/10000000: episode: 3223, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -858.000, mean reward: -4.269 [-429.000, 157.500], mean action: 3.881 [0.000, 10.000], mean observation: 37.482 [0.003, 524.500], loss: 217.580200, mae: 35.304211, mean_q: -36.601143\n",
            "  648024/10000000: episode: 3224, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: 463.200, mean reward: 2.304 [-10.000, 234.900], mean action: 4.194 [0.000, 10.000], mean observation: 27.712 [0.000, 772.000], loss: 559.935303, mae: 35.030647, mean_q: -36.442776\n",
            "  648225/10000000: episode: 3225, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -822.200, mean reward: -4.091 [-411.100, 59.200], mean action: 2.761 [0.000, 10.000], mean observation: 33.493 [0.000, 558.800], loss: 261.497162, mae: 35.635571, mean_q: -36.724388\n",
            "  648426/10000000: episode: 3226, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -364.200, mean reward: -1.812 [-182.100, 126.000], mean action: 2.607 [0.000, 10.000], mean observation: 33.384 [0.001, 645.600], loss: 333.016876, mae: 36.270248, mean_q: -37.061432\n",
            "  648627/10000000: episode: 3227, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -810.000, mean reward: -4.030 [-405.000, 131.600], mean action: 4.269 [0.000, 10.000], mean observation: 35.370 [0.001, 532.400], loss: 487.721466, mae: 36.001156, mean_q: -37.149376\n",
            "  648828/10000000: episode: 3228, duration: 1.552s, episode steps: 201, steps per second: 129, episode reward: -350.400, mean reward: -1.743 [-175.200, 99.900], mean action: 3.045 [0.000, 10.000], mean observation: 30.544 [0.000, 516.000], loss: 236.413773, mae: 36.240818, mean_q: -37.285160\n",
            "  649029/10000000: episode: 3229, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: 123.000, mean reward: 0.612 [-10.000, 293.400], mean action: 3.557 [0.000, 10.000], mean observation: 35.760 [0.000, 555.400], loss: 207.426041, mae: 36.017937, mean_q: -37.260868\n",
            "  649230/10000000: episode: 3230, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -628.000, mean reward: -3.124 [-314.000, 97.200], mean action: 2.896 [0.000, 10.000], mean observation: 33.065 [0.002, 553.100], loss: 307.534424, mae: 36.311104, mean_q: -37.166306\n",
            "  649431/10000000: episode: 3231, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: -184.200, mean reward: -0.916 [-92.100, 169.200], mean action: 2.647 [0.000, 10.000], mean observation: 37.730 [0.000, 668.100], loss: 328.789581, mae: 36.028748, mean_q: -36.806900\n",
            "  649632/10000000: episode: 3232, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -354.200, mean reward: -1.762 [-177.100, 143.100], mean action: 2.607 [0.000, 10.000], mean observation: 28.189 [0.000, 389.300], loss: 339.279785, mae: 36.121727, mean_q: -36.563908\n",
            "  649833/10000000: episode: 3233, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -324.200, mean reward: -1.613 [-162.100, 196.200], mean action: 3.219 [0.000, 10.000], mean observation: 31.116 [0.001, 587.900], loss: 380.759399, mae: 35.360737, mean_q: -36.210487\n",
            "  650034/10000000: episode: 3234, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -706.400, mean reward: -3.514 [-353.200, 53.900], mean action: 3.224 [0.000, 10.000], mean observation: 36.609 [0.000, 606.600], loss: 276.778290, mae: 35.516575, mean_q: -36.644413\n",
            "  650235/10000000: episode: 3235, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -782.600, mean reward: -3.894 [-391.300, 61.700], mean action: 2.542 [0.000, 10.000], mean observation: 34.839 [0.001, 426.200], loss: 221.369308, mae: 36.045345, mean_q: -36.466259\n",
            "  650436/10000000: episode: 3236, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: -421.000, mean reward: -2.095 [-210.500, 186.000], mean action: 3.040 [0.000, 10.000], mean observation: 34.137 [0.002, 578.200], loss: 374.472107, mae: 34.915325, mean_q: -35.701965\n",
            "  650637/10000000: episode: 3237, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -474.600, mean reward: -2.361 [-237.300, 53.100], mean action: 2.000 [0.000, 9.000], mean observation: 32.479 [0.000, 497.600], loss: 198.079041, mae: 35.024296, mean_q: -35.188480\n",
            "  650838/10000000: episode: 3238, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -313.400, mean reward: -1.559 [-156.700, 76.500], mean action: 2.736 [0.000, 10.000], mean observation: 36.141 [0.001, 527.200], loss: 385.646362, mae: 35.122040, mean_q: -35.617855\n",
            "  651039/10000000: episode: 3239, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -200.200, mean reward: -0.996 [-100.100, 72.000], mean action: 2.786 [0.000, 9.000], mean observation: 30.237 [0.001, 675.300], loss: 302.138123, mae: 35.338543, mean_q: -35.686844\n",
            "  651240/10000000: episode: 3240, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -1055.600, mean reward: -5.252 [-527.800, 30.400], mean action: 3.164 [0.000, 9.000], mean observation: 33.336 [0.000, 527.500], loss: 315.636169, mae: 34.811760, mean_q: -35.938885\n",
            "  651441/10000000: episode: 3241, duration: 1.618s, episode steps: 201, steps per second: 124, episode reward: -600.400, mean reward: -2.987 [-300.200, 80.100], mean action: 4.383 [0.000, 9.000], mean observation: 29.732 [0.001, 466.900], loss: 546.978882, mae: 34.548637, mean_q: -35.183853\n",
            "  651642/10000000: episode: 3242, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: 1019.000, mean reward: 5.070 [-10.000, 509.500], mean action: 3.945 [0.000, 10.000], mean observation: 34.298 [0.000, 544.700], loss: 350.609131, mae: 34.301788, mean_q: -35.244064\n",
            "  651843/10000000: episode: 3243, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -866.200, mean reward: -4.309 [-433.100, 93.600], mean action: 4.045 [0.000, 10.000], mean observation: 36.372 [0.002, 532.900], loss: 685.471313, mae: 34.063072, mean_q: -35.005501\n",
            "  652044/10000000: episode: 3244, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: 611.800, mean reward: 3.044 [-10.000, 552.300], mean action: 4.607 [0.000, 10.000], mean observation: 34.969 [0.001, 647.400], loss: 437.482513, mae: 33.978458, mean_q: -34.454941\n",
            "  652245/10000000: episode: 3245, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: -1031.200, mean reward: -5.130 [-515.600, 115.200], mean action: 4.985 [0.000, 9.000], mean observation: 32.593 [0.000, 602.400], loss: 468.134827, mae: 33.180351, mean_q: -33.032143\n",
            "  652446/10000000: episode: 3246, duration: 1.589s, episode steps: 201, steps per second: 127, episode reward: -1524.400, mean reward: -7.584 [-762.200, 76.500], mean action: 4.821 [0.000, 10.000], mean observation: 34.930 [0.002, 453.500], loss: 310.316254, mae: 32.317646, mean_q: -32.128632\n",
            "  652647/10000000: episode: 3247, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: 439.200, mean reward: 2.185 [-9.000, 318.600], mean action: 3.806 [0.000, 9.000], mean observation: 28.566 [0.002, 362.700], loss: 342.514465, mae: 31.996492, mean_q: -31.968025\n",
            "  652848/10000000: episode: 3248, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: 1944.200, mean reward: 9.673 [-9.000, 972.100], mean action: 3.572 [0.000, 9.000], mean observation: 31.360 [0.000, 695.400], loss: 306.518829, mae: 31.608597, mean_q: -31.661629\n",
            "  653049/10000000: episode: 3249, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -135.200, mean reward: -0.673 [-67.600, 98.400], mean action: 3.050 [0.000, 9.000], mean observation: 33.016 [0.000, 660.900], loss: 335.703308, mae: 31.411161, mean_q: -31.265854\n",
            "  653250/10000000: episode: 3250, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: 66.200, mean reward: 0.329 [-9.000, 317.700], mean action: 2.945 [0.000, 9.000], mean observation: 34.213 [0.000, 525.900], loss: 325.726196, mae: 31.433088, mean_q: -31.138891\n",
            "  653451/10000000: episode: 3251, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -478.600, mean reward: -2.381 [-239.300, 133.200], mean action: 3.348 [0.000, 9.000], mean observation: 34.133 [0.001, 446.900], loss: 367.267334, mae: 30.818830, mean_q: -30.514153\n",
            "  653652/10000000: episode: 3252, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -227.600, mean reward: -1.132 [-113.800, 360.000], mean action: 3.542 [0.000, 9.000], mean observation: 35.758 [0.000, 410.900], loss: 217.696823, mae: 30.374495, mean_q: -30.225979\n",
            "  653853/10000000: episode: 3253, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -856.400, mean reward: -4.261 [-428.200, 95.900], mean action: 3.005 [0.000, 10.000], mean observation: 28.174 [0.001, 654.600], loss: 249.334808, mae: 29.952915, mean_q: -29.772594\n",
            "  654054/10000000: episode: 3254, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -391.400, mean reward: -1.947 [-195.700, 110.700], mean action: 3.184 [0.000, 10.000], mean observation: 34.191 [0.000, 467.400], loss: 347.694061, mae: 29.967516, mean_q: -29.861063\n",
            "  654255/10000000: episode: 3255, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -70.200, mean reward: -0.349 [-35.100, 93.300], mean action: 3.279 [0.000, 9.000], mean observation: 38.019 [0.000, 697.600], loss: 346.619720, mae: 29.828775, mean_q: -29.862268\n",
            "  654456/10000000: episode: 3256, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -487.600, mean reward: -2.426 [-243.800, 73.800], mean action: 2.985 [0.000, 9.000], mean observation: 31.664 [0.000, 499.800], loss: 316.950409, mae: 29.738203, mean_q: -29.817944\n",
            "  654657/10000000: episode: 3257, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: 135.000, mean reward: 0.672 [-9.000, 240.300], mean action: 3.642 [0.000, 9.000], mean observation: 31.445 [0.001, 443.500], loss: 428.843781, mae: 28.844463, mean_q: -29.311073\n",
            "  654858/10000000: episode: 3258, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -20.200, mean reward: -0.100 [-10.100, 324.900], mean action: 3.413 [0.000, 9.000], mean observation: 32.135 [0.001, 470.800], loss: 384.280853, mae: 29.145416, mean_q: -29.714485\n",
            "  655059/10000000: episode: 3259, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -763.200, mean reward: -3.797 [-381.600, 58.100], mean action: 2.806 [0.000, 10.000], mean observation: 33.035 [0.002, 509.500], loss: 261.031647, mae: 29.647150, mean_q: -30.054285\n",
            "  655260/10000000: episode: 3260, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: 541.000, mean reward: 2.692 [-9.000, 388.800], mean action: 2.577 [0.000, 9.000], mean observation: 30.162 [0.002, 515.900], loss: 476.631927, mae: 29.811291, mean_q: -29.751743\n",
            "  655461/10000000: episode: 3261, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -509.000, mean reward: -2.532 [-254.500, 90.300], mean action: 2.473 [0.000, 9.000], mean observation: 35.518 [0.002, 467.700], loss: 308.060455, mae: 29.822195, mean_q: -29.587925\n",
            "  655662/10000000: episode: 3262, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 43.200, mean reward: 0.215 [-10.000, 185.500], mean action: 2.632 [0.000, 10.000], mean observation: 24.719 [0.002, 434.600], loss: 275.923981, mae: 29.503891, mean_q: -29.455511\n",
            "  655863/10000000: episode: 3263, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -554.800, mean reward: -2.760 [-277.400, 76.200], mean action: 2.602 [0.000, 10.000], mean observation: 36.367 [0.000, 638.600], loss: 355.164337, mae: 29.451899, mean_q: -29.578701\n",
            "  656064/10000000: episode: 3264, duration: 1.770s, episode steps: 201, steps per second: 114, episode reward: -500.600, mean reward: -2.491 [-250.300, 91.200], mean action: 3.005 [0.000, 9.000], mean observation: 32.240 [0.000, 407.100], loss: 293.528259, mae: 29.245489, mean_q: -29.628523\n",
            "  656265/10000000: episode: 3265, duration: 1.691s, episode steps: 201, steps per second: 119, episode reward: -210.800, mean reward: -1.049 [-105.400, 234.000], mean action: 3.184 [0.000, 10.000], mean observation: 34.785 [0.002, 530.500], loss: 401.503021, mae: 29.644728, mean_q: -30.315598\n",
            "  656466/10000000: episode: 3266, duration: 1.722s, episode steps: 201, steps per second: 117, episode reward: -777.200, mean reward: -3.867 [-388.600, 39.600], mean action: 3.348 [0.000, 10.000], mean observation: 35.178 [0.000, 796.200], loss: 250.862213, mae: 29.934715, mean_q: -30.887302\n",
            "  656667/10000000: episode: 3267, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: -122.400, mean reward: -0.609 [-61.200, 280.000], mean action: 3.398 [0.000, 9.000], mean observation: 36.527 [0.000, 588.400], loss: 300.879456, mae: 30.745249, mean_q: -31.662178\n",
            "  656868/10000000: episode: 3268, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -1193.600, mean reward: -5.938 [-596.800, 8.100], mean action: 3.328 [0.000, 10.000], mean observation: 30.483 [0.001, 466.300], loss: 483.626617, mae: 31.013264, mean_q: -31.571598\n",
            "  657069/10000000: episode: 3269, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -113.800, mean reward: -0.566 [-56.900, 147.400], mean action: 2.990 [0.000, 10.000], mean observation: 33.965 [0.003, 438.400], loss: 341.171600, mae: 31.723793, mean_q: -32.128361\n",
            "  657270/10000000: episode: 3270, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -544.200, mean reward: -2.707 [-272.100, 58.100], mean action: 2.716 [0.000, 10.000], mean observation: 27.060 [0.002, 545.200], loss: 299.983765, mae: 31.604591, mean_q: -31.879650\n",
            "  657471/10000000: episode: 3271, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -407.000, mean reward: -2.025 [-203.500, 79.800], mean action: 2.771 [0.000, 10.000], mean observation: 36.632 [0.001, 491.000], loss: 359.124542, mae: 31.512045, mean_q: -31.803478\n",
            "  657672/10000000: episode: 3272, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: -190.600, mean reward: -0.948 [-95.300, 134.600], mean action: 2.612 [0.000, 9.000], mean observation: 32.342 [0.000, 481.400], loss: 354.605499, mae: 30.822405, mean_q: -31.215397\n",
            "  657873/10000000: episode: 3273, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -899.400, mean reward: -4.475 [-449.700, 34.000], mean action: 2.716 [0.000, 10.000], mean observation: 30.418 [0.002, 446.500], loss: 449.390533, mae: 30.674047, mean_q: -30.974062\n",
            "  658074/10000000: episode: 3274, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -646.600, mean reward: -3.217 [-323.300, 54.600], mean action: 2.716 [0.000, 10.000], mean observation: 32.749 [0.000, 746.900], loss: 234.820084, mae: 30.504765, mean_q: -30.844902\n",
            "  658275/10000000: episode: 3275, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -19.400, mean reward: -0.097 [-10.000, 153.600], mean action: 2.562 [0.000, 10.000], mean observation: 29.604 [0.000, 583.400], loss: 368.939758, mae: 30.745235, mean_q: -30.953854\n",
            "  658476/10000000: episode: 3276, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -86.400, mean reward: -0.430 [-43.200, 200.200], mean action: 2.478 [0.000, 10.000], mean observation: 32.444 [0.000, 702.100], loss: 386.460358, mae: 30.854050, mean_q: -30.817047\n",
            "  658677/10000000: episode: 3277, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: 211.800, mean reward: 1.054 [-10.000, 298.200], mean action: 1.891 [0.000, 10.000], mean observation: 35.441 [0.000, 472.400], loss: 394.962738, mae: 31.069094, mean_q: -30.531567\n",
            "  658878/10000000: episode: 3278, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -341.200, mean reward: -1.698 [-170.600, 51.600], mean action: 2.015 [0.000, 10.000], mean observation: 35.945 [0.000, 653.600], loss: 438.290344, mae: 30.468973, mean_q: -30.349630\n",
            "  659079/10000000: episode: 3279, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -425.800, mean reward: -2.118 [-212.900, 73.500], mean action: 2.090 [0.000, 10.000], mean observation: 30.811 [0.000, 509.800], loss: 336.960083, mae: 30.041767, mean_q: -30.434801\n",
            "  659280/10000000: episode: 3280, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: 23.600, mean reward: 0.117 [-10.000, 287.100], mean action: 3.124 [0.000, 10.000], mean observation: 40.418 [0.002, 630.900], loss: 415.170258, mae: 29.425842, mean_q: -29.902245\n",
            "  659481/10000000: episode: 3281, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: 301.200, mean reward: 1.499 [-10.000, 590.100], mean action: 3.736 [0.000, 10.000], mean observation: 34.935 [0.001, 591.000], loss: 490.775574, mae: 29.822336, mean_q: -30.371145\n",
            "  659682/10000000: episode: 3282, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -818.200, mean reward: -4.071 [-409.100, 37.800], mean action: 2.930 [0.000, 10.000], mean observation: 31.736 [0.001, 532.400], loss: 259.328339, mae: 29.595436, mean_q: -29.804697\n",
            "  659883/10000000: episode: 3283, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -19.200, mean reward: -0.096 [-10.000, 92.000], mean action: 2.572 [0.000, 10.000], mean observation: 33.539 [0.001, 595.600], loss: 313.901855, mae: 29.723171, mean_q: -29.939293\n",
            "  660084/10000000: episode: 3284, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: 97.400, mean reward: 0.485 [-10.000, 169.400], mean action: 2.657 [0.000, 10.000], mean observation: 33.581 [0.000, 530.000], loss: 463.055420, mae: 29.773626, mean_q: -29.662163\n",
            "  660285/10000000: episode: 3285, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -230.600, mean reward: -1.147 [-115.300, 261.900], mean action: 2.716 [0.000, 10.000], mean observation: 29.537 [0.002, 453.300], loss: 369.460480, mae: 29.413168, mean_q: -29.428673\n",
            "  660486/10000000: episode: 3286, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -235.800, mean reward: -1.173 [-117.900, 85.400], mean action: 2.468 [0.000, 10.000], mean observation: 33.089 [0.001, 504.200], loss: 353.389069, mae: 29.410137, mean_q: -29.313341\n",
            "  660687/10000000: episode: 3287, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -145.600, mean reward: -0.724 [-72.800, 180.600], mean action: 2.443 [0.000, 10.000], mean observation: 28.558 [0.003, 591.100], loss: 480.251709, mae: 29.717781, mean_q: -29.490625\n",
            "  660888/10000000: episode: 3288, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -224.600, mean reward: -1.117 [-112.300, 79.100], mean action: 1.816 [0.000, 10.000], mean observation: 36.231 [0.001, 578.100], loss: 247.714279, mae: 29.518227, mean_q: -29.076897\n",
            "  661089/10000000: episode: 3289, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -260.800, mean reward: -1.298 [-130.400, 142.200], mean action: 2.667 [0.000, 10.000], mean observation: 31.129 [0.001, 598.700], loss: 319.750427, mae: 29.076746, mean_q: -29.011335\n",
            "  661290/10000000: episode: 3290, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -189.000, mean reward: -0.940 [-94.500, 191.700], mean action: 2.577 [0.000, 10.000], mean observation: 31.300 [0.000, 305.100], loss: 361.694702, mae: 29.327814, mean_q: -29.399511\n",
            "  661491/10000000: episode: 3291, duration: 1.615s, episode steps: 201, steps per second: 124, episode reward: 707.600, mean reward: 3.520 [-10.000, 353.800], mean action: 2.741 [0.000, 10.000], mean observation: 33.022 [0.000, 540.000], loss: 278.870758, mae: 29.440388, mean_q: -29.489889\n",
            "  661692/10000000: episode: 3292, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -513.400, mean reward: -2.554 [-256.700, 102.600], mean action: 3.025 [0.000, 10.000], mean observation: 29.992 [0.000, 663.800], loss: 577.071411, mae: 28.870918, mean_q: -29.008987\n",
            "  661893/10000000: episode: 3293, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -418.200, mean reward: -2.081 [-209.100, 110.700], mean action: 2.970 [0.000, 10.000], mean observation: 31.346 [0.000, 423.600], loss: 437.955841, mae: 28.828348, mean_q: -28.634951\n",
            "  662094/10000000: episode: 3294, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -202.400, mean reward: -1.007 [-101.200, 212.100], mean action: 3.338 [0.000, 10.000], mean observation: 34.791 [0.000, 639.500], loss: 445.992889, mae: 28.596291, mean_q: -28.381514\n",
            "  662295/10000000: episode: 3295, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -836.600, mean reward: -4.162 [-418.300, 180.000], mean action: 3.502 [0.000, 9.000], mean observation: 33.357 [0.001, 619.200], loss: 290.614105, mae: 28.865894, mean_q: -28.716778\n",
            "  662496/10000000: episode: 3296, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -146.000, mean reward: -0.726 [-73.000, 91.500], mean action: 3.035 [0.000, 10.000], mean observation: 31.978 [0.001, 651.100], loss: 278.602844, mae: 28.766378, mean_q: -28.721706\n",
            "  662697/10000000: episode: 3297, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -444.600, mean reward: -2.212 [-222.300, 59.500], mean action: 2.642 [0.000, 10.000], mean observation: 39.213 [0.000, 818.100], loss: 369.082855, mae: 28.615690, mean_q: -28.606100\n",
            "  662898/10000000: episode: 3298, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -85.400, mean reward: -0.425 [-42.700, 102.500], mean action: 2.433 [0.000, 10.000], mean observation: 32.843 [0.003, 540.500], loss: 353.501160, mae: 28.309458, mean_q: -28.272797\n",
            "  663099/10000000: episode: 3299, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -763.800, mean reward: -3.800 [-381.900, 63.000], mean action: 2.886 [0.000, 10.000], mean observation: 34.329 [0.001, 565.900], loss: 330.762939, mae: 28.546530, mean_q: -28.454840\n",
            "  663300/10000000: episode: 3300, duration: 1.602s, episode steps: 201, steps per second: 125, episode reward: -838.400, mean reward: -4.171 [-419.200, 51.100], mean action: 2.970 [0.000, 10.000], mean observation: 32.255 [0.002, 412.300], loss: 299.325256, mae: 28.523582, mean_q: -28.446751\n",
            "  663501/10000000: episode: 3301, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -657.200, mean reward: -3.270 [-328.600, 117.900], mean action: 3.139 [0.000, 10.000], mean observation: 32.282 [0.000, 720.900], loss: 339.245697, mae: 28.242483, mean_q: -28.210278\n",
            "  663702/10000000: episode: 3302, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: 6.400, mean reward: 0.032 [-10.000, 283.500], mean action: 2.507 [0.000, 10.000], mean observation: 32.103 [0.002, 624.500], loss: 416.911987, mae: 28.573189, mean_q: -28.416803\n",
            "  663903/10000000: episode: 3303, duration: 1.614s, episode steps: 201, steps per second: 125, episode reward: -737.000, mean reward: -3.667 [-368.500, 68.400], mean action: 3.020 [0.000, 10.000], mean observation: 32.986 [0.003, 497.700], loss: 376.096252, mae: 27.773617, mean_q: -28.022165\n",
            "  664104/10000000: episode: 3304, duration: 1.540s, episode steps: 201, steps per second: 130, episode reward: -346.400, mean reward: -1.723 [-173.200, 132.300], mean action: 2.811 [0.000, 10.000], mean observation: 28.064 [0.001, 419.900], loss: 260.656586, mae: 27.769585, mean_q: -27.731998\n",
            "  664305/10000000: episode: 3305, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: 6.600, mean reward: 0.033 [-10.000, 168.300], mean action: 2.587 [0.000, 10.000], mean observation: 37.036 [0.000, 615.100], loss: 341.976013, mae: 28.598206, mean_q: -28.544371\n",
            "  664506/10000000: episode: 3306, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -710.400, mean reward: -3.534 [-355.200, 64.800], mean action: 2.517 [0.000, 10.000], mean observation: 36.358 [0.000, 642.700], loss: 393.461365, mae: 28.853373, mean_q: -28.493605\n",
            "  664707/10000000: episode: 3307, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: 298.000, mean reward: 1.483 [-10.000, 268.800], mean action: 2.826 [0.000, 10.000], mean observation: 34.814 [0.000, 907.100], loss: 358.910675, mae: 28.724796, mean_q: -28.522760\n",
            "  664908/10000000: episode: 3308, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: -765.400, mean reward: -3.808 [-382.700, 58.100], mean action: 2.920 [0.000, 10.000], mean observation: 32.362 [0.003, 399.700], loss: 275.925415, mae: 28.285488, mean_q: -28.389717\n",
            "  665109/10000000: episode: 3309, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -520.000, mean reward: -2.587 [-260.000, 127.800], mean action: 2.806 [0.000, 10.000], mean observation: 36.020 [0.001, 522.800], loss: 310.532990, mae: 28.465406, mean_q: -28.535904\n",
            "  665310/10000000: episode: 3310, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: 1290.000, mean reward: 6.418 [-9.000, 648.900], mean action: 3.209 [0.000, 10.000], mean observation: 30.109 [0.000, 623.900], loss: 379.314362, mae: 28.235399, mean_q: -28.554955\n",
            "  665511/10000000: episode: 3311, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -354.400, mean reward: -1.763 [-177.200, 213.000], mean action: 3.179 [0.000, 10.000], mean observation: 28.767 [0.001, 473.800], loss: 449.922882, mae: 28.548605, mean_q: -28.608715\n",
            "  665712/10000000: episode: 3312, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -779.200, mean reward: -3.877 [-389.600, 72.000], mean action: 3.169 [0.000, 10.000], mean observation: 34.200 [0.001, 542.000], loss: 440.353088, mae: 27.868858, mean_q: -27.974293\n",
            "  665913/10000000: episode: 3313, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: 292.800, mean reward: 1.457 [-10.000, 233.400], mean action: 3.095 [0.000, 10.000], mean observation: 37.749 [0.000, 781.200], loss: 277.997589, mae: 28.236317, mean_q: -28.856941\n",
            "  666114/10000000: episode: 3314, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: 341.800, mean reward: 1.700 [-10.000, 177.300], mean action: 3.164 [0.000, 10.000], mean observation: 25.368 [0.001, 492.700], loss: 334.257477, mae: 29.045650, mean_q: -29.516582\n",
            "  666315/10000000: episode: 3315, duration: 1.779s, episode steps: 201, steps per second: 113, episode reward: 56.600, mean reward: 0.282 [-10.000, 300.600], mean action: 3.920 [0.000, 10.000], mean observation: 33.008 [0.001, 488.900], loss: 448.737946, mae: 29.041496, mean_q: -29.377352\n",
            "  666516/10000000: episode: 3316, duration: 1.817s, episode steps: 201, steps per second: 111, episode reward: 1259.000, mean reward: 6.264 [-10.000, 629.500], mean action: 4.816 [0.000, 10.000], mean observation: 33.300 [0.002, 467.200], loss: 272.608551, mae: 28.331482, mean_q: -28.746012\n",
            "  666717/10000000: episode: 3317, duration: 1.877s, episode steps: 201, steps per second: 107, episode reward: -266.600, mean reward: -1.326 [-133.300, 284.200], mean action: 4.557 [0.000, 10.000], mean observation: 35.469 [0.000, 511.400], loss: 256.503937, mae: 28.179358, mean_q: -28.650444\n",
            "  666918/10000000: episode: 3318, duration: 1.771s, episode steps: 201, steps per second: 113, episode reward: -749.000, mean reward: -3.726 [-374.500, 63.000], mean action: 3.960 [0.000, 10.000], mean observation: 34.402 [0.000, 764.600], loss: 374.157928, mae: 28.026503, mean_q: -28.585150\n",
            "  667119/10000000: episode: 3319, duration: 1.757s, episode steps: 201, steps per second: 114, episode reward: -210.800, mean reward: -1.049 [-105.400, 157.000], mean action: 2.731 [0.000, 10.000], mean observation: 26.894 [0.004, 498.200], loss: 309.353851, mae: 28.428352, mean_q: -28.670498\n",
            "  667320/10000000: episode: 3320, duration: 1.688s, episode steps: 201, steps per second: 119, episode reward: -565.400, mean reward: -2.813 [-282.700, 148.400], mean action: 3.328 [0.000, 10.000], mean observation: 33.432 [0.000, 598.900], loss: 515.288147, mae: 28.135212, mean_q: -28.474281\n",
            "  667521/10000000: episode: 3321, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -809.600, mean reward: -4.028 [-404.800, 88.900], mean action: 3.567 [0.000, 10.000], mean observation: 37.364 [0.002, 508.700], loss: 455.094055, mae: 27.599167, mean_q: -27.866459\n",
            "  667722/10000000: episode: 3322, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -1168.600, mean reward: -5.814 [-584.300, 62.600], mean action: 3.358 [0.000, 10.000], mean observation: 36.320 [0.000, 792.800], loss: 329.102936, mae: 26.780403, mean_q: -27.140129\n",
            "  667923/10000000: episode: 3323, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -916.800, mean reward: -4.561 [-458.400, 47.700], mean action: 2.781 [0.000, 9.000], mean observation: 32.549 [0.000, 746.700], loss: 521.589417, mae: 26.430887, mean_q: -26.554823\n",
            "  668124/10000000: episode: 3324, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -343.000, mean reward: -1.706 [-171.500, 117.900], mean action: 3.373 [0.000, 10.000], mean observation: 34.608 [0.001, 593.600], loss: 377.061188, mae: 26.438475, mean_q: -26.766212\n",
            "  668325/10000000: episode: 3325, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: 1489.000, mean reward: 7.408 [-10.000, 1045.800], mean action: 3.502 [0.000, 10.000], mean observation: 37.351 [0.001, 541.500], loss: 307.822815, mae: 26.636427, mean_q: -27.049902\n",
            "  668526/10000000: episode: 3326, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -432.200, mean reward: -2.150 [-216.100, 117.000], mean action: 2.677 [0.000, 10.000], mean observation: 32.553 [0.000, 519.300], loss: 338.136261, mae: 26.560789, mean_q: -26.931898\n",
            "  668727/10000000: episode: 3327, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -659.000, mean reward: -3.279 [-329.500, 115.200], mean action: 3.443 [0.000, 10.000], mean observation: 28.617 [0.001, 460.100], loss: 477.918640, mae: 26.315058, mean_q: -26.785938\n",
            "  668928/10000000: episode: 3328, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: 3564.800, mean reward: 17.735 [-10.000, 2032.200], mean action: 3.985 [0.000, 10.000], mean observation: 34.413 [0.003, 566.800], loss: 571.105347, mae: 26.937294, mean_q: -27.648355\n",
            "  669129/10000000: episode: 3329, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -339.800, mean reward: -1.691 [-169.900, 97.500], mean action: 3.169 [0.000, 10.000], mean observation: 34.963 [0.000, 579.300], loss: 274.938263, mae: 27.426840, mean_q: -27.540661\n",
            "  669330/10000000: episode: 3330, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -498.800, mean reward: -2.482 [-249.400, 115.200], mean action: 2.493 [0.000, 10.000], mean observation: 26.725 [0.001, 402.600], loss: 893.701355, mae: 26.962133, mean_q: -26.222239\n",
            "  669531/10000000: episode: 3331, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: 24.200, mean reward: 0.120 [-10.000, 125.300], mean action: 1.995 [0.000, 10.000], mean observation: 31.893 [0.001, 550.200], loss: 606.001221, mae: 26.099133, mean_q: -25.232212\n",
            "  669732/10000000: episode: 3332, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -583.200, mean reward: -2.901 [-291.600, 59.400], mean action: 2.672 [0.000, 9.000], mean observation: 33.943 [0.001, 493.400], loss: 446.655701, mae: 25.738150, mean_q: -25.012556\n",
            "  669933/10000000: episode: 3333, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -482.800, mean reward: -2.402 [-241.400, 70.800], mean action: 2.662 [0.000, 9.000], mean observation: 33.315 [0.001, 494.100], loss: 357.667755, mae: 25.159840, mean_q: -24.580276\n",
            "  670134/10000000: episode: 3334, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -714.800, mean reward: -3.556 [-357.400, 29.600], mean action: 2.458 [0.000, 10.000], mean observation: 41.097 [0.000, 593.600], loss: 379.062653, mae: 25.512329, mean_q: -24.861738\n",
            "  670335/10000000: episode: 3335, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: 152.000, mean reward: 0.756 [-10.000, 204.300], mean action: 2.348 [0.000, 10.000], mean observation: 34.322 [0.000, 803.400], loss: 298.330627, mae: 26.120159, mean_q: -25.488096\n",
            "  670536/10000000: episode: 3336, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: 436.000, mean reward: 2.169 [-9.000, 381.600], mean action: 2.353 [0.000, 9.000], mean observation: 32.487 [0.001, 397.400], loss: 577.874390, mae: 25.765163, mean_q: -25.346451\n",
            "  670737/10000000: episode: 3337, duration: 1.641s, episode steps: 201, steps per second: 122, episode reward: -230.000, mean reward: -1.144 [-115.000, 115.200], mean action: 2.806 [0.000, 10.000], mean observation: 34.611 [0.002, 525.700], loss: 392.676422, mae: 25.981386, mean_q: -25.584562\n",
            "  670938/10000000: episode: 3338, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -387.000, mean reward: -1.925 [-193.500, 59.400], mean action: 2.000 [0.000, 10.000], mean observation: 30.928 [0.001, 545.700], loss: 285.327454, mae: 25.906151, mean_q: -25.330866\n",
            "  671139/10000000: episode: 3339, duration: 1.601s, episode steps: 201, steps per second: 126, episode reward: -595.800, mean reward: -2.964 [-297.900, 47.200], mean action: 2.368 [0.000, 10.000], mean observation: 35.671 [0.000, 622.400], loss: 465.695862, mae: 25.863695, mean_q: -25.367300\n",
            "  671340/10000000: episode: 3340, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: -691.000, mean reward: -3.438 [-345.500, 38.500], mean action: 2.876 [0.000, 9.000], mean observation: 34.965 [0.000, 520.500], loss: 483.512878, mae: 26.422867, mean_q: -26.162367\n",
            "  671541/10000000: episode: 3341, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: 202.200, mean reward: 1.006 [-9.000, 142.800], mean action: 2.836 [0.000, 9.000], mean observation: 36.083 [0.000, 669.400], loss: 461.603149, mae: 26.378891, mean_q: -26.241541\n",
            "  671742/10000000: episode: 3342, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -735.600, mean reward: -3.660 [-367.800, 51.100], mean action: 2.562 [0.000, 9.000], mean observation: 35.598 [0.000, 556.200], loss: 315.649902, mae: 26.618961, mean_q: -26.571629\n",
            "  671943/10000000: episode: 3343, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: 82.200, mean reward: 0.409 [-9.000, 160.200], mean action: 2.910 [0.000, 9.000], mean observation: 38.362 [0.000, 663.000], loss: 382.129578, mae: 26.705736, mean_q: -26.612288\n",
            "  672144/10000000: episode: 3344, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -468.600, mean reward: -2.331 [-234.300, 74.700], mean action: 3.030 [0.000, 9.000], mean observation: 37.548 [0.002, 515.300], loss: 458.464081, mae: 26.386150, mean_q: -26.289888\n",
            "  672345/10000000: episode: 3345, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: -429.200, mean reward: -2.135 [-214.600, 133.700], mean action: 3.323 [0.000, 9.000], mean observation: 30.449 [0.000, 637.900], loss: 314.041290, mae: 26.526823, mean_q: -26.546539\n",
            "  672546/10000000: episode: 3346, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -403.000, mean reward: -2.005 [-201.500, 108.900], mean action: 3.015 [0.000, 9.000], mean observation: 33.403 [0.001, 459.000], loss: 374.066376, mae: 26.949486, mean_q: -27.061354\n",
            "  672747/10000000: episode: 3347, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -1071.400, mean reward: -5.330 [-535.700, 32.700], mean action: 3.055 [0.000, 9.000], mean observation: 34.288 [0.001, 581.100], loss: 511.686493, mae: 27.374908, mean_q: -27.513390\n",
            "  672948/10000000: episode: 3348, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: -346.200, mean reward: -1.722 [-173.100, 204.300], mean action: 3.423 [0.000, 10.000], mean observation: 34.617 [0.001, 622.800], loss: 508.913086, mae: 27.515324, mean_q: -27.365643\n",
            "  673149/10000000: episode: 3349, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -1003.000, mean reward: -4.990 [-501.500, 46.800], mean action: 3.438 [0.000, 10.000], mean observation: 29.712 [0.000, 570.400], loss: 480.118378, mae: 27.472109, mean_q: -27.273775\n",
            "  673350/10000000: episode: 3350, duration: 1.552s, episode steps: 201, steps per second: 129, episode reward: -741.400, mean reward: -3.689 [-370.700, 121.800], mean action: 3.179 [0.000, 10.000], mean observation: 37.498 [0.001, 446.000], loss: 216.003922, mae: 27.618174, mean_q: -27.610914\n",
            "  673551/10000000: episode: 3351, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -757.000, mean reward: -3.766 [-378.500, 112.700], mean action: 2.965 [0.000, 9.000], mean observation: 33.015 [0.000, 749.600], loss: 516.106628, mae: 27.794964, mean_q: -27.849905\n",
            "  673752/10000000: episode: 3352, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -1036.000, mean reward: -5.154 [-518.000, 108.900], mean action: 4.179 [0.000, 9.000], mean observation: 27.310 [0.002, 368.100], loss: 228.307297, mae: 27.772335, mean_q: -28.021883\n",
            "  673953/10000000: episode: 3353, duration: 1.603s, episode steps: 201, steps per second: 125, episode reward: -930.800, mean reward: -4.631 [-465.400, 102.300], mean action: 3.328 [0.000, 9.000], mean observation: 35.733 [0.001, 587.600], loss: 375.003357, mae: 28.354038, mean_q: -28.923626\n",
            "  674154/10000000: episode: 3354, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -731.800, mean reward: -3.641 [-365.900, 202.100], mean action: 3.930 [0.000, 10.000], mean observation: 31.103 [0.001, 512.300], loss: 1037.364746, mae: 27.806524, mean_q: -27.933632\n",
            "  674355/10000000: episode: 3355, duration: 1.606s, episode steps: 201, steps per second: 125, episode reward: 522.000, mean reward: 2.597 [-9.000, 478.100], mean action: 4.547 [0.000, 9.000], mean observation: 34.111 [0.003, 415.200], loss: 613.806274, mae: 27.986809, mean_q: -27.709162\n",
            "  674556/10000000: episode: 3356, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -1090.800, mean reward: -5.427 [-545.400, 63.200], mean action: 3.821 [0.000, 10.000], mean observation: 30.931 [0.000, 509.100], loss: 1006.390076, mae: 27.575399, mean_q: -27.454679\n",
            "  674757/10000000: episode: 3357, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -634.000, mean reward: -3.154 [-317.000, 174.600], mean action: 3.692 [0.000, 10.000], mean observation: 34.631 [0.001, 590.900], loss: 912.346802, mae: 27.268030, mean_q: -26.958612\n",
            "  674958/10000000: episode: 3358, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -703.400, mean reward: -3.500 [-351.700, 124.200], mean action: 3.592 [0.000, 10.000], mean observation: 24.256 [0.000, 446.200], loss: 263.303925, mae: 26.847574, mean_q: -26.381962\n",
            "  675159/10000000: episode: 3359, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -321.200, mean reward: -1.598 [-160.600, 172.800], mean action: 2.667 [0.000, 10.000], mean observation: 35.597 [0.001, 504.300], loss: 296.208282, mae: 26.614597, mean_q: -26.044294\n",
            "  675360/10000000: episode: 3360, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: 573.600, mean reward: 2.854 [-9.000, 286.800], mean action: 3.169 [0.000, 10.000], mean observation: 31.015 [0.001, 510.800], loss: 376.274109, mae: 26.277670, mean_q: -25.806713\n",
            "  675561/10000000: episode: 3361, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: -547.200, mean reward: -2.722 [-273.600, 119.700], mean action: 2.637 [0.000, 10.000], mean observation: 35.430 [0.002, 487.500], loss: 264.572052, mae: 25.819700, mean_q: -25.519238\n",
            "  675762/10000000: episode: 3362, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: 692.200, mean reward: 3.444 [-10.000, 346.100], mean action: 2.448 [0.000, 10.000], mean observation: 33.298 [0.000, 637.000], loss: 350.629578, mae: 26.495968, mean_q: -26.061743\n",
            "  675963/10000000: episode: 3363, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: -533.000, mean reward: -2.652 [-266.500, 55.800], mean action: 2.279 [0.000, 10.000], mean observation: 30.914 [0.000, 640.400], loss: 659.932068, mae: 26.492701, mean_q: -25.868454\n",
            "  676164/10000000: episode: 3364, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: 576.800, mean reward: 2.870 [-10.000, 467.100], mean action: 2.781 [0.000, 10.000], mean observation: 28.688 [0.001, 413.700], loss: 512.816223, mae: 26.053038, mean_q: -25.644644\n",
            "  676365/10000000: episode: 3365, duration: 1.651s, episode steps: 201, steps per second: 122, episode reward: 36.800, mean reward: 0.183 [-9.000, 172.800], mean action: 3.731 [0.000, 9.000], mean observation: 33.864 [0.001, 512.800], loss: 709.411072, mae: 26.105883, mean_q: -25.780632\n",
            "  676566/10000000: episode: 3366, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -683.200, mean reward: -3.399 [-341.600, 116.100], mean action: 3.731 [0.000, 9.000], mean observation: 30.681 [0.000, 558.800], loss: 483.266571, mae: 25.764755, mean_q: -25.503532\n",
            "  676767/10000000: episode: 3367, duration: 1.613s, episode steps: 201, steps per second: 125, episode reward: -645.400, mean reward: -3.211 [-322.700, 52.200], mean action: 2.736 [0.000, 10.000], mean observation: 41.393 [0.001, 584.600], loss: 318.703857, mae: 25.742664, mean_q: -25.409224\n",
            "  676968/10000000: episode: 3368, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -894.800, mean reward: -4.452 [-447.400, 17.200], mean action: 2.637 [0.000, 10.000], mean observation: 33.430 [0.000, 455.400], loss: 698.519592, mae: 25.911930, mean_q: -25.375877\n",
            "  677169/10000000: episode: 3369, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: -149.200, mean reward: -0.742 [-74.600, 183.600], mean action: 2.980 [0.000, 9.000], mean observation: 34.634 [0.002, 510.200], loss: 654.576782, mae: 25.522104, mean_q: -24.888866\n",
            "  677370/10000000: episode: 3370, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -461.200, mean reward: -2.295 [-230.600, 109.800], mean action: 3.363 [0.000, 10.000], mean observation: 29.496 [0.001, 486.800], loss: 543.111511, mae: 25.825274, mean_q: -25.603456\n",
            "  677571/10000000: episode: 3371, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -711.200, mean reward: -3.538 [-355.600, 70.000], mean action: 3.607 [0.000, 10.000], mean observation: 35.106 [0.000, 708.200], loss: 371.035461, mae: 25.840525, mean_q: -25.714424\n",
            "  677772/10000000: episode: 3372, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -200.800, mean reward: -0.999 [-100.400, 122.400], mean action: 2.328 [0.000, 9.000], mean observation: 36.620 [0.002, 500.200], loss: 673.560852, mae: 25.941513, mean_q: -25.574921\n",
            "  677973/10000000: episode: 3373, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: 290.800, mean reward: 1.447 [-10.000, 199.500], mean action: 2.910 [0.000, 10.000], mean observation: 35.942 [0.000, 440.500], loss: 267.285065, mae: 25.735983, mean_q: -25.539602\n",
            "  678174/10000000: episode: 3374, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -100.600, mean reward: -0.500 [-50.300, 227.500], mean action: 3.289 [0.000, 9.000], mean observation: 34.871 [0.000, 566.800], loss: 333.230225, mae: 25.671007, mean_q: -25.460213\n",
            "  678375/10000000: episode: 3375, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -724.400, mean reward: -3.604 [-362.200, 73.200], mean action: 2.960 [0.000, 10.000], mean observation: 30.416 [0.001, 571.900], loss: 308.966339, mae: 25.776678, mean_q: -25.653622\n",
            "  678576/10000000: episode: 3376, duration: 1.549s, episode steps: 201, steps per second: 130, episode reward: -1330.200, mean reward: -6.618 [-665.100, 25.600], mean action: 3.920 [0.000, 10.000], mean observation: 30.799 [0.000, 494.100], loss: 487.577423, mae: 26.101961, mean_q: -26.033043\n",
            "  678777/10000000: episode: 3377, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -680.600, mean reward: -3.386 [-340.300, 27.400], mean action: 2.398 [0.000, 10.000], mean observation: 29.138 [0.001, 465.200], loss: 336.457855, mae: 26.286125, mean_q: -25.984657\n",
            "  678978/10000000: episode: 3378, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: 305.800, mean reward: 1.521 [-9.000, 212.800], mean action: 2.264 [0.000, 9.000], mean observation: 32.259 [0.000, 798.300], loss: 728.687317, mae: 26.368782, mean_q: -25.691780\n",
            "  679179/10000000: episode: 3379, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -836.000, mean reward: -4.159 [-418.000, 27.600], mean action: 2.731 [0.000, 9.000], mean observation: 35.162 [0.001, 556.200], loss: 581.241699, mae: 25.987436, mean_q: -25.519958\n",
            "  679380/10000000: episode: 3380, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -291.200, mean reward: -1.449 [-145.600, 201.600], mean action: 3.741 [0.000, 10.000], mean observation: 29.349 [0.000, 481.500], loss: 365.653595, mae: 26.025755, mean_q: -25.755816\n",
            "  679581/10000000: episode: 3381, duration: 1.609s, episode steps: 201, steps per second: 125, episode reward: 355.400, mean reward: 1.768 [-9.000, 353.600], mean action: 4.139 [0.000, 9.000], mean observation: 31.791 [0.000, 668.900], loss: 477.483917, mae: 25.785366, mean_q: -25.811125\n",
            "  679782/10000000: episode: 3382, duration: 1.601s, episode steps: 201, steps per second: 126, episode reward: 23.200, mean reward: 0.115 [-9.000, 254.700], mean action: 3.493 [0.000, 9.000], mean observation: 32.323 [0.000, 506.300], loss: 344.167999, mae: 25.850817, mean_q: -25.924786\n",
            "  679983/10000000: episode: 3383, duration: 1.611s, episode steps: 201, steps per second: 125, episode reward: -804.600, mean reward: -4.003 [-402.300, 101.700], mean action: 3.512 [0.000, 10.000], mean observation: 36.500 [0.000, 605.900], loss: 337.846405, mae: 26.495012, mean_q: -26.551502\n",
            "  680184/10000000: episode: 3384, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: 17.600, mean reward: 0.088 [-10.000, 120.000], mean action: 3.139 [0.000, 10.000], mean observation: 33.526 [0.000, 650.000], loss: 396.743378, mae: 26.448296, mean_q: -26.479055\n",
            "  680385/10000000: episode: 3385, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: 395.000, mean reward: 1.965 [-10.000, 630.000], mean action: 2.642 [0.000, 10.000], mean observation: 35.560 [0.000, 574.000], loss: 449.402588, mae: 26.721100, mean_q: -26.676249\n",
            "  680586/10000000: episode: 3386, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -407.800, mean reward: -2.029 [-203.900, 226.100], mean action: 2.458 [0.000, 10.000], mean observation: 29.770 [0.002, 420.100], loss: 756.425598, mae: 26.083929, mean_q: -25.732565\n",
            "  680787/10000000: episode: 3387, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: 68.000, mean reward: 0.338 [-9.000, 254.100], mean action: 2.378 [0.000, 9.000], mean observation: 39.647 [0.001, 495.500], loss: 340.264618, mae: 26.250637, mean_q: -26.023272\n",
            "  680988/10000000: episode: 3388, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: -451.600, mean reward: -2.247 [-225.800, 130.500], mean action: 2.328 [0.000, 9.000], mean observation: 33.993 [0.001, 510.400], loss: 381.279846, mae: 26.265270, mean_q: -26.085556\n",
            "  681189/10000000: episode: 3389, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: 249.200, mean reward: 1.240 [-9.000, 337.500], mean action: 2.891 [0.000, 9.000], mean observation: 34.616 [0.000, 508.900], loss: 350.818573, mae: 26.246681, mean_q: -26.200825\n",
            "  681390/10000000: episode: 3390, duration: 1.620s, episode steps: 201, steps per second: 124, episode reward: -803.400, mean reward: -3.997 [-401.700, 52.500], mean action: 2.965 [0.000, 9.000], mean observation: 27.563 [0.003, 472.800], loss: 618.733765, mae: 25.827173, mean_q: -25.842712\n",
            "  681591/10000000: episode: 3391, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -212.600, mean reward: -1.058 [-106.300, 370.400], mean action: 2.886 [0.000, 10.000], mean observation: 35.522 [0.001, 618.400], loss: 331.484924, mae: 26.129848, mean_q: -26.243164\n",
            "  681792/10000000: episode: 3392, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: 19.800, mean reward: 0.099 [-9.000, 196.000], mean action: 2.622 [0.000, 9.000], mean observation: 32.384 [0.001, 512.000], loss: 362.002594, mae: 26.215555, mean_q: -26.178297\n",
            "  681993/10000000: episode: 3393, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -505.800, mean reward: -2.516 [-252.900, 191.800], mean action: 2.975 [0.000, 10.000], mean observation: 31.946 [0.000, 586.000], loss: 491.220154, mae: 25.749023, mean_q: -25.678415\n",
            "  682194/10000000: episode: 3394, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -507.800, mean reward: -2.526 [-253.900, 76.500], mean action: 2.557 [0.000, 9.000], mean observation: 31.236 [0.002, 431.000], loss: 374.041046, mae: 25.677460, mean_q: -25.530891\n",
            "  682395/10000000: episode: 3395, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -730.200, mean reward: -3.633 [-365.100, 88.000], mean action: 3.194 [0.000, 9.000], mean observation: 29.572 [0.001, 459.200], loss: 554.862244, mae: 25.775309, mean_q: -25.600918\n",
            "  682596/10000000: episode: 3396, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -689.000, mean reward: -3.428 [-344.500, 39.200], mean action: 2.259 [0.000, 9.000], mean observation: 35.136 [0.003, 422.300], loss: 688.855347, mae: 25.711651, mean_q: -25.051308\n",
            "  682797/10000000: episode: 3397, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -61.600, mean reward: -0.306 [-30.800, 202.500], mean action: 2.373 [0.000, 9.000], mean observation: 32.441 [0.001, 497.500], loss: 316.749268, mae: 25.395821, mean_q: -24.776857\n",
            "  682998/10000000: episode: 3398, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -180.600, mean reward: -0.899 [-90.300, 91.800], mean action: 2.055 [0.000, 9.000], mean observation: 28.510 [0.003, 430.700], loss: 370.987915, mae: 25.371681, mean_q: -24.573572\n",
            "  683199/10000000: episode: 3399, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -23.400, mean reward: -0.116 [-11.700, 243.900], mean action: 2.498 [0.000, 9.000], mean observation: 33.950 [0.000, 552.400], loss: 906.232971, mae: 24.967171, mean_q: -23.956196\n",
            "  683400/10000000: episode: 3400, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: -664.800, mean reward: -3.307 [-332.400, 21.700], mean action: 2.308 [0.000, 9.000], mean observation: 34.809 [0.000, 720.900], loss: 422.128967, mae: 24.519583, mean_q: -23.627213\n",
            "  683601/10000000: episode: 3401, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: -162.000, mean reward: -0.806 [-81.000, 133.700], mean action: 2.756 [0.000, 10.000], mean observation: 29.342 [0.001, 500.800], loss: 436.634338, mae: 24.436338, mean_q: -23.571123\n",
            "  683802/10000000: episode: 3402, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: 119.400, mean reward: 0.594 [-9.000, 204.500], mean action: 2.582 [0.000, 9.000], mean observation: 33.070 [0.001, 424.400], loss: 391.189667, mae: 24.611914, mean_q: -23.845266\n",
            "  684003/10000000: episode: 3403, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: 1653.200, mean reward: 8.225 [-9.000, 826.600], mean action: 3.085 [0.000, 9.000], mean observation: 31.334 [0.000, 441.700], loss: 601.565674, mae: 24.183880, mean_q: -23.226669\n",
            "  684204/10000000: episode: 3404, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -691.800, mean reward: -3.442 [-345.900, 44.100], mean action: 2.667 [0.000, 9.000], mean observation: 29.117 [0.002, 387.100], loss: 1103.489868, mae: 23.362738, mean_q: -22.301039\n",
            "  684405/10000000: episode: 3405, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -598.400, mean reward: -2.977 [-299.200, 98.100], mean action: 2.149 [0.000, 9.000], mean observation: 36.362 [0.002, 543.500], loss: 290.583374, mae: 22.967773, mean_q: -21.849426\n",
            "  684606/10000000: episode: 3406, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -565.200, mean reward: -2.812 [-282.600, 27.000], mean action: 2.323 [0.000, 10.000], mean observation: 32.864 [0.001, 562.800], loss: 805.869080, mae: 22.758507, mean_q: -21.723862\n",
            "  684807/10000000: episode: 3407, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: 1178.000, mean reward: 5.861 [-9.000, 761.600], mean action: 2.632 [0.000, 9.000], mean observation: 34.430 [0.000, 693.300], loss: 418.556396, mae: 22.296392, mean_q: -21.467142\n",
            "  685008/10000000: episode: 3408, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -337.000, mean reward: -1.677 [-168.500, 138.600], mean action: 2.637 [0.000, 9.000], mean observation: 35.880 [0.000, 727.600], loss: 416.084900, mae: 22.701263, mean_q: -21.804659\n",
            "  685209/10000000: episode: 3409, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -886.400, mean reward: -4.410 [-443.200, 123.300], mean action: 3.269 [0.000, 9.000], mean observation: 37.385 [0.000, 580.600], loss: 489.458649, mae: 22.366848, mean_q: -21.747366\n",
            "  685410/10000000: episode: 3410, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: 501.000, mean reward: 2.493 [-10.000, 282.600], mean action: 3.020 [0.000, 10.000], mean observation: 34.027 [0.001, 606.200], loss: 379.875702, mae: 22.614326, mean_q: -21.917969\n",
            "  685611/10000000: episode: 3411, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -323.800, mean reward: -1.611 [-161.900, 266.800], mean action: 3.373 [0.000, 9.000], mean observation: 31.202 [0.001, 597.900], loss: 462.218903, mae: 22.664352, mean_q: -22.253168\n",
            "  685812/10000000: episode: 3412, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -675.200, mean reward: -3.359 [-337.600, 135.100], mean action: 2.985 [0.000, 9.000], mean observation: 31.906 [0.004, 482.900], loss: 340.564606, mae: 23.311775, mean_q: -22.753881\n",
            "  686013/10000000: episode: 3413, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 22.200, mean reward: 0.110 [-10.000, 84.700], mean action: 2.119 [0.000, 10.000], mean observation: 27.849 [0.002, 435.000], loss: 602.312805, mae: 24.277563, mean_q: -23.517151\n",
            "  686214/10000000: episode: 3414, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -622.200, mean reward: -3.096 [-311.100, 29.000], mean action: 2.214 [0.000, 10.000], mean observation: 33.118 [0.001, 697.100], loss: 297.964752, mae: 24.137054, mean_q: -23.510368\n",
            "  686415/10000000: episode: 3415, duration: 1.626s, episode steps: 201, steps per second: 124, episode reward: -488.000, mean reward: -2.428 [-244.000, 84.800], mean action: 2.423 [0.000, 10.000], mean observation: 32.355 [0.001, 562.800], loss: 301.214355, mae: 24.402332, mean_q: -24.156569\n",
            "  686616/10000000: episode: 3416, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: -918.400, mean reward: -4.569 [-459.200, 31.800], mean action: 2.572 [0.000, 9.000], mean observation: 33.246 [0.000, 458.900], loss: 542.759460, mae: 24.617319, mean_q: -24.279102\n",
            "  686817/10000000: episode: 3417, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -724.200, mean reward: -3.603 [-362.100, 65.500], mean action: 2.632 [0.000, 10.000], mean observation: 33.224 [0.002, 522.200], loss: 456.029846, mae: 24.865778, mean_q: -24.475752\n",
            "  687018/10000000: episode: 3418, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: 232.800, mean reward: 1.158 [-10.000, 203.000], mean action: 2.383 [0.000, 10.000], mean observation: 29.035 [0.000, 527.200], loss: 241.532028, mae: 25.234200, mean_q: -24.805893\n",
            "  687219/10000000: episode: 3419, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -186.400, mean reward: -0.927 [-93.200, 146.700], mean action: 2.333 [0.000, 9.000], mean observation: 34.478 [0.000, 426.600], loss: 369.600037, mae: 25.283825, mean_q: -24.935867\n",
            "  687420/10000000: episode: 3420, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -729.800, mean reward: -3.631 [-364.900, 58.500], mean action: 2.751 [0.000, 9.000], mean observation: 33.486 [0.003, 451.200], loss: 622.088684, mae: 25.496962, mean_q: -25.346127\n",
            "  687621/10000000: episode: 3421, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: -211.000, mean reward: -1.050 [-105.500, 191.100], mean action: 2.682 [0.000, 10.000], mean observation: 31.112 [0.000, 765.000], loss: 334.903290, mae: 25.463621, mean_q: -25.319307\n",
            "  687822/10000000: episode: 3422, duration: 1.602s, episode steps: 201, steps per second: 125, episode reward: -320.200, mean reward: -1.593 [-160.100, 75.000], mean action: 2.353 [0.000, 10.000], mean observation: 34.378 [0.002, 439.000], loss: 382.542908, mae: 25.692745, mean_q: -25.448631\n",
            "  688023/10000000: episode: 3423, duration: 1.603s, episode steps: 201, steps per second: 125, episode reward: 282.600, mean reward: 1.406 [-10.000, 305.200], mean action: 2.796 [0.000, 10.000], mean observation: 39.544 [0.000, 793.800], loss: 358.807281, mae: 25.698278, mean_q: -25.601374\n",
            "  688224/10000000: episode: 3424, duration: 1.699s, episode steps: 201, steps per second: 118, episode reward: -263.000, mean reward: -1.308 [-131.500, 248.400], mean action: 2.846 [0.000, 10.000], mean observation: 33.600 [0.000, 633.800], loss: 858.067993, mae: 25.584461, mean_q: -25.301476\n",
            "  688425/10000000: episode: 3425, duration: 1.626s, episode steps: 201, steps per second: 124, episode reward: -835.400, mean reward: -4.156 [-417.700, 50.400], mean action: 2.572 [0.000, 9.000], mean observation: 37.996 [0.000, 701.500], loss: 287.253265, mae: 25.077618, mean_q: -24.791477\n",
            "  688626/10000000: episode: 3426, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -226.600, mean reward: -1.127 [-113.300, 139.500], mean action: 2.473 [0.000, 9.000], mean observation: 36.407 [0.002, 542.400], loss: 335.316437, mae: 25.178705, mean_q: -24.905495\n",
            "  688827/10000000: episode: 3427, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: 539.200, mean reward: 2.683 [-10.000, 269.600], mean action: 2.517 [0.000, 10.000], mean observation: 36.294 [0.000, 783.800], loss: 829.238647, mae: 24.960199, mean_q: -24.778305\n",
            "  689028/10000000: episode: 3428, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: -507.000, mean reward: -2.522 [-253.500, 93.800], mean action: 3.458 [0.000, 10.000], mean observation: 36.312 [0.002, 515.200], loss: 644.712463, mae: 24.686125, mean_q: -24.415140\n",
            "  689229/10000000: episode: 3429, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -451.800, mean reward: -2.248 [-225.900, 75.600], mean action: 2.557 [0.000, 10.000], mean observation: 34.625 [0.003, 381.500], loss: 732.438354, mae: 24.415457, mean_q: -23.872398\n",
            "  689430/10000000: episode: 3430, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -549.200, mean reward: -2.732 [-274.600, 140.500], mean action: 2.493 [0.000, 9.000], mean observation: 38.262 [0.000, 673.700], loss: 273.448700, mae: 24.280857, mean_q: -23.787411\n",
            "  689631/10000000: episode: 3431, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -272.400, mean reward: -1.355 [-136.200, 74.000], mean action: 2.398 [0.000, 10.000], mean observation: 33.552 [0.002, 446.200], loss: 682.102295, mae: 24.206491, mean_q: -23.848230\n",
            "  689832/10000000: episode: 3432, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -164.000, mean reward: -0.816 [-82.000, 242.100], mean action: 2.562 [0.000, 9.000], mean observation: 37.067 [0.001, 459.000], loss: 318.756897, mae: 24.236950, mean_q: -24.077692\n",
            "  690033/10000000: episode: 3433, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -502.200, mean reward: -2.499 [-251.100, 188.000], mean action: 2.612 [0.000, 10.000], mean observation: 33.787 [0.001, 558.800], loss: 553.716980, mae: 24.106012, mean_q: -23.878212\n",
            "  690234/10000000: episode: 3434, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: 52.200, mean reward: 0.260 [-10.000, 250.400], mean action: 2.453 [0.000, 10.000], mean observation: 34.403 [0.000, 933.200], loss: 331.220764, mae: 24.301935, mean_q: -24.141052\n",
            "  690435/10000000: episode: 3435, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -372.600, mean reward: -1.854 [-186.300, 70.000], mean action: 2.811 [0.000, 10.000], mean observation: 31.330 [0.000, 684.900], loss: 334.975555, mae: 23.990463, mean_q: -24.039009\n",
            "  690636/10000000: episode: 3436, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -673.800, mean reward: -3.352 [-336.900, 87.600], mean action: 3.194 [0.000, 10.000], mean observation: 30.184 [0.004, 507.800], loss: 295.892578, mae: 24.146839, mean_q: -24.297686\n",
            "  690837/10000000: episode: 3437, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 367.200, mean reward: 1.827 [-10.000, 429.300], mean action: 2.836 [0.000, 10.000], mean observation: 33.674 [0.000, 556.700], loss: 328.846191, mae: 24.476170, mean_q: -24.579153\n",
            "  691038/10000000: episode: 3438, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 57.400, mean reward: 0.286 [-10.000, 450.000], mean action: 3.343 [0.000, 10.000], mean observation: 38.833 [0.000, 607.900], loss: 624.073364, mae: 24.229712, mean_q: -24.567188\n",
            "  691239/10000000: episode: 3439, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -647.200, mean reward: -3.220 [-323.600, 168.600], mean action: 3.502 [0.000, 10.000], mean observation: 32.477 [0.001, 423.500], loss: 710.856018, mae: 24.254528, mean_q: -24.500851\n",
            "  691440/10000000: episode: 3440, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -623.600, mean reward: -3.102 [-311.800, 137.000], mean action: 2.950 [0.000, 10.000], mean observation: 35.869 [0.000, 528.400], loss: 400.039062, mae: 24.040670, mean_q: -24.175508\n",
            "  691641/10000000: episode: 3441, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: 1588.400, mean reward: 7.902 [-10.000, 794.200], mean action: 3.861 [0.000, 10.000], mean observation: 29.260 [0.003, 464.200], loss: 406.040100, mae: 24.295063, mean_q: -24.396801\n",
            "  691842/10000000: episode: 3442, duration: 1.658s, episode steps: 201, steps per second: 121, episode reward: -481.000, mean reward: -2.393 [-240.500, 82.800], mean action: 2.612 [0.000, 10.000], mean observation: 31.330 [0.000, 818.500], loss: 383.828583, mae: 24.125851, mean_q: -24.059843\n",
            "  692043/10000000: episode: 3443, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -831.200, mean reward: -4.135 [-415.600, 20.000], mean action: 2.692 [0.000, 10.000], mean observation: 29.785 [0.002, 441.900], loss: 550.206848, mae: 24.035904, mean_q: -23.933796\n",
            "  692244/10000000: episode: 3444, duration: 1.632s, episode steps: 201, steps per second: 123, episode reward: 315.800, mean reward: 1.571 [-10.000, 225.000], mean action: 2.776 [0.000, 10.000], mean observation: 35.899 [0.001, 619.000], loss: 338.354553, mae: 23.787640, mean_q: -23.636641\n",
            "  692445/10000000: episode: 3445, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -566.800, mean reward: -2.820 [-283.400, 107.700], mean action: 2.602 [0.000, 10.000], mean observation: 34.878 [0.002, 402.500], loss: 440.199554, mae: 24.354441, mean_q: -24.237783\n",
            "  692646/10000000: episode: 3446, duration: 1.670s, episode steps: 201, steps per second: 120, episode reward: -488.200, mean reward: -2.429 [-244.100, 62.900], mean action: 2.756 [0.000, 10.000], mean observation: 30.114 [0.000, 500.900], loss: 544.064819, mae: 24.402531, mean_q: -24.181072\n",
            "  692847/10000000: episode: 3447, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -526.200, mean reward: -2.618 [-263.100, 81.000], mean action: 2.478 [0.000, 10.000], mean observation: 39.337 [0.001, 501.600], loss: 695.086914, mae: 24.293127, mean_q: -24.137102\n",
            "  693048/10000000: episode: 3448, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -413.000, mean reward: -2.055 [-206.500, 37.600], mean action: 1.910 [0.000, 10.000], mean observation: 31.838 [0.001, 590.900], loss: 565.949829, mae: 24.450825, mean_q: -24.024015\n",
            "  693249/10000000: episode: 3449, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -569.400, mean reward: -2.833 [-284.700, 43.200], mean action: 2.104 [0.000, 10.000], mean observation: 33.435 [0.000, 747.100], loss: 480.411316, mae: 24.157526, mean_q: -23.957039\n",
            "  693450/10000000: episode: 3450, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -254.800, mean reward: -1.268 [-127.400, 112.800], mean action: 2.100 [0.000, 10.000], mean observation: 35.553 [0.001, 502.600], loss: 266.091736, mae: 23.995604, mean_q: -23.828146\n",
            "  693651/10000000: episode: 3451, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: 297.400, mean reward: 1.480 [-10.000, 216.800], mean action: 2.075 [0.000, 10.000], mean observation: 28.910 [0.000, 534.800], loss: 349.236877, mae: 24.284710, mean_q: -24.068190\n",
            "  693852/10000000: episode: 3452, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -85.200, mean reward: -0.424 [-42.600, 150.400], mean action: 1.657 [0.000, 10.000], mean observation: 28.567 [0.000, 580.300], loss: 561.385437, mae: 24.471020, mean_q: -24.038141\n",
            "  694053/10000000: episode: 3453, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -415.000, mean reward: -2.065 [-207.500, 38.600], mean action: 2.104 [0.000, 10.000], mean observation: 36.765 [0.000, 522.100], loss: 380.170685, mae: 24.491253, mean_q: -24.191673\n",
            "  694254/10000000: episode: 3454, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: 7.600, mean reward: 0.038 [-10.000, 165.600], mean action: 2.184 [0.000, 10.000], mean observation: 29.430 [0.001, 589.600], loss: 990.524902, mae: 24.335434, mean_q: -24.104801\n",
            "  694455/10000000: episode: 3455, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: -607.200, mean reward: -3.021 [-303.600, 27.000], mean action: 2.527 [0.000, 10.000], mean observation: 33.243 [0.000, 813.800], loss: 498.892578, mae: 24.080542, mean_q: -23.909403\n",
            "  694656/10000000: episode: 3456, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: 380.400, mean reward: 1.893 [-10.000, 287.000], mean action: 2.930 [0.000, 10.000], mean observation: 39.719 [0.000, 621.900], loss: 276.048065, mae: 23.798944, mean_q: -23.886757\n",
            "  694857/10000000: episode: 3457, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -425.800, mean reward: -2.118 [-212.900, 278.100], mean action: 3.418 [0.000, 10.000], mean observation: 28.173 [0.002, 632.400], loss: 356.007202, mae: 23.986086, mean_q: -24.173311\n",
            "  695058/10000000: episode: 3458, duration: 1.669s, episode steps: 201, steps per second: 120, episode reward: -981.800, mean reward: -4.885 [-490.900, 49.800], mean action: 3.229 [0.000, 9.000], mean observation: 35.200 [0.002, 519.900], loss: 524.756897, mae: 23.770098, mean_q: -23.983976\n",
            "  695259/10000000: episode: 3459, duration: 1.783s, episode steps: 201, steps per second: 113, episode reward: -1077.200, mean reward: -5.359 [-538.600, 16.800], mean action: 3.313 [0.000, 10.000], mean observation: 35.526 [0.001, 518.800], loss: 365.619629, mae: 24.127239, mean_q: -24.404982\n",
            "  695460/10000000: episode: 3460, duration: 1.773s, episode steps: 201, steps per second: 113, episode reward: 28.400, mean reward: 0.141 [-10.000, 237.600], mean action: 3.647 [0.000, 10.000], mean observation: 39.955 [0.001, 490.400], loss: 455.639526, mae: 24.449558, mean_q: -24.871386\n",
            "  695661/10000000: episode: 3461, duration: 1.735s, episode steps: 201, steps per second: 116, episode reward: 1141.000, mean reward: 5.677 [-10.000, 570.500], mean action: 3.154 [0.000, 10.000], mean observation: 36.395 [0.001, 627.200], loss: 292.312592, mae: 25.038538, mean_q: -25.561272\n",
            "  695862/10000000: episode: 3462, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 594.600, mean reward: 2.958 [-10.000, 555.300], mean action: 3.085 [0.000, 10.000], mean observation: 39.818 [0.001, 501.700], loss: 821.714417, mae: 25.200232, mean_q: -25.374245\n",
            "  696063/10000000: episode: 3463, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -379.000, mean reward: -1.886 [-189.500, 239.400], mean action: 3.647 [0.000, 10.000], mean observation: 31.904 [0.000, 604.500], loss: 373.555359, mae: 25.112446, mean_q: -25.337929\n",
            "  696264/10000000: episode: 3464, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -361.000, mean reward: -1.796 [-180.500, 84.900], mean action: 2.826 [0.000, 10.000], mean observation: 31.563 [0.001, 464.600], loss: 457.033752, mae: 25.573599, mean_q: -25.699430\n",
            "  696465/10000000: episode: 3465, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: -240.800, mean reward: -1.198 [-120.400, 177.000], mean action: 2.950 [0.000, 10.000], mean observation: 26.374 [0.001, 436.000], loss: 448.742920, mae: 25.391485, mean_q: -25.506695\n",
            "  696666/10000000: episode: 3466, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -663.200, mean reward: -3.300 [-331.600, 151.200], mean action: 3.219 [0.000, 10.000], mean observation: 37.563 [0.001, 531.100], loss: 685.052612, mae: 25.138760, mean_q: -25.278509\n",
            "  696867/10000000: episode: 3467, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -664.400, mean reward: -3.305 [-332.200, 37.000], mean action: 2.751 [0.000, 10.000], mean observation: 33.314 [0.000, 467.700], loss: 367.670502, mae: 25.485706, mean_q: -25.766960\n",
            "  697068/10000000: episode: 3468, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 497.200, mean reward: 2.474 [-10.000, 548.000], mean action: 3.060 [0.000, 10.000], mean observation: 39.612 [0.000, 784.200], loss: 661.331482, mae: 25.707813, mean_q: -25.885422\n",
            "  697269/10000000: episode: 3469, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: 11.400, mean reward: 0.057 [-10.000, 308.700], mean action: 2.453 [0.000, 10.000], mean observation: 30.992 [0.000, 507.700], loss: 267.672699, mae: 25.914639, mean_q: -26.257210\n",
            "  697470/10000000: episode: 3470, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -731.200, mean reward: -3.638 [-365.600, 50.400], mean action: 2.423 [0.000, 10.000], mean observation: 34.064 [0.001, 542.200], loss: 318.574097, mae: 26.067839, mean_q: -26.329422\n",
            "  697671/10000000: episode: 3471, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -642.400, mean reward: -3.196 [-321.200, 119.700], mean action: 3.289 [0.000, 10.000], mean observation: 31.191 [0.000, 527.300], loss: 369.210693, mae: 26.226921, mean_q: -26.560863\n",
            "  697872/10000000: episode: 3472, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -778.600, mean reward: -3.874 [-389.300, 51.500], mean action: 2.980 [0.000, 10.000], mean observation: 30.992 [0.001, 542.400], loss: 380.239868, mae: 26.088701, mean_q: -26.451706\n",
            "  698073/10000000: episode: 3473, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -611.600, mean reward: -3.043 [-305.800, 138.600], mean action: 2.876 [0.000, 10.000], mean observation: 39.180 [0.000, 694.400], loss: 299.207916, mae: 26.059891, mean_q: -26.389158\n",
            "  698274/10000000: episode: 3474, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -797.400, mean reward: -3.967 [-398.700, 30.000], mean action: 2.488 [0.000, 10.000], mean observation: 36.849 [0.000, 598.300], loss: 395.897491, mae: 26.050562, mean_q: -26.178890\n",
            "  698475/10000000: episode: 3475, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -135.400, mean reward: -0.674 [-67.700, 220.500], mean action: 2.980 [0.000, 10.000], mean observation: 29.973 [0.001, 555.700], loss: 421.148010, mae: 25.785875, mean_q: -25.818344\n",
            "  698676/10000000: episode: 3476, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -155.400, mean reward: -0.773 [-77.700, 329.700], mean action: 2.736 [0.000, 10.000], mean observation: 31.400 [0.000, 669.900], loss: 339.203369, mae: 26.159105, mean_q: -26.377682\n",
            "  698877/10000000: episode: 3477, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: -373.200, mean reward: -1.857 [-186.600, 114.800], mean action: 2.194 [0.000, 10.000], mean observation: 43.443 [0.001, 662.900], loss: 371.945892, mae: 26.617727, mean_q: -26.804174\n",
            "  699078/10000000: episode: 3478, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: 477.600, mean reward: 2.376 [-10.000, 333.900], mean action: 3.488 [0.000, 10.000], mean observation: 32.202 [0.002, 441.800], loss: 657.308167, mae: 26.590723, mean_q: -27.080194\n",
            "  699279/10000000: episode: 3479, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: 2475.400, mean reward: 12.315 [-10.000, 1237.700], mean action: 3.403 [0.000, 10.000], mean observation: 37.000 [0.001, 482.600], loss: 578.790283, mae: 26.916166, mean_q: -27.389809\n",
            "  699480/10000000: episode: 3480, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: 740.600, mean reward: 3.685 [-10.000, 370.300], mean action: 3.393 [0.000, 10.000], mean observation: 34.984 [0.000, 654.700], loss: 300.448639, mae: 27.042984, mean_q: -27.501905\n",
            "  699681/10000000: episode: 3481, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -813.800, mean reward: -4.049 [-406.900, 49.700], mean action: 3.169 [0.000, 10.000], mean observation: 32.785 [0.000, 706.400], loss: 548.981628, mae: 27.004047, mean_q: -27.465504\n",
            "  699882/10000000: episode: 3482, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -670.200, mean reward: -3.334 [-335.100, 51.600], mean action: 2.537 [0.000, 10.000], mean observation: 33.361 [0.001, 480.700], loss: 469.960175, mae: 26.986221, mean_q: -27.084936\n",
            "  700083/10000000: episode: 3483, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -820.200, mean reward: -4.081 [-410.100, 50.000], mean action: 3.478 [0.000, 10.000], mean observation: 32.019 [0.001, 463.400], loss: 619.551941, mae: 27.360716, mean_q: -27.625589\n",
            "  700284/10000000: episode: 3484, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -405.200, mean reward: -2.016 [-202.600, 163.600], mean action: 3.602 [0.000, 10.000], mean observation: 31.492 [0.001, 676.900], loss: 277.901337, mae: 27.143419, mean_q: -27.678373\n",
            "  700485/10000000: episode: 3485, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -410.600, mean reward: -2.043 [-205.300, 118.300], mean action: 3.682 [0.000, 10.000], mean observation: 37.255 [0.001, 714.300], loss: 452.260773, mae: 26.960785, mean_q: -27.665369\n",
            "  700686/10000000: episode: 3486, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -784.400, mean reward: -3.902 [-392.200, 138.600], mean action: 3.711 [0.000, 10.000], mean observation: 38.031 [0.000, 601.500], loss: 331.758362, mae: 27.357002, mean_q: -28.284412\n",
            "  700887/10000000: episode: 3487, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -563.400, mean reward: -2.803 [-281.700, 99.300], mean action: 4.493 [0.000, 10.000], mean observation: 32.124 [0.001, 582.800], loss: 578.372803, mae: 27.291906, mean_q: -28.090698\n",
            "  701088/10000000: episode: 3488, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: -1203.400, mean reward: -5.987 [-601.700, 76.300], mean action: 4.502 [0.000, 10.000], mean observation: 37.539 [0.000, 566.700], loss: 396.971771, mae: 27.248446, mean_q: -27.965260\n",
            "  701289/10000000: episode: 3489, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -604.200, mean reward: -3.006 [-302.100, 74.400], mean action: 3.697 [0.000, 10.000], mean observation: 27.615 [0.002, 508.500], loss: 410.236755, mae: 26.939594, mean_q: -27.654322\n",
            "  701490/10000000: episode: 3490, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: 609.000, mean reward: 3.030 [-10.000, 304.500], mean action: 3.532 [0.000, 10.000], mean observation: 29.590 [0.002, 499.200], loss: 405.043610, mae: 27.396114, mean_q: -28.245182\n",
            "  701691/10000000: episode: 3491, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: 194.600, mean reward: 0.968 [-10.000, 558.600], mean action: 2.980 [0.000, 10.000], mean observation: 33.283 [0.000, 376.900], loss: 240.978546, mae: 27.775330, mean_q: -28.425470\n",
            "  701892/10000000: episode: 3492, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -737.200, mean reward: -3.668 [-368.600, 20.300], mean action: 2.184 [0.000, 10.000], mean observation: 38.635 [0.001, 630.500], loss: 192.939941, mae: 28.106596, mean_q: -28.270962\n",
            "  702093/10000000: episode: 3493, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: 1113.400, mean reward: 5.539 [-10.000, 792.600], mean action: 2.940 [0.000, 10.000], mean observation: 29.659 [0.001, 518.600], loss: 491.558807, mae: 27.351053, mean_q: -27.465525\n",
            "  702294/10000000: episode: 3494, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: 1202.200, mean reward: 5.981 [-10.000, 999.900], mean action: 3.542 [0.000, 10.000], mean observation: 37.711 [0.000, 562.200], loss: 893.768982, mae: 26.794342, mean_q: -27.169941\n",
            "  702495/10000000: episode: 3495, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: 56.800, mean reward: 0.283 [-10.000, 171.900], mean action: 3.836 [0.000, 10.000], mean observation: 29.928 [0.002, 536.400], loss: 609.203796, mae: 26.963051, mean_q: -27.763638\n",
            "  702696/10000000: episode: 3496, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -838.800, mean reward: -4.173 [-419.400, 109.800], mean action: 3.174 [0.000, 10.000], mean observation: 33.927 [0.002, 637.200], loss: 659.300354, mae: 27.339832, mean_q: -28.192789\n",
            "  702897/10000000: episode: 3497, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: 67.000, mean reward: 0.333 [-10.000, 148.500], mean action: 3.045 [0.000, 10.000], mean observation: 27.651 [0.003, 607.700], loss: 564.210266, mae: 27.383862, mean_q: -27.548172\n",
            "  703098/10000000: episode: 3498, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -164.600, mean reward: -0.819 [-82.300, 114.600], mean action: 2.746 [0.000, 10.000], mean observation: 29.093 [0.001, 497.800], loss: 390.280426, mae: 27.521111, mean_q: -27.653753\n",
            "  703299/10000000: episode: 3499, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -651.800, mean reward: -3.243 [-325.900, 83.500], mean action: 3.189 [0.000, 10.000], mean observation: 35.257 [0.001, 462.800], loss: 492.756836, mae: 27.031775, mean_q: -27.433578\n",
            "  703500/10000000: episode: 3500, duration: 1.564s, episode steps: 201, steps per second: 128, episode reward: -462.000, mean reward: -2.299 [-231.000, 175.000], mean action: 4.378 [0.000, 10.000], mean observation: 39.600 [0.000, 772.000], loss: 682.554443, mae: 26.205729, mean_q: -27.106153\n",
            "  703701/10000000: episode: 3501, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: 454.600, mean reward: 2.262 [-10.000, 266.700], mean action: 3.866 [0.000, 10.000], mean observation: 27.708 [0.000, 471.500], loss: 583.280823, mae: 26.443605, mean_q: -27.336494\n",
            "  703902/10000000: episode: 3502, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: 98.800, mean reward: 0.492 [-10.000, 234.000], mean action: 2.498 [0.000, 10.000], mean observation: 32.952 [0.000, 558.800], loss: 337.245178, mae: 26.797342, mean_q: -26.896635\n",
            "  704103/10000000: episode: 3503, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -36.600, mean reward: -0.182 [-18.300, 118.400], mean action: 3.189 [0.000, 10.000], mean observation: 33.774 [0.001, 645.600], loss: 446.860321, mae: 26.852802, mean_q: -27.270172\n",
            "  704304/10000000: episode: 3504, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -711.800, mean reward: -3.541 [-355.900, 112.800], mean action: 2.950 [0.000, 10.000], mean observation: 33.241 [0.001, 487.300], loss: 389.646973, mae: 27.539248, mean_q: -27.695498\n",
            "  704505/10000000: episode: 3505, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: 486.600, mean reward: 2.421 [-10.000, 243.300], mean action: 3.358 [0.000, 10.000], mean observation: 31.524 [0.000, 555.400], loss: 936.390381, mae: 26.907640, mean_q: -27.222185\n",
            "  704706/10000000: episode: 3506, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -877.600, mean reward: -4.366 [-438.800, 240.800], mean action: 3.970 [0.000, 10.000], mean observation: 35.816 [0.000, 553.100], loss: 294.132263, mae: 26.891052, mean_q: -27.162807\n",
            "  704907/10000000: episode: 3507, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -865.600, mean reward: -4.306 [-432.800, 88.500], mean action: 3.886 [0.000, 10.000], mean observation: 34.035 [0.000, 668.100], loss: 360.485474, mae: 27.412561, mean_q: -27.837973\n",
            "  705108/10000000: episode: 3508, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: -1234.400, mean reward: -6.141 [-617.200, 24.500], mean action: 3.587 [0.000, 10.000], mean observation: 35.125 [0.000, 489.400], loss: 414.547119, mae: 27.719702, mean_q: -28.150557\n",
            "  705309/10000000: episode: 3509, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -927.200, mean reward: -4.613 [-463.600, 95.400], mean action: 4.040 [0.000, 10.000], mean observation: 29.158 [0.000, 513.300], loss: 767.851318, mae: 27.441656, mean_q: -28.053347\n",
            "  705510/10000000: episode: 3510, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -247.400, mean reward: -1.231 [-123.700, 196.200], mean action: 3.920 [0.000, 10.000], mean observation: 33.306 [0.000, 587.900], loss: 323.868317, mae: 27.810171, mean_q: -28.665306\n",
            "  705711/10000000: episode: 3511, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -192.200, mean reward: -0.956 [-96.100, 323.400], mean action: 3.562 [0.000, 10.000], mean observation: 35.546 [0.000, 606.600], loss: 523.038025, mae: 28.129446, mean_q: -28.920639\n",
            "  705912/10000000: episode: 3512, duration: 1.701s, episode steps: 201, steps per second: 118, episode reward: -487.600, mean reward: -2.426 [-243.800, 308.500], mean action: 3.323 [0.000, 10.000], mean observation: 34.988 [0.001, 578.200], loss: 571.582031, mae: 28.457430, mean_q: -28.986504\n",
            "  706113/10000000: episode: 3513, duration: 1.834s, episode steps: 201, steps per second: 110, episode reward: -646.600, mean reward: -3.217 [-323.300, 51.100], mean action: 2.871 [0.000, 10.000], mean observation: 29.745 [0.003, 461.100], loss: 512.623535, mae: 28.632837, mean_q: -28.943415\n",
            "  706314/10000000: episode: 3514, duration: 1.800s, episode steps: 201, steps per second: 112, episode reward: -599.400, mean reward: -2.982 [-299.700, 39.200], mean action: 2.701 [0.000, 10.000], mean observation: 37.307 [0.000, 497.600], loss: 270.921143, mae: 28.811443, mean_q: -29.326181\n",
            "  706515/10000000: episode: 3515, duration: 1.730s, episode steps: 201, steps per second: 116, episode reward: -559.400, mean reward: -2.783 [-279.700, 136.800], mean action: 2.965 [0.000, 10.000], mean observation: 34.922 [0.001, 675.300], loss: 515.730896, mae: 29.266085, mean_q: -29.647245\n",
            "  706716/10000000: episode: 3516, duration: 1.746s, episode steps: 201, steps per second: 115, episode reward: -671.200, mean reward: -3.339 [-335.600, 237.000], mean action: 3.950 [0.000, 10.000], mean observation: 31.841 [0.001, 605.800], loss: 283.235565, mae: 28.960262, mean_q: -29.725695\n",
            "  706917/10000000: episode: 3517, duration: 1.720s, episode steps: 201, steps per second: 117, episode reward: -815.800, mean reward: -4.059 [-407.900, 84.000], mean action: 3.726 [0.000, 10.000], mean observation: 28.992 [0.000, 527.500], loss: 443.875244, mae: 28.833529, mean_q: -29.487604\n",
            "  707118/10000000: episode: 3518, duration: 1.654s, episode steps: 201, steps per second: 122, episode reward: -570.600, mean reward: -2.839 [-285.300, 129.000], mean action: 3.701 [0.000, 10.000], mean observation: 31.384 [0.001, 466.900], loss: 418.993195, mae: 28.880045, mean_q: -29.783522\n",
            "  707319/10000000: episode: 3519, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -410.200, mean reward: -2.041 [-205.100, 140.500], mean action: 3.413 [0.000, 10.000], mean observation: 36.588 [0.000, 544.700], loss: 650.471558, mae: 28.795475, mean_q: -29.595634\n",
            "  707520/10000000: episode: 3520, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: 416.600, mean reward: 2.073 [-10.000, 291.600], mean action: 4.274 [0.000, 10.000], mean observation: 37.323 [0.002, 532.900], loss: 486.635651, mae: 28.947540, mean_q: -29.735056\n",
            "  707721/10000000: episode: 3521, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: 61.000, mean reward: 0.303 [-10.000, 710.100], mean action: 4.184 [0.000, 10.000], mean observation: 33.233 [0.001, 647.400], loss: 286.461334, mae: 29.039194, mean_q: -29.854580\n",
            "  707922/10000000: episode: 3522, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -710.000, mean reward: -3.532 [-355.000, 173.000], mean action: 3.945 [0.000, 10.000], mean observation: 31.581 [0.000, 602.400], loss: 488.661102, mae: 29.639091, mean_q: -30.470568\n",
            "  708123/10000000: episode: 3523, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -447.000, mean reward: -2.224 [-223.500, 156.000], mean action: 3.677 [0.000, 10.000], mean observation: 33.843 [0.002, 453.500], loss: 362.880402, mae: 29.330521, mean_q: -30.224195\n",
            "  708324/10000000: episode: 3524, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -59.200, mean reward: -0.295 [-29.600, 143.400], mean action: 3.577 [0.000, 10.000], mean observation: 28.648 [0.001, 458.100], loss: 453.567047, mae: 29.606373, mean_q: -30.364326\n",
            "  708525/10000000: episode: 3525, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: 725.400, mean reward: 3.609 [-10.000, 575.000], mean action: 4.055 [0.000, 10.000], mean observation: 33.047 [0.000, 695.400], loss: 422.192780, mae: 29.541786, mean_q: -30.397938\n",
            "  708726/10000000: episode: 3526, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: 863.000, mean reward: 4.294 [-10.000, 431.500], mean action: 3.697 [0.000, 10.000], mean observation: 34.313 [0.000, 660.900], loss: 390.535278, mae: 30.226805, mean_q: -31.255835\n",
            "  708927/10000000: episode: 3527, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: 534.600, mean reward: 2.660 [-10.000, 318.000], mean action: 3.234 [0.000, 10.000], mean observation: 35.000 [0.001, 472.200], loss: 545.890686, mae: 30.449884, mean_q: -31.147345\n",
            "  709128/10000000: episode: 3528, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: 934.200, mean reward: 4.648 [-10.000, 467.100], mean action: 2.478 [0.000, 10.000], mean observation: 33.685 [0.000, 446.900], loss: 617.515991, mae: 30.020266, mean_q: -30.586418\n",
            "  709329/10000000: episode: 3529, duration: 1.602s, episode steps: 201, steps per second: 126, episode reward: -1360.200, mean reward: -6.767 [-680.100, 41.100], mean action: 4.194 [0.000, 10.000], mean observation: 33.939 [0.001, 654.600], loss: 417.730957, mae: 28.920118, mean_q: -29.982855\n",
            "  709530/10000000: episode: 3530, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: 377.200, mean reward: 1.877 [-10.000, 340.900], mean action: 3.512 [0.000, 10.000], mean observation: 28.462 [0.001, 466.800], loss: 397.716187, mae: 29.935970, mean_q: -30.862926\n",
            "  709731/10000000: episode: 3531, duration: 1.639s, episode steps: 201, steps per second: 123, episode reward: -770.800, mean reward: -3.835 [-385.400, 153.600], mean action: 3.358 [0.000, 10.000], mean observation: 35.931 [0.000, 467.400], loss: 684.491760, mae: 29.798182, mean_q: -30.435955\n",
            "  709932/10000000: episode: 3532, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -797.600, mean reward: -3.968 [-398.800, 68.400], mean action: 3.851 [0.000, 10.000], mean observation: 34.867 [0.000, 697.600], loss: 445.610901, mae: 28.916182, mean_q: -29.738749\n",
            "  710133/10000000: episode: 3533, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -730.200, mean reward: -3.633 [-365.100, 114.500], mean action: 3.592 [0.000, 10.000], mean observation: 33.602 [0.000, 443.500], loss: 767.076965, mae: 29.171600, mean_q: -30.150560\n",
            "  710334/10000000: episode: 3534, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -998.600, mean reward: -4.968 [-499.300, 81.900], mean action: 3.801 [0.000, 10.000], mean observation: 30.573 [0.001, 470.800], loss: 528.676147, mae: 28.660503, mean_q: -29.883467\n",
            "  710535/10000000: episode: 3535, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -364.800, mean reward: -1.815 [-182.400, 252.700], mean action: 4.303 [0.000, 10.000], mean observation: 30.734 [0.003, 333.700], loss: 513.325806, mae: 28.911638, mean_q: -30.223080\n",
            "  710736/10000000: episode: 3536, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -1215.400, mean reward: -6.047 [-607.700, 71.100], mean action: 4.303 [0.000, 10.000], mean observation: 32.154 [0.002, 515.900], loss: 378.000458, mae: 30.046919, mean_q: -31.153028\n",
            "  710937/10000000: episode: 3537, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -896.400, mean reward: -4.460 [-448.200, 112.000], mean action: 3.632 [0.000, 10.000], mean observation: 34.652 [0.002, 454.800], loss: 329.354156, mae: 30.986727, mean_q: -31.641281\n",
            "  711138/10000000: episode: 3538, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: 379.800, mean reward: 1.890 [-10.000, 451.500], mean action: 3.348 [0.000, 10.000], mean observation: 32.738 [0.002, 467.700], loss: 539.652222, mae: 30.752550, mean_q: -31.351076\n",
            "  711339/10000000: episode: 3539, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: -410.800, mean reward: -2.044 [-205.400, 79.500], mean action: 3.493 [0.000, 10.000], mean observation: 23.775 [0.000, 497.400], loss: 359.721252, mae: 30.749084, mean_q: -31.457836\n",
            "  711540/10000000: episode: 3540, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -770.800, mean reward: -3.835 [-385.400, 129.000], mean action: 3.706 [0.000, 10.000], mean observation: 39.789 [0.000, 638.600], loss: 858.893127, mae: 30.188431, mean_q: -30.864136\n",
            "  711741/10000000: episode: 3541, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -1509.200, mean reward: -7.508 [-754.600, 86.000], mean action: 5.502 [0.000, 10.000], mean observation: 31.787 [0.000, 530.500], loss: 617.563538, mae: 30.315733, mean_q: -30.922842\n",
            "  711942/10000000: episode: 3542, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -251.200, mean reward: -1.250 [-125.600, 205.800], mean action: 3.940 [0.000, 10.000], mean observation: 31.083 [0.002, 445.400], loss: 535.487854, mae: 29.733835, mean_q: -29.949080\n",
            "  712143/10000000: episode: 3543, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -786.800, mean reward: -3.914 [-393.400, 87.000], mean action: 3.592 [0.000, 10.000], mean observation: 38.933 [0.000, 796.200], loss: 347.579132, mae: 29.635094, mean_q: -30.344511\n",
            "  712344/10000000: episode: 3544, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -345.200, mean reward: -1.717 [-172.600, 168.000], mean action: 2.706 [0.000, 10.000], mean observation: 33.029 [0.000, 507.900], loss: 550.115662, mae: 29.390598, mean_q: -29.907366\n",
            "  712545/10000000: episode: 3545, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: 623.000, mean reward: 3.100 [-10.000, 737.000], mean action: 2.930 [0.000, 10.000], mean observation: 36.108 [0.001, 466.300], loss: 259.859985, mae: 29.145346, mean_q: -29.732456\n",
            "  712746/10000000: episode: 3546, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -971.200, mean reward: -4.832 [-485.600, 33.000], mean action: 3.234 [0.000, 10.000], mean observation: 30.340 [0.003, 438.400], loss: 466.088104, mae: 29.479708, mean_q: -30.199558\n",
            "  712947/10000000: episode: 3547, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -826.400, mean reward: -4.111 [-413.200, 39.200], mean action: 3.149 [0.000, 10.000], mean observation: 26.909 [0.002, 545.200], loss: 414.821564, mae: 29.478525, mean_q: -30.124861\n",
            "  713148/10000000: episode: 3548, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -82.000, mean reward: -0.408 [-41.000, 77.000], mean action: 3.318 [0.000, 10.000], mean observation: 36.934 [0.000, 491.000], loss: 327.462921, mae: 29.841194, mean_q: -30.136736\n",
            "  713349/10000000: episode: 3549, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -471.600, mean reward: -2.346 [-235.800, 63.000], mean action: 2.876 [0.000, 10.000], mean observation: 32.256 [0.001, 481.400], loss: 369.894592, mae: 30.023228, mean_q: -30.239197\n",
            "  713550/10000000: episode: 3550, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -1164.200, mean reward: -5.792 [-582.100, 102.000], mean action: 3.945 [0.000, 10.000], mean observation: 30.378 [0.002, 455.400], loss: 513.976746, mae: 29.536930, mean_q: -30.434008\n",
            "  713751/10000000: episode: 3551, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -527.600, mean reward: -2.625 [-263.800, 252.000], mean action: 3.592 [0.000, 10.000], mean observation: 30.429 [0.000, 746.900], loss: 671.749146, mae: 29.140608, mean_q: -29.933710\n",
            "  713952/10000000: episode: 3552, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: 1228.400, mean reward: 6.111 [-10.000, 1075.200], mean action: 3.692 [0.000, 10.000], mean observation: 32.728 [0.001, 583.400], loss: 492.966797, mae: 29.534836, mean_q: -30.502747\n",
            "  714153/10000000: episode: 3553, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -673.000, mean reward: -3.348 [-336.500, 90.000], mean action: 3.289 [0.000, 10.000], mean observation: 32.729 [0.000, 702.100], loss: 333.007233, mae: 30.519484, mean_q: -31.200628\n",
            "  714354/10000000: episode: 3554, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -224.000, mean reward: -1.114 [-112.000, 86.000], mean action: 3.030 [0.000, 10.000], mean observation: 37.834 [0.000, 653.600], loss: 776.854004, mae: 30.825977, mean_q: -31.074997\n",
            "  714555/10000000: episode: 3555, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -292.400, mean reward: -1.455 [-146.200, 101.500], mean action: 2.811 [0.000, 10.000], mean observation: 29.744 [0.002, 509.800], loss: 389.488800, mae: 29.652567, mean_q: -30.142256\n",
            "  714756/10000000: episode: 3556, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -149.800, mean reward: -0.745 [-74.900, 80.000], mean action: 3.005 [0.000, 10.000], mean observation: 35.434 [0.000, 630.900], loss: 416.936493, mae: 30.072592, mean_q: -30.671438\n",
            "  714957/10000000: episode: 3557, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -21.000, mean reward: -0.104 [-10.500, 223.300], mean action: 3.423 [0.000, 10.000], mean observation: 38.493 [0.003, 571.000], loss: 538.086914, mae: 30.182589, mean_q: -31.180040\n",
            "  715158/10000000: episode: 3558, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -994.800, mean reward: -4.949 [-497.400, 44.100], mean action: 3.294 [0.000, 10.000], mean observation: 35.989 [0.001, 591.000], loss: 344.967560, mae: 30.559399, mean_q: -31.394604\n",
            "  715359/10000000: episode: 3559, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -1253.400, mean reward: -6.236 [-626.700, 63.700], mean action: 4.478 [0.000, 10.000], mean observation: 31.287 [0.001, 532.400], loss: 703.763672, mae: 30.614584, mean_q: -31.685810\n",
            "  715560/10000000: episode: 3560, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: 574.400, mean reward: 2.858 [-10.000, 288.400], mean action: 4.119 [0.000, 10.000], mean observation: 32.443 [0.001, 595.600], loss: 499.229858, mae: 30.909063, mean_q: -31.533188\n",
            "  715761/10000000: episode: 3561, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -143.400, mean reward: -0.713 [-71.700, 169.400], mean action: 3.677 [0.000, 10.000], mean observation: 30.515 [0.000, 530.000], loss: 414.366058, mae: 30.634409, mean_q: -31.172565\n",
            "  715962/10000000: episode: 3562, duration: 1.540s, episode steps: 201, steps per second: 130, episode reward: -311.000, mean reward: -1.547 [-155.500, 284.200], mean action: 4.378 [0.000, 10.000], mean observation: 30.188 [0.002, 453.300], loss: 566.619568, mae: 30.570381, mean_q: -31.133633\n",
            "  716163/10000000: episode: 3563, duration: 1.603s, episode steps: 201, steps per second: 125, episode reward: -272.000, mean reward: -1.353 [-136.000, 244.800], mean action: 3.179 [0.000, 10.000], mean observation: 34.801 [0.001, 504.200], loss: 892.269409, mae: 30.418358, mean_q: -30.906887\n",
            "  716364/10000000: episode: 3564, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -529.600, mean reward: -2.635 [-264.800, 232.200], mean action: 3.965 [0.000, 10.000], mean observation: 29.095 [0.003, 591.100], loss: 382.243713, mae: 29.792501, mean_q: -30.451725\n",
            "  716565/10000000: episode: 3565, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -1264.000, mean reward: -6.289 [-632.000, 22.200], mean action: 3.458 [0.000, 10.000], mean observation: 38.760 [0.001, 598.700], loss: 443.787354, mae: 29.667767, mean_q: -29.971073\n",
            "  716766/10000000: episode: 3566, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -393.200, mean reward: -1.956 [-196.600, 94.800], mean action: 3.284 [0.000, 10.000], mean observation: 28.064 [0.002, 369.100], loss: 572.385620, mae: 29.603905, mean_q: -29.942495\n",
            "  716967/10000000: episode: 3567, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -434.200, mean reward: -2.160 [-217.100, 113.400], mean action: 3.055 [0.000, 10.000], mean observation: 32.675 [0.000, 461.800], loss: 594.625488, mae: 29.625355, mean_q: -29.864500\n",
            "  717168/10000000: episode: 3568, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: 40.000, mean reward: 0.199 [-10.000, 122.500], mean action: 3.204 [0.000, 10.000], mean observation: 33.124 [0.000, 540.000], loss: 319.334747, mae: 28.975334, mean_q: -29.299843\n",
            "  717369/10000000: episode: 3569, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -181.800, mean reward: -0.904 [-90.900, 100.800], mean action: 3.348 [0.000, 10.000], mean observation: 27.974 [0.000, 663.800], loss: 827.494873, mae: 28.959656, mean_q: -28.939867\n",
            "  717570/10000000: episode: 3570, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -545.000, mean reward: -2.711 [-272.500, 90.000], mean action: 2.990 [0.000, 10.000], mean observation: 35.280 [0.000, 463.200], loss: 329.792633, mae: 28.240341, mean_q: -28.313375\n",
            "  717771/10000000: episode: 3571, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -339.800, mean reward: -1.691 [-169.900, 212.100], mean action: 3.318 [0.000, 10.000], mean observation: 33.117 [0.001, 639.500], loss: 557.771545, mae: 28.128036, mean_q: -28.279398\n",
            "  717972/10000000: episode: 3572, duration: 1.614s, episode steps: 201, steps per second: 125, episode reward: -579.000, mean reward: -2.881 [-289.500, 141.400], mean action: 3.582 [0.000, 10.000], mean observation: 30.644 [0.001, 619.200], loss: 372.488495, mae: 28.180124, mean_q: -28.269213\n",
            "  718173/10000000: episode: 3573, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -188.600, mean reward: -0.938 [-94.300, 213.500], mean action: 3.403 [0.000, 10.000], mean observation: 36.649 [0.001, 651.100], loss: 453.831787, mae: 28.167940, mean_q: -28.151260\n",
            "  718374/10000000: episode: 3574, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -906.400, mean reward: -4.509 [-453.200, 72.500], mean action: 3.731 [0.000, 10.000], mean observation: 38.527 [0.000, 818.100], loss: 455.199036, mae: 28.210752, mean_q: -28.736542\n",
            "  718575/10000000: episode: 3575, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -248.400, mean reward: -1.236 [-124.200, 252.000], mean action: 4.378 [0.000, 10.000], mean observation: 29.020 [0.003, 540.500], loss: 465.277374, mae: 28.194191, mean_q: -28.900595\n",
            "  718776/10000000: episode: 3576, duration: 1.601s, episode steps: 201, steps per second: 126, episode reward: -1063.800, mean reward: -5.293 [-531.900, 51.100], mean action: 3.527 [0.000, 10.000], mean observation: 34.891 [0.001, 565.900], loss: 318.645691, mae: 28.764587, mean_q: -29.175621\n",
            "  718977/10000000: episode: 3577, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -522.000, mean reward: -2.597 [-261.000, 143.500], mean action: 3.209 [0.000, 10.000], mean observation: 33.712 [0.002, 488.800], loss: 439.324707, mae: 29.109510, mean_q: -29.397894\n",
            "  719178/10000000: episode: 3578, duration: 1.603s, episode steps: 201, steps per second: 125, episode reward: -203.800, mean reward: -1.014 [-101.900, 359.800], mean action: 3.582 [0.000, 10.000], mean observation: 30.307 [0.000, 720.900], loss: 519.270813, mae: 28.861557, mean_q: -29.040651\n",
            "  719379/10000000: episode: 3579, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -833.200, mean reward: -4.145 [-416.600, 94.500], mean action: 3.443 [0.000, 10.000], mean observation: 35.963 [0.002, 624.500], loss: 286.351624, mae: 28.798241, mean_q: -29.279844\n",
            "  719580/10000000: episode: 3580, duration: 1.609s, episode steps: 201, steps per second: 125, episode reward: -635.200, mean reward: -3.160 [-317.600, 71.500], mean action: 3.174 [0.000, 10.000], mean observation: 28.345 [0.001, 488.700], loss: 491.569641, mae: 29.240831, mean_q: -29.948471\n",
            "  719781/10000000: episode: 3581, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -150.000, mean reward: -0.746 [-75.000, 102.900], mean action: 2.532 [0.000, 10.000], mean observation: 31.748 [0.002, 476.600], loss: 353.665863, mae: 29.681829, mean_q: -30.051569\n",
            "  719982/10000000: episode: 3582, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -177.200, mean reward: -0.882 [-88.600, 230.600], mean action: 2.353 [0.000, 10.000], mean observation: 38.593 [0.000, 615.100], loss: 768.594238, mae: 29.780783, mean_q: -30.139740\n",
            "  720183/10000000: episode: 3583, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -791.600, mean reward: -3.938 [-395.800, 83.300], mean action: 2.965 [0.000, 10.000], mean observation: 35.519 [0.000, 907.100], loss: 694.186096, mae: 29.344477, mean_q: -29.739807\n",
            "  720384/10000000: episode: 3584, duration: 1.634s, episode steps: 201, steps per second: 123, episode reward: -94.600, mean reward: -0.471 [-47.300, 192.000], mean action: 2.766 [0.000, 10.000], mean observation: 33.289 [0.001, 508.500], loss: 642.235901, mae: 29.289392, mean_q: -29.725872\n",
            "  720585/10000000: episode: 3585, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: 47.000, mean reward: 0.234 [-10.000, 492.500], mean action: 3.234 [0.000, 10.000], mean observation: 34.698 [0.001, 507.200], loss: 312.108032, mae: 28.843424, mean_q: -29.705132\n",
            "  720786/10000000: episode: 3586, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -707.200, mean reward: -3.518 [-353.600, 85.200], mean action: 3.249 [0.000, 10.000], mean observation: 32.296 [0.004, 522.800], loss: 634.038757, mae: 29.339664, mean_q: -30.222870\n",
            "  720987/10000000: episode: 3587, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -381.600, mean reward: -1.899 [-190.800, 139.300], mean action: 3.363 [0.000, 10.000], mean observation: 28.916 [0.000, 623.900], loss: 459.833344, mae: 29.559742, mean_q: -30.460060\n",
            "  721188/10000000: episode: 3588, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -394.000, mean reward: -1.960 [-197.000, 174.900], mean action: 2.338 [0.000, 10.000], mean observation: 30.894 [0.001, 402.000], loss: 569.502380, mae: 29.924469, mean_q: -30.357016\n",
            "  721389/10000000: episode: 3589, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: 247.200, mean reward: 1.230 [-10.000, 281.600], mean action: 3.050 [0.000, 10.000], mean observation: 34.108 [0.000, 542.000], loss: 658.975098, mae: 29.726757, mean_q: -30.153198\n",
            "  721590/10000000: episode: 3590, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -234.200, mean reward: -1.165 [-117.100, 212.000], mean action: 2.761 [0.000, 10.000], mean observation: 35.624 [0.000, 781.200], loss: 406.147003, mae: 29.750782, mean_q: -30.394051\n",
            "  721791/10000000: episode: 3591, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: 617.000, mean reward: 3.070 [-10.000, 308.500], mean action: 2.547 [0.000, 10.000], mean observation: 27.309 [0.001, 492.700], loss: 396.792084, mae: 30.024332, mean_q: -30.372410\n",
            "  721992/10000000: episode: 3592, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -175.000, mean reward: -0.871 [-87.500, 117.600], mean action: 2.403 [0.000, 10.000], mean observation: 34.233 [0.001, 488.900], loss: 444.671112, mae: 30.295008, mean_q: -30.813820\n",
            "  722193/10000000: episode: 3593, duration: 2.193s, episode steps: 201, steps per second: 92, episode reward: 198.800, mean reward: 0.989 [-10.000, 337.500], mean action: 3.224 [0.000, 10.000], mean observation: 34.439 [0.001, 511.400], loss: 414.703247, mae: 30.332270, mean_q: -31.248260\n",
            "  722394/10000000: episode: 3594, duration: 1.918s, episode steps: 201, steps per second: 105, episode reward: 232.400, mean reward: 1.156 [-10.000, 210.600], mean action: 2.940 [0.000, 10.000], mean observation: 35.863 [0.000, 571.200], loss: 627.175659, mae: 30.652897, mean_q: -31.555218\n",
            "  722595/10000000: episode: 3595, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -797.400, mean reward: -3.967 [-398.700, 62.300], mean action: 3.701 [0.000, 10.000], mean observation: 33.694 [0.000, 764.600], loss: 306.800171, mae: 30.627867, mean_q: -31.786526\n",
            "  722796/10000000: episode: 3596, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: -505.400, mean reward: -2.514 [-252.700, 171.600], mean action: 3.318 [0.000, 10.000], mean observation: 25.704 [0.001, 516.900], loss: 528.944641, mae: 30.846369, mean_q: -31.713661\n",
            "  722997/10000000: episode: 3597, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -704.000, mean reward: -3.502 [-352.000, 114.300], mean action: 2.920 [0.000, 10.000], mean observation: 32.893 [0.000, 598.900], loss: 488.736481, mae: 31.071970, mean_q: -31.846672\n",
            "  723198/10000000: episode: 3598, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -809.000, mean reward: -4.025 [-404.500, 11.800], mean action: 2.388 [0.000, 10.000], mean observation: 38.982 [0.002, 508.700], loss: 412.688934, mae: 31.754120, mean_q: -32.070721\n",
            "  723399/10000000: episode: 3599, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -308.400, mean reward: -1.534 [-154.200, 187.800], mean action: 2.433 [0.000, 10.000], mean observation: 34.230 [0.000, 792.800], loss: 392.779541, mae: 31.783175, mean_q: -32.158726\n",
            "  723600/10000000: episode: 3600, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: -456.800, mean reward: -2.273 [-228.400, 63.600], mean action: 2.532 [0.000, 10.000], mean observation: 36.022 [0.000, 746.700], loss: 340.960358, mae: 31.437155, mean_q: -32.135399\n",
            "  723801/10000000: episode: 3601, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -41.600, mean reward: -0.207 [-20.800, 87.600], mean action: 2.378 [0.000, 10.000], mean observation: 32.795 [0.001, 422.100], loss: 292.503540, mae: 31.509470, mean_q: -32.440826\n",
            "  724002/10000000: episode: 3602, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -452.800, mean reward: -2.253 [-226.400, 149.400], mean action: 2.498 [0.000, 10.000], mean observation: 34.298 [0.000, 541.500], loss: 337.938019, mae: 32.144962, mean_q: -32.941189\n",
            "  724203/10000000: episode: 3603, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 265.200, mean reward: 1.319 [-10.000, 391.000], mean action: 3.294 [0.000, 10.000], mean observation: 30.848 [0.004, 519.300], loss: 525.813354, mae: 31.853912, mean_q: -33.075855\n",
            "  724404/10000000: episode: 3604, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -920.800, mean reward: -4.581 [-460.400, 51.200], mean action: 3.662 [0.000, 10.000], mean observation: 33.898 [0.001, 460.100], loss: 670.127014, mae: 32.152115, mean_q: -33.381348\n",
            "  724605/10000000: episode: 3605, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: 516.800, mean reward: 2.571 [-10.000, 451.600], mean action: 2.736 [0.000, 10.000], mean observation: 31.446 [0.003, 566.800], loss: 585.744141, mae: 32.295330, mean_q: -33.198559\n",
            "  724806/10000000: episode: 3606, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -296.400, mean reward: -1.475 [-148.200, 174.800], mean action: 2.294 [0.000, 10.000], mean observation: 36.116 [0.000, 579.300], loss: 375.149841, mae: 32.121719, mean_q: -32.780926\n",
            "  725007/10000000: episode: 3607, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -497.400, mean reward: -2.475 [-248.700, 39.200], mean action: 2.259 [0.000, 10.000], mean observation: 27.880 [0.001, 550.200], loss: 366.260345, mae: 32.441689, mean_q: -32.351856\n",
            "  725208/10000000: episode: 3608, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -585.400, mean reward: -2.912 [-292.700, 35.800], mean action: 2.154 [0.000, 10.000], mean observation: 31.034 [0.001, 548.600], loss: 466.446686, mae: 32.046471, mean_q: -32.329952\n",
            "  725409/10000000: episode: 3609, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: 217.000, mean reward: 1.080 [-10.000, 178.200], mean action: 2.418 [0.000, 10.000], mean observation: 35.817 [0.001, 494.100], loss: 449.674713, mae: 31.551506, mean_q: -31.983227\n",
            "  725610/10000000: episode: 3610, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -889.000, mean reward: -4.423 [-444.500, 73.200], mean action: 3.025 [0.000, 10.000], mean observation: 34.078 [0.000, 525.700], loss: 462.709106, mae: 31.626036, mean_q: -32.560562\n",
            "  725811/10000000: episode: 3611, duration: 1.616s, episode steps: 201, steps per second: 124, episode reward: -1198.200, mean reward: -5.961 [-599.100, 47.500], mean action: 3.866 [0.000, 10.000], mean observation: 37.890 [0.000, 603.200], loss: 440.897400, mae: 31.551092, mean_q: -32.958595\n",
            "  726012/10000000: episode: 3612, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 361.200, mean reward: 1.797 [-10.000, 233.000], mean action: 3.244 [0.000, 10.000], mean observation: 34.329 [0.000, 803.400], loss: 351.492065, mae: 32.411598, mean_q: -33.409809\n",
            "  726213/10000000: episode: 3613, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -948.800, mean reward: -4.720 [-474.400, 84.800], mean action: 3.826 [0.000, 10.000], mean observation: 31.093 [0.001, 455.800], loss: 518.249268, mae: 33.267292, mean_q: -34.621330\n",
            "  726414/10000000: episode: 3614, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: -632.200, mean reward: -3.145 [-316.100, 43.000], mean action: 2.920 [0.000, 10.000], mean observation: 35.101 [0.002, 525.700], loss: 459.424377, mae: 34.081284, mean_q: -34.915756\n",
            "  726615/10000000: episode: 3615, duration: 1.639s, episode steps: 201, steps per second: 123, episode reward: -1139.800, mean reward: -5.671 [-569.900, 37.800], mean action: 3.542 [0.000, 10.000], mean observation: 34.259 [0.000, 545.700], loss: 411.390320, mae: 33.509815, mean_q: -34.842110\n",
            "  726816/10000000: episode: 3616, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -961.200, mean reward: -4.782 [-480.600, 115.500], mean action: 4.577 [0.000, 10.000], mean observation: 35.376 [0.001, 622.400], loss: 513.902222, mae: 33.439304, mean_q: -35.091640\n",
            "  727017/10000000: episode: 3617, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: -1046.800, mean reward: -5.208 [-523.400, 46.000], mean action: 3.716 [0.000, 10.000], mean observation: 35.582 [0.000, 430.400], loss: 524.929321, mae: 34.063267, mean_q: -35.704899\n",
            "  727218/10000000: episode: 3618, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: 259.400, mean reward: 1.291 [-10.000, 156.000], mean action: 2.687 [0.000, 10.000], mean observation: 37.003 [0.000, 669.400], loss: 538.706177, mae: 34.370663, mean_q: -35.733772\n",
            "  727419/10000000: episode: 3619, duration: 1.617s, episode steps: 201, steps per second: 124, episode reward: -944.000, mean reward: -4.697 [-472.000, 31.500], mean action: 2.970 [0.000, 10.000], mean observation: 32.909 [0.002, 511.500], loss: 498.252319, mae: 34.343388, mean_q: -35.628647\n",
            "  727620/10000000: episode: 3620, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -106.000, mean reward: -0.527 [-53.000, 156.600], mean action: 3.657 [0.000, 10.000], mean observation: 36.910 [0.000, 663.000], loss: 452.260956, mae: 33.686752, mean_q: -34.868664\n",
            "  727821/10000000: episode: 3621, duration: 1.611s, episode steps: 201, steps per second: 125, episode reward: -782.200, mean reward: -3.892 [-391.100, 45.000], mean action: 3.512 [0.000, 10.000], mean observation: 37.948 [0.000, 515.300], loss: 495.278809, mae: 33.794792, mean_q: -34.742203\n",
            "  728022/10000000: episode: 3622, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -126.800, mean reward: -0.631 [-63.400, 253.000], mean action: 3.542 [0.000, 10.000], mean observation: 30.868 [0.001, 637.900], loss: 248.809891, mae: 33.475216, mean_q: -34.328259\n",
            "  728223/10000000: episode: 3623, duration: 1.635s, episode steps: 201, steps per second: 123, episode reward: -774.400, mean reward: -3.853 [-387.200, 36.300], mean action: 3.154 [0.000, 10.000], mean observation: 32.224 [0.001, 539.300], loss: 696.134766, mae: 33.428925, mean_q: -34.497120\n",
            "  728424/10000000: episode: 3624, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -956.600, mean reward: -4.759 [-478.300, 53.700], mean action: 3.294 [0.000, 10.000], mean observation: 38.012 [0.001, 581.100], loss: 516.739197, mae: 33.253811, mean_q: -34.200859\n",
            "  728625/10000000: episode: 3625, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -719.600, mean reward: -3.580 [-359.800, 112.700], mean action: 3.279 [0.000, 10.000], mean observation: 33.394 [0.000, 622.800], loss: 483.182770, mae: 32.842201, mean_q: -34.102634\n",
            "  728826/10000000: episode: 3626, duration: 1.600s, episode steps: 201, steps per second: 126, episode reward: -521.600, mean reward: -2.595 [-260.800, 46.000], mean action: 2.438 [0.000, 10.000], mean observation: 31.722 [0.001, 570.400], loss: 336.316376, mae: 34.265301, mean_q: -35.443630\n",
            "  729027/10000000: episode: 3627, duration: 1.552s, episode steps: 201, steps per second: 129, episode reward: -737.600, mean reward: -3.670 [-368.800, 68.400], mean action: 2.572 [0.000, 10.000], mean observation: 35.766 [0.001, 444.900], loss: 316.844727, mae: 35.500626, mean_q: -36.429550\n",
            "  729228/10000000: episode: 3628, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -650.600, mean reward: -3.237 [-325.300, 84.700], mean action: 3.010 [0.000, 10.000], mean observation: 33.124 [0.000, 749.600], loss: 612.391296, mae: 34.552315, mean_q: -35.687771\n",
            "  729429/10000000: episode: 3629, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -133.200, mean reward: -0.663 [-66.600, 78.000], mean action: 2.483 [0.000, 10.000], mean observation: 28.303 [0.001, 475.200], loss: 462.617859, mae: 34.409443, mean_q: -35.100636\n",
            "  729630/10000000: episode: 3630, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -914.600, mean reward: -4.550 [-457.300, 39.000], mean action: 2.756 [0.000, 10.000], mean observation: 33.940 [0.002, 587.600], loss: 664.001465, mae: 34.314510, mean_q: -35.494011\n",
            "  729831/10000000: episode: 3631, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: -621.000, mean reward: -3.090 [-310.500, 52.200], mean action: 2.403 [0.000, 10.000], mean observation: 28.606 [0.001, 512.300], loss: 396.899261, mae: 35.066536, mean_q: -35.624683\n",
            "  730032/10000000: episode: 3632, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: -301.200, mean reward: -1.499 [-150.600, 33.300], mean action: 1.721 [0.000, 10.000], mean observation: 34.782 [0.003, 377.000], loss: 384.724670, mae: 34.708008, mean_q: -35.068790\n",
            "  730233/10000000: episode: 3633, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -490.800, mean reward: -2.442 [-245.400, 53.100], mean action: 2.274 [0.000, 10.000], mean observation: 32.099 [0.000, 590.900], loss: 267.907410, mae: 35.029922, mean_q: -35.719658\n",
            "  730434/10000000: episode: 3634, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -659.200, mean reward: -3.280 [-329.600, 90.300], mean action: 3.005 [0.000, 10.000], mean observation: 33.829 [0.003, 566.300], loss: 562.181335, mae: 34.963482, mean_q: -36.018875\n",
            "  730635/10000000: episode: 3635, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -618.400, mean reward: -3.077 [-309.200, 133.000], mean action: 3.602 [0.000, 10.000], mean observation: 24.701 [0.000, 446.200], loss: 458.322693, mae: 34.963642, mean_q: -36.310707\n",
            "  730836/10000000: episode: 3636, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -388.400, mean reward: -1.932 [-194.200, 104.300], mean action: 3.244 [0.000, 10.000], mean observation: 36.537 [0.001, 504.300], loss: 426.445587, mae: 35.134212, mean_q: -36.752296\n",
            "  731037/10000000: episode: 3637, duration: 1.615s, episode steps: 201, steps per second: 124, episode reward: 882.600, mean reward: 4.391 [-10.000, 441.300], mean action: 4.040 [0.000, 10.000], mean observation: 32.909 [0.001, 510.800], loss: 416.702576, mae: 35.261646, mean_q: -36.934555\n",
            "  731238/10000000: episode: 3638, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -454.800, mean reward: -2.263 [-227.400, 114.900], mean action: 4.403 [0.000, 10.000], mean observation: 29.579 [0.002, 487.500], loss: 563.214722, mae: 35.640423, mean_q: -37.317905\n",
            "  731439/10000000: episode: 3639, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: 165.800, mean reward: 0.825 [-10.000, 179.200], mean action: 3.697 [0.000, 10.000], mean observation: 35.908 [0.000, 637.000], loss: 460.031921, mae: 35.697235, mean_q: -37.309849\n",
            "  731640/10000000: episode: 3640, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -564.800, mean reward: -2.810 [-282.400, 124.500], mean action: 3.517 [0.000, 10.000], mean observation: 30.085 [0.000, 640.400], loss: 434.681152, mae: 35.788296, mean_q: -37.215431\n",
            "  731841/10000000: episode: 3641, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: 853.600, mean reward: 4.247 [-10.000, 490.700], mean action: 3.607 [0.000, 10.000], mean observation: 28.090 [0.001, 418.000], loss: 457.585907, mae: 35.615170, mean_q: -37.018932\n",
            "  732042/10000000: episode: 3642, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: 85.400, mean reward: 0.425 [-10.000, 146.700], mean action: 4.219 [0.000, 10.000], mean observation: 37.516 [0.002, 512.800], loss: 560.045105, mae: 35.203403, mean_q: -36.711361\n",
            "  732243/10000000: episode: 3643, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -11.400, mean reward: -0.057 [-10.000, 296.100], mean action: 4.159 [0.000, 10.000], mean observation: 29.650 [0.000, 558.800], loss: 401.333710, mae: 35.655945, mean_q: -37.129192\n",
            "  732444/10000000: episode: 3644, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -684.200, mean reward: -3.404 [-342.100, 61.000], mean action: 3.398 [0.000, 10.000], mean observation: 40.985 [0.001, 584.600], loss: 592.036377, mae: 36.373894, mean_q: -37.586441\n",
            "  732645/10000000: episode: 3645, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -729.200, mean reward: -3.628 [-364.600, 129.900], mean action: 3.488 [0.000, 10.000], mean observation: 32.760 [0.000, 455.400], loss: 361.389160, mae: 36.595417, mean_q: -37.903725\n",
            "  732846/10000000: episode: 3646, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -258.200, mean reward: -1.285 [-129.100, 91.800], mean action: 2.443 [0.000, 10.000], mean observation: 33.987 [0.001, 510.200], loss: 386.636108, mae: 37.005024, mean_q: -37.763687\n",
            "  733047/10000000: episode: 3647, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -581.600, mean reward: -2.894 [-290.800, 60.200], mean action: 3.000 [0.000, 10.000], mean observation: 32.711 [0.000, 708.200], loss: 290.318481, mae: 36.502056, mean_q: -37.677742\n",
            "  733248/10000000: episode: 3648, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: 7.600, mean reward: 0.038 [-10.000, 106.300], mean action: 2.562 [0.000, 10.000], mean observation: 33.221 [0.002, 436.000], loss: 500.742645, mae: 37.386063, mean_q: -37.954948\n",
            "  733449/10000000: episode: 3649, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: 37.600, mean reward: 0.187 [-10.000, 168.300], mean action: 3.269 [0.000, 10.000], mean observation: 37.606 [0.000, 500.200], loss: 297.706573, mae: 36.579498, mean_q: -37.433044\n",
            "  733650/10000000: episode: 3650, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: 884.400, mean reward: 4.400 [-10.000, 585.000], mean action: 3.164 [0.000, 10.000], mean observation: 35.158 [0.000, 425.200], loss: 306.058716, mae: 36.750340, mean_q: -37.752766\n",
            "  733851/10000000: episode: 3651, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -651.800, mean reward: -3.243 [-325.900, 45.000], mean action: 3.323 [0.000, 10.000], mean observation: 35.205 [0.000, 571.900], loss: 666.067322, mae: 36.411175, mean_q: -37.280087\n",
            "  734052/10000000: episode: 3652, duration: 1.540s, episode steps: 201, steps per second: 130, episode reward: -860.800, mean reward: -4.283 [-430.400, 91.500], mean action: 3.642 [0.000, 10.000], mean observation: 30.605 [0.002, 494.100], loss: 511.094208, mae: 36.144775, mean_q: -37.295830\n",
            "  734253/10000000: episode: 3653, duration: 1.714s, episode steps: 201, steps per second: 117, episode reward: -823.000, mean reward: -4.095 [-411.500, 66.600], mean action: 3.587 [0.000, 10.000], mean observation: 29.012 [0.000, 490.900], loss: 426.408203, mae: 36.628548, mean_q: -37.529125\n",
            "  734454/10000000: episode: 3654, duration: 1.755s, episode steps: 201, steps per second: 115, episode reward: -804.200, mean reward: -4.001 [-402.100, 46.900], mean action: 3.313 [0.000, 10.000], mean observation: 29.322 [0.001, 457.300], loss: 508.030792, mae: 36.746170, mean_q: -37.841072\n",
            "  734655/10000000: episode: 3655, duration: 1.762s, episode steps: 201, steps per second: 114, episode reward: -270.200, mean reward: -1.344 [-135.100, 152.000], mean action: 3.527 [0.000, 10.000], mean observation: 33.131 [0.000, 798.300], loss: 423.827820, mae: 37.368595, mean_q: -38.313309\n",
            "  734856/10000000: episode: 3656, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: -290.000, mean reward: -1.443 [-145.000, 259.200], mean action: 3.005 [0.000, 10.000], mean observation: 33.856 [0.001, 556.200], loss: 619.108582, mae: 37.701122, mean_q: -38.616676\n",
            "  735057/10000000: episode: 3657, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -1194.200, mean reward: -5.941 [-597.100, 37.800], mean action: 4.109 [0.000, 9.000], mean observation: 29.340 [0.000, 481.500], loss: 659.779358, mae: 36.758286, mean_q: -38.158848\n",
            "  735258/10000000: episode: 3658, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: 415.000, mean reward: 2.065 [-9.000, 309.400], mean action: 4.164 [0.000, 9.000], mean observation: 35.781 [0.000, 668.900], loss: 429.493469, mae: 37.308403, mean_q: -38.626343\n",
            "  735459/10000000: episode: 3659, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: -683.800, mean reward: -3.402 [-341.900, 109.200], mean action: 3.557 [0.000, 10.000], mean observation: 29.327 [0.000, 383.900], loss: 465.825806, mae: 38.309452, mean_q: -39.639507\n",
            "  735660/10000000: episode: 3660, duration: 1.626s, episode steps: 201, steps per second: 124, episode reward: -652.200, mean reward: -3.245 [-326.100, 80.000], mean action: 3.517 [0.000, 9.000], mean observation: 35.491 [0.000, 605.900], loss: 254.761978, mae: 37.715382, mean_q: -38.977287\n",
            "  735861/10000000: episode: 3661, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -77.600, mean reward: -0.386 [-38.800, 131.400], mean action: 3.000 [0.000, 10.000], mean observation: 34.990 [0.000, 650.000], loss: 353.256287, mae: 38.173771, mean_q: -39.349419\n",
            "  736062/10000000: episode: 3662, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: 917.800, mean reward: 4.566 [-10.000, 810.000], mean action: 3.119 [0.000, 10.000], mean observation: 33.362 [0.001, 574.000], loss: 505.020691, mae: 38.786636, mean_q: -39.885193\n",
            "  736263/10000000: episode: 3663, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -319.000, mean reward: -1.587 [-159.500, 290.700], mean action: 3.697 [0.000, 10.000], mean observation: 33.398 [0.002, 420.100], loss: 330.197449, mae: 38.609512, mean_q: -40.045582\n",
            "  736464/10000000: episode: 3664, duration: 1.618s, episode steps: 201, steps per second: 124, episode reward: -6.800, mean reward: -0.034 [-10.000, 196.500], mean action: 3.318 [0.000, 10.000], mean observation: 41.188 [0.001, 510.400], loss: 463.394592, mae: 39.566677, mean_q: -40.599438\n",
            "  736665/10000000: episode: 3665, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: 331.400, mean reward: 1.649 [-10.000, 187.700], mean action: 2.910 [0.000, 10.000], mean observation: 31.801 [0.002, 508.900], loss: 556.618164, mae: 39.471638, mean_q: -40.226238\n",
            "  736866/10000000: episode: 3666, duration: 1.654s, episode steps: 201, steps per second: 122, episode reward: -1061.600, mean reward: -5.282 [-530.800, 38.400], mean action: 3.537 [0.000, 10.000], mean observation: 34.543 [0.000, 447.500], loss: 457.373260, mae: 39.229858, mean_q: -40.797821\n",
            "  737067/10000000: episode: 3667, duration: 1.646s, episode steps: 201, steps per second: 122, episode reward: -420.000, mean reward: -2.090 [-210.000, 171.900], mean action: 4.055 [0.000, 10.000], mean observation: 27.028 [0.001, 618.400], loss: 308.217010, mae: 39.257584, mean_q: -41.033504\n",
            "  737268/10000000: episode: 3668, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: 1006.800, mean reward: 5.009 [-10.000, 503.400], mean action: 3.731 [0.000, 10.000], mean observation: 34.276 [0.001, 519.700], loss: 580.548645, mae: 39.770603, mean_q: -41.242607\n",
            "  737469/10000000: episode: 3669, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -583.800, mean reward: -2.904 [-291.900, 165.600], mean action: 3.955 [0.000, 10.000], mean observation: 32.033 [0.000, 586.000], loss: 471.369415, mae: 39.433903, mean_q: -41.235523\n",
            "  737670/10000000: episode: 3670, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -775.600, mean reward: -3.859 [-387.800, 191.800], mean action: 3.731 [0.000, 10.000], mean observation: 33.395 [0.001, 471.000], loss: 363.437500, mae: 39.621342, mean_q: -41.414867\n",
            "  737871/10000000: episode: 3671, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -303.400, mean reward: -1.509 [-151.700, 213.500], mean action: 3.109 [0.000, 10.000], mean observation: 29.959 [0.001, 431.000], loss: 427.599884, mae: 40.434193, mean_q: -41.896664\n",
            "  738072/10000000: episode: 3672, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -332.400, mean reward: -1.654 [-166.200, 129.600], mean action: 3.502 [0.000, 10.000], mean observation: 31.157 [0.001, 459.200], loss: 348.654175, mae: 40.793221, mean_q: -42.547600\n",
            "  738273/10000000: episode: 3673, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -881.600, mean reward: -4.386 [-440.800, 114.400], mean action: 3.821 [0.000, 10.000], mean observation: 34.064 [0.001, 497.500], loss: 591.559204, mae: 40.945194, mean_q: -42.759697\n",
            "  738474/10000000: episode: 3674, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: 225.800, mean reward: 1.123 [-10.000, 382.900], mean action: 2.622 [0.000, 10.000], mean observation: 32.162 [0.001, 439.900], loss: 478.671570, mae: 42.202805, mean_q: -43.727409\n",
            "  738675/10000000: episode: 3675, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -512.800, mean reward: -2.551 [-256.400, 83.300], mean action: 3.065 [0.000, 9.000], mean observation: 28.456 [0.002, 430.700], loss: 424.040436, mae: 42.050476, mean_q: -43.623165\n",
            "  738876/10000000: episode: 3676, duration: 1.633s, episode steps: 201, steps per second: 123, episode reward: -41.800, mean reward: -0.208 [-20.900, 287.600], mean action: 2.478 [0.000, 9.000], mean observation: 35.633 [0.000, 552.400], loss: 492.746979, mae: 42.097801, mean_q: -43.790268\n",
            "  739077/10000000: episode: 3677, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -267.000, mean reward: -1.328 [-133.500, 88.900], mean action: 2.647 [0.000, 10.000], mean observation: 32.743 [0.000, 720.900], loss: 354.351013, mae: 41.515747, mean_q: -43.212566\n",
            "  739278/10000000: episode: 3678, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -292.000, mean reward: -1.453 [-146.000, 86.800], mean action: 2.721 [0.000, 10.000], mean observation: 31.397 [0.001, 500.800], loss: 321.301300, mae: 42.232738, mean_q: -43.441963\n",
            "  739479/10000000: episode: 3679, duration: 1.620s, episode steps: 201, steps per second: 124, episode reward: 1972.200, mean reward: 9.812 [-10.000, 986.100], mean action: 3.224 [0.000, 10.000], mean observation: 33.961 [0.001, 441.700], loss: 474.301758, mae: 41.511806, mean_q: -43.429718\n",
            "  739680/10000000: episode: 3680, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: 446.200, mean reward: 2.220 [-10.000, 223.100], mean action: 2.323 [0.000, 10.000], mean observation: 28.911 [0.000, 348.700], loss: 522.017273, mae: 41.460987, mean_q: -42.984131\n",
            "  739881/10000000: episode: 3681, duration: 1.657s, episode steps: 201, steps per second: 121, episode reward: -615.200, mean reward: -3.061 [-307.600, 73.500], mean action: 2.682 [0.000, 9.000], mean observation: 33.475 [0.002, 537.100], loss: 340.234131, mae: 41.306427, mean_q: -42.871895\n",
            "  740082/10000000: episode: 3682, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -392.000, mean reward: -1.950 [-196.000, 113.400], mean action: 2.572 [0.000, 10.000], mean observation: 34.250 [0.002, 562.800], loss: 349.804230, mae: 41.940639, mean_q: -43.270916\n",
            "  740283/10000000: episode: 3683, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: 93.000, mean reward: 0.463 [-10.000, 326.400], mean action: 2.975 [0.000, 10.000], mean observation: 31.033 [0.001, 530.600], loss: 552.694275, mae: 41.261562, mean_q: -42.913353\n",
            "  740484/10000000: episode: 3684, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: -470.600, mean reward: -2.341 [-235.300, 69.000], mean action: 2.507 [0.000, 10.000], mean observation: 36.767 [0.000, 693.300], loss: 597.095337, mae: 41.256588, mean_q: -42.771473\n",
            "  740685/10000000: episode: 3685, duration: 1.678s, episode steps: 201, steps per second: 120, episode reward: -425.000, mean reward: -2.114 [-212.500, 94.500], mean action: 3.050 [0.000, 10.000], mean observation: 35.574 [0.000, 727.600], loss: 646.061829, mae: 40.319077, mean_q: -41.975563\n",
            "  740886/10000000: episode: 3686, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -481.600, mean reward: -2.396 [-240.800, 71.400], mean action: 3.219 [0.000, 10.000], mean observation: 34.798 [0.002, 479.600], loss: 490.455017, mae: 41.216301, mean_q: -43.209957\n",
            "  741087/10000000: episode: 3687, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -315.200, mean reward: -1.568 [-157.600, 147.600], mean action: 3.831 [0.000, 10.000], mean observation: 32.722 [0.001, 606.200], loss: 405.758484, mae: 41.446411, mean_q: -43.625076\n",
            "  741288/10000000: episode: 3688, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -147.400, mean reward: -0.733 [-73.700, 200.100], mean action: 3.114 [0.000, 10.000], mean observation: 34.837 [0.001, 597.900], loss: 433.404541, mae: 42.597065, mean_q: -44.255928\n",
            "  741489/10000000: episode: 3689, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -669.400, mean reward: -3.330 [-334.700, 41.000], mean action: 3.239 [0.000, 10.000], mean observation: 31.058 [0.003, 482.900], loss: 433.570435, mae: 42.933323, mean_q: -44.555233\n",
            "  741690/10000000: episode: 3690, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -485.400, mean reward: -2.415 [-242.700, 99.900], mean action: 3.741 [0.000, 10.000], mean observation: 26.317 [0.002, 435.000], loss: 526.500183, mae: 42.579853, mean_q: -44.666187\n",
            "  741891/10000000: episode: 3691, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -906.800, mean reward: -4.511 [-453.400, 42.400], mean action: 3.343 [0.000, 10.000], mean observation: 33.454 [0.001, 697.100], loss: 285.517151, mae: 42.975651, mean_q: -45.111080\n",
            "  742092/10000000: episode: 3692, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -761.200, mean reward: -3.787 [-380.600, 106.200], mean action: 3.388 [0.000, 10.000], mean observation: 31.701 [0.001, 562.800], loss: 350.693085, mae: 43.681469, mean_q: -45.445168\n",
            "  742293/10000000: episode: 3693, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -928.600, mean reward: -4.620 [-464.300, 35.700], mean action: 2.896 [0.000, 10.000], mean observation: 34.294 [0.000, 458.900], loss: 441.415863, mae: 43.418327, mean_q: -45.280064\n",
            "  742494/10000000: episode: 3694, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -791.200, mean reward: -3.936 [-395.600, 117.900], mean action: 4.542 [0.000, 10.000], mean observation: 30.985 [0.001, 522.200], loss: 410.908905, mae: 42.898952, mean_q: -44.993111\n",
            "  742695/10000000: episode: 3695, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: 2042.400, mean reward: 10.161 [-10.000, 1021.200], mean action: 4.368 [0.000, 10.000], mean observation: 30.608 [0.000, 527.200], loss: 400.428558, mae: 41.850437, mean_q: -44.128052\n",
            "  742896/10000000: episode: 3696, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -1343.400, mean reward: -6.684 [-671.700, 32.600], mean action: 4.144 [0.000, 10.000], mean observation: 35.071 [0.000, 424.800], loss: 485.883667, mae: 42.632275, mean_q: -44.416332\n",
            "  743097/10000000: episode: 3697, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: -675.400, mean reward: -3.360 [-337.700, 35.100], mean action: 3.060 [0.000, 10.000], mean observation: 34.505 [0.000, 765.000], loss: 310.347748, mae: 44.134884, mean_q: -45.685276\n",
            "  743298/10000000: episode: 3698, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: 152.600, mean reward: 0.759 [-10.000, 136.500], mean action: 2.716 [0.000, 10.000], mean observation: 30.532 [0.000, 638.500], loss: 522.778076, mae: 43.625408, mean_q: -45.348976\n",
            "  743499/10000000: episode: 3699, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: 245.000, mean reward: 1.219 [-10.000, 338.000], mean action: 4.418 [0.000, 10.000], mean observation: 35.938 [0.001, 534.900], loss: 486.670715, mae: 42.518784, mean_q: -44.609352\n",
            "  743700/10000000: episode: 3700, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: 457.200, mean reward: 2.275 [-10.000, 392.400], mean action: 3.846 [0.000, 10.000], mean observation: 37.368 [0.000, 793.800], loss: 549.389526, mae: 42.573402, mean_q: -44.330666\n",
            "  743901/10000000: episode: 3701, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -668.600, mean reward: -3.326 [-334.300, 65.200], mean action: 3.826 [0.000, 10.000], mean observation: 34.353 [0.001, 550.000], loss: 393.275177, mae: 41.954330, mean_q: -43.357044\n",
            "  744102/10000000: episode: 3702, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -992.800, mean reward: -4.939 [-496.400, 46.500], mean action: 3.448 [0.000, 10.000], mean observation: 41.457 [0.000, 701.500], loss: 377.665344, mae: 42.314617, mean_q: -43.629383\n",
            "  744303/10000000: episode: 3703, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: 1499.200, mean reward: 7.459 [-10.000, 749.600], mean action: 3.891 [0.000, 10.000], mean observation: 33.699 [0.000, 783.800], loss: 470.901123, mae: 42.100140, mean_q: -43.343655\n",
            "  744504/10000000: episode: 3704, duration: 1.549s, episode steps: 201, steps per second: 130, episode reward: 169.400, mean reward: 0.843 [-10.000, 259.000], mean action: 3.915 [0.000, 10.000], mean observation: 35.166 [0.000, 642.000], loss: 513.245117, mae: 41.037487, mean_q: -42.340454\n",
            "  744705/10000000: episode: 3705, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -385.000, mean reward: -1.915 [-192.500, 144.900], mean action: 3.811 [0.000, 10.000], mean observation: 35.789 [0.002, 515.200], loss: 477.448822, mae: 40.263924, mean_q: -41.405712\n",
            "  744906/10000000: episode: 3706, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -957.800, mean reward: -4.765 [-478.900, 81.900], mean action: 4.726 [0.000, 10.000], mean observation: 37.656 [0.001, 430.300], loss: 469.515228, mae: 39.365211, mean_q: -40.580009\n",
            "  745107/10000000: episode: 3707, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -638.200, mean reward: -3.175 [-319.100, 205.200], mean action: 3.995 [0.000, 10.000], mean observation: 35.869 [0.000, 673.700], loss: 572.716125, mae: 39.008045, mean_q: -39.917294\n",
            "  745308/10000000: episode: 3708, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -402.000, mean reward: -2.000 [-201.000, 238.000], mean action: 4.139 [0.000, 10.000], mean observation: 30.581 [0.001, 446.200], loss: 483.333801, mae: 38.765442, mean_q: -39.461800\n",
            "  745509/10000000: episode: 3709, duration: 1.665s, episode steps: 201, steps per second: 121, episode reward: 231.200, mean reward: 1.150 [-10.000, 470.000], mean action: 4.124 [0.000, 10.000], mean observation: 39.750 [0.001, 459.000], loss: 383.948181, mae: 37.581928, mean_q: -38.376026\n",
            "  745710/10000000: episode: 3710, duration: 1.779s, episode steps: 201, steps per second: 113, episode reward: -217.600, mean reward: -1.083 [-108.800, 383.600], mean action: 4.189 [0.000, 10.000], mean observation: 33.609 [0.001, 558.800], loss: 580.119141, mae: 36.534191, mean_q: -37.448372\n",
            "  745911/10000000: episode: 3711, duration: 1.728s, episode steps: 201, steps per second: 116, episode reward: 113.000, mean reward: 0.562 [-10.000, 187.800], mean action: 3.433 [0.000, 10.000], mean observation: 32.809 [0.000, 933.200], loss: 444.311340, mae: 36.780369, mean_q: -38.029419\n",
            "  746112/10000000: episode: 3712, duration: 1.810s, episode steps: 201, steps per second: 111, episode reward: 57.600, mean reward: 0.287 [-10.000, 139.300], mean action: 3.517 [0.000, 10.000], mean observation: 34.449 [0.000, 684.900], loss: 473.866058, mae: 37.008099, mean_q: -38.344006\n",
            "  746313/10000000: episode: 3713, duration: 1.846s, episode steps: 201, steps per second: 109, episode reward: -541.200, mean reward: -2.693 [-270.600, 113.400], mean action: 3.915 [0.000, 10.000], mean observation: 28.666 [0.004, 468.600], loss: 854.199951, mae: 36.323326, mean_q: -37.063568\n",
            "  746514/10000000: episode: 3714, duration: 1.734s, episode steps: 201, steps per second: 116, episode reward: -897.000, mean reward: -4.463 [-448.500, 47.700], mean action: 3.970 [0.000, 10.000], mean observation: 36.188 [0.000, 556.700], loss: 514.625854, mae: 36.245750, mean_q: -36.671700\n",
            "  746715/10000000: episode: 3715, duration: 1.675s, episode steps: 201, steps per second: 120, episode reward: -208.400, mean reward: -1.037 [-104.200, 350.000], mean action: 3.592 [0.000, 10.000], mean observation: 35.578 [0.000, 607.900], loss: 418.490204, mae: 35.772453, mean_q: -36.400154\n",
            "  746916/10000000: episode: 3716, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -456.400, mean reward: -2.271 [-228.200, 73.800], mean action: 3.025 [0.000, 10.000], mean observation: 36.646 [0.001, 423.500], loss: 348.273438, mae: 35.810230, mean_q: -36.331196\n",
            "  747117/10000000: episode: 3717, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: 218.600, mean reward: 1.088 [-10.000, 408.800], mean action: 2.587 [0.000, 10.000], mean observation: 30.483 [0.000, 528.400], loss: 465.554657, mae: 36.182457, mean_q: -36.364323\n",
            "  747318/10000000: episode: 3718, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: 697.600, mean reward: 3.471 [-10.000, 665.000], mean action: 3.607 [0.000, 10.000], mean observation: 30.306 [0.003, 464.200], loss: 338.425446, mae: 35.829098, mean_q: -36.664299\n",
            "  747519/10000000: episode: 3719, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -260.600, mean reward: -1.297 [-130.300, 82.000], mean action: 3.294 [0.000, 10.000], mean observation: 36.370 [0.000, 818.500], loss: 546.749817, mae: 36.242485, mean_q: -37.188446\n",
            "  747720/10000000: episode: 3720, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -1103.200, mean reward: -5.489 [-551.600, 70.000], mean action: 3.612 [0.000, 10.000], mean observation: 27.207 [0.001, 619.000], loss: 431.673553, mae: 36.140362, mean_q: -37.137619\n",
            "  747921/10000000: episode: 3721, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 241.800, mean reward: 1.203 [-10.000, 345.000], mean action: 3.806 [0.000, 10.000], mean observation: 33.457 [0.001, 461.700], loss: 576.565430, mae: 35.945488, mean_q: -36.642815\n",
            "  748122/10000000: episode: 3722, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -554.000, mean reward: -2.756 [-277.000, 323.100], mean action: 3.965 [0.000, 10.000], mean observation: 36.271 [0.002, 402.500], loss: 283.972778, mae: 35.766064, mean_q: -36.357616\n",
            "  748323/10000000: episode: 3723, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -1036.800, mean reward: -5.158 [-518.400, 119.000], mean action: 4.448 [0.000, 10.000], mean observation: 33.364 [0.000, 501.600], loss: 552.373474, mae: 35.178852, mean_q: -36.289909\n",
            "  748524/10000000: episode: 3724, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -616.800, mean reward: -3.069 [-308.400, 81.000], mean action: 3.055 [0.000, 10.000], mean observation: 37.159 [0.001, 492.600], loss: 604.507324, mae: 35.384487, mean_q: -36.247868\n",
            "  748725/10000000: episode: 3725, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: 1023.400, mean reward: 5.092 [-10.000, 984.000], mean action: 3.925 [0.000, 10.000], mean observation: 31.372 [0.000, 590.900], loss: 735.347412, mae: 34.711823, mean_q: -35.841228\n",
            "  748926/10000000: episode: 3726, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 216.600, mean reward: 1.078 [-10.000, 197.800], mean action: 4.050 [0.000, 10.000], mean observation: 33.069 [0.000, 747.100], loss: 471.612396, mae: 35.288551, mean_q: -36.386475\n",
            "  749127/10000000: episode: 3727, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -645.200, mean reward: -3.210 [-322.600, 172.800], mean action: 4.144 [0.000, 10.000], mean observation: 36.716 [0.000, 502.600], loss: 459.085114, mae: 36.080429, mean_q: -37.353073\n",
            "  749328/10000000: episode: 3728, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -287.800, mean reward: -1.432 [-143.900, 271.000], mean action: 5.139 [0.000, 10.000], mean observation: 25.642 [0.001, 534.800], loss: 643.626648, mae: 35.580242, mean_q: -37.058380\n",
            "  749529/10000000: episode: 3729, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -1180.400, mean reward: -5.873 [-590.200, 82.500], mean action: 4.547 [0.000, 10.000], mean observation: 30.469 [0.000, 580.300], loss: 373.630920, mae: 35.933270, mean_q: -37.590382\n",
            "  749730/10000000: episode: 3730, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -306.000, mean reward: -1.522 [-153.000, 193.000], mean action: 3.065 [0.000, 10.000], mean observation: 36.640 [0.000, 522.100], loss: 289.226868, mae: 37.246731, mean_q: -38.481625\n",
            "  749931/10000000: episode: 3731, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -244.800, mean reward: -1.218 [-122.400, 165.600], mean action: 4.159 [0.000, 10.000], mean observation: 26.880 [0.001, 590.600], loss: 573.498596, mae: 37.639187, mean_q: -38.928768\n",
            "  750132/10000000: episode: 3732, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -569.600, mean reward: -2.834 [-284.800, 144.000], mean action: 3.005 [0.000, 10.000], mean observation: 35.530 [0.000, 813.800], loss: 309.698151, mae: 38.248592, mean_q: -39.436554\n",
            "  750333/10000000: episode: 3733, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -414.000, mean reward: -2.060 [-207.000, 205.000], mean action: 3.119 [0.000, 10.000], mean observation: 39.645 [0.000, 632.400], loss: 375.678009, mae: 37.935810, mean_q: -39.273849\n",
            "  750534/10000000: episode: 3734, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -854.200, mean reward: -4.250 [-427.100, 123.600], mean action: 3.602 [0.000, 10.000], mean observation: 29.864 [0.003, 531.400], loss: 344.871338, mae: 37.963177, mean_q: -39.297508\n",
            "  750735/10000000: episode: 3735, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -310.400, mean reward: -1.544 [-155.200, 143.500], mean action: 3.144 [0.000, 10.000], mean observation: 35.602 [0.002, 519.900], loss: 435.642426, mae: 38.463638, mean_q: -39.892326\n",
            "  750936/10000000: episode: 3736, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -400.400, mean reward: -1.992 [-200.200, 142.000], mean action: 3.448 [0.000, 10.000], mean observation: 37.396 [0.001, 518.800], loss: 519.942993, mae: 38.902012, mean_q: -40.483158\n",
            "  751137/10000000: episode: 3737, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -700.400, mean reward: -3.485 [-350.200, 184.800], mean action: 4.214 [0.000, 10.000], mean observation: 37.826 [0.001, 554.400], loss: 560.398804, mae: 38.334316, mean_q: -39.519722\n",
            "  751338/10000000: episode: 3738, duration: 1.540s, episode steps: 201, steps per second: 130, episode reward: 62.600, mean reward: 0.311 [-10.000, 399.700], mean action: 4.154 [0.000, 10.000], mean observation: 38.204 [0.001, 627.200], loss: 370.067078, mae: 38.446507, mean_q: -39.611317\n",
            "  751539/10000000: episode: 3739, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: 1111.600, mean reward: 5.530 [-10.000, 555.800], mean action: 3.821 [0.000, 10.000], mean observation: 36.212 [0.000, 604.500], loss: 472.359985, mae: 38.095413, mean_q: -39.059944\n",
            "  751740/10000000: episode: 3740, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: 144.800, mean reward: 0.720 [-10.000, 254.700], mean action: 3.741 [0.000, 10.000], mean observation: 32.494 [0.001, 500.600], loss: 469.446167, mae: 38.352215, mean_q: -39.432587\n",
            "  751941/10000000: episode: 3741, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: -931.800, mean reward: -4.636 [-465.900, 90.900], mean action: 4.343 [0.000, 10.000], mean observation: 29.429 [0.001, 464.600], loss: 422.607086, mae: 38.628757, mean_q: -40.190193\n",
            "  752142/10000000: episode: 3742, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -88.800, mean reward: -0.442 [-44.400, 247.800], mean action: 3.806 [0.000, 10.000], mean observation: 30.825 [0.004, 531.100], loss: 406.692566, mae: 39.269154, mean_q: -40.703178\n",
            "  752343/10000000: episode: 3743, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: 314.200, mean reward: 1.563 [-10.000, 316.000], mean action: 4.398 [0.000, 10.000], mean observation: 35.465 [0.001, 507.400], loss: 508.555573, mae: 39.164894, mean_q: -40.209873\n",
            "  752544/10000000: episode: 3744, duration: 1.605s, episode steps: 201, steps per second: 125, episode reward: -1207.400, mean reward: -6.007 [-603.700, 74.000], mean action: 5.398 [0.000, 10.000], mean observation: 34.368 [0.000, 784.200], loss: 378.375336, mae: 37.879436, mean_q: -38.933784\n",
            "  752745/10000000: episode: 3745, duration: 1.623s, episode steps: 201, steps per second: 124, episode reward: -548.200, mean reward: -2.727 [-274.100, 109.600], mean action: 3.706 [0.000, 10.000], mean observation: 36.597 [0.000, 722.600], loss: 504.443085, mae: 38.140060, mean_q: -38.861504\n",
            "  752946/10000000: episode: 3746, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -386.600, mean reward: -1.923 [-193.300, 134.600], mean action: 3.512 [0.000, 10.000], mean observation: 32.142 [0.001, 542.200], loss: 378.858948, mae: 38.432346, mean_q: -39.250336\n",
            "  753147/10000000: episode: 3747, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: 200.200, mean reward: 0.996 [-10.000, 252.000], mean action: 3.149 [0.000, 10.000], mean observation: 36.408 [0.001, 448.100], loss: 415.223541, mae: 37.880394, mean_q: -38.700665\n",
            "  753348/10000000: episode: 3748, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -1103.200, mean reward: -5.489 [-551.600, 35.000], mean action: 3.816 [0.000, 10.000], mean observation: 29.627 [0.000, 527.300], loss: 364.918060, mae: 38.001129, mean_q: -39.204712\n",
            "  753549/10000000: episode: 3749, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -501.800, mean reward: -2.497 [-250.900, 103.000], mean action: 2.806 [0.000, 10.000], mean observation: 31.962 [0.003, 542.400], loss: 389.426605, mae: 38.507561, mean_q: -39.369431\n",
            "  753750/10000000: episode: 3750, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -734.400, mean reward: -3.654 [-367.200, 154.000], mean action: 3.065 [0.000, 10.000], mean observation: 39.274 [0.000, 694.400], loss: 358.457336, mae: 38.232079, mean_q: -38.629898\n",
            "  753951/10000000: episode: 3751, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -613.600, mean reward: -3.053 [-306.800, 104.700], mean action: 3.333 [0.000, 10.000], mean observation: 34.789 [0.000, 598.300], loss: 473.294067, mae: 37.428284, mean_q: -38.325840\n",
            "  754152/10000000: episode: 3752, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: -935.600, mean reward: -4.655 [-467.800, 46.200], mean action: 3.423 [0.000, 10.000], mean observation: 29.406 [0.004, 555.700], loss: 240.002197, mae: 37.032398, mean_q: -37.869366\n",
            "  754353/10000000: episode: 3753, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -802.200, mean reward: -3.991 [-401.100, 121.000], mean action: 3.751 [0.000, 10.000], mean observation: 35.137 [0.000, 669.900], loss: 659.975830, mae: 37.695030, mean_q: -38.550232\n",
            "  754554/10000000: episode: 3754, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -657.000, mean reward: -3.269 [-328.500, 164.000], mean action: 4.373 [0.000, 10.000], mean observation: 39.088 [0.001, 662.900], loss: 321.368561, mae: 37.904922, mean_q: -39.003006\n",
            "  754755/10000000: episode: 3755, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: 1004.400, mean reward: 4.997 [-10.000, 502.200], mean action: 2.821 [0.000, 10.000], mean observation: 35.523 [0.002, 482.600], loss: 269.791870, mae: 38.727875, mean_q: -39.223511\n",
            "  754956/10000000: episode: 3756, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -95.800, mean reward: -0.477 [-47.900, 139.200], mean action: 2.657 [0.000, 10.000], mean observation: 35.033 [0.001, 419.000], loss: 612.171631, mae: 38.134949, mean_q: -38.048344\n",
            "  755157/10000000: episode: 3757, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -178.800, mean reward: -0.890 [-89.400, 333.000], mean action: 5.388 [0.000, 10.000], mean observation: 36.961 [0.000, 654.700], loss: 672.071655, mae: 36.786804, mean_q: -37.297531\n",
            "  755358/10000000: episode: 3758, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: 114.800, mean reward: 0.571 [-10.000, 234.400], mean action: 3.428 [0.000, 10.000], mean observation: 33.468 [0.000, 706.400], loss: 446.293182, mae: 36.656513, mean_q: -36.753178\n",
            "  755559/10000000: episode: 3759, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -585.800, mean reward: -2.914 [-292.900, 120.400], mean action: 3.214 [0.000, 10.000], mean observation: 31.877 [0.001, 434.500], loss: 471.237488, mae: 36.627972, mean_q: -37.179195\n",
            "  755760/10000000: episode: 3760, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -569.400, mean reward: -2.833 [-284.700, 53.200], mean action: 3.085 [0.000, 10.000], mean observation: 32.266 [0.001, 568.100], loss: 506.143890, mae: 36.253147, mean_q: -36.645279\n",
            "  755961/10000000: episode: 3761, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -647.200, mean reward: -3.220 [-323.600, 147.700], mean action: 4.617 [0.000, 10.000], mean observation: 30.112 [0.001, 676.900], loss: 416.118225, mae: 35.571270, mean_q: -36.035027\n",
            "  756162/10000000: episode: 3762, duration: 1.617s, episode steps: 201, steps per second: 124, episode reward: -378.000, mean reward: -1.881 [-189.000, 398.000], mean action: 4.498 [0.000, 10.000], mean observation: 37.787 [0.000, 714.300], loss: 337.637604, mae: 35.817936, mean_q: -36.925014\n",
            "  756363/10000000: episode: 3763, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: -554.400, mean reward: -2.758 [-277.200, 49.200], mean action: 2.662 [0.000, 10.000], mean observation: 37.029 [0.000, 601.500], loss: 398.046967, mae: 37.449944, mean_q: -38.585880\n",
            "  756564/10000000: episode: 3764, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: -190.200, mean reward: -0.946 [-95.100, 82.800], mean action: 3.184 [0.000, 10.000], mean observation: 34.111 [0.000, 582.800], loss: 298.354919, mae: 37.975681, mean_q: -38.912708\n",
            "  756765/10000000: episode: 3765, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -110.200, mean reward: -0.548 [-55.100, 60.000], mean action: 2.851 [0.000, 10.000], mean observation: 33.951 [0.001, 498.800], loss: 551.785583, mae: 37.016445, mean_q: -37.525333\n",
            "  756966/10000000: episode: 3766, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -441.200, mean reward: -2.195 [-220.600, 136.000], mean action: 4.159 [0.000, 10.000], mean observation: 28.395 [0.002, 508.500], loss: 432.558990, mae: 36.185978, mean_q: -37.464348\n",
            "  757167/10000000: episode: 3767, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: 210.600, mean reward: 1.048 [-10.000, 245.700], mean action: 4.114 [0.000, 10.000], mean observation: 30.702 [0.004, 413.800], loss: 403.701996, mae: 36.558979, mean_q: -37.996391\n",
            "  757368/10000000: episode: 3768, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -284.400, mean reward: -1.415 [-142.200, 465.500], mean action: 3.935 [0.000, 10.000], mean observation: 35.263 [0.000, 367.900], loss: 635.463806, mae: 37.208755, mean_q: -38.324089\n",
            "  757569/10000000: episode: 3769, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -686.400, mean reward: -3.415 [-343.200, 420.700], mean action: 4.836 [0.000, 10.000], mean observation: 36.898 [0.001, 630.500], loss: 744.453186, mae: 37.373558, mean_q: -38.284584\n",
            "  757770/10000000: episode: 3770, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: 1591.600, mean reward: 7.918 [-10.000, 1321.000], mean action: 6.811 [0.000, 10.000], mean observation: 31.988 [0.001, 518.600], loss: 478.438812, mae: 35.859089, mean_q: -36.521587\n",
            "  757971/10000000: episode: 3771, duration: 1.635s, episode steps: 201, steps per second: 123, episode reward: -1446.200, mean reward: -7.195 [-723.100, 111.100], mean action: 6.070 [0.000, 10.000], mean observation: 35.127 [0.000, 562.200], loss: 630.128845, mae: 35.033421, mean_q: -34.977268\n",
            "  758172/10000000: episode: 3772, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: -729.000, mean reward: -3.627 [-364.500, 157.000], mean action: 5.129 [0.000, 10.000], mean observation: 33.011 [0.002, 637.200], loss: 340.023560, mae: 34.745525, mean_q: -34.610001\n",
            "  758373/10000000: episode: 3773, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -1289.000, mean reward: -6.413 [-644.500, 136.000], mean action: 5.627 [0.000, 10.000], mean observation: 28.096 [0.005, 381.200], loss: 498.336121, mae: 34.139744, mean_q: -34.241646\n",
            "  758574/10000000: episode: 3774, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: 485.600, mean reward: 2.416 [-10.000, 242.800], mean action: 5.308 [0.000, 10.000], mean observation: 32.083 [0.002, 607.700], loss: 597.848816, mae: 33.246407, mean_q: -33.400215\n",
            "  758775/10000000: episode: 3775, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: -906.000, mean reward: -4.507 [-453.000, 161.700], mean action: 4.468 [0.000, 10.000], mean observation: 26.821 [0.001, 497.800], loss: 436.053162, mae: 33.998631, mean_q: -33.866695\n",
            "  758976/10000000: episode: 3776, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -471.600, mean reward: -2.346 [-235.800, 167.000], mean action: 3.726 [0.000, 10.000], mean observation: 39.021 [0.001, 462.800], loss: 419.398956, mae: 33.145874, mean_q: -33.316616\n",
            "  759177/10000000: episode: 3777, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -751.000, mean reward: -3.736 [-375.500, 84.000], mean action: 3.498 [0.000, 10.000], mean observation: 34.645 [0.000, 772.000], loss: 425.058563, mae: 33.361183, mean_q: -33.384811\n",
            "  759378/10000000: episode: 3778, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -54.400, mean reward: -0.271 [-27.200, 261.000], mean action: 3.338 [0.000, 10.000], mean observation: 30.635 [0.001, 558.800], loss: 558.566956, mae: 33.041176, mean_q: -33.155846\n",
            "  759579/10000000: episode: 3779, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 557.400, mean reward: 2.773 [-10.000, 278.700], mean action: 3.627 [0.000, 10.000], mean observation: 30.522 [0.000, 501.100], loss: 628.153870, mae: 32.817669, mean_q: -33.079765\n",
            "  759780/10000000: episode: 3780, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: 46.600, mean reward: 0.232 [-10.000, 160.200], mean action: 3.408 [0.000, 10.000], mean observation: 36.295 [0.001, 645.600], loss: 620.864807, mae: 34.007259, mean_q: -34.264530\n",
            "  759981/10000000: episode: 3781, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -1049.000, mean reward: -5.219 [-524.500, 38.000], mean action: 3.836 [0.000, 10.000], mean observation: 31.484 [0.001, 487.300], loss: 612.909302, mae: 34.133827, mean_q: -34.495266\n",
            "  760182/10000000: episode: 3782, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: 912.000, mean reward: 4.537 [-10.000, 456.000], mean action: 4.149 [0.000, 10.000], mean observation: 34.657 [0.000, 555.400], loss: 473.981842, mae: 35.063789, mean_q: -35.520164\n",
            "  760383/10000000: episode: 3783, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -486.400, mean reward: -2.420 [-243.200, 240.800], mean action: 3.090 [0.000, 10.000], mean observation: 34.184 [0.000, 553.100], loss: 396.154785, mae: 34.811649, mean_q: -35.188000\n",
            "  760584/10000000: episode: 3784, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -903.600, mean reward: -4.496 [-451.800, 88.500], mean action: 3.368 [0.000, 10.000], mean observation: 35.669 [0.000, 668.100], loss: 490.796265, mae: 34.846283, mean_q: -35.017651\n",
            "  760785/10000000: episode: 3785, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -1249.400, mean reward: -6.216 [-624.700, 19.800], mean action: 3.761 [0.000, 10.000], mean observation: 33.387 [0.000, 489.400], loss: 417.748260, mae: 34.689938, mean_q: -35.540348\n",
            "  760986/10000000: episode: 3786, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -437.000, mean reward: -2.174 [-218.500, 57.600], mean action: 2.622 [0.000, 10.000], mean observation: 27.433 [0.002, 513.300], loss: 341.323456, mae: 35.014244, mean_q: -35.183807\n",
            "  761187/10000000: episode: 3787, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -407.600, mean reward: -2.028 [-203.800, 38.000], mean action: 1.930 [0.000, 10.000], mean observation: 33.749 [0.000, 606.600], loss: 434.305389, mae: 34.601738, mean_q: -34.358040\n",
            "  761388/10000000: episode: 3788, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -471.800, mean reward: -2.347 [-235.900, 123.400], mean action: 2.199 [0.000, 10.000], mean observation: 36.536 [0.001, 493.400], loss: 317.892242, mae: 34.247536, mean_q: -34.241394\n",
            "  761589/10000000: episode: 3789, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -573.200, mean reward: -2.852 [-286.600, 28.000], mean action: 2.219 [0.000, 10.000], mean observation: 33.029 [0.002, 578.200], loss: 605.732727, mae: 34.650436, mean_q: -34.930500\n",
            "  761790/10000000: episode: 3790, duration: 1.564s, episode steps: 201, steps per second: 128, episode reward: 400.400, mean reward: 1.992 [-10.000, 651.000], mean action: 2.766 [0.000, 10.000], mean observation: 31.836 [0.001, 461.100], loss: 378.770844, mae: 33.731331, mean_q: -34.372005\n",
            "  761991/10000000: episode: 3791, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -602.600, mean reward: -2.998 [-301.300, 32.100], mean action: 2.915 [0.000, 10.000], mean observation: 40.869 [0.000, 527.200], loss: 614.990540, mae: 34.190151, mean_q: -34.823265\n",
            "  762192/10000000: episode: 3792, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -487.200, mean reward: -2.424 [-243.600, 228.000], mean action: 3.821 [0.000, 10.000], mean observation: 31.283 [0.001, 675.300], loss: 358.586182, mae: 34.634903, mean_q: -35.953640\n",
            "  762393/10000000: episode: 3793, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -566.400, mean reward: -2.818 [-283.200, 34.800], mean action: 2.781 [0.000, 10.000], mean observation: 31.290 [0.001, 472.800], loss: 251.746933, mae: 35.342525, mean_q: -35.974243\n",
            "  762594/10000000: episode: 3794, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -117.200, mean reward: -0.583 [-58.600, 235.200], mean action: 3.159 [0.000, 10.000], mean observation: 32.560 [0.000, 527.500], loss: 285.924805, mae: 35.202271, mean_q: -35.940796\n",
            "  762795/10000000: episode: 3795, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: 593.400, mean reward: 2.952 [-10.000, 322.000], mean action: 3.672 [0.000, 10.000], mean observation: 29.728 [0.000, 544.700], loss: 388.690674, mae: 35.566177, mean_q: -36.708717\n",
            "  762996/10000000: episode: 3796, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -286.600, mean reward: -1.426 [-143.300, 128.100], mean action: 2.552 [0.000, 10.000], mean observation: 39.223 [0.001, 532.900], loss: 345.661835, mae: 36.124603, mean_q: -36.751587\n",
            "  763197/10000000: episode: 3797, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: 740.000, mean reward: 3.682 [-10.000, 552.300], mean action: 3.473 [0.000, 10.000], mean observation: 34.867 [0.002, 510.800], loss: 579.056885, mae: 35.798218, mean_q: -36.566032\n",
            "  763398/10000000: episode: 3798, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -828.000, mean reward: -4.119 [-414.000, 95.200], mean action: 3.736 [0.000, 10.000], mean observation: 32.535 [0.001, 647.400], loss: 388.710846, mae: 35.723087, mean_q: -36.685741\n",
            "  763599/10000000: episode: 3799, duration: 1.632s, episode steps: 201, steps per second: 123, episode reward: -574.800, mean reward: -2.860 [-287.400, 155.700], mean action: 4.592 [0.000, 10.000], mean observation: 31.589 [0.000, 602.400], loss: 437.420624, mae: 34.862427, mean_q: -35.866760\n",
            "  763800/10000000: episode: 3800, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: -1185.400, mean reward: -5.898 [-592.700, 156.000], mean action: 5.149 [0.000, 10.000], mean observation: 32.306 [0.002, 364.300], loss: 593.777161, mae: 34.724686, mean_q: -35.561420\n",
            "  764001/10000000: episode: 3801, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: 1500.400, mean reward: 7.465 [-10.000, 892.000], mean action: 3.398 [0.000, 10.000], mean observation: 28.321 [0.001, 458.100], loss: 431.287689, mae: 34.996983, mean_q: -35.669418\n",
            "  764202/10000000: episode: 3802, duration: 1.602s, episode steps: 201, steps per second: 125, episode reward: 538.000, mean reward: 2.677 [-10.000, 517.500], mean action: 4.035 [0.000, 10.000], mean observation: 32.588 [0.000, 695.400], loss: 362.940521, mae: 34.410496, mean_q: -34.985001\n",
            "  764403/10000000: episode: 3803, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: 522.800, mean reward: 2.601 [-10.000, 394.000], mean action: 2.915 [0.000, 10.000], mean observation: 38.209 [0.000, 660.900], loss: 435.551727, mae: 35.112202, mean_q: -35.445976\n",
            "  764604/10000000: episode: 3804, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: 1920.800, mean reward: 9.556 [-10.000, 1060.000], mean action: 3.488 [0.000, 10.000], mean observation: 31.595 [0.001, 400.300], loss: 340.657867, mae: 35.197582, mean_q: -35.592396\n",
            "  764805/10000000: episode: 3805, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -238.400, mean reward: -1.186 [-119.200, 335.300], mean action: 3.189 [0.000, 10.000], mean observation: 33.174 [0.000, 446.900], loss: 436.044769, mae: 35.205315, mean_q: -35.689384\n",
            "  765006/10000000: episode: 3806, duration: 1.619s, episode steps: 201, steps per second: 124, episode reward: -950.200, mean reward: -4.727 [-475.100, 66.500], mean action: 3.746 [0.000, 10.000], mean observation: 33.803 [0.001, 654.600], loss: 704.812439, mae: 35.032253, mean_q: -35.813229\n",
            "  765207/10000000: episode: 3807, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: 881.600, mean reward: 4.386 [-10.000, 487.000], mean action: 6.035 [0.000, 10.000], mean observation: 31.803 [0.000, 467.400], loss: 453.861206, mae: 34.520687, mean_q: -35.048935\n",
            "  765408/10000000: episode: 3808, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -966.200, mean reward: -4.807 [-483.100, 123.000], mean action: 4.791 [0.000, 10.000], mean observation: 36.157 [0.000, 420.600], loss: 542.175598, mae: 34.182251, mean_q: -34.455399\n",
            "  765609/10000000: episode: 3809, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -1068.000, mean reward: -5.313 [-534.000, 229.000], mean action: 5.637 [0.000, 10.000], mean observation: 32.007 [0.000, 697.600], loss: 418.018127, mae: 33.709690, mean_q: -33.653603\n",
            "  765810/10000000: episode: 3810, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -814.800, mean reward: -4.054 [-407.400, 81.900], mean action: 3.856 [0.000, 10.000], mean observation: 32.426 [0.000, 443.500], loss: 268.159698, mae: 33.487793, mean_q: -33.682713\n",
            "  766011/10000000: episode: 3811, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: 196.000, mean reward: 0.975 [-10.000, 267.000], mean action: 4.801 [0.000, 10.000], mean observation: 34.201 [0.001, 470.800], loss: 825.861145, mae: 33.053555, mean_q: -33.426823\n",
            "  766212/10000000: episode: 3812, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -755.200, mean reward: -3.757 [-377.600, 290.000], mean action: 5.378 [0.000, 10.000], mean observation: 27.875 [0.002, 400.000], loss: 490.364349, mae: 32.737789, mean_q: -32.835903\n",
            "  766413/10000000: episode: 3813, duration: 1.582s, episode steps: 201, steps per second: 127, episode reward: -524.800, mean reward: -2.611 [-262.400, 104.000], mean action: 3.493 [0.000, 10.000], mean observation: 33.461 [0.002, 515.900], loss: 304.621216, mae: 33.402893, mean_q: -33.229809\n",
            "  766614/10000000: episode: 3814, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: 4.000, mean reward: 0.020 [-10.000, 156.000], mean action: 3.070 [0.000, 10.000], mean observation: 35.487 [0.002, 467.700], loss: 366.966156, mae: 33.617474, mean_q: -33.425217\n",
            "  766815/10000000: episode: 3815, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -298.400, mean reward: -1.485 [-149.200, 106.000], mean action: 3.338 [0.000, 10.000], mean observation: 27.218 [0.005, 434.600], loss: 378.058136, mae: 33.459343, mean_q: -33.911335\n",
            "  767016/10000000: episode: 3816, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -455.200, mean reward: -2.265 [-227.600, 127.000], mean action: 4.483 [0.000, 10.000], mean observation: 28.986 [0.000, 638.600], loss: 519.052856, mae: 32.976292, mean_q: -33.689758\n",
            "  767217/10000000: episode: 3817, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -691.000, mean reward: -3.438 [-345.500, 193.000], mean action: 4.557 [0.000, 10.000], mean observation: 39.386 [0.000, 584.000], loss: 482.380402, mae: 33.119331, mean_q: -33.857979\n",
            "  767418/10000000: episode: 3818, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -309.800, mean reward: -1.541 [-154.900, 333.000], mean action: 4.632 [0.000, 10.000], mean observation: 29.605 [0.000, 530.500], loss: 457.443237, mae: 33.310452, mean_q: -33.703415\n",
            "  767619/10000000: episode: 3819, duration: 1.517s, episode steps: 201, steps per second: 133, episode reward: 148.800, mean reward: 0.740 [-10.000, 421.200], mean action: 2.950 [0.000, 10.000], mean observation: 36.256 [0.000, 796.200], loss: 644.276917, mae: 33.119900, mean_q: -32.817459\n",
            "  767820/10000000: episode: 3820, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -827.800, mean reward: -4.118 [-413.900, 71.400], mean action: 3.318 [0.000, 10.000], mean observation: 37.195 [0.000, 588.400], loss: 584.477356, mae: 32.295139, mean_q: -32.119610\n",
            "  768021/10000000: episode: 3821, duration: 1.578s, episode steps: 201, steps per second: 127, episode reward: -44.600, mean reward: -0.222 [-22.300, 560.000], mean action: 4.403 [0.000, 10.000], mean observation: 33.961 [0.000, 507.900], loss: 601.474243, mae: 31.718945, mean_q: -31.753046\n",
            "  768222/10000000: episode: 3822, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -471.800, mean reward: -2.347 [-235.900, 589.600], mean action: 5.522 [0.000, 10.000], mean observation: 33.388 [0.003, 463.800], loss: 780.911621, mae: 31.432135, mean_q: -31.406570\n",
            "  768423/10000000: episode: 3823, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: 419.200, mean reward: 2.086 [-10.000, 555.000], mean action: 5.408 [0.000, 10.000], mean observation: 29.457 [0.002, 545.200], loss: 582.816895, mae: 31.210890, mean_q: -31.161165\n",
            "  768624/10000000: episode: 3824, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -895.600, mean reward: -4.456 [-447.800, 111.000], mean action: 4.512 [0.000, 10.000], mean observation: 28.876 [0.004, 451.800], loss: 408.577576, mae: 31.633585, mean_q: -31.383806\n",
            "  768825/10000000: episode: 3825, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -592.600, mean reward: -2.948 [-296.300, 116.200], mean action: 3.881 [0.000, 10.000], mean observation: 35.684 [0.000, 491.000], loss: 555.119568, mae: 31.159981, mean_q: -31.172157\n",
            "  769026/10000000: episode: 3826, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -868.400, mean reward: -4.320 [-434.200, 70.000], mean action: 3.065 [0.000, 10.000], mean observation: 32.728 [0.002, 481.400], loss: 450.127716, mae: 31.531517, mean_q: -31.941692\n",
            "  769227/10000000: episode: 3827, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -770.600, mean reward: -3.834 [-385.300, 102.000], mean action: 3.254 [0.000, 10.000], mean observation: 31.401 [0.000, 746.900], loss: 511.052277, mae: 32.289696, mean_q: -32.472313\n",
            "  769428/10000000: episode: 3828, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -996.000, mean reward: -4.955 [-498.000, 70.200], mean action: 3.607 [0.000, 10.000], mean observation: 30.777 [0.000, 609.400], loss: 453.959381, mae: 31.548998, mean_q: -31.950453\n",
            "  769629/10000000: episode: 3829, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -887.000, mean reward: -4.413 [-443.500, 48.000], mean action: 3.159 [0.000, 10.000], mean observation: 30.065 [0.001, 583.400], loss: 500.485382, mae: 32.250713, mean_q: -32.105553\n",
            "  769830/10000000: episode: 3830, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -440.200, mean reward: -2.190 [-220.100, 90.000], mean action: 3.746 [0.000, 10.000], mean observation: 32.904 [0.000, 702.100], loss: 591.832092, mae: 31.820890, mean_q: -32.013744\n",
            "  770031/10000000: episode: 3831, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -298.000, mean reward: -1.483 [-149.000, 298.200], mean action: 3.274 [0.000, 10.000], mean observation: 42.181 [0.000, 653.600], loss: 423.747498, mae: 32.382195, mean_q: -32.347340\n",
            "  770232/10000000: episode: 3832, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -300.800, mean reward: -1.497 [-150.400, 142.100], mean action: 3.547 [0.000, 10.000], mean observation: 25.672 [0.000, 509.800], loss: 539.786011, mae: 32.237923, mean_q: -32.662071\n",
            "  770433/10000000: episode: 3833, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -659.600, mean reward: -3.282 [-329.800, 99.400], mean action: 4.284 [0.000, 10.000], mean observation: 37.522 [0.002, 630.900], loss: 549.773071, mae: 32.627102, mean_q: -33.476482\n",
            "  770634/10000000: episode: 3834, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: 216.400, mean reward: 1.077 [-10.000, 191.400], mean action: 3.652 [0.000, 10.000], mean observation: 37.030 [0.001, 571.000], loss: 439.710266, mae: 33.030067, mean_q: -33.738888\n",
            "  770835/10000000: episode: 3835, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -798.200, mean reward: -3.971 [-399.100, 96.600], mean action: 3.418 [0.000, 10.000], mean observation: 36.439 [0.001, 591.000], loss: 474.712524, mae: 33.546947, mean_q: -34.045273\n",
            "  771036/10000000: episode: 3836, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -571.800, mean reward: -2.845 [-285.900, 76.000], mean action: 3.095 [0.000, 10.000], mean observation: 29.438 [0.001, 532.400], loss: 439.366516, mae: 33.845356, mean_q: -34.108109\n",
            "  771237/10000000: episode: 3837, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: 402.600, mean reward: 2.003 [-10.000, 201.300], mean action: 2.612 [0.000, 10.000], mean observation: 34.664 [0.002, 595.600], loss: 641.438843, mae: 33.863865, mean_q: -33.709858\n",
            "  771438/10000000: episode: 3838, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: 536.400, mean reward: 2.669 [-10.000, 291.000], mean action: 3.070 [0.000, 10.000], mean observation: 32.226 [0.000, 530.000], loss: 500.788208, mae: 33.910297, mean_q: -34.264637\n",
            "  771639/10000000: episode: 3839, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: 435.400, mean reward: 2.166 [-10.000, 365.400], mean action: 4.204 [0.000, 10.000], mean observation: 28.093 [0.002, 504.200], loss: 555.482300, mae: 33.559284, mean_q: -33.846195\n",
            "  771840/10000000: episode: 3840, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -1308.000, mean reward: -6.507 [-654.000, 108.800], mean action: 5.388 [0.000, 10.000], mean observation: 34.058 [0.001, 591.100], loss: 566.850403, mae: 32.950699, mean_q: -32.969200\n",
            "  772041/10000000: episode: 3841, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: 417.800, mean reward: 2.079 [-10.000, 528.300], mean action: 4.134 [0.000, 10.000], mean observation: 30.058 [0.003, 545.800], loss: 263.656158, mae: 32.725937, mean_q: -32.826683\n",
            "  772242/10000000: episode: 3842, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -813.200, mean reward: -4.046 [-406.600, 40.500], mean action: 2.970 [0.000, 10.000], mean observation: 38.863 [0.001, 598.700], loss: 495.584564, mae: 32.986473, mean_q: -32.993797\n",
            "  772443/10000000: episode: 3843, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -777.600, mean reward: -3.869 [-388.800, 85.000], mean action: 3.866 [0.000, 10.000], mean observation: 28.479 [0.002, 308.400], loss: 422.218445, mae: 32.665276, mean_q: -33.512733\n",
            "  772644/10000000: episode: 3844, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: 22.200, mean reward: 0.110 [-10.000, 277.200], mean action: 4.433 [0.000, 10.000], mean observation: 30.642 [0.000, 479.300], loss: 753.907898, mae: 32.619915, mean_q: -33.435398\n",
            "  772845/10000000: episode: 3845, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -1144.400, mean reward: -5.694 [-572.200, 65.100], mean action: 4.368 [0.000, 10.000], mean observation: 34.006 [0.000, 663.800], loss: 597.825073, mae: 32.640789, mean_q: -33.378075\n",
            "  773046/10000000: episode: 3846, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -768.000, mean reward: -3.821 [-384.000, 79.800], mean action: 5.423 [0.000, 10.000], mean observation: 29.012 [0.001, 477.000], loss: 444.805298, mae: 32.865868, mean_q: -33.458569\n",
            "  773247/10000000: episode: 3847, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -894.800, mean reward: -4.452 [-447.400, 110.700], mean action: 3.766 [0.000, 10.000], mean observation: 34.627 [0.000, 639.500], loss: 421.298584, mae: 33.260712, mean_q: -33.457481\n",
            "  773448/10000000: episode: 3848, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -841.000, mean reward: -4.184 [-420.500, 272.700], mean action: 4.597 [0.000, 10.000], mean observation: 31.813 [0.001, 496.800], loss: 603.556946, mae: 32.353191, mean_q: -32.876419\n",
            "  773649/10000000: episode: 3849, duration: 1.745s, episode steps: 201, steps per second: 115, episode reward: -763.000, mean reward: -3.796 [-381.500, 274.500], mean action: 4.448 [0.000, 10.000], mean observation: 31.541 [0.001, 619.200], loss: 438.393890, mae: 32.220821, mean_q: -33.151196\n",
            "  773850/10000000: episode: 3850, duration: 1.687s, episode steps: 201, steps per second: 119, episode reward: -719.000, mean reward: -3.577 [-359.500, 232.200], mean action: 4.368 [0.000, 10.000], mean observation: 36.152 [0.001, 651.100], loss: 407.803711, mae: 33.273071, mean_q: -34.339760\n",
            "  774051/10000000: episode: 3851, duration: 1.703s, episode steps: 201, steps per second: 118, episode reward: -798.000, mean reward: -3.970 [-399.000, 145.000], mean action: 4.234 [0.000, 10.000], mean observation: 37.687 [0.000, 818.100], loss: 551.790405, mae: 33.837761, mean_q: -34.855137\n",
            "  774252/10000000: episode: 3852, duration: 1.618s, episode steps: 201, steps per second: 124, episode reward: -450.600, mean reward: -2.242 [-225.300, 220.000], mean action: 4.204 [0.000, 10.000], mean observation: 30.983 [0.003, 540.500], loss: 461.669006, mae: 34.467308, mean_q: -35.577499\n",
            "  774453/10000000: episode: 3853, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -932.000, mean reward: -4.637 [-466.000, 63.000], mean action: 3.020 [0.000, 10.000], mean observation: 34.525 [0.001, 565.900], loss: 526.294617, mae: 35.405563, mean_q: -35.727509\n",
            "  774654/10000000: episode: 3854, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -843.000, mean reward: -4.194 [-421.500, 91.700], mean action: 2.945 [0.000, 10.000], mean observation: 34.040 [0.000, 720.900], loss: 640.286072, mae: 35.516094, mean_q: -36.024616\n",
            "  774855/10000000: episode: 3855, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: 1231.400, mean reward: 6.126 [-10.000, 615.700], mean action: 3.517 [0.000, 10.000], mean observation: 30.296 [0.002, 624.500], loss: 768.213867, mae: 35.718292, mean_q: -36.209820\n",
            "  775056/10000000: episode: 3856, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -469.800, mean reward: -2.337 [-234.900, 315.000], mean action: 4.746 [0.000, 10.000], mean observation: 35.127 [0.002, 503.100], loss: 396.762360, mae: 35.341026, mean_q: -36.344921\n",
            "  775257/10000000: episode: 3857, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -827.800, mean reward: -4.118 [-413.900, 110.000], mean action: 3.612 [0.000, 10.000], mean observation: 26.726 [0.001, 462.800], loss: 553.793213, mae: 36.170563, mean_q: -36.705456\n",
            "  775458/10000000: episode: 3858, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -180.200, mean reward: -0.897 [-90.100, 115.300], mean action: 3.418 [0.000, 10.000], mean observation: 35.112 [0.000, 476.600], loss: 483.169922, mae: 35.280769, mean_q: -35.954563\n",
            "  775659/10000000: episode: 3859, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: -1292.400, mean reward: -6.430 [-646.200, 97.200], mean action: 5.095 [0.000, 10.000], mean observation: 39.459 [0.001, 642.700], loss: 465.366486, mae: 34.717049, mean_q: -35.751675\n",
            "  775860/10000000: episode: 3860, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: -1110.200, mean reward: -5.523 [-555.100, 119.000], mean action: 5.055 [0.000, 10.000], mean observation: 35.352 [0.000, 907.100], loss: 558.812683, mae: 35.306484, mean_q: -36.542305\n",
            "  776061/10000000: episode: 3861, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -144.600, mean reward: -0.719 [-72.300, 295.500], mean action: 5.164 [0.000, 10.000], mean observation: 30.172 [0.001, 399.700], loss: 258.076416, mae: 35.475334, mean_q: -36.634937\n",
            "  776262/10000000: episode: 3862, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -1326.400, mean reward: -6.599 [-663.200, 83.000], mean action: 4.582 [0.000, 10.000], mean observation: 34.798 [0.001, 507.200], loss: 671.553406, mae: 34.987942, mean_q: -35.585907\n",
            "  776463/10000000: episode: 3863, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -297.200, mean reward: -1.479 [-148.600, 191.700], mean action: 3.348 [0.000, 10.000], mean observation: 30.694 [0.002, 522.800], loss: 494.958252, mae: 35.932552, mean_q: -36.040531\n",
            "  776664/10000000: episode: 3864, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: 572.200, mean reward: 2.847 [-10.000, 286.100], mean action: 2.776 [0.000, 10.000], mean observation: 30.016 [0.000, 623.900], loss: 456.854279, mae: 35.947090, mean_q: -36.177589\n",
            "  776865/10000000: episode: 3865, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 44.200, mean reward: 0.220 [-10.000, 191.700], mean action: 2.582 [0.000, 10.000], mean observation: 34.201 [0.002, 542.000], loss: 437.724579, mae: 36.028427, mean_q: -36.531734\n",
            "  777066/10000000: episode: 3866, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: 2572.400, mean reward: 12.798 [-10.000, 1286.200], mean action: 2.980 [0.000, 10.000], mean observation: 32.889 [0.000, 493.100], loss: 480.589539, mae: 36.215004, mean_q: -36.599659\n",
            "  777267/10000000: episode: 3867, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: 420.600, mean reward: 2.093 [-10.000, 485.000], mean action: 2.935 [0.000, 10.000], mean observation: 32.485 [0.000, 781.200], loss: 615.900391, mae: 35.906963, mean_q: -36.438419\n",
            "  777468/10000000: episode: 3868, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: 468.200, mean reward: 2.329 [-10.000, 445.900], mean action: 3.050 [0.000, 10.000], mean observation: 31.168 [0.001, 492.700], loss: 378.841888, mae: 35.990929, mean_q: -36.707840\n",
            "  777669/10000000: episode: 3869, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -475.000, mean reward: -2.363 [-237.500, 54.900], mean action: 2.657 [0.000, 10.000], mean observation: 35.298 [0.001, 467.200], loss: 706.934082, mae: 36.363873, mean_q: -36.910503\n",
            "  777870/10000000: episode: 3870, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -572.200, mean reward: -2.847 [-286.100, 202.500], mean action: 3.692 [0.000, 10.000], mean observation: 33.349 [0.001, 511.400], loss: 769.359619, mae: 35.426849, mean_q: -36.643677\n",
            "  778071/10000000: episode: 3871, duration: 1.579s, episode steps: 201, steps per second: 127, episode reward: 105.000, mean reward: 0.522 [-10.000, 163.800], mean action: 4.547 [0.000, 10.000], mean observation: 35.070 [0.000, 764.600], loss: 753.940735, mae: 34.865246, mean_q: -36.279469\n",
            "  778272/10000000: episode: 3872, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -1209.200, mean reward: -6.016 [-604.600, 81.000], mean action: 4.239 [0.000, 10.000], mean observation: 31.121 [0.000, 498.200], loss: 533.538452, mae: 34.550163, mean_q: -35.741711\n",
            "  778473/10000000: episode: 3873, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -1098.200, mean reward: -5.464 [-549.100, 45.000], mean action: 4.104 [0.000, 10.000], mean observation: 26.144 [0.001, 516.900], loss: 633.119141, mae: 35.130463, mean_q: -36.281799\n",
            "  778674/10000000: episode: 3874, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -1054.200, mean reward: -5.245 [-527.100, 50.800], mean action: 4.229 [0.000, 10.000], mean observation: 34.597 [0.000, 598.900], loss: 865.982178, mae: 34.888531, mean_q: -35.729889\n",
            "  778875/10000000: episode: 3875, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -1225.000, mean reward: -6.095 [-612.500, 35.400], mean action: 3.647 [0.000, 10.000], mean observation: 42.000 [0.000, 792.800], loss: 348.101807, mae: 34.651443, mean_q: -35.416801\n",
            "  779076/10000000: episode: 3876, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -471.400, mean reward: -2.345 [-235.700, 143.000], mean action: 3.368 [0.000, 10.000], mean observation: 27.455 [0.001, 420.000], loss: 420.134399, mae: 35.437775, mean_q: -35.830685\n",
            "  779277/10000000: episode: 3877, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -601.000, mean reward: -2.990 [-300.500, 50.400], mean action: 2.403 [0.000, 10.000], mean observation: 41.734 [0.000, 746.700], loss: 407.327728, mae: 35.633778, mean_q: -36.250290\n",
            "  779478/10000000: episode: 3878, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: 238.000, mean reward: 1.184 [-10.000, 219.000], mean action: 3.000 [0.000, 10.000], mean observation: 30.894 [0.001, 375.100], loss: 553.743225, mae: 35.757278, mean_q: -36.553780\n",
            "  779679/10000000: episode: 3879, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -879.200, mean reward: -4.374 [-439.600, 42.300], mean action: 3.279 [0.000, 10.000], mean observation: 33.188 [0.000, 541.500], loss: 621.144348, mae: 35.868587, mean_q: -36.981079\n",
            "  779880/10000000: episode: 3880, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -338.200, mean reward: -1.683 [-169.100, 128.000], mean action: 3.303 [0.000, 10.000], mean observation: 29.893 [0.001, 519.300], loss: 487.939087, mae: 36.114117, mean_q: -37.298431\n",
            "  780081/10000000: episode: 3881, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -759.800, mean reward: -3.780 [-379.900, 119.700], mean action: 3.473 [0.000, 10.000], mean observation: 34.721 [0.002, 460.100], loss: 643.084290, mae: 36.355026, mean_q: -37.583145\n",
            "  780282/10000000: episode: 3882, duration: 1.633s, episode steps: 201, steps per second: 123, episode reward: 486.800, mean reward: 2.422 [-10.000, 451.600], mean action: 4.124 [0.000, 10.000], mean observation: 35.455 [0.000, 579.300], loss: 412.884186, mae: 36.402565, mean_q: -38.039360\n",
            "  780483/10000000: episode: 3883, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: -639.800, mean reward: -3.183 [-319.900, 115.200], mean action: 3.194 [0.000, 10.000], mean observation: 32.292 [0.001, 576.200], loss: 382.467987, mae: 36.829037, mean_q: -38.005665\n",
            "  780684/10000000: episode: 3884, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -317.000, mean reward: -1.577 [-158.500, 161.100], mean action: 3.060 [0.000, 10.000], mean observation: 28.537 [0.001, 550.200], loss: 575.662109, mae: 37.362511, mean_q: -38.296452\n",
            "  780885/10000000: episode: 3885, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: -755.400, mean reward: -3.758 [-377.700, 55.500], mean action: 2.677 [0.000, 10.000], mean observation: 32.708 [0.001, 548.600], loss: 488.062378, mae: 36.919376, mean_q: -37.430008\n",
            "  781086/10000000: episode: 3886, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: 386.000, mean reward: 1.920 [-10.000, 264.000], mean action: 3.199 [0.000, 10.000], mean observation: 34.289 [0.001, 494.100], loss: 578.068359, mae: 36.154720, mean_q: -36.847233\n",
            "  781287/10000000: episode: 3887, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -940.200, mean reward: -4.678 [-470.100, 24.400], mean action: 2.925 [0.000, 10.000], mean observation: 34.088 [0.000, 525.700], loss: 679.057861, mae: 35.933670, mean_q: -36.051010\n",
            "  781488/10000000: episode: 3888, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -823.200, mean reward: -4.096 [-411.600, 99.400], mean action: 3.891 [0.000, 10.000], mean observation: 43.100 [0.000, 803.400], loss: 575.311279, mae: 35.894421, mean_q: -36.853271\n",
            "  781689/10000000: episode: 3889, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: 464.200, mean reward: 2.309 [-10.000, 326.200], mean action: 3.706 [0.000, 10.000], mean observation: 27.912 [0.000, 597.200], loss: 500.591431, mae: 36.084026, mean_q: -36.891342\n",
            "  781890/10000000: episode: 3890, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -278.800, mean reward: -1.387 [-139.400, 296.800], mean action: 3.637 [0.000, 10.000], mean observation: 30.028 [0.002, 455.800], loss: 573.211365, mae: 36.344101, mean_q: -37.132355\n",
            "  782091/10000000: episode: 3891, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -607.000, mean reward: -3.020 [-303.500, 72.100], mean action: 2.915 [0.000, 10.000], mean observation: 41.563 [0.001, 545.700], loss: 393.605499, mae: 35.863831, mean_q: -36.450073\n",
            "  782292/10000000: episode: 3892, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -890.800, mean reward: -4.432 [-445.400, 59.400], mean action: 2.930 [0.000, 10.000], mean observation: 30.045 [0.000, 581.400], loss: 674.051025, mae: 35.886600, mean_q: -36.245701\n",
            "  782493/10000000: episode: 3893, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -664.400, mean reward: -3.305 [-332.200, 36.900], mean action: 2.333 [0.000, 9.000], mean observation: 35.593 [0.000, 622.400], loss: 526.941467, mae: 35.567154, mean_q: -35.940346\n",
            "  782694/10000000: episode: 3894, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -467.400, mean reward: -2.325 [-233.700, 103.500], mean action: 2.786 [0.000, 10.000], mean observation: 35.272 [0.001, 430.400], loss: 502.440186, mae: 35.341511, mean_q: -35.803410\n",
            "  782895/10000000: episode: 3895, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: 1116.200, mean reward: 5.553 [-10.000, 558.100], mean action: 3.164 [0.000, 10.000], mean observation: 37.282 [0.000, 669.400], loss: 617.239380, mae: 35.548931, mean_q: -36.355038\n",
            "  783096/10000000: episode: 3896, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -771.600, mean reward: -3.839 [-385.800, 106.800], mean action: 4.134 [0.000, 10.000], mean observation: 33.182 [0.002, 511.500], loss: 631.820984, mae: 35.583092, mean_q: -36.500111\n",
            "  783297/10000000: episode: 3897, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: 448.400, mean reward: 2.231 [-10.000, 522.000], mean action: 3.905 [0.000, 10.000], mean observation: 37.568 [0.000, 663.000], loss: 332.417450, mae: 35.603764, mean_q: -36.343357\n",
            "  783498/10000000: episode: 3898, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -596.200, mean reward: -2.966 [-298.100, 66.500], mean action: 2.920 [0.000, 10.000], mean observation: 37.424 [0.000, 637.900], loss: 659.192932, mae: 35.990398, mean_q: -36.283508\n",
            "  783699/10000000: episode: 3899, duration: 1.591s, episode steps: 201, steps per second: 126, episode reward: -317.200, mean reward: -1.578 [-158.600, 86.100], mean action: 2.796 [0.000, 10.000], mean observation: 33.042 [0.002, 542.900], loss: 639.873047, mae: 34.976379, mean_q: -35.153698\n",
            "  783900/10000000: episode: 3900, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -853.200, mean reward: -4.245 [-426.600, 145.000], mean action: 3.657 [0.000, 10.000], mean observation: 33.111 [0.001, 581.100], loss: 679.644104, mae: 34.369961, mean_q: -35.096302\n",
            "  784101/10000000: episode: 3901, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: -1159.200, mean reward: -5.767 [-579.600, 17.900], mean action: 3.234 [0.000, 10.000], mean observation: 32.410 [0.001, 469.000], loss: 715.457092, mae: 35.010719, mean_q: -36.035099\n",
            "  784302/10000000: episode: 3902, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -689.800, mean reward: -3.432 [-344.900, 204.300], mean action: 3.269 [0.000, 10.000], mean observation: 36.229 [0.000, 622.800], loss: 767.675842, mae: 35.016979, mean_q: -35.815186\n",
            "  784503/10000000: episode: 3903, duration: 1.665s, episode steps: 201, steps per second: 121, episode reward: -1152.600, mean reward: -5.734 [-576.300, 41.400], mean action: 3.567 [0.000, 10.000], mean observation: 33.690 [0.001, 446.000], loss: 433.220703, mae: 34.693230, mean_q: -35.917465\n",
            "  784704/10000000: episode: 3904, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -925.400, mean reward: -4.604 [-462.700, 82.400], mean action: 3.144 [0.000, 10.000], mean observation: 32.876 [0.001, 424.400], loss: 680.416199, mae: 35.955021, mean_q: -36.815292\n",
            "  784905/10000000: episode: 3905, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -1428.000, mean reward: -7.104 [-714.000, 48.400], mean action: 3.851 [0.000, 10.000], mean observation: 31.543 [0.000, 749.600], loss: 735.413086, mae: 35.167057, mean_q: -36.687241\n",
            "  785106/10000000: episode: 3906, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: 456.600, mean reward: 2.272 [-10.000, 770.000], mean action: 4.647 [0.000, 10.000], mean observation: 30.010 [0.001, 587.600], loss: 607.222900, mae: 35.273354, mean_q: -36.684486\n",
            "  785307/10000000: episode: 3907, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: 105.000, mean reward: 0.522 [-10.000, 404.200], mean action: 3.657 [0.000, 10.000], mean observation: 37.231 [0.002, 516.600], loss: 688.043396, mae: 35.642174, mean_q: -36.709736\n",
            "  785508/10000000: episode: 3908, duration: 1.654s, episode steps: 201, steps per second: 122, episode reward: -612.000, mean reward: -3.045 [-306.000, 140.000], mean action: 4.025 [0.000, 10.000], mean observation: 25.954 [0.001, 415.200], loss: 638.993408, mae: 35.603809, mean_q: -36.579441\n",
            "  785709/10000000: episode: 3909, duration: 1.886s, episode steps: 201, steps per second: 107, episode reward: 151.000, mean reward: 0.751 [-10.000, 614.700], mean action: 5.214 [0.000, 10.000], mean observation: 36.904 [0.004, 509.100], loss: 770.228638, mae: 35.663383, mean_q: -37.017685\n",
            "  785910/10000000: episode: 3910, duration: 1.785s, episode steps: 201, steps per second: 113, episode reward: -1047.600, mean reward: -5.212 [-523.800, 129.000], mean action: 5.408 [0.000, 10.000], mean observation: 28.394 [0.000, 590.900], loss: 707.376526, mae: 36.341904, mean_q: -37.481529\n",
            "  786111/10000000: episode: 3911, duration: 1.724s, episode steps: 201, steps per second: 117, episode reward: -1303.800, mean reward: -6.487 [-651.900, 77.600], mean action: 4.577 [0.000, 10.000], mean observation: 32.261 [0.003, 566.300], loss: 652.214722, mae: 37.190376, mean_q: -38.433868\n",
            "  786312/10000000: episode: 3912, duration: 1.778s, episode steps: 201, steps per second: 113, episode reward: -882.800, mean reward: -4.392 [-441.400, 105.300], mean action: 4.095 [0.000, 10.000], mean observation: 28.262 [0.000, 486.700], loss: 714.233215, mae: 37.729774, mean_q: -38.895130\n",
            "  786513/10000000: episode: 3913, duration: 1.682s, episode steps: 201, steps per second: 120, episode reward: -138.600, mean reward: -0.690 [-69.300, 225.000], mean action: 3.652 [0.000, 10.000], mean observation: 35.789 [0.001, 510.800], loss: 582.207153, mae: 37.827499, mean_q: -38.750633\n",
            "  786714/10000000: episode: 3914, duration: 1.697s, episode steps: 201, steps per second: 118, episode reward: 146.400, mean reward: 0.728 [-10.000, 451.800], mean action: 4.891 [0.000, 10.000], mean observation: 31.987 [0.001, 423.300], loss: 641.997986, mae: 37.551903, mean_q: -38.652905\n",
            "  786915/10000000: episode: 3915, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: 1023.600, mean reward: 5.093 [-10.000, 511.800], mean action: 4.259 [0.000, 10.000], mean observation: 33.580 [0.000, 637.000], loss: 502.564911, mae: 38.458111, mean_q: -38.869171\n",
            "  787116/10000000: episode: 3916, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -75.000, mean reward: -0.373 [-37.500, 296.000], mean action: 3.502 [0.000, 10.000], mean observation: 31.529 [0.002, 439.600], loss: 531.398071, mae: 37.813480, mean_q: -38.150074\n",
            "  787317/10000000: episode: 3917, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -492.800, mean reward: -2.452 [-246.400, 259.500], mean action: 4.229 [0.000, 10.000], mean observation: 30.683 [0.000, 640.400], loss: 559.479065, mae: 37.521736, mean_q: -37.959785\n",
            "  787518/10000000: episode: 3918, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: 866.000, mean reward: 4.308 [-10.000, 701.000], mean action: 3.274 [0.000, 10.000], mean observation: 32.097 [0.001, 512.800], loss: 586.784302, mae: 37.881710, mean_q: -38.247856\n",
            "  787719/10000000: episode: 3919, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -467.600, mean reward: -2.326 [-233.800, 192.000], mean action: 4.428 [0.000, 10.000], mean observation: 32.066 [0.001, 558.800], loss: 550.255737, mae: 37.113304, mean_q: -37.412735\n",
            "  787920/10000000: episode: 3920, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: 444.000, mean reward: 2.209 [-10.000, 423.000], mean action: 3.050 [0.000, 10.000], mean observation: 35.845 [0.000, 539.800], loss: 581.483765, mae: 37.144733, mean_q: -37.513603\n",
            "  788121/10000000: episode: 3921, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -426.400, mean reward: -2.121 [-213.200, 137.000], mean action: 3.970 [0.000, 10.000], mean observation: 35.868 [0.001, 584.600], loss: 603.615173, mae: 36.239998, mean_q: -36.897305\n",
            "  788322/10000000: episode: 3922, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -1261.800, mean reward: -6.278 [-630.900, 204.000], mean action: 5.945 [0.000, 10.000], mean observation: 35.952 [0.000, 455.400], loss: 747.181152, mae: 35.409706, mean_q: -36.076134\n",
            "  788523/10000000: episode: 3923, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -565.000, mean reward: -2.811 [-282.500, 153.000], mean action: 5.239 [0.000, 10.000], mean observation: 32.291 [0.001, 510.200], loss: 495.054871, mae: 35.594505, mean_q: -36.763474\n",
            "  788724/10000000: episode: 3924, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -1256.400, mean reward: -6.251 [-628.200, 67.500], mean action: 4.299 [0.000, 10.000], mean observation: 31.842 [0.000, 708.200], loss: 440.396454, mae: 35.983948, mean_q: -37.611671\n",
            "  788925/10000000: episode: 3925, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -393.800, mean reward: -1.959 [-196.900, 106.300], mean action: 3.159 [0.000, 10.000], mean observation: 35.234 [0.002, 500.200], loss: 559.133911, mae: 36.580158, mean_q: -38.095276\n",
            "  789126/10000000: episode: 3926, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -290.400, mean reward: -1.445 [-145.200, 171.000], mean action: 3.254 [0.000, 10.000], mean observation: 37.205 [0.000, 440.500], loss: 611.014343, mae: 36.503609, mean_q: -37.887966\n",
            "  789327/10000000: episode: 3927, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: 1002.800, mean reward: 4.989 [-10.000, 501.400], mean action: 3.209 [0.000, 10.000], mean observation: 34.512 [0.000, 566.800], loss: 590.447937, mae: 36.820164, mean_q: -37.837227\n",
            "  789528/10000000: episode: 3928, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -626.600, mean reward: -3.117 [-313.300, 52.200], mean action: 2.279 [0.000, 10.000], mean observation: 30.856 [0.001, 571.900], loss: 599.596069, mae: 36.928574, mean_q: -37.426582\n",
            "  789729/10000000: episode: 3929, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -633.000, mean reward: -3.149 [-316.500, 36.600], mean action: 2.234 [0.000, 10.000], mean observation: 34.409 [0.000, 494.100], loss: 616.333801, mae: 36.740555, mean_q: -37.008171\n",
            "  789930/10000000: episode: 3930, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -390.800, mean reward: -1.944 [-195.400, 215.200], mean action: 2.667 [0.000, 10.000], mean observation: 28.207 [0.002, 465.200], loss: 649.925415, mae: 36.559258, mean_q: -36.748070\n",
            "  790131/10000000: episode: 3931, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -273.400, mean reward: -1.360 [-136.700, 156.800], mean action: 2.328 [0.000, 10.000], mean observation: 30.688 [0.001, 533.900], loss: 844.993835, mae: 35.659126, mean_q: -35.714878\n",
            "  790332/10000000: episode: 3932, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -202.800, mean reward: -1.009 [-101.400, 132.000], mean action: 2.204 [0.000, 10.000], mean observation: 33.905 [0.000, 798.300], loss: 385.806824, mae: 35.532906, mean_q: -35.346596\n",
            "  790533/10000000: episode: 3933, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -602.400, mean reward: -2.997 [-301.200, 40.400], mean action: 2.264 [0.000, 10.000], mean observation: 30.639 [0.001, 434.300], loss: 611.775391, mae: 35.483696, mean_q: -35.753811\n",
            "  790734/10000000: episode: 3934, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -636.600, mean reward: -3.167 [-318.300, 80.100], mean action: 2.488 [0.000, 10.000], mean observation: 32.348 [0.000, 668.900], loss: 757.953064, mae: 35.551598, mean_q: -35.909519\n",
            "  790935/10000000: episode: 3935, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: 672.600, mean reward: 3.346 [-10.000, 397.800], mean action: 2.791 [0.000, 10.000], mean observation: 34.356 [0.000, 506.300], loss: 797.548584, mae: 34.634640, mean_q: -35.362957\n",
            "  791136/10000000: episode: 3936, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -583.800, mean reward: -2.904 [-291.900, 51.000], mean action: 2.542 [0.000, 10.000], mean observation: 29.504 [0.003, 383.900], loss: 535.249329, mae: 35.252701, mean_q: -35.572571\n",
            "  791337/10000000: episode: 3937, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -658.200, mean reward: -3.275 [-329.100, 43.200], mean action: 2.239 [0.000, 10.000], mean observation: 37.009 [0.000, 605.900], loss: 470.547333, mae: 35.307205, mean_q: -35.184113\n",
            "  791538/10000000: episode: 3938, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -545.000, mean reward: -2.711 [-272.500, 65.700], mean action: 2.488 [0.000, 10.000], mean observation: 35.610 [0.000, 650.000], loss: 625.429626, mae: 35.376026, mean_q: -35.553894\n",
            "  791739/10000000: episode: 3939, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -524.600, mean reward: -2.610 [-262.300, 117.600], mean action: 2.811 [0.000, 10.000], mean observation: 27.978 [0.003, 432.300], loss: 676.110229, mae: 34.352303, mean_q: -34.783165\n",
            "  791940/10000000: episode: 3940, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -126.200, mean reward: -0.628 [-63.100, 130.500], mean action: 2.264 [0.000, 10.000], mean observation: 36.210 [0.002, 420.100], loss: 417.946198, mae: 34.120621, mean_q: -33.963783\n",
            "  792141/10000000: episode: 3941, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: 611.400, mean reward: 3.042 [-10.000, 393.000], mean action: 3.100 [0.000, 10.000], mean observation: 41.403 [0.001, 510.400], loss: 538.492065, mae: 34.242050, mean_q: -34.616173\n",
            "  792342/10000000: episode: 3942, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: 2511.800, mean reward: 12.497 [-10.000, 1689.300], mean action: 3.965 [0.000, 10.000], mean observation: 31.784 [0.000, 508.900], loss: 621.124939, mae: 33.108036, mean_q: -34.321030\n",
            "  792543/10000000: episode: 3943, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: 5610.000, mean reward: 27.910 [-10.000, 3375.000], mean action: 3.741 [0.000, 10.000], mean observation: 32.804 [0.002, 447.500], loss: 913.823975, mae: 33.087139, mean_q: -34.248165\n",
            "  792744/10000000: episode: 3944, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -815.400, mean reward: -4.057 [-407.700, 58.300], mean action: 3.637 [0.000, 10.000], mean observation: 31.788 [0.001, 618.400], loss: 698.934692, mae: 33.339626, mean_q: -34.626247\n",
            "  792945/10000000: episode: 3945, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -837.400, mean reward: -4.166 [-418.700, 80.200], mean action: 3.975 [0.000, 10.000], mean observation: 31.998 [0.001, 512.000], loss: 526.413574, mae: 33.915123, mean_q: -35.391602\n",
            "  793146/10000000: episode: 3946, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -462.600, mean reward: -2.301 [-231.300, 191.800], mean action: 3.756 [0.000, 10.000], mean observation: 33.540 [0.000, 586.000], loss: 453.220001, mae: 34.460800, mean_q: -35.694836\n",
            "  793347/10000000: episode: 3947, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -652.800, mean reward: -3.248 [-326.400, 61.200], mean action: 2.856 [0.000, 10.000], mean observation: 31.758 [0.001, 423.700], loss: 515.942932, mae: 34.826981, mean_q: -35.354549\n",
            "  793548/10000000: episode: 3948, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -126.400, mean reward: -0.629 [-63.200, 152.500], mean action: 2.841 [0.000, 10.000], mean observation: 31.120 [0.001, 459.200], loss: 633.839478, mae: 34.620560, mean_q: -34.784927\n",
            "  793749/10000000: episode: 3949, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -755.600, mean reward: -3.759 [-377.800, 43.500], mean action: 2.871 [0.000, 10.000], mean observation: 31.111 [0.001, 422.300], loss: 335.237671, mae: 34.891747, mean_q: -35.242699\n",
            "  793950/10000000: episode: 3950, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -253.200, mean reward: -1.260 [-126.600, 57.600], mean action: 1.821 [0.000, 9.000], mean observation: 32.762 [0.001, 497.500], loss: 1117.265137, mae: 34.440144, mean_q: -33.806038\n",
            "  794151/10000000: episode: 3951, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -198.000, mean reward: -0.985 [-99.000, 83.300], mean action: 2.378 [0.000, 9.000], mean observation: 31.581 [0.002, 439.900], loss: 802.289673, mae: 33.894722, mean_q: -33.708324\n",
            "  794352/10000000: episode: 3952, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -791.800, mean reward: -3.939 [-395.900, 54.600], mean action: 2.896 [0.000, 10.000], mean observation: 29.588 [0.002, 527.400], loss: 336.059906, mae: 33.438862, mean_q: -33.876415\n",
            "  794553/10000000: episode: 3953, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -157.600, mean reward: -0.784 [-78.800, 143.800], mean action: 2.388 [0.000, 10.000], mean observation: 37.035 [0.000, 720.900], loss: 482.652679, mae: 34.127289, mean_q: -34.232788\n",
            "  794754/10000000: episode: 3954, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -209.000, mean reward: -1.040 [-104.500, 76.200], mean action: 2.035 [0.000, 10.000], mean observation: 31.748 [0.001, 500.800], loss: 1185.047974, mae: 33.960724, mean_q: -33.709335\n",
            "  794955/10000000: episode: 3955, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: 1400.200, mean reward: 6.966 [-10.000, 700.100], mean action: 2.697 [0.000, 10.000], mean observation: 31.926 [0.001, 424.400], loss: 1751.077393, mae: 32.960976, mean_q: -33.367107\n",
            "  795156/10000000: episode: 3956, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -253.400, mean reward: -1.261 [-126.700, 208.800], mean action: 2.338 [0.000, 10.000], mean observation: 32.247 [0.000, 441.700], loss: 1282.805664, mae: 32.574268, mean_q: -32.366295\n",
            "  795357/10000000: episode: 3957, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: 658.400, mean reward: 3.276 [-10.000, 329.200], mean action: 2.721 [0.000, 10.000], mean observation: 27.375 [0.003, 380.000], loss: 615.890747, mae: 32.385860, mean_q: -32.363533\n",
            "  795558/10000000: episode: 3958, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -738.400, mean reward: -3.674 [-369.200, 54.500], mean action: 2.622 [0.000, 10.000], mean observation: 35.478 [0.002, 537.100], loss: 490.649414, mae: 31.777052, mean_q: -31.974480\n",
            "  795759/10000000: episode: 3959, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -91.800, mean reward: -0.457 [-45.900, 264.600], mean action: 2.801 [0.000, 10.000], mean observation: 32.825 [0.001, 562.800], loss: 584.172058, mae: 31.312830, mean_q: -31.566742\n",
            "  795960/10000000: episode: 3960, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -180.800, mean reward: -0.900 [-90.400, 230.000], mean action: 2.443 [0.000, 10.000], mean observation: 31.479 [0.001, 530.600], loss: 568.881470, mae: 31.887846, mean_q: -32.033398\n",
            "  796161/10000000: episode: 3961, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -542.800, mean reward: -2.700 [-271.400, 153.000], mean action: 2.955 [0.000, 10.000], mean observation: 41.083 [0.000, 693.300], loss: 618.175903, mae: 31.971388, mean_q: -32.483215\n",
            "  796362/10000000: episode: 3962, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -612.000, mean reward: -3.045 [-306.000, 123.300], mean action: 2.652 [0.000, 10.000], mean observation: 33.157 [0.000, 727.600], loss: 656.486938, mae: 31.901814, mean_q: -32.669495\n",
            "  796563/10000000: episode: 3963, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: -432.400, mean reward: -2.151 [-216.200, 86.000], mean action: 2.050 [0.000, 10.000], mean observation: 33.518 [0.002, 453.000], loss: 506.484680, mae: 32.077381, mean_q: -32.583561\n",
            "  796764/10000000: episode: 3964, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -378.800, mean reward: -1.885 [-189.400, 65.600], mean action: 2.662 [0.000, 10.000], mean observation: 33.638 [0.001, 606.200], loss: 1507.825562, mae: 32.490044, mean_q: -33.362869\n",
            "  796965/10000000: episode: 3965, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: -824.000, mean reward: -4.100 [-412.000, 133.400], mean action: 3.378 [0.000, 10.000], mean observation: 35.399 [0.003, 593.100], loss: 427.260925, mae: 32.390732, mean_q: -33.632538\n",
            "  797166/10000000: episode: 3966, duration: 1.609s, episode steps: 201, steps per second: 125, episode reward: -979.600, mean reward: -4.874 [-489.800, 38.600], mean action: 3.194 [0.000, 10.000], mean observation: 30.540 [0.002, 482.900], loss: 433.694855, mae: 32.777470, mean_q: -34.030861\n",
            "  797367/10000000: episode: 3967, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: -422.200, mean reward: -2.100 [-211.100, 108.900], mean action: 3.328 [0.000, 10.000], mean observation: 29.672 [0.001, 697.100], loss: 1380.274902, mae: 33.019497, mean_q: -34.359810\n",
            "  797568/10000000: episode: 3968, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -1134.400, mean reward: -5.644 [-567.200, 106.000], mean action: 3.831 [0.000, 10.000], mean observation: 29.589 [0.001, 562.800], loss: 589.695557, mae: 33.532673, mean_q: -34.883411\n",
            "  797769/10000000: episode: 3969, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -755.400, mean reward: -3.758 [-377.700, 108.000], mean action: 3.667 [0.000, 10.000], mean observation: 30.400 [0.001, 471.400], loss: 570.624207, mae: 34.672993, mean_q: -35.742214\n",
            "  797970/10000000: episode: 3970, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: -895.200, mean reward: -4.454 [-447.600, 51.600], mean action: 2.985 [0.000, 10.000], mean observation: 35.792 [0.000, 522.200], loss: 422.277832, mae: 35.057915, mean_q: -36.048111\n",
            "  798171/10000000: episode: 3971, duration: 1.560s, episode steps: 201, steps per second: 129, episode reward: -4.200, mean reward: -0.021 [-10.000, 131.400], mean action: 2.891 [0.000, 10.000], mean observation: 29.427 [0.001, 521.600], loss: 482.720001, mae: 35.378269, mean_q: -36.044113\n",
            "  798372/10000000: episode: 3972, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: 97.200, mean reward: 0.484 [-10.000, 230.000], mean action: 2.204 [0.000, 10.000], mean observation: 32.292 [0.000, 527.200], loss: 546.117554, mae: 36.266277, mean_q: -36.581871\n",
            "  798573/10000000: episode: 3973, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -727.000, mean reward: -3.617 [-363.500, 65.200], mean action: 2.423 [0.000, 10.000], mean observation: 35.818 [0.000, 451.200], loss: 1648.006470, mae: 34.663544, mean_q: -35.155521\n",
            "  798774/10000000: episode: 3974, duration: 1.612s, episode steps: 201, steps per second: 125, episode reward: -511.800, mean reward: -2.546 [-255.900, 66.500], mean action: 2.925 [0.000, 10.000], mean observation: 30.476 [0.000, 765.000], loss: 800.492065, mae: 34.557468, mean_q: -35.345146\n",
            "  798975/10000000: episode: 3975, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: 863.400, mean reward: 4.296 [-10.000, 729.000], mean action: 3.030 [0.000, 10.000], mean observation: 35.104 [0.000, 638.500], loss: 1068.609741, mae: 34.319626, mean_q: -35.048851\n",
            "  799176/10000000: episode: 3976, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: 32.600, mean reward: 0.162 [-10.000, 169.000], mean action: 2.169 [0.000, 10.000], mean observation: 33.031 [0.000, 534.900], loss: 658.046326, mae: 33.759762, mean_q: -33.777054\n",
            "  799377/10000000: episode: 3977, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -833.400, mean reward: -4.146 [-416.700, 30.000], mean action: 2.697 [0.000, 10.000], mean observation: 39.261 [0.000, 793.800], loss: 1470.492432, mae: 33.174789, mean_q: -33.386150\n",
            "  799578/10000000: episode: 3978, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -179.600, mean reward: -0.894 [-89.800, 138.000], mean action: 2.219 [0.000, 10.000], mean observation: 34.247 [0.001, 681.600], loss: 449.041382, mae: 32.971119, mean_q: -33.085350\n",
            "  799779/10000000: episode: 3979, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -525.200, mean reward: -2.613 [-262.600, 15.500], mean action: 1.697 [0.000, 10.000], mean observation: 42.447 [0.000, 701.500], loss: 483.299133, mae: 33.071049, mean_q: -32.914997\n",
            "  799980/10000000: episode: 3980, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 520.600, mean reward: 2.590 [-10.000, 260.300], mean action: 1.816 [0.000, 10.000], mean observation: 32.815 [0.000, 783.800], loss: 510.799103, mae: 33.133251, mean_q: -32.865387\n",
            "  800181/10000000: episode: 3981, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: 42.000, mean reward: 0.209 [-10.000, 95.400], mean action: 2.219 [0.000, 10.000], mean observation: 37.145 [0.000, 642.000], loss: 1281.627319, mae: 32.780209, mean_q: -32.602978\n",
            "  800382/10000000: episode: 3982, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -565.600, mean reward: -2.814 [-282.800, 49.000], mean action: 2.129 [0.000, 10.000], mean observation: 33.478 [0.002, 515.200], loss: 524.774170, mae: 32.147713, mean_q: -32.121445\n",
            "  800583/10000000: episode: 3983, duration: 1.932s, episode steps: 201, steps per second: 104, episode reward: -565.000, mean reward: -2.811 [-282.500, 40.200], mean action: 2.219 [0.000, 10.000], mean observation: 35.230 [0.001, 430.300], loss: 434.149109, mae: 31.883949, mean_q: -31.903170\n",
            "  800784/10000000: episode: 3984, duration: 2.125s, episode steps: 201, steps per second: 95, episode reward: -437.800, mean reward: -2.178 [-218.900, 46.000], mean action: 2.279 [0.000, 10.000], mean observation: 40.503 [0.000, 673.700], loss: 592.658447, mae: 31.939142, mean_q: -31.749023\n",
            "  800985/10000000: episode: 3985, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -298.600, mean reward: -1.486 [-149.300, 169.000], mean action: 2.796 [0.000, 10.000], mean observation: 29.744 [0.001, 459.000], loss: 434.502136, mae: 31.594635, mean_q: -31.960272\n",
            "  801186/10000000: episode: 3986, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -475.000, mean reward: -2.363 [-237.500, 94.000], mean action: 2.821 [0.000, 10.000], mean observation: 38.855 [0.001, 558.800], loss: 711.814270, mae: 31.780390, mean_q: -32.169659\n",
            "  801387/10000000: episode: 3987, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -463.400, mean reward: -2.305 [-231.700, 274.000], mean action: 3.109 [0.000, 10.000], mean observation: 35.061 [0.000, 933.200], loss: 676.702271, mae: 31.923285, mean_q: -32.620514\n",
            "  801588/10000000: episode: 3988, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -537.600, mean reward: -2.675 [-268.800, 122.400], mean action: 3.090 [0.000, 10.000], mean observation: 30.521 [0.000, 684.900], loss: 459.645203, mae: 32.226986, mean_q: -33.014187\n",
            "  801789/10000000: episode: 3989, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: 261.400, mean reward: 1.300 [-10.000, 216.000], mean action: 2.886 [0.000, 10.000], mean observation: 32.845 [0.000, 567.300], loss: 802.299438, mae: 32.706360, mean_q: -33.622246\n",
            "  801990/10000000: episode: 3990, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -386.000, mean reward: -1.920 [-193.000, 190.800], mean action: 3.095 [0.000, 10.000], mean observation: 30.978 [0.001, 556.700], loss: 255.953415, mae: 32.587822, mean_q: -33.485085\n",
            "  802191/10000000: episode: 3991, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -665.800, mean reward: -3.312 [-332.900, 129.600], mean action: 2.801 [0.000, 10.000], mean observation: 36.051 [0.000, 444.500], loss: 334.590210, mae: 32.908733, mean_q: -33.724701\n",
            "  802392/10000000: episode: 3992, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -136.600, mean reward: -0.680 [-68.300, 500.000], mean action: 3.647 [0.000, 10.000], mean observation: 35.261 [0.000, 607.900], loss: 628.360107, mae: 32.908428, mean_q: -34.007439\n",
            "  802593/10000000: episode: 3993, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -40.400, mean reward: -0.201 [-20.200, 281.000], mean action: 3.950 [0.000, 10.000], mean observation: 36.137 [0.001, 528.400], loss: 1373.771484, mae: 33.214439, mean_q: -34.279869\n",
            "  802794/10000000: episode: 3994, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -238.200, mean reward: -1.185 [-119.100, 465.500], mean action: 3.682 [0.000, 10.000], mean observation: 31.139 [0.000, 455.800], loss: 312.326019, mae: 33.692699, mean_q: -34.902081\n",
            "  802995/10000000: episode: 3995, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -734.000, mean reward: -3.652 [-367.000, 59.000], mean action: 3.617 [0.000, 10.000], mean observation: 29.719 [0.000, 818.500], loss: 658.191467, mae: 34.300945, mean_q: -35.530483\n",
            "  803196/10000000: episode: 3996, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -466.800, mean reward: -2.322 [-233.400, 47.200], mean action: 3.015 [0.000, 10.000], mean observation: 33.329 [0.002, 441.900], loss: 545.081848, mae: 34.642689, mean_q: -35.919216\n",
            "  803397/10000000: episode: 3997, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -932.000, mean reward: -4.637 [-466.000, 21.000], mean action: 3.060 [0.000, 10.000], mean observation: 29.651 [0.001, 619.000], loss: 522.955994, mae: 35.173046, mean_q: -36.196354\n",
            "  803598/10000000: episode: 3998, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 89.000, mean reward: 0.443 [-10.000, 137.600], mean action: 3.224 [0.000, 10.000], mean observation: 32.777 [0.001, 412.100], loss: 455.371002, mae: 35.123310, mean_q: -36.134975\n",
            "  803799/10000000: episode: 3999, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: 500.400, mean reward: 2.490 [-10.000, 629.000], mean action: 3.100 [0.000, 10.000], mean observation: 35.872 [0.002, 402.500], loss: 632.499634, mae: 34.765999, mean_q: -36.278530\n",
            "  804000/10000000: episode: 4000, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -767.600, mean reward: -3.819 [-383.800, 68.000], mean action: 3.453 [0.000, 10.000], mean observation: 35.926 [0.000, 501.600], loss: 602.659729, mae: 34.721191, mean_q: -36.158192\n",
            "  804201/10000000: episode: 4001, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -376.800, mean reward: -1.875 [-188.400, 138.000], mean action: 2.706 [0.000, 10.000], mean observation: 34.459 [0.001, 492.600], loss: 522.357666, mae: 34.909733, mean_q: -36.284039\n",
            "  804402/10000000: episode: 4002, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: 646.000, mean reward: 3.214 [-10.000, 688.800], mean action: 3.328 [0.000, 10.000], mean observation: 32.155 [0.000, 590.900], loss: 1835.286743, mae: 34.939457, mean_q: -36.144138\n",
            "  804603/10000000: episode: 4003, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -459.000, mean reward: -2.284 [-229.500, 83.400], mean action: 2.622 [0.000, 10.000], mean observation: 36.655 [0.000, 747.100], loss: 904.479065, mae: 34.204273, mean_q: -35.013107\n",
            "  804804/10000000: episode: 4004, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 738.600, mean reward: 3.675 [-10.000, 604.800], mean action: 2.746 [0.000, 10.000], mean observation: 30.482 [0.000, 462.000], loss: 585.731323, mae: 34.147350, mean_q: -35.171673\n",
            "  805005/10000000: episode: 4005, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -340.000, mean reward: -1.692 [-170.000, 184.000], mean action: 3.244 [0.000, 10.000], mean observation: 30.535 [0.000, 534.800], loss: 563.363953, mae: 34.654442, mean_q: -35.575172\n",
            "  805206/10000000: episode: 4006, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -444.800, mean reward: -2.213 [-222.400, 137.500], mean action: 3.060 [0.000, 10.000], mean observation: 29.376 [0.000, 580.300], loss: 467.632568, mae: 34.823360, mean_q: -35.869297\n",
            "  805407/10000000: episode: 4007, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -638.400, mean reward: -3.176 [-319.200, 57.000], mean action: 3.527 [0.000, 10.000], mean observation: 37.984 [0.000, 522.100], loss: 616.369385, mae: 34.399319, mean_q: -35.607540\n",
            "  805608/10000000: episode: 4008, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -355.400, mean reward: -1.768 [-177.700, 148.800], mean action: 3.189 [0.000, 10.000], mean observation: 26.170 [0.000, 769.000], loss: 339.683258, mae: 34.594540, mean_q: -35.804783\n",
            "  805809/10000000: episode: 4009, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -414.000, mean reward: -2.060 [-207.000, 106.400], mean action: 2.139 [0.000, 10.000], mean observation: 37.829 [0.000, 813.800], loss: 464.721008, mae: 35.241562, mean_q: -35.831059\n",
            "  806010/10000000: episode: 4010, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -433.800, mean reward: -2.158 [-216.900, 41.000], mean action: 1.886 [0.000, 10.000], mean observation: 33.756 [0.001, 632.400], loss: 636.751648, mae: 35.720966, mean_q: -35.741070\n",
            "  806211/10000000: episode: 4011, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -420.400, mean reward: -2.092 [-210.200, 219.200], mean action: 3.159 [0.000, 10.000], mean observation: 33.314 [0.003, 531.400], loss: 1099.658447, mae: 34.124035, mean_q: -34.593338\n",
            "  806412/10000000: episode: 4012, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -540.200, mean reward: -2.688 [-270.100, 136.000], mean action: 2.597 [0.000, 10.000], mean observation: 34.521 [0.001, 519.900], loss: 761.783142, mae: 33.805531, mean_q: -34.218960\n",
            "  806613/10000000: episode: 4013, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -552.000, mean reward: -2.746 [-276.000, 64.000], mean action: 2.577 [0.000, 10.000], mean observation: 39.557 [0.001, 518.800], loss: 758.436096, mae: 33.529350, mean_q: -34.229088\n",
            "  806814/10000000: episode: 4014, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -753.200, mean reward: -3.747 [-376.600, 58.100], mean action: 3.274 [0.000, 10.000], mean observation: 36.396 [0.001, 627.200], loss: 882.292297, mae: 32.484810, mean_q: -33.701569\n",
            "  807015/10000000: episode: 4015, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -190.000, mean reward: -0.945 [-95.000, 265.300], mean action: 3.294 [0.000, 10.000], mean observation: 37.982 [0.001, 501.700], loss: 1595.784302, mae: 32.472477, mean_q: -33.705563\n",
            "  807216/10000000: episode: 4016, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: 295.400, mean reward: 1.470 [-10.000, 248.000], mean action: 3.010 [0.000, 10.000], mean observation: 34.124 [0.000, 604.500], loss: 547.709595, mae: 32.956451, mean_q: -34.113422\n",
            "  807417/10000000: episode: 4017, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -467.000, mean reward: -2.323 [-233.500, 113.200], mean action: 3.950 [0.000, 10.000], mean observation: 31.986 [0.001, 437.600], loss: 636.083862, mae: 32.838902, mean_q: -34.322666\n",
            "  807618/10000000: episode: 4018, duration: 1.517s, episode steps: 201, steps per second: 133, episode reward: -924.200, mean reward: -4.598 [-462.100, 70.200], mean action: 4.109 [0.000, 10.000], mean observation: 30.833 [0.001, 464.600], loss: 831.009888, mae: 33.128803, mean_q: -34.671799\n",
            "  807819/10000000: episode: 4019, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -292.000, mean reward: -1.453 [-146.000, 201.600], mean action: 4.498 [0.000, 10.000], mean observation: 31.916 [0.001, 531.100], loss: 429.130432, mae: 33.690025, mean_q: -35.334927\n",
            "  808020/10000000: episode: 4020, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: 22.400, mean reward: 0.111 [-10.000, 221.200], mean action: 3.100 [0.000, 10.000], mean observation: 35.022 [0.002, 439.400], loss: 1131.572754, mae: 34.045895, mean_q: -34.662342\n",
            "  808221/10000000: episode: 4021, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -480.000, mean reward: -2.388 [-240.000, 59.200], mean action: 2.398 [0.000, 10.000], mean observation: 34.960 [0.000, 784.200], loss: 476.782135, mae: 34.089600, mean_q: -34.689629\n",
            "  808422/10000000: episode: 4022, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 224.400, mean reward: 1.116 [-10.000, 328.800], mean action: 3.045 [0.000, 10.000], mean observation: 38.423 [0.000, 722.600], loss: 1441.593384, mae: 33.558475, mean_q: -34.500183\n",
            "  808623/10000000: episode: 4023, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -36.800, mean reward: -0.183 [-18.400, 240.100], mean action: 2.816 [0.000, 10.000], mean observation: 31.058 [0.001, 542.200], loss: 1070.836060, mae: 33.304317, mean_q: -33.980312\n",
            "  808824/10000000: episode: 4024, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: 1733.000, mean reward: 8.622 [-10.000, 866.500], mean action: 2.308 [0.000, 10.000], mean observation: 37.444 [0.001, 527.300], loss: 497.430878, mae: 33.176754, mean_q: -33.561047\n",
            "  809025/10000000: episode: 4025, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -833.400, mean reward: -4.146 [-416.700, 13.300], mean action: 2.274 [0.000, 10.000], mean observation: 26.617 [0.000, 529.100], loss: 607.515625, mae: 32.921211, mean_q: -33.394962\n",
            "  809226/10000000: episode: 4026, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -874.200, mean reward: -4.349 [-437.100, 37.100], mean action: 3.010 [0.000, 10.000], mean observation: 33.466 [0.000, 694.400], loss: 380.209961, mae: 32.543499, mean_q: -33.174995\n",
            "  809427/10000000: episode: 4027, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -745.400, mean reward: -3.708 [-372.700, 27.600], mean action: 2.264 [0.000, 10.000], mean observation: 40.258 [0.001, 598.300], loss: 1195.989624, mae: 32.233288, mean_q: -32.475018\n",
            "  809628/10000000: episode: 4028, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -622.600, mean reward: -3.098 [-311.300, 104.700], mean action: 3.065 [0.000, 10.000], mean observation: 33.131 [0.000, 555.700], loss: 529.149963, mae: 32.150383, mean_q: -32.815414\n",
            "  809829/10000000: episode: 4029, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -1210.200, mean reward: -6.021 [-605.100, 80.800], mean action: 3.930 [0.000, 10.000], mean observation: 28.492 [0.003, 492.800], loss: 684.320129, mae: 31.831469, mean_q: -32.910656\n",
            "  810030/10000000: episode: 4030, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -51.800, mean reward: -0.258 [-25.900, 329.700], mean action: 4.109 [0.000, 10.000], mean observation: 38.108 [0.000, 669.900], loss: 1314.199463, mae: 31.861925, mean_q: -33.097076\n",
            "  810231/10000000: episode: 4031, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: 1392.600, mean reward: 6.928 [-10.000, 791.000], mean action: 3.438 [0.000, 10.000], mean observation: 37.302 [0.001, 662.900], loss: 530.878967, mae: 32.283360, mean_q: -33.214005\n",
            "  810432/10000000: episode: 4032, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -243.400, mean reward: -1.211 [-121.700, 131.400], mean action: 2.100 [0.000, 10.000], mean observation: 34.599 [0.002, 482.600], loss: 465.412750, mae: 32.751881, mean_q: -33.244350\n",
            "  810633/10000000: episode: 4033, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: 555.000, mean reward: 2.761 [-10.000, 296.000], mean action: 2.587 [0.000, 10.000], mean observation: 39.110 [0.000, 654.700], loss: 730.568542, mae: 32.641327, mean_q: -33.234554\n",
            "  810834/10000000: episode: 4034, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -576.000, mean reward: -2.866 [-288.000, 126.700], mean action: 3.303 [0.000, 10.000], mean observation: 33.355 [0.000, 706.400], loss: 551.160461, mae: 31.580656, mean_q: -32.149567\n",
            "  811035/10000000: episode: 4035, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -1131.800, mean reward: -5.631 [-565.900, 65.600], mean action: 4.090 [0.000, 10.000], mean observation: 32.463 [0.001, 480.700], loss: 605.753235, mae: 30.937847, mean_q: -31.822115\n",
            "  811236/10000000: episode: 4036, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -19.200, mean reward: -0.096 [-10.000, 100.000], mean action: 3.214 [0.000, 10.000], mean observation: 34.665 [0.001, 422.500], loss: 842.956482, mae: 30.531925, mean_q: -31.372274\n",
            "  811437/10000000: episode: 4037, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -1024.200, mean reward: -5.096 [-512.100, 60.800], mean action: 3.303 [0.000, 10.000], mean observation: 31.844 [0.001, 676.900], loss: 1267.087280, mae: 30.970587, mean_q: -31.743282\n",
            "  811638/10000000: episode: 4038, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -581.000, mean reward: -2.891 [-290.500, 135.200], mean action: 3.607 [0.000, 10.000], mean observation: 26.963 [0.005, 497.200], loss: 796.695923, mae: 30.755857, mean_q: -31.638710\n",
            "  811839/10000000: episode: 4039, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -802.000, mean reward: -3.990 [-401.000, 231.000], mean action: 4.015 [0.000, 10.000], mean observation: 43.401 [0.000, 714.300], loss: 652.399048, mae: 30.663088, mean_q: -31.836351\n",
            "  812040/10000000: episode: 4040, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -1304.200, mean reward: -6.489 [-652.100, 39.200], mean action: 4.189 [0.000, 10.000], mean observation: 33.729 [0.000, 582.800], loss: 417.484222, mae: 30.578688, mean_q: -31.599483\n",
            "  812241/10000000: episode: 4041, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -446.400, mean reward: -2.221 [-223.200, 98.100], mean action: 3.776 [0.000, 10.000], mean observation: 33.543 [0.000, 566.700], loss: 776.160828, mae: 30.902737, mean_q: -31.806471\n",
            "  812442/10000000: episode: 4042, duration: 1.549s, episode steps: 201, steps per second: 130, episode reward: -836.000, mean reward: -4.159 [-418.000, 49.000], mean action: 3.592 [0.000, 10.000], mean observation: 30.306 [0.001, 498.800], loss: 670.758545, mae: 31.353445, mean_q: -32.313152\n",
            "  812643/10000000: episode: 4043, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: 0.600, mean reward: 0.003 [-10.000, 244.800], mean action: 3.144 [0.000, 10.000], mean observation: 32.186 [0.002, 508.500], loss: 425.602844, mae: 31.440561, mean_q: -32.385197\n",
            "  812844/10000000: episode: 4044, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: 181.800, mean reward: 0.904 [-10.000, 458.400], mean action: 2.682 [0.000, 10.000], mean observation: 31.672 [0.000, 413.800], loss: 589.990906, mae: 31.597914, mean_q: -32.267548\n",
            "  813045/10000000: episode: 4045, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -646.800, mean reward: -3.218 [-323.400, 67.000], mean action: 2.587 [0.000, 10.000], mean observation: 37.397 [0.000, 451.700], loss: 629.729187, mae: 31.839159, mean_q: -32.669285\n",
            "  813246/10000000: episode: 4046, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -555.600, mean reward: -2.764 [-277.800, 50.000], mean action: 1.930 [0.000, 10.000], mean observation: 34.886 [0.001, 630.500], loss: 604.166138, mae: 31.843521, mean_q: -32.083530\n",
            "  813447/10000000: episode: 4047, duration: 1.618s, episode steps: 201, steps per second: 124, episode reward: 971.400, mean reward: 4.833 [-10.000, 524.000], mean action: 2.761 [0.000, 10.000], mean observation: 31.575 [0.001, 527.300], loss: 818.478516, mae: 31.173527, mean_q: -31.617163\n",
            "  813648/10000000: episode: 4048, duration: 1.647s, episode steps: 201, steps per second: 122, episode reward: -324.800, mean reward: -1.616 [-162.400, 141.300], mean action: 3.284 [0.000, 10.000], mean observation: 34.583 [0.000, 562.200], loss: 1645.622192, mae: 31.288868, mean_q: -31.696430\n",
            "  813849/10000000: episode: 4049, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: 1135.600, mean reward: 5.650 [-10.000, 605.700], mean action: 3.910 [0.000, 10.000], mean observation: 33.275 [0.002, 637.200], loss: 551.493835, mae: 30.679537, mean_q: -31.544514\n",
            "  814050/10000000: episode: 4050, duration: 1.600s, episode steps: 201, steps per second: 126, episode reward: -1143.400, mean reward: -5.689 [-571.700, 48.800], mean action: 3.582 [0.000, 10.000], mean observation: 25.886 [0.003, 436.400], loss: 626.182068, mae: 31.068584, mean_q: -32.068497\n",
            "  814251/10000000: episode: 4051, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: 401.400, mean reward: 1.997 [-10.000, 200.700], mean action: 3.025 [0.000, 10.000], mean observation: 31.170 [0.001, 607.700], loss: 430.006622, mae: 31.594484, mean_q: -32.752644\n",
            "  814452/10000000: episode: 4052, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -346.000, mean reward: -1.721 [-173.000, 184.800], mean action: 2.582 [0.000, 10.000], mean observation: 33.047 [0.001, 462.800], loss: 533.653931, mae: 32.394821, mean_q: -33.414986\n",
            "  814653/10000000: episode: 4053, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -484.200, mean reward: -2.409 [-242.100, 42.000], mean action: 2.264 [0.000, 10.000], mean observation: 38.330 [0.003, 524.500], loss: 618.058838, mae: 32.741600, mean_q: -33.608047\n",
            "  814854/10000000: episode: 4054, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -414.600, mean reward: -2.063 [-207.300, 76.200], mean action: 2.756 [0.000, 10.000], mean observation: 28.630 [0.000, 772.000], loss: 339.951508, mae: 33.106190, mean_q: -34.121964\n",
            "  815055/10000000: episode: 4055, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -663.600, mean reward: -3.301 [-331.800, 93.600], mean action: 2.866 [0.000, 10.000], mean observation: 32.378 [0.000, 558.800], loss: 1602.874268, mae: 33.107426, mean_q: -34.036663\n",
            "  815256/10000000: episode: 4056, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: 848.200, mean reward: 4.220 [-10.000, 424.100], mean action: 2.856 [0.000, 10.000], mean observation: 34.040 [0.001, 501.100], loss: 1382.935913, mae: 32.592148, mean_q: -33.258400\n",
            "  815457/10000000: episode: 4057, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -398.400, mean reward: -1.982 [-199.200, 89.600], mean action: 2.194 [0.000, 10.000], mean observation: 32.726 [0.001, 645.600], loss: 1051.443604, mae: 32.048527, mean_q: -32.217773\n",
            "  815658/10000000: episode: 4058, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -645.800, mean reward: -3.213 [-322.900, 9.900], mean action: 1.995 [0.000, 10.000], mean observation: 31.952 [0.001, 516.000], loss: 390.568848, mae: 31.458925, mean_q: -31.816019\n",
            "  815859/10000000: episode: 4059, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: 57.800, mean reward: 0.288 [-10.000, 129.600], mean action: 2.169 [0.000, 10.000], mean observation: 36.033 [0.000, 555.400], loss: 427.139771, mae: 31.279398, mean_q: -31.793779\n",
            "  816060/10000000: episode: 4060, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -601.800, mean reward: -2.994 [-300.900, 34.400], mean action: 2.025 [0.000, 10.000], mean observation: 33.035 [0.000, 553.100], loss: 422.154114, mae: 31.522272, mean_q: -32.049679\n",
            "  816261/10000000: episode: 4061, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -463.200, mean reward: -2.304 [-231.600, 51.300], mean action: 2.249 [0.000, 10.000], mean observation: 37.469 [0.000, 668.100], loss: 467.536652, mae: 31.864143, mean_q: -32.477406\n",
            "  816462/10000000: episode: 4062, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -696.800, mean reward: -3.467 [-348.400, 27.000], mean action: 2.453 [0.000, 10.000], mean observation: 30.100 [0.000, 483.800], loss: 944.941101, mae: 32.139771, mean_q: -32.756447\n",
            "  816663/10000000: episode: 4063, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -424.200, mean reward: -2.110 [-212.100, 61.600], mean action: 2.453 [0.000, 10.000], mean observation: 28.150 [0.002, 513.300], loss: 711.136719, mae: 31.331667, mean_q: -32.221451\n",
            "  816864/10000000: episode: 4064, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -672.000, mean reward: -3.343 [-336.000, 88.800], mean action: 2.637 [0.000, 10.000], mean observation: 35.712 [0.000, 606.600], loss: 529.601379, mae: 31.918432, mean_q: -33.011292\n",
            "  817065/10000000: episode: 4065, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -826.400, mean reward: -4.111 [-413.200, 61.700], mean action: 2.756 [0.000, 10.000], mean observation: 38.800 [0.001, 493.400], loss: 367.140076, mae: 32.374443, mean_q: -33.344604\n",
            "  817266/10000000: episode: 4066, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: 524.800, mean reward: 2.611 [-10.000, 558.000], mean action: 2.323 [0.000, 10.000], mean observation: 32.683 [0.002, 578.200], loss: 1271.794800, mae: 32.027306, mean_q: -32.346642\n",
            "  817467/10000000: episode: 4067, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -681.000, mean reward: -3.388 [-340.500, 49.000], mean action: 2.468 [0.000, 10.000], mean observation: 32.451 [0.001, 478.600], loss: 484.724762, mae: 31.815235, mean_q: -32.314644\n",
            "  817668/10000000: episode: 4068, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -353.600, mean reward: -1.759 [-176.800, 59.000], mean action: 1.945 [0.000, 10.000], mean observation: 37.353 [0.000, 527.200], loss: 483.008698, mae: 31.644976, mean_q: -31.782858\n",
            "  817869/10000000: episode: 4069, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: 474.800, mean reward: 2.362 [-10.000, 237.400], mean action: 1.925 [0.000, 10.000], mean observation: 31.575 [0.001, 675.300], loss: 787.821350, mae: 31.218697, mean_q: -31.259571\n",
            "  818070/10000000: episode: 4070, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -238.000, mean reward: -1.184 [-119.000, 235.200], mean action: 2.289 [0.000, 10.000], mean observation: 30.264 [0.004, 432.400], loss: 323.413605, mae: 30.908920, mean_q: -31.447985\n",
            "  818271/10000000: episode: 4071, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: 357.000, mean reward: 1.776 [-10.000, 485.100], mean action: 2.224 [0.000, 10.000], mean observation: 31.453 [0.000, 527.500], loss: 938.364990, mae: 31.389664, mean_q: -31.913439\n",
            "  818472/10000000: episode: 4072, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -56.800, mean reward: -0.283 [-28.400, 225.400], mean action: 2.065 [0.000, 9.000], mean observation: 35.803 [0.000, 544.700], loss: 616.395447, mae: 30.956190, mean_q: -31.003506\n",
            "  818673/10000000: episode: 4073, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -578.400, mean reward: -2.878 [-289.200, 36.600], mean action: 2.294 [0.000, 10.000], mean observation: 36.468 [0.001, 532.900], loss: 443.247681, mae: 30.640455, mean_q: -30.839624\n",
            "  818874/10000000: episode: 4074, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -120.800, mean reward: -0.601 [-60.400, 226.800], mean action: 2.706 [0.000, 10.000], mean observation: 34.109 [0.001, 647.400], loss: 363.017426, mae: 30.374928, mean_q: -31.006481\n",
            "  819075/10000000: episode: 4075, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -906.800, mean reward: -4.511 [-453.400, 18.000], mean action: 2.677 [0.000, 10.000], mean observation: 32.864 [0.001, 602.400], loss: 420.056213, mae: 31.540146, mean_q: -31.631754\n",
            "  819276/10000000: episode: 4076, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -516.800, mean reward: -2.571 [-258.400, 89.600], mean action: 2.443 [0.000, 10.000], mean observation: 33.803 [0.000, 453.500], loss: 681.582642, mae: 31.198151, mean_q: -30.883194\n",
            "  819477/10000000: episode: 4077, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -319.800, mean reward: -1.591 [-159.900, 143.400], mean action: 2.701 [0.000, 10.000], mean observation: 28.704 [0.002, 364.300], loss: 855.057983, mae: 30.867012, mean_q: -31.312704\n",
            "  819678/10000000: episode: 4078, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: 1601.000, mean reward: 7.965 [-9.000, 800.500], mean action: 2.816 [0.000, 10.000], mean observation: 27.777 [0.001, 498.300], loss: 420.754242, mae: 30.890440, mean_q: -31.813940\n",
            "  819879/10000000: episode: 4079, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: -610.000, mean reward: -3.035 [-305.000, 141.000], mean action: 3.328 [0.000, 10.000], mean observation: 35.503 [0.000, 695.400], loss: 474.852234, mae: 31.283796, mean_q: -32.387016\n",
            "  820080/10000000: episode: 4080, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: 420.400, mean reward: 2.092 [-10.000, 212.000], mean action: 3.159 [0.000, 10.000], mean observation: 34.534 [0.000, 525.900], loss: 610.951599, mae: 32.033718, mean_q: -32.918674\n",
            "  820281/10000000: episode: 4081, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: 633.400, mean reward: 3.151 [-10.000, 583.100], mean action: 3.080 [0.000, 10.000], mean observation: 35.074 [0.001, 446.900], loss: 551.578918, mae: 31.653101, mean_q: -32.448971\n",
            "  820482/10000000: episode: 4082, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: 455.000, mean reward: 2.264 [-10.000, 287.400], mean action: 2.841 [0.000, 10.000], mean observation: 32.245 [0.000, 386.400], loss: 429.123383, mae: 31.551651, mean_q: -32.057362\n",
            "  820683/10000000: episode: 4083, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -250.600, mean reward: -1.247 [-125.300, 133.800], mean action: 2.164 [0.000, 10.000], mean observation: 31.067 [0.001, 654.600], loss: 679.719604, mae: 30.861925, mean_q: -30.910780\n",
            "  820884/10000000: episode: 4084, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: 536.000, mean reward: 2.667 [-10.000, 288.000], mean action: 2.090 [0.000, 10.000], mean observation: 35.641 [0.000, 467.400], loss: 543.882080, mae: 30.105005, mean_q: -29.984301\n",
            "  821085/10000000: episode: 4085, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -168.800, mean reward: -0.840 [-84.400, 57.600], mean action: 1.960 [0.000, 9.000], mean observation: 36.465 [0.000, 697.600], loss: 563.987427, mae: 30.182875, mean_q: -29.891619\n",
            "  821286/10000000: episode: 4086, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -365.200, mean reward: -1.817 [-182.600, 60.600], mean action: 1.980 [0.000, 10.000], mean observation: 30.873 [0.000, 499.800], loss: 1453.922852, mae: 29.909971, mean_q: -29.405186\n",
            "  821487/10000000: episode: 4087, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -546.200, mean reward: -2.717 [-273.100, 46.800], mean action: 2.647 [0.000, 10.000], mean observation: 31.241 [0.001, 443.500], loss: 619.487244, mae: 29.672312, mean_q: -30.358797\n",
            "  821688/10000000: episode: 4088, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -479.000, mean reward: -2.383 [-239.500, 74.800], mean action: 2.527 [0.000, 10.000], mean observation: 34.250 [0.001, 470.800], loss: 518.972351, mae: 29.797192, mean_q: -30.462093\n",
            "  821889/10000000: episode: 4089, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -230.200, mean reward: -1.145 [-115.100, 203.000], mean action: 2.975 [0.000, 10.000], mean observation: 29.423 [0.002, 427.500], loss: 571.910583, mae: 29.735493, mean_q: -30.538576\n",
            "  822090/10000000: episode: 4090, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -647.400, mean reward: -3.221 [-323.700, 104.000], mean action: 2.955 [0.000, 10.000], mean observation: 32.062 [0.002, 515.900], loss: 579.534363, mae: 29.473482, mean_q: -30.416676\n",
            "  822291/10000000: episode: 4091, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -84.800, mean reward: -0.422 [-42.400, 180.600], mean action: 2.935 [0.000, 10.000], mean observation: 36.091 [0.002, 467.700], loss: 668.119690, mae: 30.150944, mean_q: -31.030699\n",
            "  822492/10000000: episode: 4092, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -545.000, mean reward: -2.711 [-272.500, 65.400], mean action: 3.184 [0.000, 10.000], mean observation: 27.080 [0.002, 434.600], loss: 546.078186, mae: 29.817581, mean_q: -30.647709\n",
            "  822693/10000000: episode: 4093, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -292.600, mean reward: -1.456 [-146.300, 154.400], mean action: 3.005 [0.000, 10.000], mean observation: 32.622 [0.000, 638.600], loss: 520.399536, mae: 29.978424, mean_q: -31.040161\n",
            "  822894/10000000: episode: 4094, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -831.400, mean reward: -4.136 [-415.700, 103.200], mean action: 3.617 [0.000, 10.000], mean observation: 33.968 [0.000, 407.100], loss: 656.455505, mae: 29.555157, mean_q: -30.492697\n",
            "  823095/10000000: episode: 4095, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -149.400, mean reward: -0.743 [-74.700, 240.100], mean action: 2.871 [0.000, 10.000], mean observation: 35.488 [0.000, 530.500], loss: 500.686829, mae: 29.538940, mean_q: -30.272144\n",
            "  823296/10000000: episode: 4096, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: 243.000, mean reward: 1.209 [-10.000, 327.600], mean action: 2.632 [0.000, 10.000], mean observation: 33.960 [0.000, 796.200], loss: 566.897339, mae: 29.181973, mean_q: -29.773739\n",
            "  823497/10000000: episode: 4097, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -763.600, mean reward: -3.799 [-381.800, 112.000], mean action: 2.965 [0.000, 10.000], mean observation: 38.491 [0.000, 588.400], loss: 1084.538452, mae: 29.730902, mean_q: -30.230484\n",
            "  823698/10000000: episode: 4098, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: 467.600, mean reward: 2.326 [-10.000, 672.800], mean action: 2.896 [0.000, 10.000], mean observation: 29.665 [0.001, 466.300], loss: 1200.253540, mae: 29.533573, mean_q: -29.445074\n",
            "  823899/10000000: episode: 4099, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -444.600, mean reward: -2.212 [-222.300, 147.400], mean action: 2.398 [0.000, 10.000], mean observation: 33.866 [0.003, 438.400], loss: 489.125427, mae: 29.026329, mean_q: -29.176556\n",
            "  824100/10000000: episode: 4100, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -545.800, mean reward: -2.715 [-272.900, 66.600], mean action: 2.881 [0.000, 10.000], mean observation: 28.300 [0.002, 545.200], loss: 591.476562, mae: 28.557451, mean_q: -28.941751\n",
            "  824301/10000000: episode: 4101, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -825.600, mean reward: -4.107 [-412.800, 46.200], mean action: 3.174 [0.000, 10.000], mean observation: 34.291 [0.001, 491.000], loss: 741.122498, mae: 29.153807, mean_q: -29.749683\n",
            "  824502/10000000: episode: 4102, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 58.400, mean reward: 0.291 [-10.000, 269.200], mean action: 3.154 [0.000, 10.000], mean observation: 30.790 [0.000, 452.700], loss: 493.760742, mae: 28.734999, mean_q: -29.412119\n",
            "  824703/10000000: episode: 4103, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -912.800, mean reward: -4.541 [-456.400, 12.600], mean action: 2.393 [0.000, 10.000], mean observation: 35.138 [0.002, 481.400], loss: 577.816223, mae: 28.612898, mean_q: -28.877069\n",
            "  824904/10000000: episode: 4104, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -687.000, mean reward: -3.418 [-343.500, 35.400], mean action: 2.711 [0.000, 10.000], mean observation: 28.138 [0.000, 746.900], loss: 475.027313, mae: 28.503088, mean_q: -28.772455\n",
            "  825105/10000000: episode: 4105, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -647.600, mean reward: -3.222 [-323.800, 33.000], mean action: 2.388 [0.000, 10.000], mean observation: 33.140 [0.000, 609.400], loss: 345.862213, mae: 28.120871, mean_q: -28.060589\n",
            "  825306/10000000: episode: 4106, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -267.200, mean reward: -1.329 [-133.600, 307.200], mean action: 2.542 [0.000, 10.000], mean observation: 31.452 [0.000, 702.100], loss: 625.087097, mae: 27.863058, mean_q: -27.771479\n",
            "  825507/10000000: episode: 4107, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -763.600, mean reward: -3.799 [-381.800, 28.600], mean action: 2.522 [0.000, 10.000], mean observation: 34.487 [0.000, 472.400], loss: 395.683563, mae: 27.861526, mean_q: -27.920170\n",
            "  825708/10000000: episode: 4108, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -268.000, mean reward: -1.333 [-134.000, 80.000], mean action: 2.184 [0.000, 10.000], mean observation: 38.164 [0.000, 653.600], loss: 1591.203979, mae: 27.685017, mean_q: -27.416422\n",
            "  825909/10000000: episode: 4109, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: 133.800, mean reward: 0.666 [-8.000, 171.500], mean action: 2.129 [0.000, 10.000], mean observation: 27.893 [0.000, 509.800], loss: 1000.600281, mae: 26.662153, mean_q: -26.311354\n",
            "  826110/10000000: episode: 4110, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -467.200, mean reward: -2.324 [-233.600, 63.000], mean action: 2.582 [0.000, 10.000], mean observation: 39.233 [0.002, 630.900], loss: 599.829346, mae: 26.548565, mean_q: -26.866528\n",
            "  826311/10000000: episode: 4111, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -412.600, mean reward: -2.053 [-206.300, 168.600], mean action: 3.438 [0.000, 10.000], mean observation: 33.088 [0.001, 571.000], loss: 1276.377808, mae: 26.997183, mean_q: -27.451973\n",
            "  826512/10000000: episode: 4112, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -702.400, mean reward: -3.495 [-351.200, 82.800], mean action: 2.801 [0.000, 10.000], mean observation: 36.593 [0.001, 591.000], loss: 1484.297729, mae: 27.412151, mean_q: -27.595655\n",
            "  826713/10000000: episode: 4113, duration: 1.742s, episode steps: 201, steps per second: 115, episode reward: -293.800, mean reward: -1.462 [-146.900, 94.000], mean action: 3.090 [0.000, 10.000], mean observation: 31.201 [0.001, 595.600], loss: 554.461182, mae: 27.019352, mean_q: -27.369097\n",
            "  826914/10000000: episode: 4114, duration: 1.682s, episode steps: 201, steps per second: 120, episode reward: 1126.400, mean reward: 5.604 [-10.000, 749.000], mean action: 2.871 [0.000, 10.000], mean observation: 34.029 [0.000, 530.000], loss: 424.911743, mae: 27.435343, mean_q: -27.454592\n",
            "  827115/10000000: episode: 4115, duration: 1.677s, episode steps: 201, steps per second: 120, episode reward: 1.000, mean reward: 0.005 [-10.000, 145.200], mean action: 2.373 [0.000, 10.000], mean observation: 30.944 [0.001, 453.300], loss: 1180.944092, mae: 27.225889, mean_q: -27.093407\n",
            "  827316/10000000: episode: 4116, duration: 1.660s, episode steps: 201, steps per second: 121, episode reward: -541.400, mean reward: -2.694 [-270.700, 85.400], mean action: 3.095 [0.000, 10.000], mean observation: 31.347 [0.006, 504.200], loss: 1263.867310, mae: 26.394064, mean_q: -26.741735\n",
            "  827517/10000000: episode: 4117, duration: 1.687s, episode steps: 201, steps per second: 119, episode reward: -680.000, mean reward: -3.383 [-340.000, 190.400], mean action: 4.164 [0.000, 10.000], mean observation: 31.514 [0.001, 591.100], loss: 783.984436, mae: 25.731615, mean_q: -26.511402\n",
            "  827718/10000000: episode: 4118, duration: 1.667s, episode steps: 201, steps per second: 121, episode reward: -329.400, mean reward: -1.639 [-164.700, 410.900], mean action: 4.741 [0.000, 10.000], mean observation: 33.478 [0.001, 578.100], loss: 657.990845, mae: 26.221466, mean_q: -26.952211\n",
            "  827919/10000000: episode: 4119, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -475.400, mean reward: -2.365 [-237.700, 263.200], mean action: 3.512 [0.000, 10.000], mean observation: 34.937 [0.001, 598.700], loss: 355.266632, mae: 26.627867, mean_q: -27.274044\n",
            "  828120/10000000: episode: 4120, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 9.400, mean reward: 0.047 [-10.000, 345.000], mean action: 4.100 [0.000, 10.000], mean observation: 28.465 [0.003, 308.400], loss: 693.179871, mae: 26.635811, mean_q: -27.031162\n",
            "  828321/10000000: episode: 4121, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -333.600, mean reward: -1.660 [-166.800, 162.000], mean action: 4.353 [0.000, 10.000], mean observation: 32.580 [0.000, 540.000], loss: 412.079651, mae: 26.378065, mean_q: -26.461123\n",
            "  828522/10000000: episode: 4122, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -1201.400, mean reward: -5.977 [-600.700, 105.000], mean action: 4.055 [0.000, 10.000], mean observation: 30.248 [0.000, 663.800], loss: 324.421844, mae: 26.348270, mean_q: -26.883512\n",
            "  828723/10000000: episode: 4123, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -898.600, mean reward: -4.471 [-449.300, 51.000], mean action: 3.443 [0.000, 10.000], mean observation: 30.925 [0.002, 477.000], loss: 357.136200, mae: 26.892881, mean_q: -27.503881\n",
            "  828924/10000000: episode: 4124, duration: 1.533s, episode steps: 201, steps per second: 131, episode reward: -599.000, mean reward: -2.980 [-299.500, 73.800], mean action: 3.015 [0.000, 10.000], mean observation: 34.595 [0.000, 639.500], loss: 433.896759, mae: 27.494432, mean_q: -28.059963\n",
            "  829125/10000000: episode: 4125, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -194.400, mean reward: -0.967 [-97.200, 181.800], mean action: 3.144 [0.000, 10.000], mean observation: 36.798 [0.001, 619.200], loss: 550.718140, mae: 27.681818, mean_q: -28.206457\n",
            "  829326/10000000: episode: 4126, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -908.200, mean reward: -4.518 [-454.100, 40.400], mean action: 3.224 [0.000, 10.000], mean observation: 29.501 [0.001, 651.100], loss: 1414.603516, mae: 27.211695, mean_q: -27.851358\n",
            "  829527/10000000: episode: 4127, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -767.000, mean reward: -3.816 [-383.500, 116.100], mean action: 4.473 [0.000, 10.000], mean observation: 38.197 [0.000, 818.100], loss: 474.861176, mae: 27.163494, mean_q: -28.107939\n",
            "  829728/10000000: episode: 4128, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -900.000, mean reward: -4.478 [-450.000, 116.000], mean action: 3.841 [0.000, 10.000], mean observation: 32.762 [0.002, 429.000], loss: 347.313385, mae: 27.661381, mean_q: -28.587040\n",
            "  829929/10000000: episode: 4129, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -751.800, mean reward: -3.740 [-375.900, 73.000], mean action: 3.478 [0.000, 10.000], mean observation: 33.251 [0.001, 565.900], loss: 1562.897339, mae: 27.591042, mean_q: -27.948446\n",
            "  830130/10000000: episode: 4130, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -1223.400, mean reward: -6.087 [-611.700, 49.000], mean action: 4.408 [0.000, 10.000], mean observation: 33.244 [0.002, 446.700], loss: 767.301392, mae: 26.820549, mean_q: -27.398991\n",
            "  830331/10000000: episode: 4131, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -1066.600, mean reward: -5.306 [-533.300, 35.000], mean action: 3.498 [0.000, 10.000], mean observation: 33.163 [0.000, 720.900], loss: 1097.988892, mae: 27.077148, mean_q: -27.558498\n",
            "  830532/10000000: episode: 4132, duration: 1.552s, episode steps: 201, steps per second: 129, episode reward: -843.000, mean reward: -4.194 [-421.500, 41.000], mean action: 2.746 [0.000, 10.000], mean observation: 30.510 [0.003, 624.500], loss: 388.475677, mae: 26.893021, mean_q: -27.069298\n",
            "  830733/10000000: episode: 4133, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -164.600, mean reward: -0.819 [-82.300, 220.500], mean action: 2.841 [0.000, 10.000], mean observation: 36.067 [0.002, 503.100], loss: 568.335205, mae: 26.834475, mean_q: -26.981197\n",
            "  830934/10000000: episode: 4134, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -312.000, mean reward: -1.552 [-156.000, 85.800], mean action: 2.557 [0.000, 10.000], mean observation: 24.672 [0.001, 462.800], loss: 326.445312, mae: 27.272747, mean_q: -27.321825\n",
            "  831135/10000000: episode: 4135, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: 1156.400, mean reward: 5.753 [-10.000, 807.100], mean action: 2.597 [0.000, 10.000], mean observation: 38.293 [0.000, 476.600], loss: 424.692169, mae: 27.067785, mean_q: -26.695700\n",
            "  831336/10000000: episode: 4136, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -664.800, mean reward: -3.307 [-332.400, 75.600], mean action: 2.617 [0.000, 10.000], mean observation: 37.471 [0.000, 642.700], loss: 462.117615, mae: 26.910065, mean_q: -26.222923\n",
            "  831537/10000000: episode: 4137, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -98.800, mean reward: -0.492 [-49.400, 296.100], mean action: 3.622 [0.000, 10.000], mean observation: 34.277 [0.000, 907.100], loss: 1831.035645, mae: 25.919830, mean_q: -25.912415\n",
            "  831738/10000000: episode: 4138, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: 422.000, mean reward: 2.100 [-10.000, 689.500], mean action: 3.876 [0.000, 10.000], mean observation: 31.616 [0.004, 399.700], loss: 451.917450, mae: 26.198561, mean_q: -26.400503\n",
            "  831939/10000000: episode: 4139, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -1042.200, mean reward: -5.185 [-521.100, 106.500], mean action: 4.154 [0.000, 10.000], mean observation: 35.494 [0.001, 522.800], loss: 553.437561, mae: 26.353292, mean_q: -26.332006\n",
            "  832140/10000000: episode: 4140, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: 263.800, mean reward: 1.312 [-10.000, 504.700], mean action: 3.891 [0.000, 10.000], mean observation: 29.013 [0.000, 476.000], loss: 725.856628, mae: 25.836142, mean_q: -25.900713\n",
            "  832341/10000000: episode: 4141, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: 567.800, mean reward: 2.825 [-10.000, 349.800], mean action: 4.129 [0.000, 10.000], mean observation: 31.252 [0.001, 623.900], loss: 764.012695, mae: 25.324364, mean_q: -25.385660\n",
            "  832542/10000000: episode: 4142, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -1188.000, mean reward: -5.910 [-594.000, 27.300], mean action: 3.592 [0.000, 10.000], mean observation: 31.217 [0.002, 542.000], loss: 525.812256, mae: 25.580851, mean_q: -25.933302\n",
            "  832743/10000000: episode: 4143, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: 280.400, mean reward: 1.395 [-10.000, 216.000], mean action: 3.303 [0.000, 10.000], mean observation: 39.695 [0.000, 515.500], loss: 538.754150, mae: 25.985867, mean_q: -26.523935\n",
            "  832944/10000000: episode: 4144, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -54.800, mean reward: -0.273 [-27.400, 194.000], mean action: 3.453 [0.000, 10.000], mean observation: 27.318 [0.000, 781.200], loss: 503.900177, mae: 26.461016, mean_q: -26.936562\n",
            "  833145/10000000: episode: 4145, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -778.800, mean reward: -3.875 [-389.400, 70.500], mean action: 3.343 [0.000, 10.000], mean observation: 31.835 [0.001, 492.700], loss: 1429.703613, mae: 26.771816, mean_q: -27.133461\n",
            "  833346/10000000: episode: 4146, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -476.000, mean reward: -2.368 [-238.000, 160.500], mean action: 3.716 [0.000, 10.000], mean observation: 35.800 [0.002, 467.200], loss: 533.442627, mae: 26.515104, mean_q: -27.419497\n",
            "  833547/10000000: episode: 4147, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -181.000, mean reward: -0.900 [-90.500, 135.000], mean action: 3.303 [0.000, 10.000], mean observation: 33.702 [0.000, 511.400], loss: 541.066284, mae: 26.772335, mean_q: -27.431238\n",
            "  833748/10000000: episode: 4148, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -878.200, mean reward: -4.369 [-439.100, 96.000], mean action: 3.741 [0.000, 10.000], mean observation: 34.204 [0.000, 764.600], loss: 396.821045, mae: 26.999990, mean_q: -27.660370\n",
            "  833949/10000000: episode: 4149, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -195.000, mean reward: -0.970 [-97.500, 286.000], mean action: 3.040 [0.000, 10.000], mean observation: 28.865 [0.004, 498.200], loss: 507.328033, mae: 27.132633, mean_q: -27.725935\n",
            "  834150/10000000: episode: 4150, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -579.200, mean reward: -2.882 [-289.600, 111.300], mean action: 3.219 [0.000, 10.000], mean observation: 31.042 [0.000, 598.900], loss: 628.156311, mae: 27.465752, mean_q: -28.174318\n",
            "  834351/10000000: episode: 4151, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -892.800, mean reward: -4.442 [-446.400, 54.000], mean action: 3.124 [0.000, 10.000], mean observation: 34.145 [0.001, 508.700], loss: 1356.269287, mae: 27.418812, mean_q: -28.098349\n",
            "  834552/10000000: episode: 4152, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -553.000, mean reward: -2.751 [-276.500, 143.000], mean action: 3.333 [0.000, 10.000], mean observation: 38.714 [0.000, 792.800], loss: 559.723206, mae: 27.567924, mean_q: -28.137083\n",
            "  834753/10000000: episode: 4153, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -1153.200, mean reward: -5.737 [-576.600, 63.600], mean action: 3.502 [0.000, 10.000], mean observation: 31.276 [0.002, 455.000], loss: 594.082153, mae: 27.716440, mean_q: -28.566736\n",
            "  834954/10000000: episode: 4154, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -1074.600, mean reward: -5.346 [-537.300, 52.400], mean action: 3.980 [0.000, 10.000], mean observation: 38.761 [0.000, 746.700], loss: 568.370850, mae: 27.672754, mean_q: -28.684849\n",
            "  835155/10000000: episode: 4155, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: 313.000, mean reward: 1.557 [-10.000, 262.800], mean action: 3.612 [0.000, 10.000], mean observation: 33.469 [0.001, 541.500], loss: 919.631042, mae: 27.503542, mean_q: -28.707432\n",
            "  835356/10000000: episode: 4156, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -372.800, mean reward: -1.855 [-186.400, 391.000], mean action: 3.955 [0.000, 10.000], mean observation: 32.097 [0.000, 516.300], loss: 461.192383, mae: 28.179703, mean_q: -29.568476\n",
            "  835557/10000000: episode: 4157, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -355.600, mean reward: -1.769 [-177.800, 99.900], mean action: 3.204 [0.000, 10.000], mean observation: 31.038 [0.001, 519.300], loss: 531.714539, mae: 29.146519, mean_q: -30.325672\n",
            "  835758/10000000: episode: 4158, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -477.400, mean reward: -2.375 [-238.700, 225.800], mean action: 2.995 [0.000, 10.000], mean observation: 35.712 [0.003, 566.800], loss: 996.240723, mae: 29.822790, mean_q: -30.603651\n",
            "  835959/10000000: episode: 4159, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: 799.000, mean reward: 3.975 [-10.000, 699.200], mean action: 2.612 [0.000, 10.000], mean observation: 32.759 [0.000, 579.300], loss: 1456.694824, mae: 29.928480, mean_q: -30.789616\n",
            "  836160/10000000: episode: 4160, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -787.400, mean reward: -3.917 [-393.700, 115.200], mean action: 3.244 [0.000, 10.000], mean observation: 28.601 [0.001, 576.200], loss: 2226.981445, mae: 29.220430, mean_q: -30.081125\n",
            "  836361/10000000: episode: 4161, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -395.400, mean reward: -1.967 [-197.700, 132.000], mean action: 3.378 [0.000, 10.000], mean observation: 32.895 [0.001, 550.200], loss: 546.591553, mae: 29.132282, mean_q: -30.379524\n",
            "  836562/10000000: episode: 4162, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -1030.800, mean reward: -5.128 [-515.400, 66.600], mean action: 3.582 [0.000, 10.000], mean observation: 31.154 [0.001, 493.400], loss: 503.880554, mae: 29.334089, mean_q: -30.456802\n",
            "  836763/10000000: episode: 4163, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -344.000, mean reward: -1.711 [-172.000, 177.000], mean action: 3.478 [0.000, 10.000], mean observation: 34.123 [0.001, 494.100], loss: 380.849274, mae: 29.610025, mean_q: -30.625725\n",
            "  836964/10000000: episode: 4164, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -905.800, mean reward: -4.506 [-452.900, 68.400], mean action: 3.284 [0.000, 10.000], mean observation: 37.458 [0.000, 525.700], loss: 1316.662476, mae: 29.452921, mean_q: -30.056126\n",
            "  837165/10000000: episode: 4165, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: 215.800, mean reward: 1.074 [-10.000, 326.200], mean action: 3.821 [0.000, 10.000], mean observation: 38.181 [0.000, 803.400], loss: 2728.687744, mae: 28.563469, mean_q: -29.546328\n",
            "  837366/10000000: episode: 4166, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -1325.200, mean reward: -6.593 [-662.600, 15.000], mean action: 3.905 [0.000, 10.000], mean observation: 31.711 [0.000, 597.200], loss: 1434.008057, mae: 28.562092, mean_q: -29.525522\n",
            "  837567/10000000: episode: 4167, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -878.000, mean reward: -4.368 [-439.000, 169.600], mean action: 3.881 [0.000, 10.000], mean observation: 32.235 [0.002, 525.700], loss: 437.838623, mae: 28.889444, mean_q: -30.083220\n",
            "  837768/10000000: episode: 4168, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -886.000, mean reward: -4.408 [-443.000, 52.800], mean action: 3.572 [0.000, 10.000], mean observation: 35.449 [0.001, 545.700], loss: 331.339478, mae: 29.750210, mean_q: -31.059008\n",
            "  837969/10000000: episode: 4169, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -1159.400, mean reward: -5.768 [-579.700, 45.600], mean action: 3.826 [0.000, 10.000], mean observation: 32.534 [0.000, 622.400], loss: 728.264771, mae: 29.972361, mean_q: -31.238876\n",
            "  838170/10000000: episode: 4170, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -943.800, mean reward: -4.696 [-471.900, 77.000], mean action: 3.846 [0.000, 10.000], mean observation: 39.056 [0.000, 592.700], loss: 491.429657, mae: 29.743233, mean_q: -30.787233\n",
            "  838371/10000000: episode: 4171, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -173.400, mean reward: -0.863 [-86.700, 391.000], mean action: 4.338 [0.000, 10.000], mean observation: 35.031 [0.000, 669.400], loss: 1272.516357, mae: 29.379091, mean_q: -30.406580\n",
            "  838572/10000000: episode: 4172, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -318.600, mean reward: -1.585 [-159.300, 204.400], mean action: 4.323 [0.000, 10.000], mean observation: 32.917 [0.000, 556.200], loss: 2826.570312, mae: 29.231684, mean_q: -29.766560\n",
            "  838773/10000000: episode: 4173, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -438.000, mean reward: -2.179 [-219.000, 302.000], mean action: 4.433 [0.000, 10.000], mean observation: 35.590 [0.002, 511.500], loss: 1102.589600, mae: 28.790651, mean_q: -29.266771\n",
            "  838974/10000000: episode: 4174, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -509.800, mean reward: -2.536 [-254.900, 178.000], mean action: 3.975 [0.000, 10.000], mean observation: 40.890 [0.000, 663.000], loss: 558.257629, mae: 28.684755, mean_q: -29.296431\n",
            "  839175/10000000: episode: 4175, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -1012.400, mean reward: -5.037 [-506.200, 75.000], mean action: 4.045 [0.000, 10.000], mean observation: 32.561 [0.000, 637.900], loss: 502.863373, mae: 28.883976, mean_q: -29.530199\n",
            "  839376/10000000: episode: 4176, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: -556.000, mean reward: -2.766 [-278.000, 123.000], mean action: 3.736 [0.000, 10.000], mean observation: 34.011 [0.001, 459.000], loss: 379.760406, mae: 29.235628, mean_q: -29.947153\n",
            "  839577/10000000: episode: 4177, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -1176.400, mean reward: -5.853 [-588.200, 70.700], mean action: 3.756 [0.000, 10.000], mean observation: 32.902 [0.001, 581.100], loss: 1310.193848, mae: 29.512312, mean_q: -30.240849\n",
            "  839778/10000000: episode: 4178, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -881.600, mean reward: -4.386 [-440.800, 158.900], mean action: 4.269 [0.000, 10.000], mean observation: 32.489 [0.001, 622.800], loss: 384.824615, mae: 29.072250, mean_q: -29.829111\n",
            "  839979/10000000: episode: 4179, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -937.400, mean reward: -4.664 [-468.700, 140.000], mean action: 3.547 [0.000, 10.000], mean observation: 32.710 [0.000, 570.400], loss: 1113.783081, mae: 29.015999, mean_q: -29.706810\n",
            "  840180/10000000: episode: 4180, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -722.600, mean reward: -3.595 [-361.300, 123.000], mean action: 3.124 [0.000, 10.000], mean observation: 36.174 [0.001, 446.000], loss: 614.383545, mae: 28.993980, mean_q: -29.838663\n",
            "  840381/10000000: episode: 4181, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -1425.800, mean reward: -7.094 [-712.900, 57.000], mean action: 4.383 [0.000, 10.000], mean observation: 32.301 [0.002, 498.600], loss: 1523.472778, mae: 28.630194, mean_q: -29.400478\n",
            "  840582/10000000: episode: 4182, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -710.600, mean reward: -3.535 [-355.300, 308.000], mean action: 4.398 [0.000, 10.000], mean observation: 30.387 [0.000, 749.600], loss: 1333.917603, mae: 28.817286, mean_q: -29.467585\n",
            "  840783/10000000: episode: 4183, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -987.000, mean reward: -4.910 [-493.500, 136.400], mean action: 5.224 [0.000, 10.000], mean observation: 34.133 [0.001, 587.600], loss: 557.206421, mae: 28.339495, mean_q: -28.800188\n",
            "  840984/10000000: episode: 4184, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: 3620.400, mean reward: 18.012 [-10.000, 2021.000], mean action: 4.433 [0.000, 10.000], mean observation: 33.741 [0.001, 516.600], loss: 473.434998, mae: 28.693190, mean_q: -29.445917\n",
            "  841185/10000000: episode: 4185, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -831.600, mean reward: -4.137 [-415.800, 112.000], mean action: 3.990 [0.000, 10.000], mean observation: 27.922 [0.003, 415.200], loss: 403.266724, mae: 29.060207, mean_q: -29.935522\n",
            "  841386/10000000: episode: 4186, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -229.200, mean reward: -1.140 [-114.600, 168.800], mean action: 3.975 [0.000, 10.000], mean observation: 34.709 [0.004, 509.100], loss: 1089.613159, mae: 29.066507, mean_q: -29.799458\n",
            "  841587/10000000: episode: 4187, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -640.400, mean reward: -3.186 [-320.200, 68.000], mean action: 3.393 [0.000, 10.000], mean observation: 34.042 [0.000, 590.900], loss: 498.589539, mae: 28.777359, mean_q: -29.560825\n",
            "  841788/10000000: episode: 4188, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -390.200, mean reward: -1.941 [-195.100, 194.000], mean action: 3.000 [0.000, 10.000], mean observation: 25.200 [0.000, 359.900], loss: 307.492859, mae: 29.178921, mean_q: -30.043615\n",
            "  841989/10000000: episode: 4189, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -472.600, mean reward: -2.351 [-236.300, 117.000], mean action: 3.090 [0.000, 10.000], mean observation: 34.367 [0.001, 504.300], loss: 1071.011719, mae: 29.859692, mean_q: -30.472565\n",
            "  842190/10000000: episode: 4190, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -146.200, mean reward: -0.727 [-73.100, 196.000], mean action: 3.552 [0.000, 10.000], mean observation: 31.209 [0.003, 510.800], loss: 1335.351562, mae: 29.671518, mean_q: -30.451439\n",
            "  842391/10000000: episode: 4191, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: 873.000, mean reward: 4.343 [-10.000, 502.000], mean action: 3.423 [0.000, 10.000], mean observation: 32.068 [0.001, 487.500], loss: 394.220551, mae: 29.639208, mean_q: -30.453594\n",
            "  842592/10000000: episode: 4192, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: 175.600, mean reward: 0.874 [-10.000, 209.600], mean action: 2.617 [0.000, 10.000], mean observation: 35.888 [0.000, 637.000], loss: 502.148834, mae: 29.804007, mean_q: -30.255875\n",
            "  842793/10000000: episode: 4193, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -673.200, mean reward: -3.349 [-336.600, 58.000], mean action: 2.672 [0.000, 10.000], mean observation: 31.013 [0.000, 640.400], loss: 396.883362, mae: 29.841156, mean_q: -30.515188\n",
            "  842994/10000000: episode: 4194, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: 547.600, mean reward: 2.724 [-10.000, 519.000], mean action: 3.030 [0.000, 10.000], mean observation: 30.176 [0.001, 413.700], loss: 524.761475, mae: 29.988098, mean_q: -30.917318\n",
            "  843195/10000000: episode: 4195, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -794.800, mean reward: -3.954 [-397.400, 41.000], mean action: 3.109 [0.000, 10.000], mean observation: 31.801 [0.001, 512.800], loss: 468.232269, mae: 30.281904, mean_q: -31.331211\n",
            "  843396/10000000: episode: 4196, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -410.600, mean reward: -2.043 [-205.300, 81.500], mean action: 2.831 [0.000, 10.000], mean observation: 30.635 [0.000, 558.800], loss: 670.986023, mae: 30.822567, mean_q: -31.889912\n",
            "  843597/10000000: episode: 4197, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -752.800, mean reward: -3.745 [-376.400, 42.300], mean action: 2.995 [0.000, 10.000], mean observation: 38.722 [0.001, 539.800], loss: 345.407227, mae: 31.371664, mean_q: -32.405685\n",
            "  843798/10000000: episode: 4198, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -702.200, mean reward: -3.494 [-351.100, 28.000], mean action: 2.637 [0.000, 10.000], mean observation: 35.005 [0.000, 584.600], loss: 388.349060, mae: 31.351130, mean_q: -32.251453\n",
            "  843999/10000000: episode: 4199, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -670.400, mean reward: -3.335 [-335.200, 91.800], mean action: 2.801 [0.000, 10.000], mean observation: 38.255 [0.002, 510.200], loss: 324.566956, mae: 31.189922, mean_q: -32.161446\n",
            "  844200/10000000: episode: 4200, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -350.800, mean reward: -1.745 [-175.400, 110.400], mean action: 2.269 [0.000, 10.000], mean observation: 29.133 [0.001, 486.800], loss: 442.436890, mae: 31.549610, mean_q: -32.342205\n",
            "  844401/10000000: episode: 4201, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: 422.400, mean reward: 2.101 [-10.000, 425.200], mean action: 2.677 [0.000, 10.000], mean observation: 31.930 [0.000, 708.200], loss: 822.374817, mae: 31.976353, mean_q: -32.655331\n",
            "  844602/10000000: episode: 4202, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -686.200, mean reward: -3.414 [-343.100, 52.500], mean action: 2.736 [0.000, 10.000], mean observation: 38.461 [0.002, 500.200], loss: 296.413849, mae: 31.643570, mean_q: -32.523750\n",
            "  844803/10000000: episode: 4203, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: 1060.200, mean reward: 5.275 [-10.000, 649.000], mean action: 3.393 [0.000, 10.000], mean observation: 35.374 [0.000, 440.500], loss: 536.491516, mae: 31.816921, mean_q: -32.930542\n",
            "  845004/10000000: episode: 4204, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: 124.400, mean reward: 0.619 [-10.000, 227.500], mean action: 3.567 [0.000, 10.000], mean observation: 35.323 [0.000, 566.800], loss: 310.447113, mae: 32.188099, mean_q: -33.504223\n",
            "  845205/10000000: episode: 4205, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -385.400, mean reward: -1.917 [-192.700, 122.000], mean action: 3.134 [0.000, 10.000], mean observation: 29.135 [0.001, 571.900], loss: 384.593018, mae: 32.735229, mean_q: -34.026203\n",
            "  845406/10000000: episode: 4206, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -87.000, mean reward: -0.433 [-43.500, 564.000], mean action: 3.692 [0.000, 10.000], mean observation: 32.257 [0.000, 494.100], loss: 695.830505, mae: 33.170025, mean_q: -34.398949\n",
            "  845607/10000000: episode: 4207, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -414.000, mean reward: -2.060 [-207.000, 269.000], mean action: 3.328 [0.000, 10.000], mean observation: 30.768 [0.001, 465.200], loss: 719.596497, mae: 33.444336, mean_q: -34.704113\n",
            "  845808/10000000: episode: 4208, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -86.800, mean reward: -0.432 [-43.400, 215.600], mean action: 3.622 [0.000, 10.000], mean observation: 32.016 [0.000, 798.300], loss: 412.877319, mae: 33.416107, mean_q: -34.554020\n",
            "  846009/10000000: episode: 4209, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -1026.800, mean reward: -5.108 [-513.400, 35.500], mean action: 3.488 [0.000, 10.000], mean observation: 33.296 [0.002, 556.200], loss: 415.101685, mae: 33.691174, mean_q: -34.596882\n",
            "  846210/10000000: episode: 4210, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -763.600, mean reward: -3.799 [-381.800, 63.000], mean action: 3.179 [0.000, 10.000], mean observation: 26.427 [0.001, 434.300], loss: 534.802490, mae: 34.024975, mean_q: -34.916428\n",
            "  846411/10000000: episode: 4211, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -921.200, mean reward: -4.583 [-460.600, 32.400], mean action: 3.224 [0.000, 10.000], mean observation: 35.315 [0.000, 668.900], loss: 310.701965, mae: 34.200771, mean_q: -35.162857\n",
            "  846612/10000000: episode: 4212, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -403.800, mean reward: -2.009 [-201.900, 175.000], mean action: 3.060 [0.000, 10.000], mean observation: 32.417 [0.000, 506.300], loss: 510.577576, mae: 33.428875, mean_q: -34.090992\n",
            "  846813/10000000: episode: 4213, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -370.600, mean reward: -1.844 [-185.300, 253.000], mean action: 3.463 [0.000, 10.000], mean observation: 32.447 [0.000, 531.100], loss: 384.080658, mae: 33.325680, mean_q: -34.154228\n",
            "  847014/10000000: episode: 4214, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -400.000, mean reward: -1.990 [-200.000, 54.000], mean action: 2.607 [0.000, 10.000], mean observation: 37.091 [0.000, 650.000], loss: 804.892700, mae: 33.517742, mean_q: -34.363640\n",
            "  847215/10000000: episode: 4215, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: -463.800, mean reward: -2.307 [-231.900, 65.700], mean action: 2.443 [0.000, 10.000], mean observation: 36.541 [0.000, 574.000], loss: 582.066956, mae: 33.507225, mean_q: -34.058426\n",
            "  847416/10000000: episode: 4216, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -186.600, mean reward: -0.928 [-93.300, 247.100], mean action: 2.303 [0.000, 10.000], mean observation: 26.958 [0.002, 420.100], loss: 363.668854, mae: 33.385616, mean_q: -33.951790\n",
            "  847617/10000000: episode: 4217, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -99.600, mean reward: -0.496 [-49.800, 131.000], mean action: 2.458 [0.000, 10.000], mean observation: 39.635 [0.002, 470.300], loss: 420.236481, mae: 33.415001, mean_q: -34.256985\n",
            "  847818/10000000: episode: 4218, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -723.600, mean reward: -3.600 [-361.800, 62.400], mean action: 3.035 [0.000, 10.000], mean observation: 36.596 [0.001, 510.400], loss: 843.816406, mae: 32.945805, mean_q: -33.868797\n",
            "  848019/10000000: episode: 4219, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: 607.200, mean reward: 3.021 [-10.000, 375.400], mean action: 2.856 [0.000, 10.000], mean observation: 33.689 [0.000, 508.900], loss: 316.894775, mae: 32.726055, mean_q: -33.574482\n",
            "  848220/10000000: episode: 4220, duration: 1.616s, episode steps: 201, steps per second: 124, episode reward: -659.600, mean reward: -3.282 [-329.800, 38.400], mean action: 2.338 [0.000, 10.000], mean observation: 31.672 [0.002, 472.800], loss: 647.565125, mae: 32.779213, mean_q: -33.291225\n",
            "  848421/10000000: episode: 4221, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -216.800, mean reward: -1.079 [-108.400, 138.000], mean action: 2.398 [0.000, 10.000], mean observation: 29.314 [0.001, 618.400], loss: 443.776184, mae: 32.846153, mean_q: -33.326168\n",
            "  848622/10000000: episode: 4222, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: 155.400, mean reward: 0.773 [-10.000, 140.000], mean action: 2.080 [0.000, 10.000], mean observation: 33.380 [0.001, 512.000], loss: 323.461212, mae: 33.351971, mean_q: -33.979385\n",
            "  848823/10000000: episode: 4223, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -526.800, mean reward: -2.621 [-263.400, 55.200], mean action: 2.413 [0.000, 10.000], mean observation: 33.822 [0.000, 586.000], loss: 746.379028, mae: 33.591824, mean_q: -34.198853\n",
            "  849024/10000000: episode: 4224, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -839.800, mean reward: -4.178 [-419.900, 22.000], mean action: 2.517 [0.000, 10.000], mean observation: 32.337 [0.001, 431.000], loss: 303.525085, mae: 33.770683, mean_q: -34.596992\n",
            "  849225/10000000: episode: 4225, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: 50.000, mean reward: 0.249 [-10.000, 86.400], mean action: 2.010 [0.000, 10.000], mean observation: 28.659 [0.001, 459.200], loss: 812.422424, mae: 33.848206, mean_q: -34.582409\n",
            "  849426/10000000: episode: 4226, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -834.200, mean reward: -4.150 [-417.100, 60.900], mean action: 2.687 [0.000, 10.000], mean observation: 33.807 [0.001, 422.300], loss: 581.378296, mae: 33.825325, mean_q: -34.613132\n",
            "  849627/10000000: episode: 4227, duration: 1.587s, episode steps: 201, steps per second: 127, episode reward: 99.800, mean reward: 0.497 [-10.000, 228.800], mean action: 3.075 [0.000, 10.000], mean observation: 33.886 [0.001, 497.500], loss: 690.645264, mae: 33.709370, mean_q: -34.261757\n",
            "  849828/10000000: episode: 4228, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -732.200, mean reward: -3.643 [-366.100, 40.800], mean action: 3.000 [0.000, 10.000], mean observation: 28.972 [0.002, 430.700], loss: 450.863800, mae: 33.824142, mean_q: -34.366135\n",
            "  850029/10000000: episode: 4229, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -326.600, mean reward: -1.625 [-163.300, 108.400], mean action: 2.871 [0.000, 10.000], mean observation: 30.864 [0.002, 527.400], loss: 1041.525757, mae: 34.051315, mean_q: -34.633076\n",
            "  850230/10000000: episode: 4230, duration: 1.586s, episode steps: 201, steps per second: 127, episode reward: -391.000, mean reward: -1.945 [-195.500, 287.600], mean action: 2.940 [0.000, 10.000], mean observation: 36.396 [0.000, 720.900], loss: 447.171600, mae: 34.048584, mean_q: -34.775295\n",
            "  850431/10000000: episode: 4231, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: 106.000, mean reward: 0.527 [-10.000, 116.800], mean action: 3.353 [0.000, 10.000], mean observation: 31.068 [0.001, 500.800], loss: 469.055573, mae: 33.740692, mean_q: -34.526867\n",
            "  850632/10000000: episode: 4232, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -717.800, mean reward: -3.571 [-358.900, 127.600], mean action: 3.423 [0.000, 10.000], mean observation: 31.696 [0.001, 424.400], loss: 454.132294, mae: 33.183723, mean_q: -33.590443\n",
            "  850833/10000000: episode: 4233, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: 618.800, mean reward: 3.079 [-10.000, 309.400], mean action: 2.990 [0.000, 10.000], mean observation: 32.573 [0.000, 441.700], loss: 317.708313, mae: 32.818455, mean_q: -33.563046\n",
            "  851034/10000000: episode: 4234, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -222.600, mean reward: -1.107 [-111.300, 208.400], mean action: 3.144 [0.000, 10.000], mean observation: 29.630 [0.002, 387.100], loss: 428.987091, mae: 33.439629, mean_q: -34.146156\n",
            "  851235/10000000: episode: 4235, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -388.600, mean reward: -1.933 [-194.300, 75.600], mean action: 2.532 [0.000, 10.000], mean observation: 33.403 [0.002, 537.100], loss: 332.838806, mae: 33.286324, mean_q: -33.891144\n",
            "  851436/10000000: episode: 4236, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -137.200, mean reward: -0.683 [-68.600, 151.200], mean action: 2.353 [0.000, 10.000], mean observation: 35.397 [0.001, 562.800], loss: 472.879730, mae: 33.190609, mean_q: -33.812439\n",
            "  851637/10000000: episode: 4237, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: 33.600, mean reward: 0.167 [-10.000, 161.000], mean action: 2.269 [0.000, 10.000], mean observation: 30.311 [0.000, 693.300], loss: 487.172882, mae: 33.638870, mean_q: -34.487106\n",
            "  851838/10000000: episode: 4238, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -897.400, mean reward: -4.465 [-448.700, 56.700], mean action: 2.975 [0.000, 10.000], mean observation: 40.724 [0.001, 600.600], loss: 505.287689, mae: 34.112988, mean_q: -35.279957\n",
            "  852039/10000000: episode: 4239, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -845.000, mean reward: -4.204 [-422.500, 69.300], mean action: 3.199 [0.000, 10.000], mean observation: 35.070 [0.000, 727.600], loss: 614.865601, mae: 34.298805, mean_q: -35.326054\n",
            "  852240/10000000: episode: 4240, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -4.800, mean reward: -0.024 [-10.000, 129.000], mean action: 3.552 [0.000, 10.000], mean observation: 34.432 [0.001, 606.200], loss: 423.443237, mae: 34.522179, mean_q: -35.818607\n",
            "  852441/10000000: episode: 4241, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -677.600, mean reward: -3.371 [-338.800, 52.500], mean action: 2.771 [0.000, 10.000], mean observation: 31.015 [0.001, 597.900], loss: 682.283569, mae: 35.425800, mean_q: -36.616253\n",
            "  852642/10000000: episode: 4242, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 204.800, mean reward: 1.019 [-10.000, 400.200], mean action: 2.527 [0.000, 10.000], mean observation: 33.672 [0.003, 593.100], loss: 410.656281, mae: 35.264378, mean_q: -36.458473\n",
            "  852843/10000000: episode: 4243, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -536.000, mean reward: -2.667 [-268.000, 77.200], mean action: 2.562 [0.000, 10.000], mean observation: 29.690 [0.002, 435.000], loss: 453.642578, mae: 35.419323, mean_q: -36.288319\n",
            "  853044/10000000: episode: 4244, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -343.600, mean reward: -1.709 [-171.800, 76.800], mean action: 2.318 [0.000, 10.000], mean observation: 30.809 [0.001, 697.100], loss: 385.900574, mae: 35.385918, mean_q: -36.120850\n",
            "  853245/10000000: episode: 4245, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -169.600, mean reward: -0.844 [-84.800, 86.000], mean action: 2.109 [0.000, 10.000], mean observation: 30.931 [0.001, 562.800], loss: 268.174469, mae: 35.607559, mean_q: -36.358120\n",
            "  853446/10000000: episode: 4246, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -904.800, mean reward: -4.501 [-452.400, 20.400], mean action: 2.483 [0.000, 10.000], mean observation: 30.680 [0.001, 388.800], loss: 466.666077, mae: 35.184956, mean_q: -36.072109\n",
            "  853647/10000000: episode: 4247, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -754.600, mean reward: -3.754 [-377.300, 25.800], mean action: 2.572 [0.000, 10.000], mean observation: 34.895 [0.000, 522.200], loss: 393.412872, mae: 35.322674, mean_q: -36.104126\n",
            "  853848/10000000: episode: 4248, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -160.000, mean reward: -0.796 [-80.000, 172.200], mean action: 3.075 [0.000, 10.000], mean observation: 29.834 [0.001, 521.600], loss: 422.435242, mae: 35.152328, mean_q: -36.278866\n",
            "  854049/10000000: episode: 4249, duration: 1.639s, episode steps: 201, steps per second: 123, episode reward: -329.200, mean reward: -1.638 [-164.600, 123.600], mean action: 2.453 [0.000, 10.000], mean observation: 33.278 [0.000, 527.200], loss: 467.702881, mae: 35.942513, mean_q: -36.907806\n",
            "  854250/10000000: episode: 4250, duration: 1.682s, episode steps: 201, steps per second: 120, episode reward: -630.600, mean reward: -3.137 [-315.300, 48.900], mean action: 2.254 [0.000, 8.000], mean observation: 34.412 [0.002, 451.200], loss: 475.663605, mae: 36.531132, mean_q: -37.376991\n",
            "  854451/10000000: episode: 4251, duration: 1.682s, episode steps: 201, steps per second: 119, episode reward: -280.200, mean reward: -1.394 [-140.100, 170.000], mean action: 2.711 [0.000, 10.000], mean observation: 31.999 [0.000, 765.000], loss: 429.373291, mae: 36.361950, mean_q: -37.337357\n",
            "  854652/10000000: episode: 4252, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: 148.000, mean reward: 0.736 [-10.000, 236.600], mean action: 2.124 [0.000, 10.000], mean observation: 36.385 [0.002, 439.000], loss: 415.955688, mae: 37.106537, mean_q: -37.852055\n",
            "  854853/10000000: episode: 4253, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -656.000, mean reward: -3.264 [-328.000, 29.100], mean action: 1.995 [0.000, 8.000], mean observation: 35.402 [0.000, 793.800], loss: 357.563965, mae: 37.276028, mean_q: -38.066677\n",
            "  855054/10000000: episode: 4254, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -149.200, mean reward: -0.742 [-74.600, 82.800], mean action: 2.617 [0.000, 10.000], mean observation: 34.989 [0.000, 633.800], loss: 491.184845, mae: 37.775673, mean_q: -38.728054\n",
            "  855255/10000000: episode: 4255, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -672.800, mean reward: -3.347 [-336.400, 44.100], mean action: 2.687 [0.000, 10.000], mean observation: 36.103 [0.000, 701.500], loss: 489.455017, mae: 37.506031, mean_q: -38.515949\n",
            "  855456/10000000: episode: 4256, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -794.000, mean reward: -3.950 [-397.000, 38.800], mean action: 2.507 [0.000, 10.000], mean observation: 40.134 [0.002, 542.400], loss: 666.498657, mae: 37.006859, mean_q: -37.765427\n",
            "  855657/10000000: episode: 4257, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: 979.200, mean reward: 4.872 [-10.000, 489.600], mean action: 2.468 [0.000, 10.000], mean observation: 32.091 [0.000, 783.800], loss: 529.005981, mae: 36.521385, mean_q: -36.667648\n",
            "  855858/10000000: episode: 4258, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -347.000, mean reward: -1.726 [-173.500, 42.000], mean action: 2.403 [0.000, 10.000], mean observation: 38.557 [0.000, 620.700], loss: 673.177917, mae: 35.292519, mean_q: -35.148987\n",
            "  856059/10000000: episode: 4259, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -544.600, mean reward: -2.709 [-272.300, 65.000], mean action: 2.463 [0.000, 10.000], mean observation: 35.123 [0.002, 515.200], loss: 594.411987, mae: 34.075798, mean_q: -33.737797\n",
            "  856260/10000000: episode: 4260, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: 123.000, mean reward: 0.612 [-8.000, 136.000], mean action: 2.224 [0.000, 10.000], mean observation: 35.336 [0.000, 673.700], loss: 475.733856, mae: 34.081730, mean_q: -34.288918\n",
            "  856461/10000000: episode: 4261, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -533.800, mean reward: -2.656 [-266.900, 87.900], mean action: 2.567 [0.000, 10.000], mean observation: 36.891 [0.002, 446.200], loss: 463.283112, mae: 33.786564, mean_q: -34.505066\n",
            "  856662/10000000: episode: 4262, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -468.800, mean reward: -2.332 [-234.400, 102.000], mean action: 2.891 [0.000, 10.000], mean observation: 34.230 [0.001, 459.000], loss: 698.598755, mae: 34.209393, mean_q: -35.301037\n",
            "  856863/10000000: episode: 4263, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 660.800, mean reward: 3.288 [-10.000, 376.000], mean action: 2.801 [0.000, 10.000], mean observation: 36.642 [0.001, 558.800], loss: 824.112122, mae: 34.342735, mean_q: -35.321667\n",
            "  857064/10000000: episode: 4264, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: 953.600, mean reward: 4.744 [-10.000, 476.800], mean action: 2.453 [0.000, 10.000], mean observation: 33.181 [0.000, 933.200], loss: 433.362183, mae: 34.978741, mean_q: -36.062569\n",
            "  857265/10000000: episode: 4265, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -229.800, mean reward: -1.143 [-114.900, 84.000], mean action: 2.254 [0.000, 10.000], mean observation: 32.574 [0.000, 684.900], loss: 587.986084, mae: 35.063961, mean_q: -35.455345\n",
            "  857466/10000000: episode: 4266, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -319.600, mean reward: -1.590 [-159.800, 99.500], mean action: 2.876 [0.000, 10.000], mean observation: 30.681 [0.002, 507.800], loss: 352.599182, mae: 35.314140, mean_q: -36.320473\n",
            "  857667/10000000: episode: 4267, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: 10.800, mean reward: 0.054 [-10.000, 190.800], mean action: 3.020 [0.000, 10.000], mean observation: 33.448 [0.000, 556.700], loss: 424.997894, mae: 35.711330, mean_q: -36.674625\n",
            "  857868/10000000: episode: 4268, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -625.200, mean reward: -3.110 [-312.600, 81.000], mean action: 2.761 [0.000, 10.000], mean observation: 34.585 [0.001, 444.500], loss: 343.557251, mae: 36.954338, mean_q: -37.762146\n",
            "  858069/10000000: episode: 4269, duration: 1.517s, episode steps: 201, steps per second: 133, episode reward: -151.400, mean reward: -0.753 [-75.700, 200.000], mean action: 2.343 [0.000, 10.000], mean observation: 35.461 [0.000, 607.900], loss: 374.402679, mae: 36.818405, mean_q: -37.390678\n",
            "  858270/10000000: episode: 4270, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -286.000, mean reward: -1.423 [-143.000, 79.200], mean action: 2.692 [0.000, 10.000], mean observation: 37.226 [0.000, 528.400], loss: 636.179443, mae: 36.559196, mean_q: -37.421257\n",
            "  858471/10000000: episode: 4271, duration: 1.483s, episode steps: 201, steps per second: 135, episode reward: -215.800, mean reward: -1.074 [-107.900, 292.000], mean action: 2.706 [0.000, 10.000], mean observation: 28.822 [0.001, 455.800], loss: 727.208801, mae: 36.818634, mean_q: -37.536877\n",
            "  858672/10000000: episode: 4272, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -238.600, mean reward: -1.187 [-119.300, 192.000], mean action: 3.632 [0.000, 10.000], mean observation: 31.309 [0.000, 818.500], loss: 985.474182, mae: 36.172047, mean_q: -37.204170\n",
            "  858873/10000000: episode: 4273, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -1179.000, mean reward: -5.866 [-589.500, 82.000], mean action: 4.055 [0.000, 10.000], mean observation: 31.716 [0.002, 441.900], loss: 302.405304, mae: 35.869312, mean_q: -37.042484\n",
            "  859074/10000000: episode: 4274, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: -230.600, mean reward: -1.147 [-115.300, 375.000], mean action: 4.080 [0.000, 10.000], mean observation: 32.855 [0.001, 619.000], loss: 492.638489, mae: 36.241020, mean_q: -37.688633\n",
            "  859275/10000000: episode: 4275, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -494.800, mean reward: -2.462 [-247.400, 103.200], mean action: 3.647 [0.000, 10.000], mean observation: 34.031 [0.001, 412.100], loss: 389.848572, mae: 36.000767, mean_q: -37.398273\n",
            "  859476/10000000: episode: 4276, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: 345.200, mean reward: 1.717 [-10.000, 629.000], mean action: 3.408 [0.000, 10.000], mean observation: 33.248 [0.000, 500.900], loss: 503.050293, mae: 35.837433, mean_q: -36.986118\n",
            "  859677/10000000: episode: 4277, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -442.600, mean reward: -2.202 [-221.300, 170.000], mean action: 3.229 [0.000, 10.000], mean observation: 38.342 [0.001, 501.600], loss: 408.291138, mae: 35.313133, mean_q: -36.360668\n",
            "  859878/10000000: episode: 4278, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -401.400, mean reward: -1.997 [-200.700, 97.200], mean action: 2.766 [0.000, 10.000], mean observation: 31.694 [0.001, 590.900], loss: 301.498993, mae: 35.548363, mean_q: -36.304382\n",
            "  860079/10000000: episode: 4279, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 388.600, mean reward: 1.933 [-7.000, 296.700], mean action: 2.478 [0.000, 10.000], mean observation: 30.212 [0.000, 567.300], loss: 624.225586, mae: 35.647724, mean_q: -35.843674\n",
            "  860280/10000000: episode: 4280, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -371.600, mean reward: -1.849 [-185.800, 67.600], mean action: 2.403 [0.000, 10.000], mean observation: 37.412 [0.000, 747.100], loss: 403.713867, mae: 35.578568, mean_q: -35.921074\n",
            "  860481/10000000: episode: 4281, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -103.200, mean reward: -0.513 [-51.600, 172.800], mean action: 2.144 [0.000, 7.000], mean observation: 30.533 [0.000, 534.800], loss: 392.685913, mae: 35.348782, mean_q: -35.664337\n",
            "  860682/10000000: episode: 4282, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -170.200, mean reward: -0.847 [-85.100, 110.000], mean action: 2.244 [0.000, 10.000], mean observation: 29.682 [0.000, 478.800], loss: 643.534851, mae: 35.302624, mean_q: -35.303051\n",
            "  860883/10000000: episode: 4283, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -557.200, mean reward: -2.772 [-278.600, 55.000], mean action: 2.677 [0.000, 10.000], mean observation: 33.522 [0.000, 580.300], loss: 741.843445, mae: 34.347153, mean_q: -33.927185\n",
            "  861084/10000000: episode: 4284, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -167.800, mean reward: -0.835 [-83.900, 108.400], mean action: 3.109 [0.000, 10.000], mean observation: 33.995 [0.000, 589.600], loss: 854.216187, mae: 34.148937, mean_q: -34.372936\n",
            "  861285/10000000: episode: 4285, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -716.000, mean reward: -3.562 [-358.000, 73.600], mean action: 3.015 [0.000, 10.000], mean observation: 30.183 [0.000, 813.800], loss: 412.949738, mae: 33.993256, mean_q: -34.684395\n",
            "  861486/10000000: episode: 4286, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -905.800, mean reward: -4.506 [-452.900, 30.400], mean action: 2.562 [0.000, 10.000], mean observation: 36.913 [0.000, 621.900], loss: 445.273621, mae: 34.515488, mean_q: -35.040298\n",
            "  861687/10000000: episode: 4287, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -267.400, mean reward: -1.330 [-133.700, 164.000], mean action: 2.303 [0.000, 10.000], mean observation: 31.615 [0.002, 632.400], loss: 705.019531, mae: 34.416897, mean_q: -34.575439\n",
            "  861888/10000000: episode: 4288, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -132.800, mean reward: -0.661 [-66.400, 191.800], mean action: 2.448 [0.000, 10.000], mean observation: 33.689 [0.003, 531.400], loss: 934.661438, mae: 34.386288, mean_q: -35.005924\n",
            "  862089/10000000: episode: 4289, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -913.800, mean reward: -4.546 [-456.900, 41.000], mean action: 2.811 [0.000, 10.000], mean observation: 35.468 [0.001, 519.900], loss: 367.281525, mae: 34.548603, mean_q: -35.604637\n",
            "  862290/10000000: episode: 4290, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -723.600, mean reward: -3.600 [-361.800, 71.000], mean action: 3.219 [0.000, 10.000], mean observation: 37.485 [0.001, 447.800], loss: 480.882721, mae: 34.724106, mean_q: -35.790470\n",
            "  862491/10000000: episode: 4291, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 282.400, mean reward: 1.405 [-10.000, 228.400], mean action: 2.527 [0.000, 10.000], mean observation: 39.954 [0.001, 627.200], loss: 322.389648, mae: 35.182381, mean_q: -36.078888\n",
            "  862692/10000000: episode: 4292, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: 221.600, mean reward: 1.102 [-10.000, 308.500], mean action: 2.851 [0.000, 10.000], mean observation: 39.185 [0.003, 501.700], loss: 452.021759, mae: 35.155533, mean_q: -36.266342\n",
            "  862893/10000000: episode: 4293, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -142.400, mean reward: -0.708 [-71.200, 193.000], mean action: 2.806 [0.000, 10.000], mean observation: 29.410 [0.000, 604.500], loss: 369.121552, mae: 35.697224, mean_q: -36.915142\n",
            "  863094/10000000: episode: 4294, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: 171.200, mean reward: 0.852 [-10.000, 283.000], mean action: 2.557 [0.000, 10.000], mean observation: 32.140 [0.001, 437.600], loss: 784.964844, mae: 36.171341, mean_q: -36.599518\n",
            "  863295/10000000: episode: 4295, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -181.800, mean reward: -0.904 [-90.900, 141.600], mean action: 2.970 [0.000, 10.000], mean observation: 30.210 [0.001, 464.600], loss: 400.691345, mae: 36.479252, mean_q: -37.171806\n",
            "  863496/10000000: episode: 4296, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -340.600, mean reward: -1.695 [-170.300, 67.000], mean action: 3.179 [0.000, 10.000], mean observation: 33.278 [0.001, 531.100], loss: 246.475983, mae: 36.618309, mean_q: -37.454815\n",
            "  863697/10000000: episode: 4297, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -681.200, mean reward: -3.389 [-340.600, 31.600], mean action: 2.323 [0.000, 10.000], mean observation: 38.612 [0.000, 467.700], loss: 366.806274, mae: 36.924934, mean_q: -37.576344\n",
            "  863898/10000000: episode: 4298, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: 521.400, mean reward: 2.594 [-7.000, 383.600], mean action: 2.119 [0.000, 10.000], mean observation: 35.557 [0.000, 784.200], loss: 371.967041, mae: 37.283344, mean_q: -37.796814\n",
            "  864099/10000000: episode: 4299, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: 439.600, mean reward: 2.187 [-10.000, 403.800], mean action: 2.697 [0.000, 10.000], mean observation: 32.777 [0.000, 507.700], loss: 710.747437, mae: 37.147537, mean_q: -38.041553\n",
            "  864300/10000000: episode: 4300, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -710.600, mean reward: -3.535 [-355.300, 34.400], mean action: 2.413 [0.000, 10.000], mean observation: 32.394 [0.001, 542.200], loss: 365.201508, mae: 37.280499, mean_q: -38.094376\n",
            "  864501/10000000: episode: 4301, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -531.600, mean reward: -2.645 [-265.800, 86.800], mean action: 3.060 [0.000, 10.000], mean observation: 34.884 [0.001, 527.300], loss: 606.049683, mae: 37.094776, mean_q: -38.829742\n",
            "  864702/10000000: episode: 4302, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -1010.800, mean reward: -5.029 [-505.400, 40.000], mean action: 2.980 [0.000, 10.000], mean observation: 30.027 [0.000, 542.400], loss: 292.970795, mae: 37.300762, mean_q: -39.078495\n",
            "  864903/10000000: episode: 4303, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -837.000, mean reward: -4.164 [-418.500, 31.800], mean action: 3.030 [0.000, 10.000], mean observation: 32.090 [0.000, 694.400], loss: 593.469604, mae: 38.435337, mean_q: -39.578968\n",
            "  865104/10000000: episode: 4304, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -910.200, mean reward: -4.528 [-455.100, 28.500], mean action: 2.642 [0.000, 10.000], mean observation: 44.343 [0.000, 598.300], loss: 329.925995, mae: 38.417206, mean_q: -39.450981\n",
            "  865305/10000000: episode: 4305, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -419.800, mean reward: -2.089 [-209.900, 70.700], mean action: 2.313 [0.000, 10.000], mean observation: 30.602 [0.001, 555.700], loss: 287.655701, mae: 39.091110, mean_q: -39.950146\n",
            "  865506/10000000: episode: 4306, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: 40.400, mean reward: 0.201 [-10.000, 188.400], mean action: 2.876 [0.000, 10.000], mean observation: 29.139 [0.003, 492.800], loss: 516.455872, mae: 39.164673, mean_q: -40.590809\n",
            "  865707/10000000: episode: 4307, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -687.200, mean reward: -3.419 [-343.600, 45.900], mean action: 2.657 [0.000, 8.000], mean observation: 40.104 [0.000, 669.900], loss: 469.828888, mae: 39.985527, mean_q: -41.547634\n",
            "  865908/10000000: episode: 4308, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -46.400, mean reward: -0.231 [-23.200, 158.200], mean action: 2.701 [0.000, 10.000], mean observation: 34.136 [0.002, 547.800], loss: 271.505554, mae: 39.819893, mean_q: -41.164673\n",
            "  866109/10000000: episode: 4309, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: 977.000, mean reward: 4.861 [-10.000, 588.800], mean action: 2.871 [0.000, 10.000], mean observation: 36.207 [0.002, 482.600], loss: 391.805023, mae: 40.254894, mean_q: -41.969143\n",
            "  866310/10000000: episode: 4310, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -611.600, mean reward: -3.043 [-305.800, 41.200], mean action: 2.458 [0.000, 10.000], mean observation: 36.635 [0.000, 654.700], loss: 413.796265, mae: 41.154186, mean_q: -42.562061\n",
            "  866511/10000000: episode: 4311, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -43.200, mean reward: -0.215 [-21.600, 255.000], mean action: 2.567 [0.000, 10.000], mean observation: 35.244 [0.000, 706.400], loss: 568.203735, mae: 40.663021, mean_q: -41.813831\n",
            "  866712/10000000: episode: 4312, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -850.200, mean reward: -4.230 [-425.100, 49.700], mean action: 3.199 [0.000, 10.000], mean observation: 30.509 [0.001, 480.700], loss: 629.345215, mae: 40.513306, mean_q: -41.793461\n",
            "  866913/10000000: episode: 4313, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: 42.600, mean reward: 0.212 [-10.000, 198.000], mean action: 2.657 [0.000, 10.000], mean observation: 34.849 [0.001, 444.500], loss: 353.839386, mae: 41.126911, mean_q: -42.309589\n",
            "  867114/10000000: episode: 4314, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: 41.800, mean reward: 0.208 [-10.000, 245.400], mean action: 2.284 [0.000, 10.000], mean observation: 30.690 [0.001, 676.900], loss: 350.755737, mae: 42.067898, mean_q: -42.897102\n",
            "  867315/10000000: episode: 4315, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -181.400, mean reward: -0.902 [-90.700, 84.500], mean action: 2.607 [0.000, 10.000], mean observation: 31.698 [0.001, 714.300], loss: 414.979126, mae: 42.408962, mean_q: -43.359772\n",
            "  867516/10000000: episode: 4316, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -389.800, mean reward: -1.939 [-194.900, 65.600], mean action: 1.741 [0.000, 10.000], mean observation: 43.129 [0.000, 601.500], loss: 279.798248, mae: 41.771507, mean_q: -42.746075\n",
            "  867717/10000000: episode: 4317, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -578.200, mean reward: -2.877 [-289.100, 71.400], mean action: 2.358 [0.000, 10.000], mean observation: 31.660 [0.000, 582.800], loss: 352.128418, mae: 42.245483, mean_q: -42.917667\n",
            "  867918/10000000: episode: 4318, duration: 1.627s, episode steps: 201, steps per second: 124, episode reward: 129.600, mean reward: 0.645 [-10.000, 79.000], mean action: 2.493 [0.000, 10.000], mean observation: 32.681 [0.000, 566.700], loss: 399.975677, mae: 41.751816, mean_q: -42.871296\n",
            "  868119/10000000: episode: 4319, duration: 1.712s, episode steps: 201, steps per second: 117, episode reward: -262.800, mean reward: -1.307 [-131.400, 107.100], mean action: 2.308 [0.000, 10.000], mean observation: 30.228 [0.001, 498.800], loss: 669.960327, mae: 41.357723, mean_q: -42.150574\n",
            "  868320/10000000: episode: 4320, duration: 1.669s, episode steps: 201, steps per second: 120, episode reward: -290.000, mean reward: -1.443 [-145.000, 123.000], mean action: 2.647 [0.000, 10.000], mean observation: 32.478 [0.002, 508.500], loss: 668.209045, mae: 42.189507, mean_q: -43.303406\n",
            "  868521/10000000: episode: 4321, duration: 1.669s, episode steps: 201, steps per second: 120, episode reward: 73.800, mean reward: 0.367 [-10.000, 372.400], mean action: 2.841 [0.000, 10.000], mean observation: 32.955 [0.000, 413.800], loss: 539.661133, mae: 41.019455, mean_q: -42.738407\n",
            "  868722/10000000: episode: 4322, duration: 1.683s, episode steps: 201, steps per second: 119, episode reward: -854.400, mean reward: -4.251 [-427.200, 51.000], mean action: 2.776 [0.000, 10.000], mean observation: 38.180 [0.000, 630.500], loss: 353.044617, mae: 41.092129, mean_q: -42.514183\n",
            "  868923/10000000: episode: 4323, duration: 1.653s, episode steps: 201, steps per second: 122, episode reward: -646.000, mean reward: -3.214 [-323.000, 180.300], mean action: 3.323 [0.000, 10.000], mean observation: 30.926 [0.001, 518.600], loss: 415.978790, mae: 40.982830, mean_q: -42.735832\n",
            "  869124/10000000: episode: 4324, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: 880.800, mean reward: 4.382 [-10.000, 666.600], mean action: 3.070 [0.000, 10.000], mean observation: 35.525 [0.000, 562.200], loss: 644.188538, mae: 41.146854, mean_q: -42.794567\n",
            "  869325/10000000: episode: 4325, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -582.000, mean reward: -2.896 [-291.000, 52.200], mean action: 2.761 [0.000, 10.000], mean observation: 30.741 [0.002, 468.200], loss: 286.867920, mae: 41.160545, mean_q: -42.657337\n",
            "  869526/10000000: episode: 4326, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: 289.000, mean reward: 1.438 [-10.000, 201.900], mean action: 2.557 [0.000, 10.000], mean observation: 34.348 [0.002, 637.200], loss: 346.846008, mae: 41.221283, mean_q: -42.661537\n",
            "  869727/10000000: episode: 4327, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -317.000, mean reward: -1.577 [-158.500, 166.800], mean action: 2.741 [0.000, 10.000], mean observation: 28.648 [0.003, 515.700], loss: 575.484497, mae: 41.312187, mean_q: -42.977962\n",
            "  869928/10000000: episode: 4328, duration: 1.589s, episode steps: 201, steps per second: 127, episode reward: 422.000, mean reward: 2.100 [-10.000, 211.000], mean action: 2.393 [0.000, 10.000], mean observation: 28.800 [0.001, 607.700], loss: 776.639526, mae: 40.724037, mean_q: -41.866905\n",
            "  870129/10000000: episode: 4329, duration: 1.574s, episode steps: 201, steps per second: 128, episode reward: -208.400, mean reward: -1.037 [-104.200, 138.600], mean action: 2.119 [0.000, 10.000], mean observation: 34.206 [0.001, 462.800], loss: 395.062653, mae: 41.022305, mean_q: -42.017212\n",
            "  870330/10000000: episode: 4330, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -633.000, mean reward: -3.149 [-316.500, 42.000], mean action: 2.368 [0.000, 10.000], mean observation: 37.802 [0.003, 524.500], loss: 392.788177, mae: 40.649124, mean_q: -41.815838\n",
            "  870531/10000000: episode: 4331, duration: 1.640s, episode steps: 201, steps per second: 123, episode reward: -127.400, mean reward: -0.634 [-63.700, 114.300], mean action: 2.363 [0.000, 8.000], mean observation: 27.419 [0.000, 772.000], loss: 274.804382, mae: 41.072685, mean_q: -42.160000\n",
            "  870732/10000000: episode: 4332, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -221.400, mean reward: -1.101 [-110.700, 160.000], mean action: 2.488 [0.000, 10.000], mean observation: 33.493 [0.000, 558.800], loss: 474.314362, mae: 41.768909, mean_q: -43.306664\n",
            "  870933/10000000: episode: 4333, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: 717.800, mean reward: 3.571 [-10.000, 592.000], mean action: 2.224 [0.000, 10.000], mean observation: 34.393 [0.001, 645.600], loss: 254.189499, mae: 41.882370, mean_q: -43.507771\n",
            "  871134/10000000: episode: 4334, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: -581.800, mean reward: -2.895 [-290.900, 71.200], mean action: 2.801 [0.000, 10.000], mean observation: 34.648 [0.001, 532.400], loss: 310.833923, mae: 41.946537, mean_q: -43.693951\n",
            "  871335/10000000: episode: 4335, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: 12.200, mean reward: 0.061 [-10.000, 111.000], mean action: 2.706 [0.000, 10.000], mean observation: 30.295 [0.000, 516.000], loss: 691.807312, mae: 42.164787, mean_q: -43.489677\n",
            "  871536/10000000: episode: 4336, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: 489.800, mean reward: 2.437 [-10.000, 326.000], mean action: 3.159 [0.000, 10.000], mean observation: 37.206 [0.000, 555.400], loss: 680.174988, mae: 41.485977, mean_q: -43.161263\n",
            "  871737/10000000: episode: 4337, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -680.800, mean reward: -3.387 [-340.400, 108.000], mean action: 2.761 [0.000, 10.000], mean observation: 32.154 [0.002, 464.000], loss: 699.918213, mae: 41.694817, mean_q: -43.349045\n",
            "  871938/10000000: episode: 4338, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -371.200, mean reward: -1.847 [-185.600, 177.000], mean action: 2.552 [0.000, 10.000], mean observation: 37.521 [0.000, 668.100], loss: 383.990112, mae: 41.667679, mean_q: -43.313950\n",
            "  872139/10000000: episode: 4339, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: -784.200, mean reward: -3.901 [-392.100, 27.000], mean action: 2.627 [0.000, 10.000], mean observation: 28.628 [0.000, 389.300], loss: 331.620636, mae: 41.579258, mean_q: -43.046349\n",
            "  872340/10000000: episode: 4340, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -186.400, mean reward: -0.927 [-93.200, 130.800], mean action: 2.617 [0.000, 10.000], mean observation: 31.003 [0.001, 587.900], loss: 369.504578, mae: 41.698318, mean_q: -42.870155\n",
            "  872541/10000000: episode: 4341, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -190.400, mean reward: -0.947 [-95.200, 215.600], mean action: 2.244 [0.000, 10.000], mean observation: 36.358 [0.000, 606.600], loss: 425.255920, mae: 41.856857, mean_q: -43.398006\n",
            "  872742/10000000: episode: 4342, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -511.800, mean reward: -2.546 [-255.900, 123.400], mean action: 2.080 [0.000, 10.000], mean observation: 35.304 [0.001, 426.200], loss: 516.328369, mae: 42.236469, mean_q: -43.624474\n",
            "  872943/10000000: episode: 4343, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -655.000, mean reward: -3.259 [-327.500, 93.000], mean action: 2.532 [0.000, 10.000], mean observation: 33.208 [0.002, 578.200], loss: 383.021606, mae: 42.223160, mean_q: -43.804302\n",
            "  873144/10000000: episode: 4344, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -848.200, mean reward: -4.220 [-424.100, 70.000], mean action: 3.214 [0.000, 10.000], mean observation: 33.119 [0.000, 497.600], loss: 327.212189, mae: 41.903927, mean_q: -43.856636\n",
            "  873345/10000000: episode: 4345, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -555.200, mean reward: -2.762 [-277.600, 62.000], mean action: 2.910 [0.000, 10.000], mean observation: 36.069 [0.001, 527.200], loss: 368.219360, mae: 42.459484, mean_q: -44.365341\n",
            "  873546/10000000: episode: 4346, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -321.800, mean reward: -1.601 [-160.900, 165.900], mean action: 2.746 [0.000, 10.000], mean observation: 30.615 [0.001, 675.300], loss: 209.830414, mae: 42.749699, mean_q: -44.685234\n",
            "  873747/10000000: episode: 4347, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: 2327.400, mean reward: 11.579 [-10.000, 1163.700], mean action: 3.428 [0.000, 10.000], mean observation: 33.488 [0.000, 527.500], loss: 365.510803, mae: 43.138329, mean_q: -45.241543\n",
            "  873948/10000000: episode: 4348, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -697.400, mean reward: -3.470 [-348.700, 49.600], mean action: 2.811 [0.000, 10.000], mean observation: 29.325 [0.001, 466.900], loss: 411.609924, mae: 43.911263, mean_q: -45.489952\n",
            "  874149/10000000: episode: 4349, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: 65.000, mean reward: 0.323 [-10.000, 140.500], mean action: 2.552 [0.000, 10.000], mean observation: 33.744 [0.000, 544.700], loss: 574.245239, mae: 42.648613, mean_q: -43.843624\n",
            "  874350/10000000: episode: 4350, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -534.800, mean reward: -2.661 [-267.400, 78.000], mean action: 2.552 [0.000, 10.000], mean observation: 36.513 [0.002, 532.900], loss: 391.412537, mae: 42.434727, mean_q: -43.970043\n",
            "  874551/10000000: episode: 4351, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: 260.800, mean reward: 1.298 [-10.000, 315.600], mean action: 2.343 [0.000, 10.000], mean observation: 34.691 [0.001, 647.400], loss: 467.204987, mae: 42.772816, mean_q: -44.223667\n",
            "  874752/10000000: episode: 4352, duration: 1.506s, episode steps: 201, steps per second: 134, episode reward: -621.600, mean reward: -3.093 [-310.800, 86.500], mean action: 2.985 [0.000, 10.000], mean observation: 32.803 [0.000, 602.400], loss: 423.952881, mae: 42.327747, mean_q: -43.744728\n",
            "  874953/10000000: episode: 4353, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -927.400, mean reward: -4.614 [-463.700, 2.700], mean action: 2.398 [0.000, 10.000], mean observation: 35.720 [0.002, 453.500], loss: 415.026367, mae: 42.637867, mean_q: -43.972515\n",
            "  875154/10000000: episode: 4354, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: 60.400, mean reward: 0.300 [-10.000, 141.600], mean action: 2.697 [0.000, 10.000], mean observation: 27.593 [0.002, 362.700], loss: 368.003296, mae: 42.746117, mean_q: -44.145832\n",
            "  875355/10000000: episode: 4355, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: 595.200, mean reward: 2.961 [-10.000, 297.600], mean action: 2.612 [0.000, 10.000], mean observation: 31.485 [0.000, 695.400], loss: 323.355133, mae: 42.979836, mean_q: -44.500881\n",
            "  875556/10000000: episode: 4356, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: 52.400, mean reward: 0.261 [-10.000, 94.000], mean action: 2.114 [0.000, 10.000], mean observation: 33.026 [0.000, 660.900], loss: 343.508057, mae: 42.922100, mean_q: -44.140423\n",
            "  875757/10000000: episode: 4357, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -610.600, mean reward: -3.038 [-305.300, 39.400], mean action: 2.313 [0.000, 10.000], mean observation: 34.590 [0.000, 525.900], loss: 403.511047, mae: 43.279114, mean_q: -44.552998\n",
            "  875958/10000000: episode: 4358, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -353.200, mean reward: -1.757 [-176.600, 166.600], mean action: 2.254 [0.000, 10.000], mean observation: 34.010 [0.001, 446.900], loss: 280.277679, mae: 43.452106, mean_q: -44.859074\n",
            "  876159/10000000: episode: 4359, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: 267.600, mean reward: 1.331 [-10.000, 199.500], mean action: 2.393 [0.000, 10.000], mean observation: 35.403 [0.000, 410.900], loss: 357.762177, mae: 43.244801, mean_q: -44.815842\n",
            "  876360/10000000: episode: 4360, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: 1768.600, mean reward: 8.799 [-10.000, 884.300], mean action: 3.005 [0.000, 10.000], mean observation: 28.150 [0.001, 654.600], loss: 317.682983, mae: 42.566349, mean_q: -44.350075\n",
            "  876561/10000000: episode: 4361, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -262.000, mean reward: -1.303 [-131.000, 76.800], mean action: 2.562 [0.000, 10.000], mean observation: 34.526 [0.000, 467.400], loss: 598.701477, mae: 42.355080, mean_q: -43.628750\n",
            "  876762/10000000: episode: 4362, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -48.200, mean reward: -0.240 [-24.100, 311.000], mean action: 2.781 [0.000, 10.000], mean observation: 37.875 [0.000, 697.600], loss: 710.552551, mae: 41.892181, mean_q: -43.354294\n",
            "  876963/10000000: episode: 4363, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -462.200, mean reward: -2.300 [-231.100, 68.700], mean action: 2.308 [0.000, 10.000], mean observation: 31.475 [0.000, 499.800], loss: 364.900269, mae: 42.426514, mean_q: -44.079704\n",
            "  877164/10000000: episode: 4364, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: 346.200, mean reward: 1.722 [-10.000, 215.000], mean action: 2.055 [0.000, 10.000], mean observation: 32.097 [0.001, 443.500], loss: 500.849854, mae: 42.911419, mean_q: -44.194958\n",
            "  877365/10000000: episode: 4365, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -137.000, mean reward: -0.682 [-68.500, 100.800], mean action: 2.100 [0.000, 10.000], mean observation: 31.699 [0.001, 470.800], loss: 317.330841, mae: 42.847763, mean_q: -44.235065\n",
            "  877566/10000000: episode: 4366, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -379.400, mean reward: -1.888 [-189.700, 116.000], mean action: 2.448 [0.000, 10.000], mean observation: 32.863 [0.002, 509.500], loss: 368.903442, mae: 42.975410, mean_q: -44.447517\n",
            "  877767/10000000: episode: 4367, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -493.200, mean reward: -2.454 [-246.600, 78.400], mean action: 2.731 [0.000, 10.000], mean observation: 30.252 [0.002, 515.900], loss: 733.932068, mae: 42.870705, mean_q: -44.727005\n",
            "  877968/10000000: episode: 4368, duration: 1.494s, episode steps: 201, steps per second: 134, episode reward: -85.200, mean reward: -0.424 [-42.600, 126.700], mean action: 2.224 [0.000, 10.000], mean observation: 35.342 [0.002, 467.700], loss: 311.212402, mae: 43.065800, mean_q: -44.692123\n",
            "  878169/10000000: episode: 4369, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -184.000, mean reward: -0.915 [-92.000, 53.000], mean action: 2.114 [0.000, 10.000], mean observation: 25.027 [0.002, 434.600], loss: 649.259094, mae: 43.477455, mean_q: -44.877449\n",
            "  878370/10000000: episode: 4370, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -631.200, mean reward: -3.140 [-315.600, 77.200], mean action: 2.114 [0.000, 10.000], mean observation: 36.630 [0.000, 638.600], loss: 576.915710, mae: 43.010826, mean_q: -44.065113\n",
            "  878571/10000000: episode: 4371, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -168.800, mean reward: -0.840 [-84.400, 133.200], mean action: 2.005 [0.000, 10.000], mean observation: 31.852 [0.000, 407.100], loss: 735.182922, mae: 42.889118, mean_q: -43.920372\n",
            "  878772/10000000: episode: 4372, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -29.000, mean reward: -0.144 [-14.500, 72.500], mean action: 2.095 [0.000, 10.000], mean observation: 34.626 [0.002, 530.500], loss: 533.359314, mae: 42.585152, mean_q: -43.826172\n",
            "  878973/10000000: episode: 4373, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -403.000, mean reward: -2.005 [-201.500, 52.200], mean action: 2.408 [0.000, 10.000], mean observation: 35.216 [0.000, 796.200], loss: 636.434143, mae: 42.378056, mean_q: -43.795219\n",
            "  879174/10000000: episode: 4374, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 284.400, mean reward: 1.415 [-10.000, 252.300], mean action: 2.085 [0.000, 10.000], mean observation: 36.716 [0.000, 588.400], loss: 417.577026, mae: 42.553047, mean_q: -43.815327\n",
            "  879375/10000000: episode: 4375, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -780.400, mean reward: -3.883 [-390.200, 37.000], mean action: 2.512 [0.000, 10.000], mean observation: 30.271 [0.001, 466.300], loss: 480.277130, mae: 42.192364, mean_q: -43.482231\n",
            "  879576/10000000: episode: 4376, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 2607.400, mean reward: 12.972 [-10.000, 1474.000], mean action: 2.687 [0.000, 10.000], mean observation: 34.514 [0.003, 438.400], loss: 531.537781, mae: 42.057442, mean_q: -43.263268\n",
            "  879777/10000000: episode: 4377, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -469.600, mean reward: -2.336 [-234.800, 111.000], mean action: 2.881 [0.000, 10.000], mean observation: 26.782 [0.002, 545.200], loss: 985.276611, mae: 41.727058, mean_q: -42.753929\n",
            "  879978/10000000: episode: 4378, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -698.600, mean reward: -3.476 [-349.300, 45.600], mean action: 2.423 [0.000, 10.000], mean observation: 36.338 [0.001, 491.000], loss: 322.940094, mae: 41.476902, mean_q: -42.464916\n",
            "  880179/10000000: episode: 4379, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -8.400, mean reward: -0.042 [-10.000, 269.200], mean action: 2.428 [0.000, 10.000], mean observation: 32.501 [0.000, 481.400], loss: 316.044189, mae: 41.213551, mean_q: -42.522938\n",
            "  880380/10000000: episode: 4380, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -1086.600, mean reward: -5.406 [-543.300, 4.600], mean action: 2.791 [0.000, 10.000], mean observation: 30.394 [0.002, 446.500], loss: 490.762817, mae: 41.476650, mean_q: -43.001381\n",
            "  880581/10000000: episode: 4381, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -755.200, mean reward: -3.757 [-377.600, 41.300], mean action: 2.577 [0.000, 10.000], mean observation: 32.628 [0.000, 746.900], loss: 450.417603, mae: 40.841145, mean_q: -42.477394\n",
            "  880782/10000000: episode: 4382, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -301.400, mean reward: -1.500 [-150.700, 153.600], mean action: 2.876 [0.000, 10.000], mean observation: 29.759 [0.000, 583.400], loss: 432.375153, mae: 40.653095, mean_q: -42.289127\n",
            "  880983/10000000: episode: 4383, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -1121.000, mean reward: -5.577 [-560.500, 45.000], mean action: 3.826 [0.000, 10.000], mean observation: 33.116 [0.000, 702.100], loss: 399.088776, mae: 40.674713, mean_q: -42.461296\n",
            "  881184/10000000: episode: 4384, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: -382.200, mean reward: -1.901 [-191.100, 127.800], mean action: 2.881 [0.000, 10.000], mean observation: 35.372 [0.000, 472.400], loss: 364.735931, mae: 40.810108, mean_q: -42.532585\n",
            "  881385/10000000: episode: 4385, duration: 2.060s, episode steps: 201, steps per second: 98, episode reward: 199.000, mean reward: 0.990 [-10.000, 203.000], mean action: 2.861 [0.000, 10.000], mean observation: 35.176 [0.000, 653.600], loss: 554.550293, mae: 40.771610, mean_q: -42.345474\n",
            "  881586/10000000: episode: 4386, duration: 1.903s, episode steps: 201, steps per second: 106, episode reward: -422.000, mean reward: -2.100 [-211.000, 122.500], mean action: 3.244 [0.000, 10.000], mean observation: 30.804 [0.000, 509.800], loss: 619.531494, mae: 41.069157, mean_q: -42.908405\n",
            "  881787/10000000: episode: 4387, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -376.400, mean reward: -1.873 [-188.200, 106.500], mean action: 2.716 [0.000, 10.000], mean observation: 40.482 [0.002, 630.900], loss: 394.204712, mae: 41.234657, mean_q: -43.011204\n",
            "  881988/10000000: episode: 4388, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -810.000, mean reward: -4.030 [-405.000, 84.300], mean action: 3.229 [0.000, 10.000], mean observation: 35.265 [0.001, 591.000], loss: 400.092651, mae: 41.363338, mean_q: -43.119102\n",
            "  882189/10000000: episode: 4389, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -633.000, mean reward: -3.149 [-316.500, 69.000], mean action: 2.771 [0.000, 10.000], mean observation: 32.134 [0.001, 532.400], loss: 337.110352, mae: 41.196663, mean_q: -43.067329\n",
            "  882390/10000000: episode: 4390, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: 352.200, mean reward: 1.752 [-10.000, 184.000], mean action: 3.279 [0.000, 10.000], mean observation: 32.855 [0.001, 595.600], loss: 632.529297, mae: 41.030357, mean_q: -42.912884\n",
            "  882591/10000000: episode: 4391, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -16.800, mean reward: -0.084 [-10.000, 224.700], mean action: 3.612 [0.000, 10.000], mean observation: 33.666 [0.000, 530.000], loss: 368.665466, mae: 41.222775, mean_q: -42.893650\n",
            "  882792/10000000: episode: 4392, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: 297.200, mean reward: 1.479 [-10.000, 371.400], mean action: 3.010 [0.000, 10.000], mean observation: 29.995 [0.002, 453.300], loss: 829.839172, mae: 40.724739, mean_q: -42.089504\n",
            "  882993/10000000: episode: 4393, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -150.200, mean reward: -0.747 [-75.100, 136.000], mean action: 2.682 [0.000, 10.000], mean observation: 32.639 [0.001, 504.200], loss: 824.179077, mae: 40.501102, mean_q: -41.615192\n",
            "  883194/10000000: episode: 4394, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -366.600, mean reward: -1.824 [-183.300, 206.400], mean action: 2.806 [0.000, 10.000], mean observation: 28.458 [0.003, 591.100], loss: 292.994019, mae: 39.894199, mean_q: -41.182098\n",
            "  883395/10000000: episode: 4395, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -874.000, mean reward: -4.348 [-437.000, 58.700], mean action: 2.846 [0.000, 10.000], mean observation: 37.155 [0.001, 578.100], loss: 664.563171, mae: 40.003586, mean_q: -41.231304\n",
            "  883596/10000000: episode: 4396, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -406.600, mean reward: -2.023 [-203.300, 131.600], mean action: 2.841 [0.000, 10.000], mean observation: 30.107 [0.001, 598.700], loss: 233.931946, mae: 40.071182, mean_q: -41.437061\n",
            "  883797/10000000: episode: 4397, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -173.800, mean reward: -0.865 [-86.900, 115.000], mean action: 2.493 [0.000, 10.000], mean observation: 32.128 [0.000, 305.100], loss: 450.635651, mae: 40.059200, mean_q: -41.398205\n",
            "  883998/10000000: episode: 4398, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: 610.800, mean reward: 3.039 [-10.000, 308.000], mean action: 2.562 [0.000, 10.000], mean observation: 32.546 [0.000, 540.000], loss: 327.404144, mae: 40.277493, mean_q: -41.723423\n",
            "  884199/10000000: episode: 4399, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -51.800, mean reward: -0.258 [-25.900, 141.600], mean action: 3.010 [0.000, 10.000], mean observation: 30.171 [0.000, 663.800], loss: 351.864899, mae: 40.525654, mean_q: -42.101086\n",
            "  884400/10000000: episode: 4400, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -247.600, mean reward: -1.232 [-123.800, 143.000], mean action: 2.831 [0.000, 10.000], mean observation: 31.320 [0.000, 423.600], loss: 297.512238, mae: 40.889683, mean_q: -42.490692\n",
            "  884601/10000000: episode: 4401, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -319.000, mean reward: -1.587 [-159.500, 121.200], mean action: 2.338 [0.000, 10.000], mean observation: 34.441 [0.000, 639.500], loss: 274.879028, mae: 41.068501, mean_q: -42.460335\n",
            "  884802/10000000: episode: 4402, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -477.200, mean reward: -2.374 [-238.600, 202.000], mean action: 2.891 [0.000, 10.000], mean observation: 33.808 [0.001, 619.200], loss: 377.170044, mae: 40.805534, mean_q: -42.417358\n",
            "  885003/10000000: episode: 4403, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -22.200, mean reward: -0.110 [-11.100, 91.500], mean action: 1.915 [0.000, 10.000], mean observation: 32.863 [0.001, 651.100], loss: 357.133514, mae: 40.893166, mean_q: -42.087547\n",
            "  885204/10000000: episode: 4404, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: -414.000, mean reward: -2.060 [-207.000, 28.800], mean action: 1.871 [0.000, 10.000], mean observation: 37.830 [0.000, 818.100], loss: 659.938599, mae: 40.391438, mean_q: -41.254894\n",
            "  885405/10000000: episode: 4405, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -248.000, mean reward: -1.234 [-124.000, 82.000], mean action: 2.343 [0.000, 10.000], mean observation: 33.197 [0.003, 540.500], loss: 311.954224, mae: 39.673733, mean_q: -40.806576\n",
            "  885606/10000000: episode: 4406, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -770.600, mean reward: -3.834 [-385.300, 135.900], mean action: 3.483 [0.000, 10.000], mean observation: 34.460 [0.001, 565.900], loss: 310.767395, mae: 39.559692, mean_q: -40.852947\n",
            "  885807/10000000: episode: 4407, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -710.800, mean reward: -3.536 [-355.400, 51.100], mean action: 2.388 [0.000, 10.000], mean observation: 32.448 [0.002, 412.300], loss: 263.046600, mae: 39.589622, mean_q: -40.328205\n",
            "  886008/10000000: episode: 4408, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -441.600, mean reward: -2.197 [-220.800, 109.000], mean action: 2.284 [0.000, 10.000], mean observation: 31.649 [0.000, 720.900], loss: 1275.770752, mae: 39.241070, mean_q: -39.607193\n",
            "  886209/10000000: episode: 4409, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: 2089.600, mean reward: 10.396 [-10.000, 1071.000], mean action: 3.104 [0.000, 10.000], mean observation: 32.011 [0.002, 624.500], loss: 308.573547, mae: 38.734608, mean_q: -39.298344\n",
            "  886410/10000000: episode: 4410, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -659.800, mean reward: -3.283 [-329.900, 22.800], mean action: 2.100 [0.000, 10.000], mean observation: 33.339 [0.003, 497.700], loss: 309.361969, mae: 38.377048, mean_q: -38.729782\n",
            "  886611/10000000: episode: 4411, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -498.000, mean reward: -2.478 [-249.000, 88.000], mean action: 2.303 [0.000, 10.000], mean observation: 27.798 [0.001, 419.900], loss: 452.145691, mae: 38.218605, mean_q: -38.891296\n",
            "  886812/10000000: episode: 4412, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: 1987.000, mean reward: 9.886 [-10.000, 1153.000], mean action: 2.965 [0.000, 10.000], mean observation: 37.474 [0.000, 615.100], loss: 860.858215, mae: 37.661907, mean_q: -38.674171\n",
            "  887013/10000000: episode: 4413, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -935.400, mean reward: -4.654 [-467.700, 59.500], mean action: 3.468 [0.000, 10.000], mean observation: 36.179 [0.000, 642.700], loss: 617.072327, mae: 37.145523, mean_q: -38.081375\n",
            "  887214/10000000: episode: 4414, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -591.800, mean reward: -2.944 [-295.900, 76.800], mean action: 2.925 [0.000, 10.000], mean observation: 35.240 [0.000, 907.100], loss: 446.578735, mae: 37.395473, mean_q: -38.154655\n",
            "  887415/10000000: episode: 4415, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -461.800, mean reward: -2.298 [-230.900, 98.500], mean action: 2.488 [0.000, 10.000], mean observation: 32.755 [0.001, 507.200], loss: 579.495117, mae: 37.306576, mean_q: -37.841530\n",
            "  887616/10000000: episode: 4416, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -750.400, mean reward: -3.733 [-375.200, 85.200], mean action: 2.542 [0.000, 10.000], mean observation: 34.944 [0.002, 522.800], loss: 451.718140, mae: 36.630356, mean_q: -36.862782\n",
            "  887817/10000000: episode: 4417, duration: 1.550s, episode steps: 201, steps per second: 130, episode reward: 249.800, mean reward: 1.243 [-10.000, 199.000], mean action: 3.134 [0.000, 10.000], mean observation: 30.113 [0.000, 623.900], loss: 496.521759, mae: 36.333588, mean_q: -36.700340\n",
            "  888018/10000000: episode: 4418, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: 357.200, mean reward: 1.777 [-10.000, 213.000], mean action: 2.239 [0.000, 10.000], mean observation: 28.826 [0.001, 473.800], loss: 477.100433, mae: 35.870148, mean_q: -35.916973\n",
            "  888219/10000000: episode: 4419, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -1154.600, mean reward: -5.744 [-577.300, 9.300], mean action: 3.154 [0.000, 10.000], mean observation: 35.163 [0.000, 542.000], loss: 542.297913, mae: 35.242172, mean_q: -35.560799\n",
            "  888420/10000000: episode: 4420, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -1094.200, mean reward: -5.444 [-547.100, 127.200], mean action: 4.423 [0.000, 10.000], mean observation: 36.556 [0.000, 781.200], loss: 518.719299, mae: 34.496517, mean_q: -34.803165\n",
            "  888621/10000000: episode: 4421, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: 441.600, mean reward: 2.197 [-10.000, 637.000], mean action: 4.124 [0.000, 10.000], mean observation: 25.557 [0.001, 492.700], loss: 630.973816, mae: 33.603214, mean_q: -33.633595\n",
            "  888822/10000000: episode: 4422, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -740.600, mean reward: -3.685 [-370.300, 168.000], mean action: 4.333 [0.000, 10.000], mean observation: 33.645 [0.001, 488.900], loss: 343.655548, mae: 33.018963, mean_q: -32.952286\n",
            "  889023/10000000: episode: 4423, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -101.400, mean reward: -0.504 [-50.700, 128.400], mean action: 4.020 [0.000, 10.000], mean observation: 32.744 [0.002, 467.200], loss: 492.804352, mae: 32.681004, mean_q: -32.874348\n",
            "  889224/10000000: episode: 4424, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -135.600, mean reward: -0.675 [-67.800, 236.000], mean action: 4.935 [0.000, 10.000], mean observation: 35.464 [0.000, 511.400], loss: 664.984924, mae: 32.521755, mean_q: -32.339211\n",
            "  889425/10000000: episode: 4425, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -607.800, mean reward: -3.024 [-303.900, 104.300], mean action: 3.831 [0.000, 10.000], mean observation: 35.133 [0.000, 764.600], loss: 316.922485, mae: 32.247047, mean_q: -32.429707\n",
            "  889626/10000000: episode: 4426, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -897.000, mean reward: -4.463 [-448.500, 54.000], mean action: 3.065 [0.000, 10.000], mean observation: 27.722 [0.004, 516.900], loss: 982.442627, mae: 32.165104, mean_q: -32.022053\n",
            "  889827/10000000: episode: 4427, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -514.600, mean reward: -2.560 [-257.300, 103.000], mean action: 2.955 [0.000, 10.000], mean observation: 32.118 [0.000, 598.900], loss: 357.989227, mae: 31.298342, mean_q: -31.160414\n",
            "  890028/10000000: episode: 4428, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -564.400, mean reward: -2.808 [-282.200, 79.000], mean action: 2.811 [0.000, 10.000], mean observation: 37.512 [0.002, 508.700], loss: 507.511169, mae: 31.086039, mean_q: -31.000862\n",
            "  890229/10000000: episode: 4429, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -706.200, mean reward: -3.513 [-353.100, 119.200], mean action: 3.279 [0.000, 10.000], mean observation: 36.432 [0.000, 792.800], loss: 378.137238, mae: 31.201511, mean_q: -31.104357\n",
            "  890430/10000000: episode: 4430, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -351.400, mean reward: -1.748 [-175.700, 135.000], mean action: 2.388 [0.000, 10.000], mean observation: 31.850 [0.000, 746.700], loss: 170.080811, mae: 31.104671, mean_q: -30.924994\n",
            "  890631/10000000: episode: 4431, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -94.200, mean reward: -0.469 [-47.100, 175.200], mean action: 2.199 [0.000, 10.000], mean observation: 34.899 [0.001, 593.600], loss: 578.804504, mae: 30.711920, mean_q: -30.363459\n",
            "  890832/10000000: episode: 4432, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -60.800, mean reward: -0.302 [-30.400, 137.000], mean action: 1.473 [0.000, 10.000], mean observation: 37.010 [0.001, 541.500], loss: 455.370850, mae: 29.743605, mean_q: -28.828634\n",
            "  891033/10000000: episode: 4433, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -148.200, mean reward: -0.737 [-74.100, 156.400], mean action: 1.836 [0.000, 10.000], mean observation: 32.781 [0.000, 519.300], loss: 548.972534, mae: 29.172949, mean_q: -28.048979\n",
            "  891234/10000000: episode: 4434, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -312.200, mean reward: -1.553 [-156.100, 26.000], mean action: 1.547 [0.000, 10.000], mean observation: 28.383 [0.001, 460.100], loss: 388.335175, mae: 28.183973, mean_q: -26.523880\n",
            "  891435/10000000: episode: 4435, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -362.000, mean reward: -1.801 [-181.000, 85.500], mean action: 2.075 [0.000, 10.000], mean observation: 34.732 [0.003, 566.800], loss: 351.614502, mae: 28.389524, mean_q: -27.539110\n",
            "  891636/10000000: episode: 4436, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: 511.600, mean reward: 2.545 [-10.000, 349.600], mean action: 1.896 [0.000, 10.000], mean observation: 35.128 [0.000, 579.300], loss: 502.682983, mae: 27.819942, mean_q: -26.922720\n",
            "  891837/10000000: episode: 4437, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -243.800, mean reward: -1.213 [-121.900, 192.000], mean action: 2.159 [0.000, 10.000], mean observation: 26.424 [0.001, 402.600], loss: 423.106415, mae: 27.553860, mean_q: -26.309061\n",
            "  892038/10000000: episode: 4438, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -173.800, mean reward: -0.865 [-86.900, 98.700], mean action: 2.279 [0.000, 7.000], mean observation: 32.618 [0.001, 550.200], loss: 314.494873, mae: 27.008356, mean_q: -25.853767\n",
            "  892239/10000000: episode: 4439, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -441.800, mean reward: -2.198 [-220.900, 79.200], mean action: 2.846 [0.000, 10.000], mean observation: 33.748 [0.001, 493.400], loss: 511.569733, mae: 27.101667, mean_q: -26.571537\n",
            "  892440/10000000: episode: 4440, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -199.000, mean reward: -0.990 [-99.500, 111.600], mean action: 2.100 [0.000, 10.000], mean observation: 33.093 [0.001, 494.100], loss: 482.629456, mae: 27.262030, mean_q: -26.345665\n",
            "  892641/10000000: episode: 4441, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -668.200, mean reward: -3.324 [-334.100, 44.400], mean action: 2.284 [0.000, 10.000], mean observation: 41.224 [0.000, 593.600], loss: 563.470154, mae: 26.920582, mean_q: -26.004971\n",
            "  892842/10000000: episode: 4442, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -531.400, mean reward: -2.644 [-265.700, 58.800], mean action: 2.567 [0.000, 10.000], mean observation: 33.767 [0.000, 803.400], loss: 446.106018, mae: 26.603918, mean_q: -25.822023\n",
            "  893043/10000000: episode: 4443, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -409.200, mean reward: -2.036 [-204.600, 127.200], mean action: 2.413 [0.000, 10.000], mean observation: 33.571 [0.001, 455.800], loss: 508.073608, mae: 26.480034, mean_q: -25.510384\n",
            "  893244/10000000: episode: 4444, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -513.200, mean reward: -2.553 [-256.600, 110.000], mean action: 2.677 [0.000, 10.000], mean observation: 33.510 [0.002, 525.700], loss: 403.346313, mae: 26.557510, mean_q: -25.713057\n",
            "  893445/10000000: episode: 4445, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -750.200, mean reward: -3.732 [-375.100, 29.600], mean action: 2.318 [0.000, 10.000], mean observation: 32.007 [0.001, 545.700], loss: 246.054382, mae: 26.449345, mean_q: -25.517723\n",
            "  893646/10000000: episode: 4446, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -801.400, mean reward: -3.987 [-400.700, 34.200], mean action: 2.582 [0.000, 10.000], mean observation: 35.679 [0.000, 622.400], loss: 387.214020, mae: 26.383833, mean_q: -25.715004\n",
            "  893847/10000000: episode: 4447, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -577.800, mean reward: -2.875 [-288.900, 154.000], mean action: 2.572 [0.000, 10.000], mean observation: 33.912 [0.000, 520.500], loss: 358.027039, mae: 26.749403, mean_q: -26.267179\n",
            "  894048/10000000: episode: 4448, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: 361.800, mean reward: 1.800 [-10.000, 180.900], mean action: 2.453 [0.000, 10.000], mean observation: 36.069 [0.000, 669.400], loss: 234.659317, mae: 26.985989, mean_q: -26.494356\n",
            "  894249/10000000: episode: 4449, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -585.000, mean reward: -2.910 [-292.500, 102.200], mean action: 2.338 [0.000, 10.000], mean observation: 35.500 [0.000, 556.200], loss: 514.856018, mae: 27.605469, mean_q: -27.230991\n",
            "  894450/10000000: episode: 4450, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -543.200, mean reward: -2.702 [-271.600, 44.000], mean action: 2.423 [0.000, 10.000], mean observation: 38.492 [0.000, 663.000], loss: 238.255371, mae: 27.916887, mean_q: -27.876427\n",
            "  894651/10000000: episode: 4451, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -642.400, mean reward: -3.196 [-321.200, 17.800], mean action: 2.005 [0.000, 10.000], mean observation: 38.619 [0.002, 515.300], loss: 276.417969, mae: 28.149527, mean_q: -27.759605\n",
            "  894852/10000000: episode: 4452, duration: 1.705s, episode steps: 201, steps per second: 118, episode reward: -650.200, mean reward: -3.235 [-325.100, 47.500], mean action: 2.463 [0.000, 10.000], mean observation: 29.438 [0.000, 637.900], loss: 806.207458, mae: 27.393385, mean_q: -26.951492\n",
            "  895053/10000000: episode: 4453, duration: 1.696s, episode steps: 201, steps per second: 119, episode reward: -566.600, mean reward: -2.819 [-283.300, 50.500], mean action: 1.980 [0.000, 10.000], mean observation: 33.791 [0.001, 459.000], loss: 489.565918, mae: 26.579412, mean_q: -25.547941\n",
            "  895254/10000000: episode: 4454, duration: 1.620s, episode steps: 201, steps per second: 124, episode reward: -556.600, mean reward: -2.769 [-278.300, 19.200], mean action: 1.637 [0.000, 10.000], mean observation: 35.487 [0.001, 581.100], loss: 408.260132, mae: 26.958883, mean_q: -25.910793\n",
            "  895455/10000000: episode: 4455, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -535.000, mean reward: -2.662 [-267.500, 80.500], mean action: 2.104 [0.000, 10.000], mean observation: 33.226 [0.001, 622.800], loss: 274.309479, mae: 26.949780, mean_q: -25.926699\n",
            "  895656/10000000: episode: 4456, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -316.800, mean reward: -1.576 [-158.400, 62.000], mean action: 1.826 [0.000, 10.000], mean observation: 30.378 [0.000, 570.400], loss: 667.311401, mae: 26.485167, mean_q: -25.357224\n",
            "  895857/10000000: episode: 4457, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -419.600, mean reward: -2.088 [-209.800, 69.600], mean action: 1.791 [0.000, 10.000], mean observation: 36.471 [0.001, 446.000], loss: 297.075226, mae: 26.053215, mean_q: -24.978714\n",
            "  896058/10000000: episode: 4458, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -472.800, mean reward: -2.352 [-236.400, 82.400], mean action: 1.786 [0.000, 10.000], mean observation: 34.009 [0.000, 749.600], loss: 494.101044, mae: 25.650846, mean_q: -24.601170\n",
            "  896259/10000000: episode: 4459, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -65.000, mean reward: -0.323 [-32.500, 231.000], mean action: 2.174 [0.000, 10.000], mean observation: 26.998 [0.002, 368.100], loss: 251.403488, mae: 25.974190, mean_q: -25.168112\n",
            "  896460/10000000: episode: 4460, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -289.200, mean reward: -1.439 [-144.600, 102.300], mean action: 2.179 [0.000, 10.000], mean observation: 35.544 [0.001, 587.600], loss: 337.649323, mae: 26.090689, mean_q: -25.296780\n",
            "  896661/10000000: episode: 4461, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 542.000, mean reward: 2.697 [-10.000, 606.300], mean action: 2.015 [0.000, 10.000], mean observation: 31.386 [0.001, 512.300], loss: 356.326416, mae: 26.066187, mean_q: -25.528612\n",
            "  896862/10000000: episode: 4462, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: 549.800, mean reward: 2.735 [-10.000, 341.500], mean action: 2.279 [0.000, 10.000], mean observation: 33.430 [0.003, 415.200], loss: 352.745453, mae: 26.292442, mean_q: -25.837473\n",
            "  897063/10000000: episode: 4463, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -655.600, mean reward: -3.262 [-327.800, 20.400], mean action: 2.030 [0.000, 10.000], mean observation: 31.305 [0.000, 509.100], loss: 512.645691, mae: 26.396803, mean_q: -25.395563\n",
            "  897264/10000000: episode: 4464, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -444.800, mean reward: -2.213 [-222.400, 40.800], mean action: 2.199 [0.000, 8.000], mean observation: 34.545 [0.001, 590.900], loss: 170.574738, mae: 26.647305, mean_q: -25.922646\n",
            "  897465/10000000: episode: 4465, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -390.000, mean reward: -1.940 [-195.000, 82.800], mean action: 2.164 [0.000, 8.000], mean observation: 23.905 [0.000, 446.200], loss: 545.120850, mae: 26.632534, mean_q: -25.950394\n",
            "  897666/10000000: episode: 4466, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -17.800, mean reward: -0.089 [-10.000, 89.400], mean action: 1.547 [0.000, 10.000], mean observation: 35.897 [0.001, 504.300], loss: 229.947144, mae: 26.939877, mean_q: -25.423653\n",
            "  897867/10000000: episode: 4467, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -325.400, mean reward: -1.619 [-162.700, 59.500], mean action: 1.831 [0.000, 10.000], mean observation: 30.905 [0.001, 510.800], loss: 353.676544, mae: 26.866816, mean_q: -25.935843\n",
            "  898068/10000000: episode: 4468, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -371.400, mean reward: -1.848 [-185.700, 46.000], mean action: 2.085 [0.000, 8.000], mean observation: 35.471 [0.002, 487.500], loss: 547.694763, mae: 26.745914, mean_q: -26.014868\n",
            "  898269/10000000: episode: 4469, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: 732.600, mean reward: 3.645 [-10.000, 366.300], mean action: 2.169 [0.000, 10.000], mean observation: 33.631 [0.000, 637.000], loss: 298.761597, mae: 26.266947, mean_q: -25.796803\n",
            "  898470/10000000: episode: 4470, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -562.400, mean reward: -2.798 [-281.200, 18.600], mean action: 2.045 [0.000, 10.000], mean observation: 30.669 [0.000, 640.400], loss: 247.489548, mae: 26.261427, mean_q: -25.845568\n",
            "  898671/10000000: episode: 4471, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 420.800, mean reward: 2.094 [-10.000, 253.600], mean action: 1.940 [0.000, 10.000], mean observation: 28.783 [0.001, 413.700], loss: 174.696320, mae: 27.013351, mean_q: -26.315065\n",
            "  898872/10000000: episode: 4472, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -272.200, mean reward: -1.354 [-136.100, 45.900], mean action: 1.771 [0.000, 10.000], mean observation: 33.375 [0.001, 512.800], loss: 352.334778, mae: 26.957396, mean_q: -26.194916\n",
            "  899073/10000000: episode: 4473, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -361.000, mean reward: -1.796 [-180.500, 42.300], mean action: 2.274 [0.000, 10.000], mean observation: 32.063 [0.000, 558.800], loss: 361.680817, mae: 26.890276, mean_q: -26.670572\n",
            "  899274/10000000: episode: 4474, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -534.000, mean reward: -2.657 [-267.000, 76.000], mean action: 2.711 [0.000, 10.000], mean observation: 40.646 [0.001, 584.600], loss: 374.955566, mae: 27.018501, mean_q: -27.001293\n",
            "  899475/10000000: episode: 4475, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -646.200, mean reward: -3.215 [-323.100, 25.800], mean action: 2.164 [0.000, 10.000], mean observation: 32.986 [0.000, 455.400], loss: 463.599335, mae: 27.001963, mean_q: -26.637276\n",
            "  899676/10000000: episode: 4476, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -68.000, mean reward: -0.338 [-34.000, 303.100], mean action: 2.383 [0.000, 10.000], mean observation: 34.787 [0.002, 510.200], loss: 415.579163, mae: 26.665661, mean_q: -26.174154\n",
            "  899877/10000000: episode: 4477, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -293.400, mean reward: -1.460 [-146.700, 110.400], mean action: 2.502 [0.000, 10.000], mean observation: 29.216 [0.001, 486.800], loss: 348.276184, mae: 25.986681, mean_q: -25.678190\n",
            "  900078/10000000: episode: 4478, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -262.000, mean reward: -1.303 [-131.000, 63.600], mean action: 2.144 [0.000, 10.000], mean observation: 35.167 [0.000, 708.200], loss: 502.743011, mae: 26.324770, mean_q: -25.642895\n",
            "  900279/10000000: episode: 4479, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -498.800, mean reward: -2.482 [-249.400, 100.500], mean action: 2.756 [0.000, 10.000], mean observation: 37.575 [0.002, 500.200], loss: 442.384888, mae: 26.248606, mean_q: -26.404743\n",
            "  900480/10000000: episode: 4480, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: 690.600, mean reward: 3.436 [-10.000, 345.300], mean action: 2.050 [0.000, 10.000], mean observation: 34.986 [0.000, 440.500], loss: 475.970886, mae: 26.363791, mean_q: -25.918108\n",
            "  900681/10000000: episode: 4481, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -387.400, mean reward: -1.927 [-193.700, 59.200], mean action: 2.333 [0.000, 10.000], mean observation: 34.825 [0.000, 566.800], loss: 455.725586, mae: 25.774015, mean_q: -25.514168\n",
            "  900882/10000000: episode: 4482, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -741.600, mean reward: -3.690 [-370.800, 34.000], mean action: 3.000 [0.000, 10.000], mean observation: 31.110 [0.001, 571.900], loss: 262.147614, mae: 26.233414, mean_q: -25.969736\n",
            "  901083/10000000: episode: 4483, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -757.000, mean reward: -3.766 [-378.500, 12.000], mean action: 2.169 [0.000, 10.000], mean observation: 31.233 [0.000, 494.100], loss: 288.484406, mae: 26.255049, mean_q: -25.628044\n",
            "  901284/10000000: episode: 4484, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -312.800, mean reward: -1.556 [-156.400, 161.400], mean action: 2.139 [0.000, 10.000], mean observation: 28.544 [0.001, 457.300], loss: 258.473328, mae: 26.165838, mean_q: -25.689915\n",
            "  901485/10000000: episode: 4485, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 78.400, mean reward: 0.390 [-10.000, 136.200], mean action: 2.299 [0.000, 10.000], mean observation: 32.156 [0.000, 798.300], loss: 262.510284, mae: 26.368038, mean_q: -25.793957\n",
            "  901686/10000000: episode: 4486, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -615.200, mean reward: -3.061 [-307.600, 21.600], mean action: 1.891 [0.000, 10.000], mean observation: 34.919 [0.001, 556.200], loss: 413.381439, mae: 26.124590, mean_q: -25.086531\n",
            "  901887/10000000: episode: 4487, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -126.200, mean reward: -0.628 [-63.100, 202.000], mean action: 1.771 [0.000, 10.000], mean observation: 30.339 [0.000, 481.500], loss: 384.717743, mae: 25.926758, mean_q: -24.836479\n",
            "  902088/10000000: episode: 4488, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -52.000, mean reward: -0.259 [-26.000, 105.000], mean action: 1.781 [0.000, 8.000], mean observation: 31.169 [0.000, 668.900], loss: 246.165848, mae: 25.237713, mean_q: -24.215942\n",
            "  902289/10000000: episode: 4489, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -172.400, mean reward: -0.858 [-86.200, 78.800], mean action: 2.100 [0.000, 10.000], mean observation: 32.014 [0.000, 506.300], loss: 196.661987, mae: 25.004259, mean_q: -24.147263\n",
            "  902490/10000000: episode: 4490, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -682.000, mean reward: -3.393 [-341.000, 29.200], mean action: 2.244 [0.000, 10.000], mean observation: 36.528 [0.000, 605.900], loss: 295.840393, mae: 24.877735, mean_q: -24.016914\n",
            "  902691/10000000: episode: 4491, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: -168.400, mean reward: -0.838 [-84.200, 120.000], mean action: 1.791 [0.000, 10.000], mean observation: 34.038 [0.000, 650.000], loss: 557.807495, mae: 24.772825, mean_q: -23.536692\n",
            "  902892/10000000: episode: 4492, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -133.000, mean reward: -0.662 [-66.500, 180.000], mean action: 1.831 [0.000, 10.000], mean observation: 35.069 [0.000, 574.000], loss: 511.137115, mae: 25.107777, mean_q: -23.682322\n",
            "  903093/10000000: episode: 4493, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 64.800, mean reward: 0.322 [-10.000, 193.800], mean action: 1.975 [0.000, 10.000], mean observation: 31.271 [0.002, 420.100], loss: 423.337524, mae: 25.029411, mean_q: -23.783827\n",
            "  903294/10000000: episode: 4494, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 887.200, mean reward: 4.414 [-10.000, 443.600], mean action: 1.826 [0.000, 10.000], mean observation: 38.170 [0.001, 495.500], loss: 255.699738, mae: 24.583036, mean_q: -23.772078\n",
            "  903495/10000000: episode: 4495, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -365.800, mean reward: -1.820 [-182.900, 58.000], mean action: 2.333 [0.000, 10.000], mean observation: 34.030 [0.001, 510.400], loss: 356.188660, mae: 24.865545, mean_q: -23.887671\n",
            "  903696/10000000: episode: 4496, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: 1540.600, mean reward: 7.665 [-10.000, 1012.500], mean action: 2.373 [0.000, 10.000], mean observation: 35.533 [0.000, 508.900], loss: 205.174362, mae: 24.907207, mean_q: -24.159801\n",
            "  903897/10000000: episode: 4497, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -513.400, mean reward: -2.554 [-256.700, 53.500], mean action: 2.428 [0.000, 10.000], mean observation: 26.278 [0.003, 472.800], loss: 324.958649, mae: 25.000870, mean_q: -24.369791\n",
            "  904098/10000000: episode: 4498, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: 51.200, mean reward: 0.255 [-10.000, 291.500], mean action: 2.015 [0.000, 10.000], mean observation: 35.851 [0.001, 618.400], loss: 218.914688, mae: 25.170034, mean_q: -24.713839\n",
            "  904299/10000000: episode: 4499, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -240.800, mean reward: -1.198 [-120.400, 82.800], mean action: 2.239 [0.000, 10.000], mean observation: 32.499 [0.001, 512.000], loss: 587.653076, mae: 24.736219, mean_q: -24.159712\n",
            "  904500/10000000: episode: 4500, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -541.600, mean reward: -2.695 [-270.800, 25.600], mean action: 2.134 [0.000, 10.000], mean observation: 31.720 [0.000, 586.000], loss: 327.380096, mae: 24.379524, mean_q: -23.938288\n",
            "  904701/10000000: episode: 4501, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -174.000, mean reward: -0.866 [-87.000, 232.500], mean action: 2.289 [0.000, 10.000], mean observation: 31.034 [0.002, 431.000], loss: 630.771667, mae: 24.220198, mean_q: -23.720465\n",
            "  904902/10000000: episode: 4502, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -312.600, mean reward: -1.555 [-156.300, 52.800], mean action: 2.244 [0.000, 10.000], mean observation: 30.051 [0.001, 459.200], loss: 450.726837, mae: 23.907066, mean_q: -23.430250\n",
            "  905103/10000000: episode: 4503, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: -484.400, mean reward: -2.410 [-242.200, 114.400], mean action: 2.378 [0.000, 10.000], mean observation: 34.666 [0.003, 422.300], loss: 288.608215, mae: 23.730400, mean_q: -22.911478\n",
            "  905304/10000000: episode: 4504, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -285.000, mean reward: -1.418 [-142.500, 164.100], mean action: 1.731 [0.000, 10.000], mean observation: 32.398 [0.001, 497.500], loss: 312.936676, mae: 22.788839, mean_q: -21.558254\n",
            "  905505/10000000: episode: 4505, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -489.200, mean reward: -2.434 [-244.600, 35.200], mean action: 2.090 [0.000, 10.000], mean observation: 28.564 [0.003, 430.700], loss: 574.951294, mae: 22.302589, mean_q: -21.047449\n",
            "  905706/10000000: episode: 4506, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -51.400, mean reward: -0.256 [-25.700, 215.700], mean action: 2.368 [0.000, 10.000], mean observation: 34.002 [0.000, 552.400], loss: 422.972351, mae: 22.061136, mean_q: -21.067728\n",
            "  905907/10000000: episode: 4507, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -729.200, mean reward: -3.628 [-364.600, 54.000], mean action: 2.448 [0.000, 10.000], mean observation: 34.901 [0.000, 720.900], loss: 312.308228, mae: 21.912577, mean_q: -21.026508\n",
            "  906108/10000000: episode: 4508, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -173.000, mean reward: -0.861 [-86.500, 57.300], mean action: 2.408 [0.000, 10.000], mean observation: 30.623 [0.001, 500.800], loss: 280.133636, mae: 22.111170, mean_q: -21.177164\n",
            "  906309/10000000: episode: 4509, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: 438.600, mean reward: 2.182 [-10.000, 219.300], mean action: 2.174 [0.000, 10.000], mean observation: 31.630 [0.001, 418.500], loss: 689.574341, mae: 21.701672, mean_q: -20.711798\n",
            "  906510/10000000: episode: 4510, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: 495.800, mean reward: 2.467 [-10.000, 355.200], mean action: 2.090 [0.000, 10.000], mean observation: 31.397 [0.000, 441.700], loss: 272.440094, mae: 21.105059, mean_q: -19.956705\n",
            "  906711/10000000: episode: 4511, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -519.400, mean reward: -2.584 [-259.700, 26.400], mean action: 2.254 [0.000, 10.000], mean observation: 29.505 [0.002, 387.100], loss: 298.324646, mae: 21.070156, mean_q: -20.150112\n",
            "  906912/10000000: episode: 4512, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -724.600, mean reward: -3.605 [-362.300, 75.600], mean action: 3.224 [0.000, 10.000], mean observation: 36.155 [0.002, 543.500], loss: 495.762909, mae: 21.110140, mean_q: -20.732117\n",
            "  907113/10000000: episode: 4513, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -626.000, mean reward: -3.114 [-313.000, 50.400], mean action: 2.930 [0.000, 10.000], mean observation: 32.844 [0.001, 562.800], loss: 268.093536, mae: 21.535482, mean_q: -21.374361\n",
            "  907314/10000000: episode: 4514, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -356.400, mean reward: -1.773 [-178.200, 108.800], mean action: 2.537 [0.000, 10.000], mean observation: 34.480 [0.000, 693.300], loss: 316.310089, mae: 21.851942, mean_q: -21.395136\n",
            "  907515/10000000: episode: 4515, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -665.800, mean reward: -3.312 [-332.900, 63.000], mean action: 2.453 [0.000, 10.000], mean observation: 37.147 [0.000, 727.600], loss: 470.424622, mae: 22.649574, mean_q: -22.042561\n",
            "  907716/10000000: episode: 4516, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -689.800, mean reward: -3.432 [-344.900, 24.900], mean action: 2.522 [0.000, 10.000], mean observation: 36.045 [0.000, 479.600], loss: 269.755981, mae: 22.739166, mean_q: -22.317043\n",
            "  907917/10000000: episode: 4517, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: 325.800, mean reward: 1.621 [-10.000, 219.800], mean action: 2.308 [0.000, 10.000], mean observation: 33.999 [0.001, 606.200], loss: 245.300095, mae: 23.174566, mean_q: -22.669861\n",
            "  908118/10000000: episode: 4518, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -131.800, mean reward: -0.656 [-65.900, 200.100], mean action: 2.478 [0.000, 10.000], mean observation: 31.091 [0.001, 597.900], loss: 208.549133, mae: 23.255791, mean_q: -22.957058\n",
            "  908319/10000000: episode: 4519, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -596.400, mean reward: -2.967 [-298.200, 77.200], mean action: 2.856 [0.000, 10.000], mean observation: 32.862 [0.004, 482.900], loss: 337.984253, mae: 23.361727, mean_q: -23.295189\n",
            "  908520/10000000: episode: 4520, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -34.600, mean reward: -0.172 [-17.300, 107.000], mean action: 2.796 [0.000, 10.000], mean observation: 27.108 [0.002, 435.000], loss: 477.920776, mae: 23.566067, mean_q: -23.242723\n",
            "  908721/10000000: episode: 4521, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -419.800, mean reward: -2.089 [-209.900, 57.000], mean action: 1.886 [0.000, 10.000], mean observation: 33.263 [0.001, 697.100], loss: 275.767090, mae: 23.972555, mean_q: -23.260437\n",
            "  908922/10000000: episode: 4522, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -431.400, mean reward: -2.146 [-215.700, 118.000], mean action: 2.408 [0.000, 10.000], mean observation: 31.927 [0.001, 562.800], loss: 586.441284, mae: 23.330132, mean_q: -22.432287\n",
            "  909123/10000000: episode: 4523, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -672.000, mean reward: -3.343 [-336.000, 15.900], mean action: 2.020 [0.000, 10.000], mean observation: 33.643 [0.000, 458.900], loss: 277.939178, mae: 23.203825, mean_q: -22.674824\n",
            "  909324/10000000: episode: 4524, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: -540.800, mean reward: -2.691 [-270.400, 39.300], mean action: 2.000 [0.000, 10.000], mean observation: 32.920 [0.002, 522.200], loss: 432.157562, mae: 23.241405, mean_q: -22.451424\n",
            "  909525/10000000: episode: 4525, duration: 1.676s, episode steps: 201, steps per second: 120, episode reward: 36.000, mean reward: 0.179 [-10.000, 123.600], mean action: 2.104 [0.000, 10.000], mean observation: 29.133 [0.000, 527.200], loss: 349.970825, mae: 23.454720, mean_q: -22.624193\n",
            "  909726/10000000: episode: 4526, duration: 1.722s, episode steps: 201, steps per second: 117, episode reward: -334.200, mean reward: -1.663 [-167.100, 92.000], mean action: 1.786 [0.000, 10.000], mean observation: 35.001 [0.000, 426.600], loss: 200.421188, mae: 23.492531, mean_q: -22.443933\n",
            "  909927/10000000: episode: 4527, duration: 1.760s, episode steps: 201, steps per second: 114, episode reward: -642.000, mean reward: -3.194 [-321.000, 11.700], mean action: 1.826 [0.000, 10.000], mean observation: 32.696 [0.003, 451.200], loss: 339.671021, mae: 23.201483, mean_q: -22.114126\n",
            "  910128/10000000: episode: 4528, duration: 1.765s, episode steps: 201, steps per second: 114, episode reward: -166.800, mean reward: -0.830 [-83.400, 255.000], mean action: 2.567 [0.000, 10.000], mean observation: 31.445 [0.000, 765.000], loss: 386.068085, mae: 22.653141, mean_q: -21.993944\n",
            "  910329/10000000: episode: 4529, duration: 1.688s, episode steps: 201, steps per second: 119, episode reward: 455.000, mean reward: 2.264 [-10.000, 227.500], mean action: 2.622 [0.000, 10.000], mean observation: 34.118 [0.002, 439.000], loss: 578.251770, mae: 21.894981, mean_q: -21.451769\n",
            "  910530/10000000: episode: 4530, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: -95.600, mean reward: -0.476 [-47.800, 135.500], mean action: 2.920 [0.000, 10.000], mean observation: 39.538 [0.000, 793.800], loss: 475.794800, mae: 22.002663, mean_q: -22.074471\n",
            "  910731/10000000: episode: 4531, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -538.200, mean reward: -2.678 [-269.100, 30.000], mean action: 2.333 [0.000, 10.000], mean observation: 33.673 [0.000, 633.800], loss: 473.058929, mae: 22.393423, mean_q: -21.985203\n",
            "  910932/10000000: episode: 4532, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -753.000, mean reward: -3.746 [-376.500, 48.900], mean action: 2.716 [0.000, 10.000], mean observation: 38.792 [0.000, 701.500], loss: 250.761200, mae: 22.202372, mean_q: -21.994549\n",
            "  911133/10000000: episode: 4533, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -215.200, mean reward: -1.071 [-107.600, 231.600], mean action: 2.876 [0.000, 10.000], mean observation: 35.778 [0.002, 542.400], loss: 437.498444, mae: 22.377829, mean_q: -21.888252\n",
            "  911334/10000000: episode: 4534, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: 1053.400, mean reward: 5.241 [-10.000, 526.700], mean action: 2.751 [0.000, 10.000], mean observation: 36.555 [0.000, 783.800], loss: 550.921631, mae: 22.354649, mean_q: -21.947309\n",
            "  911535/10000000: episode: 4535, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -201.800, mean reward: -1.004 [-100.900, 134.000], mean action: 2.577 [0.000, 10.000], mean observation: 36.225 [0.002, 515.200], loss: 360.100677, mae: 22.238972, mean_q: -21.540623\n",
            "  911736/10000000: episode: 4536, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 11.400, mean reward: 0.057 [-10.000, 281.400], mean action: 2.592 [0.000, 10.000], mean observation: 34.214 [0.003, 381.500], loss: 332.433716, mae: 22.257517, mean_q: -21.877516\n",
            "  911937/10000000: episode: 4537, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -219.800, mean reward: -1.094 [-109.900, 239.400], mean action: 2.597 [0.000, 10.000], mean observation: 38.720 [0.000, 673.700], loss: 337.198059, mae: 22.191765, mean_q: -21.738558\n",
            "  912138/10000000: episode: 4538, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -220.600, mean reward: -1.098 [-110.300, 102.000], mean action: 2.015 [0.000, 10.000], mean observation: 33.714 [0.002, 446.200], loss: 204.244095, mae: 22.221170, mean_q: -21.490988\n",
            "  912339/10000000: episode: 4539, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -270.200, mean reward: -1.344 [-135.100, 80.700], mean action: 2.403 [0.000, 10.000], mean observation: 36.515 [0.001, 459.000], loss: 268.466156, mae: 22.421282, mean_q: -21.983952\n",
            "  912540/10000000: episode: 4540, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -657.800, mean reward: -3.273 [-328.900, 30.800], mean action: 2.075 [0.000, 10.000], mean observation: 34.478 [0.001, 558.800], loss: 410.277252, mae: 23.114807, mean_q: -22.258343\n",
            "  912741/10000000: episode: 4541, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -292.800, mean reward: -1.457 [-146.400, 27.200], mean action: 1.562 [0.000, 10.000], mean observation: 33.918 [0.000, 933.200], loss: 210.510132, mae: 22.583250, mean_q: -21.146034\n",
            "  912942/10000000: episode: 4542, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -231.000, mean reward: -1.149 [-115.500, 63.000], mean action: 2.433 [0.000, 10.000], mean observation: 31.402 [0.000, 684.900], loss: 481.294434, mae: 22.832394, mean_q: -22.324224\n",
            "  913143/10000000: episode: 4543, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -529.600, mean reward: -2.635 [-264.800, 39.800], mean action: 1.866 [0.000, 10.000], mean observation: 29.772 [0.004, 507.800], loss: 331.099670, mae: 22.857765, mean_q: -22.133921\n",
            "  913344/10000000: episode: 4544, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -505.000, mean reward: -2.512 [-252.500, 28.000], mean action: 1.975 [0.000, 10.000], mean observation: 34.514 [0.000, 556.700], loss: 510.291748, mae: 23.062840, mean_q: -22.304901\n",
            "  913545/10000000: episode: 4545, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -470.800, mean reward: -2.342 [-235.400, 84.000], mean action: 2.060 [0.000, 10.000], mean observation: 38.170 [0.000, 607.900], loss: 382.087219, mae: 23.372051, mean_q: -22.419373\n",
            "  913746/10000000: episode: 4546, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -321.200, mean reward: -1.598 [-160.600, 84.300], mean action: 1.617 [0.000, 10.000], mean observation: 32.704 [0.001, 423.500], loss: 652.216675, mae: 23.376240, mean_q: -22.298435\n",
            "  913947/10000000: episode: 4547, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -576.800, mean reward: -2.870 [-288.400, 41.100], mean action: 2.308 [0.000, 10.000], mean observation: 35.898 [0.000, 528.400], loss: 306.287537, mae: 22.941090, mean_q: -22.614370\n",
            "  914148/10000000: episode: 4548, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: 87.000, mean reward: 0.433 [-10.000, 233.600], mean action: 2.831 [0.000, 10.000], mean observation: 29.437 [0.003, 464.200], loss: 426.390625, mae: 23.033428, mean_q: -22.771994\n",
            "  914349/10000000: episode: 4549, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -391.400, mean reward: -1.947 [-195.700, 41.400], mean action: 2.030 [0.000, 10.000], mean observation: 31.475 [0.000, 818.500], loss: 288.164978, mae: 23.712303, mean_q: -23.090899\n",
            "  914550/10000000: episode: 4550, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -634.600, mean reward: -3.157 [-317.300, 30.800], mean action: 2.343 [0.000, 10.000], mean observation: 30.142 [0.002, 449.500], loss: 206.952469, mae: 23.983028, mean_q: -22.744617\n",
            "  914751/10000000: episode: 4551, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: 126.000, mean reward: 0.627 [-7.000, 187.500], mean action: 2.308 [0.000, 7.000], mean observation: 34.989 [0.001, 619.000], loss: 321.985382, mae: 24.228834, mean_q: -23.765606\n",
            "  914952/10000000: episode: 4552, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -401.200, mean reward: -1.996 [-200.600, 71.800], mean action: 2.368 [0.000, 10.000], mean observation: 35.550 [0.002, 402.500], loss: 438.221558, mae: 24.025656, mean_q: -23.752537\n",
            "  915153/10000000: episode: 4553, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -617.800, mean reward: -3.074 [-308.900, 67.000], mean action: 2.657 [0.000, 10.000], mean observation: 29.989 [0.000, 500.900], loss: 456.876678, mae: 24.153610, mean_q: -24.007526\n",
            "  915354/10000000: episode: 4554, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -294.000, mean reward: -1.463 [-147.000, 51.000], mean action: 2.567 [0.000, 10.000], mean observation: 40.424 [0.001, 501.600], loss: 636.975342, mae: 24.126759, mean_q: -23.746988\n",
            "  915555/10000000: episode: 4555, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -407.800, mean reward: -2.029 [-203.900, 54.000], mean action: 2.602 [0.000, 10.000], mean observation: 31.276 [0.001, 590.900], loss: 278.467743, mae: 24.107670, mean_q: -23.784195\n",
            "  915756/10000000: episode: 4556, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 69.400, mean reward: 0.345 [-10.000, 296.700], mean action: 2.512 [0.000, 10.000], mean observation: 32.248 [0.000, 747.100], loss: 564.732971, mae: 23.378109, mean_q: -23.132845\n",
            "  915957/10000000: episode: 4557, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -540.800, mean reward: -2.691 [-270.400, 66.000], mean action: 3.050 [0.000, 10.000], mean observation: 35.707 [0.001, 502.600], loss: 299.363342, mae: 23.609816, mean_q: -23.688080\n",
            "  916158/10000000: episode: 4558, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -23.200, mean reward: -0.115 [-11.600, 83.500], mean action: 2.527 [0.000, 10.000], mean observation: 29.090 [0.000, 534.800], loss: 347.437195, mae: 23.996672, mean_q: -23.703329\n",
            "  916359/10000000: episode: 4559, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -165.000, mean reward: -0.821 [-82.500, 188.000], mean action: 2.393 [0.000, 10.000], mean observation: 28.883 [0.000, 580.300], loss: 200.159088, mae: 24.195934, mean_q: -24.050222\n",
            "  916560/10000000: episode: 4560, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -372.000, mean reward: -1.851 [-186.000, 96.500], mean action: 2.129 [0.000, 10.000], mean observation: 36.684 [0.000, 522.100], loss: 234.148209, mae: 24.532003, mean_q: -24.207508\n",
            "  916761/10000000: episode: 4561, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 66.800, mean reward: 0.332 [-10.000, 68.500], mean action: 2.343 [0.000, 10.000], mean observation: 28.932 [0.001, 589.600], loss: 345.636230, mae: 25.552145, mean_q: -24.629768\n",
            "  916962/10000000: episode: 4562, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -674.200, mean reward: -3.354 [-337.100, 25.200], mean action: 2.144 [0.000, 10.000], mean observation: 34.370 [0.000, 813.800], loss: 250.926575, mae: 24.603540, mean_q: -23.818874\n",
            "  917163/10000000: episode: 4563, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -365.200, mean reward: -1.817 [-182.600, 57.600], mean action: 1.930 [0.000, 10.000], mean observation: 38.451 [0.000, 621.900], loss: 230.192627, mae: 25.147636, mean_q: -24.498333\n",
            "  917364/10000000: episode: 4564, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -505.000, mean reward: -2.512 [-252.500, 27.400], mean action: 1.701 [0.000, 10.000], mean observation: 29.099 [0.002, 632.400], loss: 352.249451, mae: 25.611591, mean_q: -23.901663\n",
            "  917565/10000000: episode: 4565, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -353.600, mean reward: -1.759 [-176.800, 83.000], mean action: 1.642 [0.000, 10.000], mean observation: 35.368 [0.002, 519.900], loss: 344.978180, mae: 24.677351, mean_q: -23.565084\n",
            "  917766/10000000: episode: 4566, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -326.400, mean reward: -1.624 [-163.200, 106.500], mean action: 2.194 [0.000, 10.000], mean observation: 34.665 [0.001, 518.800], loss: 481.395355, mae: 23.786526, mean_q: -23.388706\n",
            "  917967/10000000: episode: 4567, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -189.600, mean reward: -0.943 [-94.800, 61.000], mean action: 2.109 [0.000, 10.000], mean observation: 40.675 [0.001, 490.400], loss: 285.062225, mae: 23.587885, mean_q: -23.287039\n",
            "  918168/10000000: episode: 4568, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: 375.600, mean reward: 1.869 [-10.000, 303.200], mean action: 2.413 [0.000, 10.000], mean observation: 35.906 [0.001, 627.200], loss: 548.784668, mae: 23.488813, mean_q: -23.080608\n",
            "  918369/10000000: episode: 4569, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -201.200, mean reward: -1.001 [-100.600, 246.800], mean action: 2.542 [0.000, 10.000], mean observation: 39.584 [0.001, 501.700], loss: 496.650665, mae: 23.624275, mean_q: -23.553560\n",
            "  918570/10000000: episode: 4570, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -552.600, mean reward: -2.749 [-276.300, 120.000], mean action: 3.438 [0.000, 10.000], mean observation: 32.306 [0.000, 604.500], loss: 525.792847, mae: 23.578474, mean_q: -23.352602\n",
            "  918771/10000000: episode: 4571, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -230.000, mean reward: -1.144 [-115.000, 283.000], mean action: 3.851 [0.000, 10.000], mean observation: 31.174 [0.001, 464.600], loss: 327.358612, mae: 23.740910, mean_q: -23.630600\n",
            "  918972/10000000: episode: 4572, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -18.200, mean reward: -0.091 [-10.000, 142.000], mean action: 3.134 [0.000, 10.000], mean observation: 27.592 [0.001, 531.100], loss: 333.673889, mae: 23.883289, mean_q: -23.784325\n",
            "  919173/10000000: episode: 4573, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -444.000, mean reward: -2.209 [-222.000, 120.300], mean action: 3.254 [0.000, 10.000], mean observation: 36.172 [0.001, 507.400], loss: 525.206726, mae: 23.566708, mean_q: -23.183167\n",
            "  919374/10000000: episode: 4574, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -50.600, mean reward: -0.252 [-25.300, 316.000], mean action: 3.333 [0.000, 10.000], mean observation: 33.702 [0.000, 467.700], loss: 372.035400, mae: 23.331156, mean_q: -23.149229\n",
            "  919575/10000000: episode: 4575, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -397.200, mean reward: -1.976 [-198.600, 109.600], mean action: 2.826 [0.000, 10.000], mean observation: 39.344 [0.000, 784.200], loss: 328.607819, mae: 23.578753, mean_q: -23.446493\n",
            "  919776/10000000: episode: 4576, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: 212.200, mean reward: 1.056 [-10.000, 205.800], mean action: 2.761 [0.000, 10.000], mean observation: 31.231 [0.000, 507.700], loss: 373.455200, mae: 23.584381, mean_q: -23.506079\n",
            "  919977/10000000: episode: 4577, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -279.400, mean reward: -1.390 [-139.700, 175.100], mean action: 2.388 [0.000, 10.000], mean observation: 34.236 [0.001, 542.200], loss: 544.975830, mae: 23.579485, mean_q: -23.549236\n",
            "  920178/10000000: episode: 4578, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -781.800, mean reward: -3.890 [-390.900, 17.900], mean action: 2.587 [0.000, 10.000], mean observation: 30.806 [0.000, 527.300], loss: 282.734772, mae: 23.806713, mean_q: -23.532810\n",
            "  920379/10000000: episode: 4579, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -526.400, mean reward: -2.619 [-263.200, 74.000], mean action: 2.368 [0.000, 10.000], mean observation: 30.886 [0.001, 542.400], loss: 309.610046, mae: 24.068508, mean_q: -23.742420\n",
            "  920580/10000000: episode: 4580, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -639.600, mean reward: -3.182 [-319.800, 15.400], mean action: 2.065 [0.000, 10.000], mean observation: 39.095 [0.000, 694.400], loss: 369.848419, mae: 23.856941, mean_q: -23.414364\n",
            "  920781/10000000: episode: 4581, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -300.400, mean reward: -1.495 [-150.200, 88.800], mean action: 2.174 [0.000, 10.000], mean observation: 37.114 [0.000, 598.300], loss: 290.323914, mae: 23.549965, mean_q: -23.069613\n",
            "  920982/10000000: episode: 4582, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -511.800, mean reward: -2.546 [-255.900, 30.300], mean action: 1.796 [0.000, 10.000], mean observation: 29.845 [0.001, 555.700], loss: 366.144348, mae: 24.098475, mean_q: -23.568218\n",
            "  921183/10000000: episode: 4583, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -526.400, mean reward: -2.619 [-263.200, 17.000], mean action: 1.488 [0.000, 10.000], mean observation: 31.398 [0.000, 669.900], loss: 320.777191, mae: 24.721825, mean_q: -23.211020\n",
            "  921384/10000000: episode: 4584, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -108.400, mean reward: -0.539 [-54.200, 72.600], mean action: 1.562 [0.000, 10.000], mean observation: 43.483 [0.001, 662.900], loss: 855.766479, mae: 24.209429, mean_q: -21.929548\n",
            "  921585/10000000: episode: 4585, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: 28.800, mean reward: 0.143 [-10.000, 74.200], mean action: 1.313 [0.000, 10.000], mean observation: 32.673 [0.002, 441.800], loss: 392.715057, mae: 22.546171, mean_q: -19.967354\n",
            "  921786/10000000: episode: 4586, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -310.400, mean reward: -1.544 [-155.200, 57.000], mean action: 1.642 [0.000, 10.000], mean observation: 37.276 [0.001, 482.600], loss: 349.297424, mae: 22.404119, mean_q: -20.233717\n",
            "  921987/10000000: episode: 4587, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -104.800, mean reward: -0.521 [-52.400, 111.000], mean action: 2.204 [0.000, 10.000], mean observation: 34.674 [0.000, 654.700], loss: 263.509796, mae: 22.428764, mean_q: -21.004242\n",
            "  922188/10000000: episode: 4588, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -444.200, mean reward: -2.210 [-222.100, 58.600], mean action: 2.114 [0.000, 10.000], mean observation: 32.913 [0.000, 706.400], loss: 195.777557, mae: 22.677380, mean_q: -21.599928\n",
            "  922389/10000000: episode: 4589, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -417.400, mean reward: -2.077 [-208.700, 34.400], mean action: 1.806 [0.000, 10.000], mean observation: 33.059 [0.001, 480.700], loss: 312.019287, mae: 22.779875, mean_q: -21.762962\n",
            "  922590/10000000: episode: 4590, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -251.200, mean reward: -1.250 [-125.600, 65.600], mean action: 2.264 [0.000, 10.000], mean observation: 32.350 [0.001, 463.400], loss: 280.623108, mae: 22.871958, mean_q: -21.828665\n",
            "  922791/10000000: episode: 4591, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: 29.600, mean reward: 0.147 [-10.000, 245.400], mean action: 2.090 [0.000, 10.000], mean observation: 31.552 [0.001, 676.900], loss: 159.793655, mae: 23.129631, mean_q: -22.191830\n",
            "  922992/10000000: episode: 4592, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -411.000, mean reward: -2.045 [-205.500, 43.800], mean action: 2.050 [0.000, 10.000], mean observation: 36.788 [0.001, 714.300], loss: 590.685608, mae: 23.306879, mean_q: -22.452799\n",
            "  923193/10000000: episode: 4593, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -416.000, mean reward: -2.070 [-208.000, 79.600], mean action: 2.408 [0.000, 10.000], mean observation: 37.941 [0.000, 601.500], loss: 204.723465, mae: 22.942083, mean_q: -22.950308\n",
            "  923394/10000000: episode: 4594, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -1.800, mean reward: -0.009 [-10.000, 99.300], mean action: 2.512 [0.000, 10.000], mean observation: 32.110 [0.001, 582.800], loss: 189.593735, mae: 23.748617, mean_q: -23.692642\n",
            "  923595/10000000: episode: 4595, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -270.200, mean reward: -1.344 [-135.100, 36.900], mean action: 2.060 [0.000, 10.000], mean observation: 37.267 [0.000, 566.700], loss: 379.227234, mae: 24.389080, mean_q: -23.631105\n",
            "  923796/10000000: episode: 4596, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -373.400, mean reward: -1.858 [-186.700, 45.900], mean action: 2.109 [0.000, 10.000], mean observation: 27.810 [0.002, 508.500], loss: 373.475403, mae: 24.221752, mean_q: -23.652252\n",
            "  923997/10000000: episode: 4597, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -119.400, mean reward: -0.594 [-59.700, 163.200], mean action: 2.308 [0.000, 10.000], mean observation: 30.423 [0.002, 499.200], loss: 311.343475, mae: 23.958452, mean_q: -23.347715\n",
            "  924198/10000000: episode: 4598, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: 0.600, mean reward: 0.003 [-10.000, 279.300], mean action: 2.189 [0.000, 10.000], mean observation: 33.565 [0.000, 376.900], loss: 269.715515, mae: 23.994068, mean_q: -23.561411\n",
            "  924399/10000000: episode: 4599, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -691.400, mean reward: -3.440 [-345.700, 16.000], mean action: 2.065 [0.000, 10.000], mean observation: 37.639 [0.001, 630.500], loss: 327.856201, mae: 24.316769, mean_q: -23.576502\n",
            "  924600/10000000: episode: 4600, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: 76.600, mean reward: 0.381 [-10.000, 157.200], mean action: 2.463 [0.000, 10.000], mean observation: 30.483 [0.001, 518.600], loss: 331.501099, mae: 23.989603, mean_q: -23.049974\n",
            "  924801/10000000: episode: 4601, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: 194.800, mean reward: 0.969 [-10.000, 333.300], mean action: 2.214 [0.000, 10.000], mean observation: 36.683 [0.000, 562.200], loss: 318.345520, mae: 23.733961, mean_q: -23.367727\n",
            "  925002/10000000: episode: 4602, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: 556.200, mean reward: 2.767 [-10.000, 278.100], mean action: 3.443 [0.000, 10.000], mean observation: 30.344 [0.002, 536.400], loss: 271.106934, mae: 23.512173, mean_q: -23.781847\n",
            "  925203/10000000: episode: 4603, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -1049.600, mean reward: -5.222 [-524.800, 40.000], mean action: 3.259 [0.000, 10.000], mean observation: 33.769 [0.002, 637.200], loss: 295.745209, mae: 23.858685, mean_q: -23.927799\n",
            "  925404/10000000: episode: 4604, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -252.200, mean reward: -1.255 [-126.100, 82.500], mean action: 2.463 [0.000, 10.000], mean observation: 27.905 [0.002, 607.700], loss: 210.124573, mae: 23.942484, mean_q: -23.727234\n",
            "  925605/10000000: episode: 4605, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -358.000, mean reward: -1.781 [-179.000, 106.800], mean action: 2.796 [0.000, 10.000], mean observation: 29.010 [0.001, 497.800], loss: 423.232941, mae: 24.148174, mean_q: -24.082424\n",
            "  925806/10000000: episode: 4606, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -708.600, mean reward: -3.525 [-354.300, 18.300], mean action: 2.398 [0.000, 10.000], mean observation: 35.294 [0.001, 462.800], loss: 223.650299, mae: 24.161415, mean_q: -23.871048\n",
            "  926007/10000000: episode: 4607, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -395.000, mean reward: -1.965 [-197.500, 70.000], mean action: 2.905 [0.000, 10.000], mean observation: 39.214 [0.000, 772.000], loss: 203.266830, mae: 24.202236, mean_q: -24.137148\n",
            "  926208/10000000: episode: 4608, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -285.600, mean reward: -1.421 [-142.800, 114.300], mean action: 2.582 [0.000, 10.000], mean observation: 27.920 [0.000, 471.500], loss: 359.426270, mae: 24.565659, mean_q: -24.331331\n",
            "  926409/10000000: episode: 4609, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -470.400, mean reward: -2.340 [-235.200, 93.600], mean action: 2.925 [0.000, 10.000], mean observation: 32.530 [0.000, 558.800], loss: 362.804230, mae: 24.226852, mean_q: -24.284245\n",
            "  926610/10000000: episode: 4610, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: 70.200, mean reward: 0.349 [-10.000, 224.000], mean action: 3.279 [0.000, 10.000], mean observation: 34.008 [0.001, 645.600], loss: 279.766449, mae: 24.125206, mean_q: -24.169878\n",
            "  926811/10000000: episode: 4611, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -828.800, mean reward: -4.123 [-414.400, 12.000], mean action: 2.721 [0.000, 10.000], mean observation: 33.153 [0.001, 487.300], loss: 289.559479, mae: 24.271786, mean_q: -24.035652\n",
            "  927012/10000000: episode: 4612, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 77.400, mean reward: 0.385 [-10.000, 88.000], mean action: 2.179 [0.000, 10.000], mean observation: 31.434 [0.000, 555.400], loss: 232.785492, mae: 24.658293, mean_q: -24.265293\n",
            "  927213/10000000: episode: 4613, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -753.600, mean reward: -3.749 [-376.800, 20.400], mean action: 2.259 [0.000, 10.000], mean observation: 36.069 [0.000, 553.100], loss: 202.755173, mae: 24.901756, mean_q: -24.584419\n",
            "  927414/10000000: episode: 4614, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -615.000, mean reward: -3.060 [-307.500, 55.500], mean action: 2.502 [0.000, 10.000], mean observation: 33.877 [0.000, 668.100], loss: 373.301300, mae: 24.990788, mean_q: -24.710949\n",
            "  927615/10000000: episode: 4615, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -476.400, mean reward: -2.370 [-238.200, 45.600], mean action: 2.239 [0.000, 10.000], mean observation: 35.166 [0.000, 489.400], loss: 360.992859, mae: 25.572622, mean_q: -25.780134\n",
            "  927816/10000000: episode: 4616, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -224.400, mean reward: -1.116 [-112.200, 79.500], mean action: 2.209 [0.000, 10.000], mean observation: 29.241 [0.000, 513.300], loss: 343.152100, mae: 25.887909, mean_q: -25.838493\n",
            "  928017/10000000: episode: 4617, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -572.400, mean reward: -2.848 [-286.200, 32.000], mean action: 2.692 [0.000, 10.000], mean observation: 33.179 [0.000, 587.900], loss: 376.059540, mae: 26.063816, mean_q: -25.935919\n",
            "  928218/10000000: episode: 4618, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -340.000, mean reward: -1.692 [-170.000, 161.700], mean action: 2.398 [0.000, 10.000], mean observation: 36.109 [0.000, 606.600], loss: 201.333176, mae: 26.144495, mean_q: -25.998276\n",
            "  928419/10000000: episode: 4619, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -437.800, mean reward: -2.178 [-218.900, 61.700], mean action: 2.015 [0.000, 10.000], mean observation: 34.371 [0.001, 578.200], loss: 328.803558, mae: 25.972786, mean_q: -25.687361\n",
            "  928620/10000000: episode: 4620, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -90.200, mean reward: -0.449 [-45.100, 186.000], mean action: 2.294 [0.000, 10.000], mean observation: 29.817 [0.003, 461.100], loss: 324.865723, mae: 26.220963, mean_q: -25.947781\n",
            "  928821/10000000: episode: 4621, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -488.200, mean reward: -2.429 [-244.100, 44.000], mean action: 2.572 [0.000, 10.000], mean observation: 38.097 [0.000, 497.600], loss: 361.551453, mae: 26.359146, mean_q: -26.606474\n",
            "  929022/10000000: episode: 4622, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -538.200, mean reward: -2.678 [-269.100, 74.900], mean action: 2.811 [0.000, 10.000], mean observation: 34.198 [0.001, 675.300], loss: 345.375580, mae: 26.161793, mean_q: -26.650131\n",
            "  929223/10000000: episode: 4623, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -555.200, mean reward: -2.762 [-277.600, 99.300], mean action: 2.687 [0.000, 10.000], mean observation: 31.868 [0.001, 605.800], loss: 245.419128, mae: 26.510576, mean_q: -26.592205\n",
            "  929424/10000000: episode: 4624, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -417.600, mean reward: -2.078 [-208.800, 60.000], mean action: 2.488 [0.000, 10.000], mean observation: 29.042 [0.000, 527.500], loss: 282.227570, mae: 26.560799, mean_q: -26.635452\n",
            "  929625/10000000: episode: 4625, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -43.600, mean reward: -0.217 [-21.800, 75.000], mean action: 1.925 [0.000, 10.000], mean observation: 31.835 [0.001, 466.900], loss: 232.853775, mae: 26.730406, mean_q: -26.849277\n",
            "  929826/10000000: episode: 4626, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: 295.800, mean reward: 1.472 [-10.000, 164.700], mean action: 2.468 [0.000, 10.000], mean observation: 36.027 [0.000, 544.700], loss: 337.141357, mae: 26.721876, mean_q: -26.937098\n",
            "  930027/10000000: episode: 4627, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -298.400, mean reward: -1.485 [-149.200, 97.200], mean action: 2.572 [0.000, 10.000], mean observation: 37.889 [0.002, 532.900], loss: 257.521210, mae: 26.597162, mean_q: -26.860100\n",
            "  930228/10000000: episode: 4628, duration: 1.567s, episode steps: 201, steps per second: 128, episode reward: 655.400, mean reward: 3.261 [-10.000, 631.200], mean action: 2.950 [0.000, 10.000], mean observation: 33.099 [0.001, 647.400], loss: 383.899414, mae: 26.686399, mean_q: -26.889683\n",
            "  930429/10000000: episode: 4629, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -633.600, mean reward: -3.152 [-316.800, 65.000], mean action: 2.219 [0.000, 10.000], mean observation: 31.219 [0.000, 602.400], loss: 220.802704, mae: 26.818865, mean_q: -26.986229\n",
            "  930630/10000000: episode: 4630, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -544.000, mean reward: -2.706 [-272.000, 78.000], mean action: 2.542 [0.000, 10.000], mean observation: 34.124 [0.002, 453.500], loss: 163.631912, mae: 26.557384, mean_q: -26.912138\n",
            "  930831/10000000: episode: 4631, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -155.600, mean reward: -0.774 [-77.800, 63.600], mean action: 2.090 [0.000, 10.000], mean observation: 28.467 [0.001, 458.100], loss: 303.070557, mae: 26.936426, mean_q: -26.914185\n",
            "  931032/10000000: episode: 4632, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -341.000, mean reward: -1.697 [-170.500, 89.200], mean action: 1.876 [0.000, 10.000], mean observation: 32.738 [0.000, 695.400], loss: 330.456940, mae: 26.863119, mean_q: -26.848551\n",
            "  931233/10000000: episode: 4633, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: 105.000, mean reward: 0.522 [-10.000, 107.200], mean action: 2.771 [0.000, 10.000], mean observation: 34.336 [0.000, 660.900], loss: 293.689575, mae: 26.686941, mean_q: -26.845829\n",
            "  931434/10000000: episode: 4634, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -597.600, mean reward: -2.973 [-298.800, 38.000], mean action: 2.478 [0.000, 10.000], mean observation: 34.895 [0.001, 472.200], loss: 255.351685, mae: 26.736464, mean_q: -27.082197\n",
            "  931635/10000000: episode: 4635, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: 307.200, mean reward: 1.528 [-10.000, 333.200], mean action: 2.866 [0.000, 10.000], mean observation: 33.803 [0.000, 446.900], loss: 273.423767, mae: 26.914938, mean_q: -26.950632\n",
            "  931836/10000000: episode: 4636, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -95.800, mean reward: -0.477 [-47.900, 143.700], mean action: 2.050 [0.000, 10.000], mean observation: 34.158 [0.001, 654.600], loss: 382.599884, mae: 26.926237, mean_q: -26.682859\n",
            "  932037/10000000: episode: 4637, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -241.400, mean reward: -1.201 [-120.700, 97.400], mean action: 2.189 [0.000, 10.000], mean observation: 28.154 [0.001, 466.800], loss: 173.150497, mae: 26.953264, mean_q: -26.661804\n",
            "  932238/10000000: episode: 4638, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -209.400, mean reward: -1.042 [-104.700, 72.200], mean action: 2.075 [0.000, 10.000], mean observation: 36.419 [0.000, 467.400], loss: 249.183167, mae: 28.136047, mean_q: -27.729282\n",
            "  932439/10000000: episode: 4639, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -385.800, mean reward: -1.919 [-192.900, 93.300], mean action: 2.159 [0.000, 10.000], mean observation: 34.605 [0.000, 697.600], loss: 178.492050, mae: 28.119720, mean_q: -27.695322\n",
            "  932640/10000000: episode: 4640, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -541.600, mean reward: -2.695 [-270.800, 26.700], mean action: 2.114 [0.000, 10.000], mean observation: 33.813 [0.000, 443.500], loss: 387.716309, mae: 27.889196, mean_q: -28.029238\n",
            "  932841/10000000: episode: 4641, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: 235.800, mean reward: 1.173 [-10.000, 278.600], mean action: 2.368 [0.000, 10.000], mean observation: 30.394 [0.001, 470.800], loss: 288.211700, mae: 27.935610, mean_q: -28.262243\n",
            "  933042/10000000: episode: 4642, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: 198.800, mean reward: 0.989 [-10.000, 261.000], mean action: 2.040 [0.000, 10.000], mean observation: 30.832 [0.002, 333.700], loss: 351.406860, mae: 27.977879, mean_q: -27.951517\n",
            "  933243/10000000: episode: 4643, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -446.600, mean reward: -2.222 [-223.300, 43.200], mean action: 1.811 [0.000, 10.000], mean observation: 32.172 [0.002, 515.900], loss: 231.306229, mae: 27.704170, mean_q: -27.650988\n",
            "  933444/10000000: episode: 4644, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -426.000, mean reward: -2.119 [-213.000, 93.600], mean action: 1.791 [0.000, 10.000], mean observation: 34.470 [0.002, 454.800], loss: 279.422638, mae: 27.623724, mean_q: -27.352598\n",
            "  933645/10000000: episode: 4645, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: 162.200, mean reward: 0.807 [-10.000, 270.900], mean action: 1.776 [0.000, 10.000], mean observation: 32.558 [0.002, 467.700], loss: 273.379242, mae: 27.752119, mean_q: -27.338581\n",
            "  933846/10000000: episode: 4646, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -97.400, mean reward: -0.485 [-48.700, 53.000], mean action: 2.114 [0.000, 10.000], mean observation: 25.525 [0.000, 638.600], loss: 161.741669, mae: 27.522694, mean_q: -26.929085\n",
            "  934047/10000000: episode: 4647, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -350.400, mean reward: -1.743 [-175.200, 46.400], mean action: 1.582 [0.000, 10.000], mean observation: 38.676 [0.001, 584.000], loss: 299.402710, mae: 27.198729, mean_q: -26.780886\n",
            "  934248/10000000: episode: 4648, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -454.000, mean reward: -2.259 [-227.000, 43.000], mean action: 2.149 [0.000, 10.000], mean observation: 31.343 [0.000, 530.500], loss: 286.403625, mae: 26.860582, mean_q: -27.002592\n",
            "  934449/10000000: episode: 4649, duration: 1.517s, episode steps: 201, steps per second: 133, episode reward: -3.400, mean reward: -0.017 [-10.000, 145.000], mean action: 2.677 [0.000, 10.000], mean observation: 31.405 [0.002, 445.400], loss: 309.830505, mae: 27.081211, mean_q: -27.430014\n",
            "  934650/10000000: episode: 4650, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: -473.800, mean reward: -2.357 [-236.900, 41.600], mean action: 2.318 [0.000, 10.000], mean observation: 39.742 [0.000, 796.200], loss: 406.646973, mae: 27.126316, mean_q: -27.277794\n",
            "  934851/10000000: episode: 4651, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -134.000, mean reward: -0.667 [-67.000, 185.000], mean action: 2.338 [0.000, 10.000], mean observation: 32.040 [0.000, 507.900], loss: 207.449310, mae: 27.253309, mean_q: -27.246767\n",
            "  935052/10000000: episode: 4652, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -586.200, mean reward: -2.916 [-293.100, 28.600], mean action: 2.229 [0.000, 10.000], mean observation: 36.906 [0.001, 466.300], loss: 234.274094, mae: 27.430803, mean_q: -27.390118\n",
            "  935253/10000000: episode: 4653, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -373.600, mean reward: -1.859 [-186.800, 44.200], mean action: 2.144 [0.000, 10.000], mean observation: 29.524 [0.003, 438.400], loss: 284.993500, mae: 27.441275, mean_q: -27.540359\n",
            "  935454/10000000: episode: 4654, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -407.600, mean reward: -2.028 [-203.800, 111.000], mean action: 2.547 [0.000, 10.000], mean observation: 26.716 [0.002, 545.200], loss: 186.728500, mae: 27.633114, mean_q: -28.062826\n",
            "  935655/10000000: episode: 4655, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: 9.800, mean reward: 0.049 [-9.000, 83.000], mean action: 2.274 [0.000, 10.000], mean observation: 37.546 [0.000, 491.000], loss: 227.690002, mae: 27.861313, mean_q: -28.336069\n",
            "  935856/10000000: episode: 4656, duration: 1.649s, episode steps: 201, steps per second: 122, episode reward: -696.400, mean reward: -3.465 [-348.200, 70.200], mean action: 2.776 [0.000, 10.000], mean observation: 31.781 [0.001, 481.400], loss: 215.836639, mae: 28.313992, mean_q: -28.928509\n",
            "  936057/10000000: episode: 4657, duration: 1.645s, episode steps: 201, steps per second: 122, episode reward: -504.600, mean reward: -2.510 [-252.300, 136.000], mean action: 2.876 [0.000, 10.000], mean observation: 30.177 [0.002, 455.400], loss: 292.745636, mae: 28.715334, mean_q: -29.270357\n",
            "  936258/10000000: episode: 4658, duration: 1.649s, episode steps: 201, steps per second: 122, episode reward: -853.000, mean reward: -4.244 [-426.500, 54.600], mean action: 2.935 [0.000, 10.000], mean observation: 30.415 [0.000, 746.900], loss: 273.205658, mae: 28.544605, mean_q: -29.295155\n",
            "  936459/10000000: episode: 4659, duration: 1.601s, episode steps: 201, steps per second: 126, episode reward: -50.600, mean reward: -0.252 [-25.300, 153.600], mean action: 2.353 [0.000, 10.000], mean observation: 33.532 [0.001, 583.400], loss: 243.616135, mae: 29.925781, mean_q: -30.183516\n",
            "  936660/10000000: episode: 4660, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -162.000, mean reward: -0.806 [-81.000, 171.600], mean action: 2.065 [0.000, 10.000], mean observation: 32.538 [0.000, 702.100], loss: 186.488968, mae: 30.569357, mean_q: -30.432535\n",
            "  936861/10000000: episode: 4661, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -543.800, mean reward: -2.705 [-271.900, 37.800], mean action: 2.159 [0.000, 10.000], mean observation: 37.664 [0.000, 653.600], loss: 285.344299, mae: 29.415773, mean_q: -29.653584\n",
            "  937062/10000000: episode: 4662, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -143.600, mean reward: -0.714 [-71.800, 108.000], mean action: 2.418 [0.000, 10.000], mean observation: 29.701 [0.002, 509.800], loss: 230.947998, mae: 29.204117, mean_q: -29.778383\n",
            "  937263/10000000: episode: 4663, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -132.600, mean reward: -0.660 [-66.300, 42.600], mean action: 2.289 [0.000, 10.000], mean observation: 36.379 [0.000, 630.900], loss: 169.638199, mae: 29.324314, mean_q: -29.664692\n",
            "  937464/10000000: episode: 4664, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: 170.400, mean reward: 0.848 [-9.000, 95.700], mean action: 1.672 [0.000, 10.000], mean observation: 37.121 [0.003, 571.000], loss: 202.177048, mae: 29.806602, mean_q: -29.297579\n",
            "  937665/10000000: episode: 4665, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -427.400, mean reward: -2.126 [-213.700, 44.100], mean action: 1.935 [0.000, 9.000], mean observation: 36.929 [0.001, 591.000], loss: 204.725372, mae: 29.366638, mean_q: -28.853167\n",
            "  937866/10000000: episode: 4666, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -304.800, mean reward: -1.516 [-152.400, 55.300], mean action: 2.458 [0.000, 10.000], mean observation: 30.885 [0.001, 532.400], loss: 148.087936, mae: 29.439152, mean_q: -28.973400\n",
            "  938067/10000000: episode: 4667, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: 526.200, mean reward: 2.618 [-10.000, 263.100], mean action: 2.383 [0.000, 10.000], mean observation: 33.204 [0.001, 595.600], loss: 279.036987, mae: 28.777563, mean_q: -28.724163\n",
            "  938268/10000000: episode: 4668, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -235.200, mean reward: -1.170 [-117.600, 46.200], mean action: 2.866 [0.000, 10.000], mean observation: 29.410 [0.000, 530.000], loss: 196.600937, mae: 28.687172, mean_q: -28.606350\n",
            "  938469/10000000: episode: 4669, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: 522.200, mean reward: 2.598 [-10.000, 371.400], mean action: 2.592 [0.000, 10.000], mean observation: 30.250 [0.002, 453.300], loss: 239.277359, mae: 28.489080, mean_q: -28.304644\n",
            "  938670/10000000: episode: 4670, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -393.000, mean reward: -1.955 [-196.500, 47.200], mean action: 2.443 [0.000, 10.000], mean observation: 34.785 [0.001, 504.200], loss: 274.377747, mae: 28.331532, mean_q: -27.863432\n",
            "  938871/10000000: episode: 4671, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -317.800, mean reward: -1.581 [-158.900, 41.300], mean action: 2.249 [0.000, 10.000], mean observation: 28.941 [0.003, 591.100], loss: 189.113266, mae: 27.936798, mean_q: -27.335724\n",
            "  939072/10000000: episode: 4672, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -35.000, mean reward: -0.174 [-17.500, 394.800], mean action: 2.562 [0.000, 10.000], mean observation: 39.426 [0.001, 598.700], loss: 265.141327, mae: 28.182655, mean_q: -27.952679\n",
            "  939273/10000000: episode: 4673, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -223.400, mean reward: -1.111 [-111.700, 59.500], mean action: 2.269 [0.000, 10.000], mean observation: 27.553 [0.002, 369.100], loss: 141.948868, mae: 28.418148, mean_q: -28.002283\n",
            "  939474/10000000: episode: 4674, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -306.200, mean reward: -1.523 [-153.100, 86.800], mean action: 1.736 [0.000, 9.000], mean observation: 32.905 [0.000, 461.800], loss: 224.250122, mae: 28.590885, mean_q: -27.879051\n",
            "  939675/10000000: episode: 4675, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: 117.400, mean reward: 0.584 [-9.000, 101.000], mean action: 1.955 [0.000, 10.000], mean observation: 32.799 [0.000, 540.000], loss: 123.147224, mae: 28.594898, mean_q: -27.505983\n",
            "  939876/10000000: episode: 4676, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -56.800, mean reward: -0.283 [-28.400, 72.000], mean action: 2.015 [0.000, 10.000], mean observation: 28.686 [0.000, 663.800], loss: 199.126694, mae: 28.014771, mean_q: -27.327229\n",
            "  940077/10000000: episode: 4677, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -410.800, mean reward: -2.044 [-205.400, 20.000], mean action: 1.677 [0.000, 9.000], mean observation: 34.777 [0.000, 463.200], loss: 146.934570, mae: 27.784689, mean_q: -26.823816\n",
            "  940278/10000000: episode: 4678, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -507.600, mean reward: -2.525 [-253.800, 30.400], mean action: 2.159 [0.000, 10.000], mean observation: 32.743 [0.001, 639.500], loss: 120.061821, mae: 27.295710, mean_q: -26.906677\n",
            "  940479/10000000: episode: 4679, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -377.400, mean reward: -1.878 [-188.700, 100.000], mean action: 2.328 [0.000, 10.000], mean observation: 31.087 [0.001, 619.200], loss: 166.152267, mae: 26.978525, mean_q: -27.018379\n",
            "  940680/10000000: episode: 4680, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -194.800, mean reward: -0.969 [-97.400, 77.400], mean action: 2.279 [0.000, 10.000], mean observation: 36.244 [0.001, 651.100], loss: 196.436920, mae: 27.057930, mean_q: -27.098734\n",
            "  940881/10000000: episode: 4681, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -389.000, mean reward: -1.935 [-194.500, 57.600], mean action: 2.284 [0.000, 10.000], mean observation: 38.930 [0.000, 818.100], loss: 250.875580, mae: 27.704758, mean_q: -27.848661\n",
            "  941082/10000000: episode: 4682, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -579.600, mean reward: -2.884 [-289.800, 102.500], mean action: 2.403 [0.000, 8.000], mean observation: 28.653 [0.003, 540.500], loss: 198.382736, mae: 27.678133, mean_q: -27.741570\n",
            "  941283/10000000: episode: 4683, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -873.000, mean reward: -4.343 [-436.500, 49.000], mean action: 3.045 [0.000, 10.000], mean observation: 35.485 [0.001, 565.900], loss: 237.255447, mae: 27.226385, mean_q: -27.240822\n",
            "  941484/10000000: episode: 4684, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -623.000, mean reward: -3.100 [-311.500, 61.500], mean action: 2.542 [0.000, 10.000], mean observation: 33.327 [0.002, 488.800], loss: 227.726471, mae: 27.263884, mean_q: -27.390553\n",
            "  941685/10000000: episode: 4685, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -255.400, mean reward: -1.271 [-127.700, 102.800], mean action: 1.891 [0.000, 9.000], mean observation: 30.143 [0.000, 720.900], loss: 169.161194, mae: 27.783724, mean_q: -27.564060\n",
            "  941886/10000000: episode: 4686, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -546.000, mean reward: -2.716 [-273.000, 76.000], mean action: 2.174 [0.000, 10.000], mean observation: 35.911 [0.002, 624.500], loss: 235.396225, mae: 28.336491, mean_q: -28.305197\n",
            "  942087/10000000: episode: 4687, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -568.000, mean reward: -2.826 [-284.000, 34.200], mean action: 2.104 [0.000, 10.000], mean observation: 28.579 [0.001, 488.700], loss: 184.283157, mae: 28.965384, mean_q: -28.981091\n",
            "  942288/10000000: episode: 4688, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -179.400, mean reward: -0.893 [-89.700, 74.800], mean action: 2.363 [0.000, 10.000], mean observation: 31.770 [0.002, 476.600], loss: 205.332886, mae: 28.874388, mean_q: -28.977493\n",
            "  942489/10000000: episode: 4689, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -326.200, mean reward: -1.623 [-163.100, 51.800], mean action: 2.149 [0.000, 8.000], mean observation: 38.859 [0.000, 615.100], loss: 217.148636, mae: 29.390732, mean_q: -29.613586\n",
            "  942690/10000000: episode: 4690, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -677.400, mean reward: -3.370 [-338.700, 18.900], mean action: 2.095 [0.000, 10.000], mean observation: 35.758 [0.000, 907.100], loss: 140.624557, mae: 29.748621, mean_q: -30.018688\n",
            "  942891/10000000: episode: 4691, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -214.400, mean reward: -1.067 [-107.200, 76.800], mean action: 2.020 [0.000, 10.000], mean observation: 32.630 [0.001, 508.500], loss: 183.956528, mae: 29.694813, mean_q: -29.856043\n",
            "  943092/10000000: episode: 4692, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: 131.800, mean reward: 0.656 [-10.000, 295.500], mean action: 2.015 [0.000, 10.000], mean observation: 35.384 [0.001, 507.200], loss: 205.627075, mae: 30.218552, mean_q: -30.329277\n",
            "  943293/10000000: episode: 4693, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -396.800, mean reward: -1.974 [-198.400, 87.000], mean action: 2.323 [0.000, 10.000], mean observation: 31.900 [0.002, 522.800], loss: 231.136154, mae: 30.067568, mean_q: -30.328241\n",
            "  943494/10000000: episode: 4694, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -154.400, mean reward: -0.768 [-77.200, 123.300], mean action: 2.363 [0.000, 9.000], mean observation: 28.844 [0.000, 623.900], loss: 261.291992, mae: 29.606464, mean_q: -29.762827\n",
            "  943695/10000000: episode: 4695, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -630.000, mean reward: -3.134 [-315.000, 34.800], mean action: 2.323 [0.000, 10.000], mean observation: 30.920 [0.001, 402.000], loss: 173.729004, mae: 29.435890, mean_q: -29.837652\n",
            "  943896/10000000: episode: 4696, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: 1457.000, mean reward: 7.249 [-10.000, 844.800], mean action: 2.184 [0.000, 10.000], mean observation: 34.535 [0.000, 542.000], loss: 261.770111, mae: 29.191425, mean_q: -29.613417\n",
            "  944097/10000000: episode: 4697, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -377.000, mean reward: -1.876 [-188.500, 48.600], mean action: 2.254 [0.000, 10.000], mean observation: 34.997 [0.000, 781.200], loss: 204.912003, mae: 29.165651, mean_q: -29.835981\n",
            "  944298/10000000: episode: 4698, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: 182.800, mean reward: 0.909 [-10.000, 191.100], mean action: 2.149 [0.000, 10.000], mean observation: 27.445 [0.001, 492.700], loss: 182.154144, mae: 29.653255, mean_q: -29.864998\n",
            "  944499/10000000: episode: 4699, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: 80.000, mean reward: 0.398 [-10.000, 233.800], mean action: 1.995 [0.000, 10.000], mean observation: 34.189 [0.001, 488.900], loss: 196.006104, mae: 29.951939, mean_q: -30.372011\n",
            "  944700/10000000: episode: 4700, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: 83.800, mean reward: 0.417 [-10.000, 337.500], mean action: 2.333 [0.000, 10.000], mean observation: 34.407 [0.001, 511.400], loss: 190.793228, mae: 29.724274, mean_q: -30.087648\n",
            "  944901/10000000: episode: 4701, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -195.200, mean reward: -0.971 [-97.600, 162.400], mean action: 2.428 [0.000, 10.000], mean observation: 35.710 [0.000, 571.200], loss: 166.739822, mae: 29.434723, mean_q: -29.805643\n",
            "  945102/10000000: episode: 4702, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -338.000, mean reward: -1.682 [-169.000, 44.500], mean action: 2.388 [0.000, 10.000], mean observation: 34.109 [0.000, 764.600], loss: 225.594055, mae: 29.038757, mean_q: -29.287066\n",
            "  945303/10000000: episode: 4703, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -210.000, mean reward: -1.045 [-105.000, 94.200], mean action: 2.866 [0.000, 10.000], mean observation: 25.207 [0.001, 516.900], loss: 265.564789, mae: 28.854200, mean_q: -29.349424\n",
            "  945504/10000000: episode: 4704, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -410.400, mean reward: -2.042 [-205.200, 72.100], mean action: 2.075 [0.000, 10.000], mean observation: 33.818 [0.000, 598.900], loss: 121.029114, mae: 29.251797, mean_q: -29.443413\n",
            "  945705/10000000: episode: 4705, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -544.000, mean reward: -2.706 [-272.000, 11.800], mean action: 1.637 [0.000, 10.000], mean observation: 38.305 [0.002, 508.700], loss: 216.922363, mae: 29.728064, mean_q: -29.504726\n",
            "  945906/10000000: episode: 4706, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -209.400, mean reward: -1.042 [-104.700, 125.200], mean action: 1.627 [0.000, 10.000], mean observation: 34.312 [0.000, 792.800], loss: 191.214783, mae: 29.689720, mean_q: -29.387545\n",
            "  946107/10000000: episode: 4707, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -112.200, mean reward: -0.558 [-56.100, 106.000], mean action: 1.836 [0.000, 10.000], mean observation: 37.111 [0.000, 746.700], loss: 168.707581, mae: 29.585846, mean_q: -29.368759\n",
            "  946308/10000000: episode: 4708, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -424.800, mean reward: -2.113 [-212.400, 44.700], mean action: 2.119 [0.000, 10.000], mean observation: 31.475 [0.001, 422.100], loss: 253.765350, mae: 29.451208, mean_q: -29.524515\n",
            "  946509/10000000: episode: 4709, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -25.400, mean reward: -0.126 [-12.700, 298.800], mean action: 1.910 [0.000, 10.000], mean observation: 34.751 [0.000, 541.500], loss: 173.209305, mae: 29.761015, mean_q: -29.786219\n",
            "  946710/10000000: episode: 4710, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -454.400, mean reward: -2.261 [-227.200, 65.000], mean action: 2.264 [0.000, 10.000], mean observation: 31.471 [0.004, 519.300], loss: 136.294327, mae: 29.677530, mean_q: -29.814310\n",
            "  946911/10000000: episode: 4711, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -46.200, mean reward: -0.230 [-23.100, 89.600], mean action: 2.279 [0.000, 10.000], mean observation: 34.069 [0.001, 460.100], loss: 195.652252, mae: 29.713842, mean_q: -30.166300\n",
            "  947112/10000000: episode: 4712, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 610.200, mean reward: 3.036 [-10.000, 451.600], mean action: 2.721 [0.000, 10.000], mean observation: 30.503 [0.003, 566.800], loss: 106.564972, mae: 29.481176, mean_q: -30.149715\n",
            "  947313/10000000: episode: 4713, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: 280.800, mean reward: 1.397 [-10.000, 524.400], mean action: 2.761 [0.000, 10.000], mean observation: 36.070 [0.000, 579.300], loss: 281.753540, mae: 29.438438, mean_q: -29.855259\n",
            "  947514/10000000: episode: 4714, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -262.600, mean reward: -1.306 [-131.300, 115.200], mean action: 2.493 [0.000, 10.000], mean observation: 27.530 [0.001, 550.200], loss: 300.846558, mae: 29.101652, mean_q: -29.323044\n",
            "  947715/10000000: episode: 4715, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -857.600, mean reward: -4.267 [-428.800, 53.700], mean action: 3.045 [0.000, 10.000], mean observation: 31.441 [0.001, 548.600], loss: 276.087189, mae: 29.101164, mean_q: -29.773153\n",
            "  947916/10000000: episode: 4716, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -316.800, mean reward: -1.576 [-158.400, 56.000], mean action: 2.095 [0.000, 10.000], mean observation: 36.265 [0.002, 494.100], loss: 179.727493, mae: 29.685104, mean_q: -30.013336\n",
            "  948117/10000000: episode: 4717, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -466.400, mean reward: -2.320 [-233.200, 52.500], mean action: 2.035 [0.000, 10.000], mean observation: 33.750 [0.000, 525.700], loss: 214.217621, mae: 29.796570, mean_q: -29.760582\n",
            "  948318/10000000: episode: 4718, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -587.200, mean reward: -2.921 [-293.600, 22.800], mean action: 1.861 [0.000, 10.000], mean observation: 37.707 [0.000, 603.200], loss: 185.066177, mae: 29.361008, mean_q: -29.589764\n",
            "  948519/10000000: episode: 4719, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -130.600, mean reward: -0.650 [-65.300, 93.200], mean action: 2.204 [0.000, 10.000], mean observation: 34.501 [0.000, 803.400], loss: 219.635345, mae: 29.147150, mean_q: -29.347734\n",
            "  948720/10000000: episode: 4720, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -576.000, mean reward: -2.866 [-288.000, 42.400], mean action: 2.393 [0.000, 10.000], mean observation: 30.527 [0.001, 455.800], loss: 264.680145, mae: 29.286858, mean_q: -29.669237\n",
            "  948921/10000000: episode: 4721, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -341.800, mean reward: -1.700 [-170.900, 51.500], mean action: 2.139 [0.000, 10.000], mean observation: 35.337 [0.002, 525.700], loss: 113.494286, mae: 29.817602, mean_q: -30.248072\n",
            "  949122/10000000: episode: 4722, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -387.200, mean reward: -1.926 [-193.600, 66.000], mean action: 2.199 [0.000, 10.000], mean observation: 34.394 [0.000, 545.700], loss: 165.879623, mae: 30.458698, mean_q: -30.626465\n",
            "  949323/10000000: episode: 4723, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -415.400, mean reward: -2.067 [-207.700, 38.500], mean action: 1.761 [0.000, 10.000], mean observation: 35.126 [0.001, 622.400], loss: 192.942245, mae: 30.543726, mean_q: -30.737467\n",
            "  949524/10000000: episode: 4724, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -464.400, mean reward: -2.310 [-232.200, 28.700], mean action: 2.144 [0.000, 10.000], mean observation: 35.711 [0.000, 430.400], loss: 118.534561, mae: 30.756296, mean_q: -31.000183\n",
            "  949725/10000000: episode: 4725, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: 56.600, mean reward: 0.282 [-8.000, 97.500], mean action: 1.990 [0.000, 8.000], mean observation: 37.065 [0.000, 669.400], loss: 194.546219, mae: 30.901730, mean_q: -31.053434\n",
            "  949926/10000000: episode: 4726, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -499.200, mean reward: -2.484 [-249.600, 22.800], mean action: 1.721 [0.000, 10.000], mean observation: 33.184 [0.002, 511.500], loss: 216.621460, mae: 31.361336, mean_q: -31.288101\n",
            "  950127/10000000: episode: 4727, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -166.400, mean reward: -0.828 [-83.200, 42.000], mean action: 1.721 [0.000, 10.000], mean observation: 37.042 [0.000, 663.000], loss: 167.351761, mae: 30.875729, mean_q: -30.850275\n",
            "  950328/10000000: episode: 4728, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -696.600, mean reward: -3.466 [-348.300, 34.800], mean action: 2.438 [0.000, 10.000], mean observation: 37.766 [0.000, 515.300], loss: 207.589081, mae: 30.908630, mean_q: -31.437679\n",
            "  950529/10000000: episode: 4729, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -307.800, mean reward: -1.531 [-153.900, 57.300], mean action: 2.124 [0.000, 10.000], mean observation: 31.130 [0.001, 637.900], loss: 216.417450, mae: 30.936224, mean_q: -31.147596\n",
            "  950730/10000000: episode: 4730, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -523.000, mean reward: -2.602 [-261.500, 23.200], mean action: 1.920 [0.000, 10.000], mean observation: 31.647 [0.001, 539.300], loss: 135.965927, mae: 31.385857, mean_q: -31.537136\n",
            "  950931/10000000: episode: 4731, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -520.200, mean reward: -2.588 [-260.100, 53.700], mean action: 1.841 [0.000, 10.000], mean observation: 37.798 [0.001, 581.100], loss: 125.624458, mae: 31.391218, mean_q: -31.506039\n",
            "  951132/10000000: episode: 4732, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -656.400, mean reward: -3.266 [-328.200, 28.000], mean action: 2.229 [0.000, 10.000], mean observation: 33.785 [0.000, 622.800], loss: 198.364502, mae: 31.512365, mean_q: -31.830040\n",
            "  951333/10000000: episode: 4733, duration: 1.700s, episode steps: 201, steps per second: 118, episode reward: -653.600, mean reward: -3.252 [-326.800, 36.900], mean action: 2.388 [0.000, 10.000], mean observation: 33.473 [0.001, 570.400], loss: 167.594116, mae: 31.577347, mean_q: -32.091457\n",
            "  951534/10000000: episode: 4734, duration: 1.640s, episode steps: 201, steps per second: 123, episode reward: -355.800, mean reward: -1.770 [-177.900, 123.600], mean action: 1.816 [0.000, 10.000], mean observation: 34.436 [0.001, 424.400], loss: 119.165230, mae: 31.424139, mean_q: -31.930033\n",
            "  951735/10000000: episode: 4735, duration: 1.672s, episode steps: 201, steps per second: 120, episode reward: -503.800, mean reward: -2.506 [-251.900, 32.200], mean action: 1.876 [0.000, 10.000], mean observation: 33.080 [0.000, 749.600], loss: 152.355255, mae: 31.713959, mean_q: -31.934866\n",
            "  951936/10000000: episode: 4736, duration: 1.712s, episode steps: 201, steps per second: 117, episode reward: -461.800, mean reward: -2.298 [-230.900, 50.000], mean action: 2.358 [0.000, 10.000], mean observation: 29.150 [0.001, 587.600], loss: 126.653099, mae: 31.355774, mean_q: -31.884008\n",
            "  952137/10000000: episode: 4737, duration: 1.710s, episode steps: 201, steps per second: 118, episode reward: 1365.200, mean reward: 6.792 [-10.000, 1010.500], mean action: 2.269 [0.000, 10.000], mean observation: 32.621 [0.002, 516.600], loss: 180.256821, mae: 31.758381, mean_q: -32.172848\n",
            "  952338/10000000: episode: 4738, duration: 1.700s, episode steps: 201, steps per second: 118, episode reward: -674.600, mean reward: -3.356 [-337.300, 40.600], mean action: 2.512 [0.000, 10.000], mean observation: 28.391 [0.001, 512.300], loss: 244.423462, mae: 31.805450, mean_q: -32.360233\n",
            "  952539/10000000: episode: 4739, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -202.600, mean reward: -1.008 [-101.300, 68.300], mean action: 2.164 [0.000, 10.000], mean observation: 34.908 [0.003, 377.000], loss: 163.625977, mae: 31.452400, mean_q: -32.099503\n",
            "  952740/10000000: episode: 4740, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: -466.400, mean reward: -2.320 [-233.200, 36.300], mean action: 2.393 [0.000, 10.000], mean observation: 32.305 [0.000, 590.900], loss: 162.216354, mae: 31.511032, mean_q: -32.072971\n",
            "  952941/10000000: episode: 4741, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -510.200, mean reward: -2.538 [-255.100, 116.400], mean action: 2.781 [0.000, 10.000], mean observation: 33.574 [0.003, 566.300], loss: 224.911270, mae: 31.447649, mean_q: -32.066235\n",
            "  953142/10000000: episode: 4742, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -492.400, mean reward: -2.450 [-246.200, 95.000], mean action: 2.403 [0.000, 10.000], mean observation: 24.704 [0.000, 446.200], loss: 190.105347, mae: 31.859287, mean_q: -32.407074\n",
            "  953343/10000000: episode: 4743, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -216.000, mean reward: -1.075 [-108.000, 135.000], mean action: 2.353 [0.000, 10.000], mean observation: 36.645 [0.001, 504.300], loss: 258.198822, mae: 32.046894, mean_q: -32.583755\n",
            "  953544/10000000: episode: 4744, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 621.800, mean reward: 3.094 [-10.000, 310.900], mean action: 2.567 [0.000, 10.000], mean observation: 32.761 [0.001, 510.800], loss: 153.901901, mae: 31.881407, mean_q: -32.642212\n",
            "  953745/10000000: episode: 4745, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: 17.600, mean reward: 0.088 [-10.000, 114.900], mean action: 2.333 [0.000, 10.000], mean observation: 29.562 [0.002, 487.500], loss: 207.755585, mae: 31.979881, mean_q: -32.498184\n",
            "  953946/10000000: episode: 4746, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 33.600, mean reward: 0.167 [-10.000, 108.600], mean action: 2.294 [0.000, 10.000], mean observation: 35.939 [0.000, 637.000], loss: 206.622864, mae: 32.245930, mean_q: -32.968143\n",
            "  954147/10000000: episode: 4747, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 65.800, mean reward: 0.327 [-10.000, 259.500], mean action: 2.721 [0.000, 10.000], mean observation: 30.100 [0.000, 640.400], loss: 152.421310, mae: 32.321693, mean_q: -33.205219\n",
            "  954348/10000000: episode: 4748, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -184.000, mean reward: -0.915 [-92.000, 70.100], mean action: 2.617 [0.000, 10.000], mean observation: 28.110 [0.001, 418.000], loss: 129.837143, mae: 32.536297, mean_q: -33.542870\n",
            "  954549/10000000: episode: 4749, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -169.000, mean reward: -0.841 [-84.500, 45.900], mean action: 2.289 [0.000, 10.000], mean observation: 37.870 [0.002, 512.800], loss: 179.978470, mae: 32.652149, mean_q: -33.266842\n",
            "  954750/10000000: episode: 4750, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -553.600, mean reward: -2.754 [-276.800, 64.500], mean action: 3.060 [0.000, 10.000], mean observation: 30.604 [0.000, 558.800], loss: 150.347961, mae: 32.424225, mean_q: -33.481575\n",
            "  954951/10000000: episode: 4751, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: -407.200, mean reward: -2.026 [-203.600, 64.800], mean action: 2.149 [0.000, 10.000], mean observation: 39.970 [0.001, 584.600], loss: 136.220184, mae: 32.645905, mean_q: -33.395916\n",
            "  955152/10000000: episode: 4752, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -558.800, mean reward: -2.780 [-279.400, 109.600], mean action: 2.577 [0.000, 10.000], mean observation: 32.842 [0.000, 455.400], loss: 134.642609, mae: 32.631302, mean_q: -33.411526\n",
            "  955353/10000000: episode: 4753, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -663.200, mean reward: -3.300 [-331.600, 85.000], mean action: 2.726 [0.000, 10.000], mean observation: 34.660 [0.001, 510.200], loss: 158.013474, mae: 32.215538, mean_q: -33.035534\n",
            "  955554/10000000: episode: 4754, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -570.800, mean reward: -2.840 [-285.400, 73.200], mean action: 2.488 [0.000, 10.000], mean observation: 31.512 [0.000, 708.200], loss: 220.923859, mae: 32.155014, mean_q: -33.148872\n",
            "  955755/10000000: episode: 4755, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: 378.400, mean reward: 1.883 [-10.000, 212.600], mean action: 2.219 [0.000, 10.000], mean observation: 33.331 [0.002, 436.000], loss: 313.734192, mae: 32.297001, mean_q: -32.934158\n",
            "  955956/10000000: episode: 4756, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -348.800, mean reward: -1.735 [-174.400, 88.200], mean action: 2.532 [0.000, 10.000], mean observation: 37.772 [0.000, 500.200], loss: 266.893463, mae: 32.405167, mean_q: -33.110329\n",
            "  956157/10000000: episode: 4757, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: 244.800, mean reward: 1.218 [-10.000, 171.000], mean action: 2.274 [0.000, 10.000], mean observation: 35.538 [0.000, 425.200], loss: 201.071213, mae: 32.282986, mean_q: -33.056328\n",
            "  956358/10000000: episode: 4758, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -525.000, mean reward: -2.612 [-262.500, 60.800], mean action: 2.423 [0.000, 10.000], mean observation: 34.578 [0.000, 571.900], loss: 318.067474, mae: 32.363064, mean_q: -33.236301\n",
            "  956559/10000000: episode: 4759, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -450.200, mean reward: -2.240 [-225.100, 85.000], mean action: 2.647 [0.000, 10.000], mean observation: 30.911 [0.002, 494.100], loss: 228.488708, mae: 32.460831, mean_q: -33.268147\n",
            "  956760/10000000: episode: 4760, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -513.400, mean reward: -2.554 [-256.700, 44.400], mean action: 2.433 [0.000, 10.000], mean observation: 28.856 [0.000, 490.900], loss: 271.367157, mae: 32.350471, mean_q: -33.205624\n",
            "  956961/10000000: episode: 4761, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -312.800, mean reward: -1.556 [-156.400, 82.200], mean action: 2.274 [0.000, 10.000], mean observation: 29.368 [0.001, 457.300], loss: 188.577469, mae: 32.414669, mean_q: -33.152672\n",
            "  957162/10000000: episode: 4762, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -394.200, mean reward: -1.961 [-197.100, 91.200], mean action: 2.403 [0.000, 10.000], mean observation: 33.567 [0.000, 798.300], loss: 121.716415, mae: 32.417412, mean_q: -33.299019\n",
            "  957363/10000000: episode: 4763, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -606.400, mean reward: -3.017 [-303.200, 25.200], mean action: 2.264 [0.000, 10.000], mean observation: 33.190 [0.001, 556.200], loss: 230.641815, mae: 32.364830, mean_q: -33.102997\n",
            "  957564/10000000: episode: 4764, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -543.000, mean reward: -2.701 [-271.500, 36.000], mean action: 2.169 [0.000, 10.000], mean observation: 30.565 [0.000, 481.500], loss: 155.812546, mae: 32.803646, mean_q: -33.810902\n",
            "  957765/10000000: episode: 4765, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -170.400, mean reward: -0.848 [-85.200, 82.800], mean action: 1.925 [0.000, 10.000], mean observation: 34.839 [0.000, 668.900], loss: 184.136154, mae: 33.117825, mean_q: -33.940872\n",
            "  957966/10000000: episode: 4766, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -476.200, mean reward: -2.369 [-238.100, 50.600], mean action: 2.413 [0.000, 10.000], mean observation: 29.832 [0.000, 383.900], loss: 198.050934, mae: 33.434322, mean_q: -34.348320\n",
            "  958167/10000000: episode: 4767, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -555.400, mean reward: -2.763 [-277.700, 33.900], mean action: 2.398 [0.000, 10.000], mean observation: 35.325 [0.000, 605.900], loss: 162.188400, mae: 33.547382, mean_q: -34.396587\n",
            "  958368/10000000: episode: 4768, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -73.600, mean reward: -0.366 [-36.800, 197.100], mean action: 2.935 [0.000, 10.000], mean observation: 35.125 [0.000, 650.000], loss: 227.826843, mae: 33.209610, mean_q: -34.302792\n",
            "  958569/10000000: episode: 4769, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -53.200, mean reward: -0.265 [-26.600, 270.000], mean action: 2.612 [0.000, 10.000], mean observation: 32.617 [0.001, 574.000], loss: 160.356979, mae: 33.405258, mean_q: -34.295006\n",
            "  958770/10000000: episode: 4770, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -624.200, mean reward: -3.105 [-312.100, 56.700], mean action: 2.861 [0.000, 10.000], mean observation: 33.483 [0.002, 420.100], loss: 123.124123, mae: 33.247139, mean_q: -34.514530\n",
            "  958971/10000000: episode: 4771, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -428.000, mean reward: -2.129 [-214.000, 108.900], mean action: 2.527 [0.000, 9.000], mean observation: 41.496 [0.001, 510.400], loss: 157.190720, mae: 33.457294, mean_q: -34.368965\n",
            "  959172/10000000: episode: 4772, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -117.400, mean reward: -0.584 [-58.700, 187.700], mean action: 2.692 [0.000, 10.000], mean observation: 32.011 [0.002, 508.900], loss: 259.429230, mae: 33.123756, mean_q: -34.083996\n",
            "  959373/10000000: episode: 4773, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: 455.800, mean reward: 2.268 [-10.000, 675.000], mean action: 2.547 [0.000, 10.000], mean observation: 34.777 [0.000, 447.500], loss: 124.047661, mae: 33.562241, mean_q: -34.645256\n",
            "  959574/10000000: episode: 4774, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -620.600, mean reward: -3.088 [-310.300, 35.000], mean action: 2.507 [0.000, 10.000], mean observation: 27.263 [0.001, 618.400], loss: 236.930771, mae: 33.702873, mean_q: -34.749622\n",
            "  959775/10000000: episode: 4775, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: 785.200, mean reward: 3.906 [-10.000, 392.600], mean action: 2.821 [0.000, 10.000], mean observation: 33.230 [0.001, 519.700], loss: 126.667152, mae: 34.019833, mean_q: -35.312550\n",
            "  959976/10000000: episode: 4776, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -120.600, mean reward: -0.600 [-60.300, 138.000], mean action: 2.980 [0.000, 10.000], mean observation: 32.644 [0.000, 586.000], loss: 230.975204, mae: 34.088696, mean_q: -35.578510\n",
            "  960177/10000000: episode: 4777, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -470.200, mean reward: -2.339 [-235.100, 164.400], mean action: 2.925 [0.000, 10.000], mean observation: 32.889 [0.001, 471.000], loss: 155.534073, mae: 34.422134, mean_q: -35.745327\n",
            "  960378/10000000: episode: 4778, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 441.400, mean reward: 2.196 [-10.000, 305.000], mean action: 2.891 [0.000, 10.000], mean observation: 30.505 [0.001, 431.000], loss: 146.144669, mae: 34.266853, mean_q: -35.501682\n",
            "  960579/10000000: episode: 4779, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -643.000, mean reward: -3.199 [-321.500, 52.200], mean action: 2.672 [0.000, 10.000], mean observation: 30.464 [0.001, 459.200], loss: 218.492447, mae: 34.039200, mean_q: -35.138470\n",
            "  960780/10000000: episode: 4780, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -678.000, mean reward: -3.373 [-339.000, 114.400], mean action: 2.950 [0.000, 10.000], mean observation: 34.563 [0.001, 497.500], loss: 110.618401, mae: 33.837994, mean_q: -34.966862\n",
            "  960981/10000000: episode: 4781, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -641.600, mean reward: -3.192 [-320.800, 91.500], mean action: 3.244 [0.000, 10.000], mean observation: 31.628 [0.001, 439.900], loss: 161.611633, mae: 33.833645, mean_q: -35.183975\n",
            "  961182/10000000: episode: 4782, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -810.000, mean reward: -4.030 [-405.000, 62.400], mean action: 3.010 [0.000, 10.000], mean observation: 28.463 [0.002, 430.700], loss: 240.622742, mae: 33.997402, mean_q: -35.567226\n",
            "  961383/10000000: episode: 4783, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -607.600, mean reward: -3.023 [-303.800, 81.300], mean action: 3.403 [0.000, 10.000], mean observation: 36.289 [0.000, 552.400], loss: 253.819717, mae: 33.765556, mean_q: -35.312683\n",
            "  961584/10000000: episode: 4784, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -638.000, mean reward: -3.174 [-319.000, 49.200], mean action: 3.025 [0.000, 10.000], mean observation: 32.092 [0.000, 720.900], loss: 172.053101, mae: 34.156948, mean_q: -35.442814\n",
            "  961785/10000000: episode: 4785, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: 283.000, mean reward: 1.408 [-10.000, 141.500], mean action: 2.701 [0.000, 10.000], mean observation: 31.961 [0.001, 500.800], loss: 151.447174, mae: 33.938492, mean_q: -35.025253\n",
            "  961986/10000000: episode: 4786, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -133.600, mean reward: -0.665 [-66.800, 292.400], mean action: 2.856 [0.000, 10.000], mean observation: 33.518 [0.001, 441.700], loss: 173.128891, mae: 33.967327, mean_q: -35.141277\n",
            "  962187/10000000: episode: 4787, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: 1084.000, mean reward: 5.393 [-8.000, 542.000], mean action: 2.592 [0.000, 9.000], mean observation: 28.899 [0.000, 348.700], loss: 155.359283, mae: 34.570644, mean_q: -35.687080\n",
            "  962388/10000000: episode: 4788, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -746.000, mean reward: -3.711 [-373.000, 37.200], mean action: 2.970 [0.000, 10.000], mean observation: 34.051 [0.002, 537.100], loss: 151.154770, mae: 34.626434, mean_q: -35.846691\n",
            "  962589/10000000: episode: 4789, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -683.200, mean reward: -3.399 [-341.600, 50.400], mean action: 2.970 [0.000, 10.000], mean observation: 33.615 [0.002, 562.800], loss: 213.590958, mae: 35.017128, mean_q: -36.378841\n",
            "  962790/10000000: episode: 4790, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: 898.000, mean reward: 4.468 [-10.000, 652.800], mean action: 2.537 [0.000, 10.000], mean observation: 30.979 [0.001, 530.600], loss: 299.466217, mae: 35.143185, mean_q: -36.357479\n",
            "  962991/10000000: episode: 4791, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -406.600, mean reward: -2.023 [-203.300, 124.600], mean action: 2.826 [0.000, 10.000], mean observation: 37.932 [0.000, 693.300], loss: 185.741028, mae: 34.988724, mean_q: -36.219227\n",
            "  963192/10000000: episode: 4792, duration: 1.594s, episode steps: 201, steps per second: 126, episode reward: -475.000, mean reward: -2.363 [-237.500, 94.500], mean action: 2.512 [0.000, 10.000], mean observation: 34.783 [0.000, 727.600], loss: 149.568634, mae: 34.785259, mean_q: -35.848984\n",
            "  963393/10000000: episode: 4793, duration: 2.204s, episode steps: 201, steps per second: 91, episode reward: -224.600, mean reward: -1.117 [-112.300, 86.000], mean action: 2.124 [0.000, 9.000], mean observation: 35.061 [0.002, 479.600], loss: 184.611145, mae: 34.766079, mean_q: -35.477390\n",
            "  963594/10000000: episode: 4794, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: -218.600, mean reward: -1.088 [-109.300, 35.700], mean action: 1.960 [0.000, 7.000], mean observation: 33.347 [0.001, 606.200], loss: 295.329834, mae: 34.864792, mean_q: -35.410542\n",
            "  963795/10000000: episode: 4795, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -351.000, mean reward: -1.746 [-175.500, 133.400], mean action: 2.667 [0.000, 9.000], mean observation: 33.976 [0.002, 597.900], loss: 211.092972, mae: 34.527908, mean_q: -35.307869\n",
            "  963996/10000000: episode: 4796, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -360.000, mean reward: -1.791 [-180.000, 96.500], mean action: 2.632 [0.000, 9.000], mean observation: 30.770 [0.003, 482.900], loss: 101.847206, mae: 34.253025, mean_q: -34.979382\n",
            "  964197/10000000: episode: 4797, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -186.200, mean reward: -0.926 [-93.100, 72.600], mean action: 2.353 [0.000, 9.000], mean observation: 26.370 [0.002, 435.000], loss: 170.182907, mae: 34.279285, mean_q: -34.836685\n",
            "  964398/10000000: episode: 4798, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -568.000, mean reward: -2.826 [-284.000, 86.000], mean action: 2.945 [0.000, 10.000], mean observation: 33.524 [0.001, 697.100], loss: 215.981262, mae: 33.747852, mean_q: -34.491009\n",
            "  964599/10000000: episode: 4799, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -428.200, mean reward: -2.130 [-214.100, 82.600], mean action: 2.935 [0.000, 9.000], mean observation: 32.393 [0.001, 562.800], loss: 291.449402, mae: 33.878132, mean_q: -34.606747\n",
            "  964800/10000000: episode: 4800, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -849.800, mean reward: -4.228 [-424.900, 30.600], mean action: 2.498 [0.000, 8.000], mean observation: 33.881 [0.000, 458.900], loss: 158.810074, mae: 33.750290, mean_q: -34.342274\n",
            "  965001/10000000: episode: 4801, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -340.600, mean reward: -1.695 [-170.300, 52.400], mean action: 2.050 [0.000, 10.000], mean observation: 30.814 [0.001, 522.200], loss: 290.164978, mae: 33.896233, mean_q: -34.276943\n",
            "  965202/10000000: episode: 4802, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: 477.200, mean reward: 2.374 [-9.000, 238.600], mean action: 2.219 [0.000, 9.000], mean observation: 30.530 [0.000, 527.200], loss: 230.266403, mae: 33.413799, mean_q: -33.709927\n",
            "  965403/10000000: episode: 4803, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -863.200, mean reward: -4.295 [-431.600, 24.600], mean action: 2.468 [0.000, 8.000], mean observation: 34.955 [0.000, 424.800], loss: 203.630875, mae: 33.143589, mean_q: -33.467365\n",
            "  965604/10000000: episode: 4804, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -307.000, mean reward: -1.527 [-153.500, 79.800], mean action: 2.348 [0.000, 8.000], mean observation: 34.739 [0.000, 765.000], loss: 230.404266, mae: 32.884583, mean_q: -33.058449\n",
            "  965805/10000000: episode: 4805, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -282.400, mean reward: -1.405 [-141.200, 145.800], mean action: 2.463 [0.000, 10.000], mean observation: 31.259 [0.000, 638.500], loss: 229.576859, mae: 33.009640, mean_q: -32.989182\n",
            "  966006/10000000: episode: 4806, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: -635.400, mean reward: -3.161 [-317.700, 30.000], mean action: 2.418 [0.000, 9.000], mean observation: 35.308 [0.001, 534.900], loss: 284.107880, mae: 32.932880, mean_q: -32.997391\n",
            "  966207/10000000: episode: 4807, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: 2.000, mean reward: 0.010 [-9.000, 87.200], mean action: 2.303 [0.000, 9.000], mean observation: 37.286 [0.000, 793.800], loss: 141.131027, mae: 33.217724, mean_q: -33.396809\n",
            "  966408/10000000: episode: 4808, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -401.800, mean reward: -1.999 [-200.900, 114.100], mean action: 2.905 [0.000, 10.000], mean observation: 35.792 [0.001, 681.600], loss: 223.535294, mae: 33.045223, mean_q: -33.218147\n",
            "  966609/10000000: episode: 4809, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -903.200, mean reward: -4.494 [-451.600, 37.800], mean action: 3.000 [0.000, 10.000], mean observation: 39.862 [0.000, 701.500], loss: 179.181885, mae: 32.821613, mean_q: -32.915749\n",
            "  966810/10000000: episode: 4810, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: 296.200, mean reward: 1.474 [-7.000, 314.100], mean action: 2.532 [0.000, 7.000], mean observation: 34.459 [0.000, 783.800], loss: 169.742325, mae: 32.493488, mean_q: -32.462170\n",
            "  967011/10000000: episode: 4811, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 161.200, mean reward: 0.802 [-7.000, 124.200], mean action: 2.766 [0.000, 8.000], mean observation: 36.254 [0.000, 642.000], loss: 171.890060, mae: 32.244675, mean_q: -32.393452\n",
            "  967212/10000000: episode: 4812, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -384.200, mean reward: -1.911 [-192.100, 79.800], mean action: 2.517 [0.000, 9.000], mean observation: 34.203 [0.002, 515.200], loss: 391.727020, mae: 31.939432, mean_q: -32.170135\n",
            "  967413/10000000: episode: 4813, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -353.000, mean reward: -1.756 [-176.500, 80.400], mean action: 2.463 [0.000, 7.000], mean observation: 37.255 [0.001, 430.300], loss: 274.544922, mae: 31.714140, mean_q: -31.968969\n",
            "  967614/10000000: episode: 4814, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -301.200, mean reward: -1.499 [-150.600, 140.500], mean action: 2.294 [0.000, 8.000], mean observation: 36.998 [0.000, 673.700], loss: 177.624191, mae: 31.515339, mean_q: -31.832396\n",
            "  967815/10000000: episode: 4815, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -385.600, mean reward: -1.918 [-192.800, 34.000], mean action: 2.055 [0.000, 9.000], mean observation: 29.750 [0.001, 446.200], loss: 174.384567, mae: 31.852859, mean_q: -31.995262\n",
            "  968016/10000000: episode: 4816, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -139.600, mean reward: -0.695 [-69.800, 188.000], mean action: 2.095 [0.000, 7.000], mean observation: 40.754 [0.001, 558.800], loss: 138.764725, mae: 31.815788, mean_q: -31.887423\n",
            "  968217/10000000: episode: 4817, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -630.000, mean reward: -3.134 [-315.000, 54.800], mean action: 2.393 [0.000, 9.000], mean observation: 32.752 [0.002, 533.200], loss: 282.576172, mae: 31.263901, mean_q: -31.601164\n",
            "  968418/10000000: episode: 4818, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -376.800, mean reward: -1.875 [-188.400, 68.000], mean action: 2.154 [0.000, 9.000], mean observation: 32.927 [0.000, 933.200], loss: 181.420319, mae: 31.765280, mean_q: -31.840000\n",
            "  968619/10000000: episode: 4819, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 62.600, mean reward: 0.311 [-10.000, 99.500], mean action: 3.010 [0.000, 10.000], mean observation: 34.019 [0.000, 684.900], loss: 320.193787, mae: 30.924528, mean_q: -31.514944\n",
            "  968820/10000000: episode: 4820, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -544.000, mean reward: -2.706 [-272.000, 129.600], mean action: 2.950 [0.000, 10.000], mean observation: 29.121 [0.004, 468.600], loss: 168.176468, mae: 31.099983, mean_q: -31.708902\n",
            "  969021/10000000: episode: 4821, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: 59.400, mean reward: 0.296 [-8.000, 333.900], mean action: 2.577 [0.000, 8.000], mean observation: 35.793 [0.000, 556.700], loss: 208.417953, mae: 30.813807, mean_q: -30.992586\n",
            "  969222/10000000: episode: 4822, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -257.800, mean reward: -1.283 [-128.900, 300.000], mean action: 2.816 [0.000, 8.000], mean observation: 36.101 [0.000, 607.900], loss: 197.564758, mae: 31.140198, mean_q: -31.303526\n",
            "  969423/10000000: episode: 4823, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -218.000, mean reward: -1.085 [-109.000, 84.300], mean action: 2.075 [0.000, 8.000], mean observation: 35.977 [0.001, 423.500], loss: 136.875076, mae: 31.472961, mean_q: -31.590679\n",
            "  969624/10000000: episode: 4824, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -490.200, mean reward: -2.439 [-245.100, 68.500], mean action: 2.274 [0.000, 10.000], mean observation: 30.962 [0.000, 528.400], loss: 176.920578, mae: 32.020771, mean_q: -32.273724\n",
            "  969825/10000000: episode: 4825, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: 84.000, mean reward: 0.418 [-8.000, 332.500], mean action: 2.184 [0.000, 8.000], mean observation: 29.817 [0.003, 464.200], loss: 176.259140, mae: 32.497688, mean_q: -32.623611\n",
            "  970026/10000000: episode: 4826, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -36.000, mean reward: -0.179 [-18.000, 70.800], mean action: 2.139 [0.000, 10.000], mean observation: 36.927 [0.000, 818.500], loss: 274.182495, mae: 32.628086, mean_q: -32.581070\n",
            "  970227/10000000: episode: 4827, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -745.600, mean reward: -3.709 [-372.800, 12.000], mean action: 2.085 [0.000, 8.000], mean observation: 26.727 [0.001, 619.000], loss: 203.389725, mae: 32.328075, mean_q: -32.635120\n",
            "  970428/10000000: episode: 4828, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: 556.600, mean reward: 2.769 [-8.000, 345.000], mean action: 2.428 [0.000, 8.000], mean observation: 33.799 [0.001, 461.700], loss: 153.191391, mae: 32.501293, mean_q: -32.885521\n",
            "  970629/10000000: episode: 4829, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -309.000, mean reward: -1.537 [-154.500, 125.800], mean action: 2.393 [0.000, 8.000], mean observation: 36.083 [0.002, 402.500], loss: 198.091324, mae: 32.131786, mean_q: -32.517193\n",
            "  970830/10000000: episode: 4830, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -486.800, mean reward: -2.422 [-243.400, 61.500], mean action: 2.289 [0.000, 8.000], mean observation: 33.674 [0.000, 501.600], loss: 164.815033, mae: 32.146545, mean_q: -32.549557\n",
            "  971031/10000000: episode: 4831, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -86.400, mean reward: -0.430 [-43.200, 81.000], mean action: 2.274 [0.000, 10.000], mean observation: 37.237 [0.001, 492.600], loss: 154.212128, mae: 31.879025, mean_q: -32.240910\n",
            "  971232/10000000: episode: 4832, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: 266.200, mean reward: 1.324 [-10.000, 295.200], mean action: 2.259 [0.000, 10.000], mean observation: 31.260 [0.000, 590.900], loss: 213.423065, mae: 32.343815, mean_q: -32.666748\n",
            "  971433/10000000: episode: 4833, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -20.000, mean reward: -0.100 [-10.000, 296.700], mean action: 2.697 [0.000, 10.000], mean observation: 33.662 [0.000, 747.100], loss: 226.163162, mae: 32.469265, mean_q: -33.033016\n",
            "  971634/10000000: episode: 4834, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -631.800, mean reward: -3.143 [-315.900, 33.800], mean action: 2.597 [0.000, 8.000], mean observation: 35.716 [0.000, 462.000], loss: 354.863037, mae: 31.977762, mean_q: -32.439198\n",
            "  971835/10000000: episode: 4835, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -595.400, mean reward: -2.962 [-297.700, 44.000], mean action: 3.139 [0.000, 10.000], mean observation: 25.547 [0.001, 534.800], loss: 207.187012, mae: 32.158669, mean_q: -32.920193\n",
            "  972036/10000000: episode: 4836, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -465.800, mean reward: -2.317 [-232.900, 137.500], mean action: 2.781 [0.000, 9.000], mean observation: 30.378 [0.000, 580.300], loss: 289.482788, mae: 32.390194, mean_q: -33.137608\n",
            "  972237/10000000: episode: 4837, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -811.600, mean reward: -4.038 [-405.800, 34.200], mean action: 3.085 [0.000, 10.000], mean observation: 36.605 [0.000, 522.100], loss: 239.057755, mae: 32.588421, mean_q: -33.219727\n",
            "  972438/10000000: episode: 4838, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: -21.000, mean reward: -0.104 [-10.500, 110.400], mean action: 2.900 [0.000, 8.000], mean observation: 27.323 [0.001, 590.600], loss: 181.722168, mae: 32.576008, mean_q: -33.211369\n",
            "  972639/10000000: episode: 4839, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -678.400, mean reward: -3.375 [-339.200, 45.600], mean action: 2.682 [0.000, 8.000], mean observation: 35.074 [0.000, 813.800], loss: 257.701233, mae: 32.804329, mean_q: -33.437550\n",
            "  972840/10000000: episode: 4840, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -769.200, mean reward: -3.827 [-384.600, 41.000], mean action: 2.607 [0.000, 8.000], mean observation: 39.934 [0.000, 632.400], loss: 213.805634, mae: 32.511681, mean_q: -32.793304\n",
            "  973041/10000000: episode: 4841, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -1075.400, mean reward: -5.350 [-537.700, 21.600], mean action: 3.154 [0.000, 9.000], mean observation: 29.834 [0.003, 531.400], loss: 277.949432, mae: 32.625389, mean_q: -33.001904\n",
            "  973242/10000000: episode: 4842, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -466.600, mean reward: -2.321 [-233.300, 60.000], mean action: 2.697 [0.000, 7.000], mean observation: 35.778 [0.002, 519.900], loss: 158.974243, mae: 32.289455, mean_q: -32.804131\n",
            "  973443/10000000: episode: 4843, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -530.800, mean reward: -2.641 [-265.400, 34.300], mean action: 2.139 [0.000, 10.000], mean observation: 37.268 [0.001, 518.800], loss: 307.090149, mae: 32.683575, mean_q: -32.862453\n",
            "  973644/10000000: episode: 4844, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -395.800, mean reward: -1.969 [-197.900, 79.200], mean action: 2.408 [0.000, 8.000], mean observation: 38.525 [0.001, 554.400], loss: 158.492203, mae: 32.617771, mean_q: -33.057320\n",
            "  973845/10000000: episode: 4845, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: 609.800, mean reward: 3.034 [-8.000, 304.900], mean action: 2.368 [0.000, 8.000], mean observation: 38.247 [0.001, 627.200], loss: 159.829483, mae: 32.735214, mean_q: -33.254253\n",
            "  974046/10000000: episode: 4846, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: 689.000, mean reward: 3.428 [-7.000, 344.500], mean action: 2.040 [0.000, 7.000], mean observation: 35.321 [0.000, 604.500], loss: 263.057983, mae: 33.200188, mean_q: -33.454948\n",
            "  974247/10000000: episode: 4847, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: -385.400, mean reward: -1.917 [-192.700, 33.200], mean action: 1.980 [0.000, 8.000], mean observation: 32.615 [0.001, 500.600], loss: 176.107666, mae: 33.193275, mean_q: -33.422001\n",
            "  974448/10000000: episode: 4848, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -394.000, mean reward: -1.960 [-197.000, 78.000], mean action: 2.149 [0.000, 8.000], mean observation: 29.390 [0.001, 464.600], loss: 407.977814, mae: 32.635300, mean_q: -32.700352\n",
            "  974649/10000000: episode: 4849, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -357.000, mean reward: -1.776 [-178.500, 70.800], mean action: 2.532 [0.000, 10.000], mean observation: 30.604 [0.004, 531.100], loss: 193.884155, mae: 32.671947, mean_q: -33.097324\n",
            "  974850/10000000: episode: 4850, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -197.400, mean reward: -0.982 [-98.700, 144.000], mean action: 2.353 [0.000, 7.000], mean observation: 35.734 [0.001, 507.400], loss: 252.876480, mae: 33.066833, mean_q: -33.493584\n",
            "  975051/10000000: episode: 4851, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -514.800, mean reward: -2.561 [-257.400, 66.000], mean action: 2.488 [0.000, 9.000], mean observation: 34.119 [0.000, 784.200], loss: 359.605499, mae: 33.059563, mean_q: -33.308559\n",
            "  975252/10000000: episode: 4852, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -638.600, mean reward: -3.177 [-319.300, 54.800], mean action: 2.224 [0.000, 8.000], mean observation: 36.534 [0.000, 722.600], loss: 164.495804, mae: 33.223679, mean_q: -33.466930\n",
            "  975453/10000000: episode: 4853, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -608.400, mean reward: -3.027 [-304.200, 67.300], mean action: 2.383 [0.000, 10.000], mean observation: 32.665 [0.001, 542.200], loss: 150.765610, mae: 32.845211, mean_q: -33.218094\n",
            "  975654/10000000: episode: 4854, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: 789.800, mean reward: 3.929 [-8.000, 525.300], mean action: 3.060 [0.000, 8.000], mean observation: 36.204 [0.001, 448.100], loss: 344.641937, mae: 32.312054, mean_q: -32.662655\n",
            "  975855/10000000: episode: 4855, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -801.200, mean reward: -3.986 [-400.600, 51.300], mean action: 2.791 [0.000, 7.000], mean observation: 29.500 [0.000, 527.300], loss: 219.932831, mae: 32.163582, mean_q: -32.669987\n",
            "  976056/10000000: episode: 4856, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -774.800, mean reward: -3.855 [-387.400, 30.000], mean action: 3.100 [0.000, 9.000], mean observation: 32.348 [0.003, 542.400], loss: 245.545624, mae: 32.373451, mean_q: -33.119221\n",
            "  976257/10000000: episode: 4857, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -609.200, mean reward: -3.031 [-304.600, 77.000], mean action: 2.577 [0.000, 10.000], mean observation: 39.084 [0.000, 694.400], loss: 172.502533, mae: 32.433506, mean_q: -33.115520\n",
            "  976458/10000000: episode: 4858, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -657.000, mean reward: -3.269 [-328.500, 69.800], mean action: 2.368 [0.000, 9.000], mean observation: 34.709 [0.000, 598.300], loss: 225.374954, mae: 32.877571, mean_q: -33.506481\n",
            "  976659/10000000: episode: 4859, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -580.600, mean reward: -2.889 [-290.300, 25.500], mean action: 2.348 [0.000, 9.000], mean observation: 29.155 [0.004, 555.700], loss: 290.214722, mae: 32.924538, mean_q: -33.698921\n",
            "  976860/10000000: episode: 4860, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -451.800, mean reward: -2.248 [-225.900, 57.600], mean action: 2.701 [0.000, 10.000], mean observation: 35.516 [0.000, 669.900], loss: 158.331161, mae: 33.248104, mean_q: -34.228954\n",
            "  977061/10000000: episode: 4861, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -541.600, mean reward: -2.695 [-270.800, 40.400], mean action: 2.154 [0.000, 8.000], mean observation: 39.870 [0.001, 662.900], loss: 383.566315, mae: 33.205082, mean_q: -33.779205\n",
            "  977262/10000000: episode: 4862, duration: 1.669s, episode steps: 201, steps per second: 120, episode reward: 163.600, mean reward: 0.814 [-10.000, 131.400], mean action: 2.378 [0.000, 10.000], mean observation: 35.170 [0.002, 482.600], loss: 150.034607, mae: 33.358719, mean_q: -34.036362\n",
            "  977463/10000000: episode: 4863, duration: 1.680s, episode steps: 201, steps per second: 120, episode reward: 1674.000, mean reward: 8.328 [-10.000, 883.200], mean action: 2.363 [0.000, 10.000], mean observation: 34.237 [0.001, 419.000], loss: 222.831757, mae: 33.501175, mean_q: -34.226418\n",
            "  977664/10000000: episode: 4864, duration: 1.686s, episode steps: 201, steps per second: 119, episode reward: -299.000, mean reward: -1.488 [-149.500, 72.400], mean action: 2.189 [0.000, 10.000], mean observation: 36.991 [0.000, 654.700], loss: 228.913483, mae: 33.675659, mean_q: -34.248352\n",
            "  977865/10000000: episode: 4865, duration: 1.652s, episode steps: 201, steps per second: 122, episode reward: -553.400, mean reward: -2.753 [-276.700, 40.600], mean action: 2.299 [0.000, 10.000], mean observation: 33.413 [0.000, 706.400], loss: 309.660614, mae: 33.493309, mean_q: -34.032799\n",
            "  978066/10000000: episode: 4866, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -483.200, mean reward: -2.404 [-241.600, 36.600], mean action: 2.299 [0.000, 10.000], mean observation: 32.237 [0.001, 434.500], loss: 325.902527, mae: 33.813431, mean_q: -34.479225\n",
            "  978267/10000000: episode: 4867, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 502.200, mean reward: 2.499 [-10.000, 490.800], mean action: 2.682 [0.000, 10.000], mean observation: 31.864 [0.001, 568.100], loss: 165.240082, mae: 33.870609, mean_q: -34.637417\n",
            "  978468/10000000: episode: 4868, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -493.200, mean reward: -2.454 [-246.600, 126.600], mean action: 2.891 [0.000, 10.000], mean observation: 30.887 [0.001, 676.900], loss: 339.150696, mae: 33.872108, mean_q: -34.722172\n",
            "  978669/10000000: episode: 4869, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -663.600, mean reward: -3.301 [-331.800, 43.800], mean action: 2.642 [0.000, 8.000], mean observation: 37.737 [0.000, 714.300], loss: 188.240173, mae: 33.981678, mean_q: -34.876064\n",
            "  978870/10000000: episode: 4870, duration: 1.483s, episode steps: 201, steps per second: 135, episode reward: -626.600, mean reward: -3.117 [-313.300, 56.000], mean action: 2.378 [0.000, 10.000], mean observation: 36.312 [0.000, 601.500], loss: 315.305206, mae: 34.058769, mean_q: -34.993343\n",
            "  979071/10000000: episode: 4871, duration: 1.549s, episode steps: 201, steps per second: 130, episode reward: 178.800, mean reward: 0.890 [-8.000, 165.500], mean action: 2.313 [0.000, 8.000], mean observation: 34.212 [0.000, 582.800], loss: 216.689392, mae: 33.719917, mean_q: -34.568600\n",
            "  979272/10000000: episode: 4872, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -414.600, mean reward: -2.063 [-207.300, 44.100], mean action: 2.393 [0.000, 8.000], mean observation: 34.084 [0.001, 498.800], loss: 252.456482, mae: 33.649490, mean_q: -34.270981\n",
            "  979473/10000000: episode: 4873, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -186.400, mean reward: -0.927 [-93.200, 108.500], mean action: 2.642 [0.000, 10.000], mean observation: 28.519 [0.002, 508.500], loss: 168.920441, mae: 33.518116, mean_q: -34.018570\n",
            "  979674/10000000: episode: 4874, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -109.200, mean reward: -0.543 [-54.600, 114.600], mean action: 2.229 [0.000, 10.000], mean observation: 31.200 [0.000, 413.800], loss: 144.985291, mae: 33.524525, mean_q: -34.006325\n",
            "  979875/10000000: episode: 4875, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -391.800, mean reward: -1.949 [-195.900, 93.100], mean action: 1.886 [0.000, 8.000], mean observation: 34.869 [0.000, 367.900], loss: 238.608170, mae: 33.954689, mean_q: -34.485863\n",
            "  980076/10000000: episode: 4876, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -528.800, mean reward: -2.631 [-264.400, 60.100], mean action: 1.960 [0.000, 7.000], mean observation: 37.059 [0.001, 630.500], loss: 248.093033, mae: 34.105362, mean_q: -34.409569\n",
            "  980277/10000000: episode: 4877, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: 951.800, mean reward: 4.735 [-7.000, 660.500], mean action: 2.085 [0.000, 7.000], mean observation: 31.485 [0.001, 518.600], loss: 152.316132, mae: 34.032825, mean_q: -34.503777\n",
            "  980478/10000000: episode: 4878, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: 902.400, mean reward: 4.490 [-7.000, 666.600], mean action: 2.050 [0.000, 7.000], mean observation: 35.325 [0.000, 562.200], loss: 328.770172, mae: 34.381298, mean_q: -34.835831\n",
            "  980679/10000000: episode: 4879, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: 304.400, mean reward: 1.514 [-8.000, 336.500], mean action: 2.701 [0.000, 10.000], mean observation: 33.860 [0.002, 637.200], loss: 264.427185, mae: 34.304558, mean_q: -34.845867\n",
            "  980880/10000000: episode: 4880, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -484.600, mean reward: -2.411 [-242.300, 83.400], mean action: 2.876 [0.000, 7.000], mean observation: 26.938 [0.005, 368.500], loss: 212.301346, mae: 34.442242, mean_q: -34.771816\n",
            "  981081/10000000: episode: 4881, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -107.600, mean reward: -0.535 [-53.800, 82.500], mean action: 2.493 [0.000, 7.000], mean observation: 32.621 [0.002, 607.700], loss: 179.753738, mae: 34.810429, mean_q: -35.331291\n",
            "  981282/10000000: episode: 4882, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -624.400, mean reward: -3.106 [-312.200, 23.100], mean action: 2.537 [0.000, 10.000], mean observation: 26.328 [0.001, 497.800], loss: 214.540619, mae: 35.207951, mean_q: -35.819805\n",
            "  981483/10000000: episode: 4883, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -629.800, mean reward: -3.133 [-314.900, 47.600], mean action: 2.378 [0.000, 7.000], mean observation: 39.654 [0.001, 462.800], loss: 229.138443, mae: 35.068211, mean_q: -35.733955\n",
            "  981684/10000000: episode: 4884, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -520.000, mean reward: -2.587 [-260.000, 36.000], mean action: 2.736 [0.000, 10.000], mean observation: 34.009 [0.000, 772.000], loss: 267.283844, mae: 35.058174, mean_q: -35.938194\n",
            "  981885/10000000: episode: 4885, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -192.800, mean reward: -0.959 [-96.400, 152.400], mean action: 2.582 [0.000, 10.000], mean observation: 30.768 [0.001, 558.800], loss: 271.253815, mae: 35.156963, mean_q: -36.088654\n",
            "  982086/10000000: episode: 4886, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -86.200, mean reward: -0.429 [-43.100, 165.000], mean action: 2.498 [0.000, 8.000], mean observation: 30.972 [0.000, 501.100], loss: 268.542206, mae: 34.959435, mean_q: -35.732742\n",
            "  982287/10000000: episode: 4887, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: 332.400, mean reward: 1.654 [-10.000, 236.800], mean action: 2.527 [0.000, 10.000], mean observation: 36.278 [0.001, 645.600], loss: 241.332794, mae: 35.162979, mean_q: -36.051140\n",
            "  982488/10000000: episode: 4888, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -720.600, mean reward: -3.585 [-360.300, 22.800], mean action: 2.716 [0.000, 7.000], mean observation: 30.930 [0.001, 487.300], loss: 384.505005, mae: 35.049103, mean_q: -35.832848\n",
            "  982689/10000000: episode: 4889, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 123.800, mean reward: 0.616 [-10.000, 70.400], mean action: 2.592 [0.000, 10.000], mean observation: 35.318 [0.000, 555.400], loss: 135.372116, mae: 35.434185, mean_q: -36.536873\n",
            "  982890/10000000: episode: 4890, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -590.200, mean reward: -2.936 [-295.100, 68.800], mean action: 2.647 [0.000, 10.000], mean observation: 33.552 [0.000, 553.100], loss: 188.574173, mae: 35.862209, mean_q: -36.981995\n",
            "  983091/10000000: episode: 4891, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -488.400, mean reward: -2.430 [-244.200, 163.000], mean action: 2.995 [0.000, 10.000], mean observation: 36.778 [0.000, 668.100], loss: 233.898361, mae: 36.007687, mean_q: -37.139351\n",
            "  983292/10000000: episode: 4892, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -877.600, mean reward: -4.366 [-438.800, 10.800], mean action: 2.607 [0.000, 8.000], mean observation: 32.415 [0.000, 483.800], loss: 269.530396, mae: 35.864048, mean_q: -37.007900\n",
            "  983493/10000000: episode: 4893, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: -133.200, mean reward: -0.663 [-66.600, 130.800], mean action: 2.632 [0.000, 10.000], mean observation: 27.517 [0.002, 513.300], loss: 259.883972, mae: 35.972893, mean_q: -37.194244\n",
            "  983694/10000000: episode: 4894, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -414.000, mean reward: -2.060 [-207.000, 94.800], mean action: 2.871 [0.000, 8.000], mean observation: 33.764 [0.000, 606.600], loss: 219.439423, mae: 36.658875, mean_q: -38.038074\n",
            "  983895/10000000: episode: 4895, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -577.800, mean reward: -2.875 [-288.900, 185.100], mean action: 2.945 [0.000, 10.000], mean observation: 36.344 [0.001, 493.400], loss: 237.685822, mae: 37.163456, mean_q: -38.500191\n",
            "  984096/10000000: episode: 4896, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -729.000, mean reward: -3.627 [-364.500, 20.400], mean action: 2.517 [0.000, 10.000], mean observation: 33.608 [0.002, 578.200], loss: 272.301605, mae: 37.768059, mean_q: -39.011406\n",
            "  984297/10000000: episode: 4897, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -816.600, mean reward: -4.063 [-408.300, 28.700], mean action: 2.383 [0.000, 10.000], mean observation: 31.990 [0.001, 461.100], loss: 239.699203, mae: 37.999950, mean_q: -39.035641\n",
            "  984498/10000000: episode: 4898, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -502.600, mean reward: -2.500 [-251.300, 32.100], mean action: 2.139 [0.000, 8.000], mean observation: 40.698 [0.000, 527.200], loss: 227.208801, mae: 38.133694, mean_q: -39.126865\n",
            "  984699/10000000: episode: 4899, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -682.800, mean reward: -3.397 [-341.400, 31.600], mean action: 2.657 [0.000, 10.000], mean observation: 30.827 [0.001, 675.300], loss: 349.536377, mae: 38.153091, mean_q: -39.279556\n",
            "  984900/10000000: episode: 4900, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -302.800, mean reward: -1.506 [-151.400, 118.500], mean action: 2.383 [0.000, 8.000], mean observation: 31.066 [0.001, 472.800], loss: 167.118942, mae: 38.389725, mean_q: -39.648640\n",
            "  985101/10000000: episode: 4901, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -295.200, mean reward: -1.469 [-147.600, 78.400], mean action: 2.139 [0.000, 10.000], mean observation: 32.758 [0.000, 527.500], loss: 251.990555, mae: 38.377552, mean_q: -39.325844\n",
            "  985302/10000000: episode: 4902, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -129.200, mean reward: -0.643 [-64.600, 74.400], mean action: 2.124 [0.000, 9.000], mean observation: 30.605 [0.000, 544.700], loss: 190.704605, mae: 38.353161, mean_q: -39.234558\n",
            "  985503/10000000: episode: 4903, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -424.600, mean reward: -2.112 [-212.300, 56.000], mean action: 2.433 [0.000, 8.000], mean observation: 39.290 [0.001, 532.900], loss: 227.996429, mae: 38.011166, mean_q: -38.821266\n",
            "  985704/10000000: episode: 4904, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -40.400, mean reward: -0.201 [-20.200, 129.600], mean action: 2.577 [0.000, 8.000], mean observation: 34.075 [0.002, 510.800], loss: 179.953934, mae: 37.901024, mean_q: -38.722584\n",
            "  985905/10000000: episode: 4905, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -785.600, mean reward: -3.908 [-392.800, 27.200], mean action: 2.378 [0.000, 8.000], mean observation: 32.216 [0.001, 647.400], loss: 252.149292, mae: 38.033546, mean_q: -38.584835\n",
            "  986106/10000000: episode: 4906, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -558.000, mean reward: -2.776 [-279.000, 45.000], mean action: 2.159 [0.000, 8.000], mean observation: 32.225 [0.000, 602.400], loss: 263.139801, mae: 37.727692, mean_q: -38.476154\n",
            "  986307/10000000: episode: 4907, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -464.600, mean reward: -2.311 [-232.300, 78.000], mean action: 2.308 [0.000, 8.000], mean observation: 32.767 [0.002, 364.300], loss: 343.753174, mae: 37.331505, mean_q: -38.118984\n",
            "  986508/10000000: episode: 4908, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: 797.200, mean reward: 3.966 [-8.000, 446.000], mean action: 2.144 [0.000, 8.000], mean observation: 28.029 [0.001, 458.100], loss: 323.881287, mae: 37.179920, mean_q: -37.531013\n",
            "  986709/10000000: episode: 4909, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: 159.000, mean reward: 0.791 [-10.000, 133.500], mean action: 2.040 [0.000, 10.000], mean observation: 32.404 [0.000, 695.400], loss: 193.144699, mae: 36.853947, mean_q: -36.997761\n",
            "  986910/10000000: episode: 4910, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -4.200, mean reward: -0.021 [-10.000, 164.500], mean action: 2.164 [0.000, 10.000], mean observation: 37.670 [0.000, 660.900], loss: 271.946075, mae: 36.642353, mean_q: -36.798534\n",
            "  987111/10000000: episode: 4911, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 690.600, mean reward: 3.436 [-8.000, 345.300], mean action: 2.552 [0.000, 8.000], mean observation: 31.764 [0.001, 400.300], loss: 264.376190, mae: 36.601746, mean_q: -36.959156\n",
            "  987312/10000000: episode: 4912, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -480.000, mean reward: -2.388 [-240.000, 95.800], mean action: 2.502 [0.000, 8.000], mean observation: 33.767 [0.000, 446.900], loss: 312.176941, mae: 36.483360, mean_q: -36.833115\n",
            "  987513/10000000: episode: 4913, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -355.000, mean reward: -1.766 [-177.500, 199.500], mean action: 2.493 [0.000, 8.000], mean observation: 32.997 [0.001, 654.600], loss: 234.545593, mae: 36.689682, mean_q: -37.197712\n",
            "  987714/10000000: episode: 4914, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: 1060.200, mean reward: 5.275 [-8.000, 530.100], mean action: 2.756 [0.000, 8.000], mean observation: 32.262 [0.000, 467.400], loss: 218.541779, mae: 36.286625, mean_q: -36.892155\n",
            "  987915/10000000: episode: 4915, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -681.600, mean reward: -3.391 [-340.800, 57.600], mean action: 3.234 [0.000, 8.000], mean observation: 35.658 [0.000, 420.600], loss: 266.601105, mae: 36.163418, mean_q: -37.112198\n",
            "  988116/10000000: episode: 4916, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -157.800, mean reward: -0.785 [-78.900, 93.300], mean action: 2.587 [0.000, 10.000], mean observation: 32.967 [0.000, 697.600], loss: 275.101807, mae: 36.490997, mean_q: -37.397427\n",
            "  988317/10000000: episode: 4917, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -820.200, mean reward: -4.081 [-410.100, 24.400], mean action: 2.512 [0.000, 8.000], mean observation: 31.887 [0.000, 443.500], loss: 318.340179, mae: 36.576538, mean_q: -37.324959\n",
            "  988518/10000000: episode: 4918, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -527.600, mean reward: -2.625 [-263.800, 107.500], mean action: 2.632 [0.000, 8.000], mean observation: 33.882 [0.001, 470.800], loss: 351.533478, mae: 36.140518, mean_q: -36.738190\n",
            "  988719/10000000: episode: 4919, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -587.400, mean reward: -2.922 [-293.700, 72.200], mean action: 2.557 [0.000, 8.000], mean observation: 27.896 [0.002, 400.000], loss: 283.487732, mae: 36.480915, mean_q: -37.216030\n",
            "  988920/10000000: episode: 4920, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -624.800, mean reward: -3.108 [-312.400, 43.200], mean action: 2.522 [0.000, 8.000], mean observation: 33.836 [0.002, 515.900], loss: 183.559280, mae: 36.494286, mean_q: -37.179062\n",
            "  989121/10000000: episode: 4921, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -360.400, mean reward: -1.793 [-180.200, 156.000], mean action: 2.403 [0.000, 9.000], mean observation: 35.196 [0.002, 467.700], loss: 245.975098, mae: 36.641632, mean_q: -37.428493\n",
            "  989322/10000000: episode: 4922, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 578.800, mean reward: 2.880 [-9.000, 541.800], mean action: 3.015 [0.000, 9.000], mean observation: 26.945 [0.005, 434.600], loss: 314.431244, mae: 36.843578, mean_q: -37.692444\n",
            "  989523/10000000: episode: 4923, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -484.400, mean reward: -2.410 [-242.200, 50.800], mean action: 2.647 [0.000, 10.000], mean observation: 29.055 [0.000, 638.600], loss: 310.482758, mae: 37.115978, mean_q: -37.939793\n",
            "  989724/10000000: episode: 4924, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -460.000, mean reward: -2.289 [-230.000, 56.500], mean action: 2.274 [0.000, 8.000], mean observation: 40.179 [0.000, 584.000], loss: 338.569885, mae: 36.957108, mean_q: -37.530056\n",
            "  989925/10000000: episode: 4925, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -293.400, mean reward: -1.460 [-146.700, 166.500], mean action: 2.289 [0.000, 9.000], mean observation: 29.403 [0.000, 530.500], loss: 260.862396, mae: 37.032288, mean_q: -37.546696\n",
            "  990126/10000000: episode: 4926, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -279.400, mean reward: -1.390 [-139.700, 26.400], mean action: 1.746 [0.000, 9.000], mean observation: 36.136 [0.000, 796.200], loss: 299.910706, mae: 36.703018, mean_q: -36.464874\n",
            "  990327/10000000: episode: 4927, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -568.200, mean reward: -2.827 [-284.100, 87.000], mean action: 2.279 [0.000, 9.000], mean observation: 36.828 [0.000, 588.400], loss: 217.409119, mae: 35.926407, mean_q: -36.305897\n",
            "  990528/10000000: episode: 4928, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: 593.000, mean reward: 2.950 [-10.000, 504.600], mean action: 2.781 [0.000, 10.000], mean observation: 34.505 [0.000, 507.900], loss: 223.495407, mae: 35.470978, mean_q: -36.388401\n",
            "  990729/10000000: episode: 4929, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -382.400, mean reward: -1.902 [-191.200, 147.400], mean action: 2.811 [0.000, 8.000], mean observation: 33.109 [0.003, 463.800], loss: 213.888947, mae: 36.103073, mean_q: -36.749866\n",
            "  990930/10000000: episode: 4930, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -220.200, mean reward: -1.096 [-110.100, 111.000], mean action: 2.930 [0.000, 8.000], mean observation: 29.100 [0.002, 545.200], loss: 453.794220, mae: 35.928120, mean_q: -36.606781\n",
            "  991131/10000000: episode: 4931, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -618.000, mean reward: -3.075 [-309.000, 44.400], mean action: 2.920 [0.000, 10.000], mean observation: 28.856 [0.004, 451.800], loss: 307.078247, mae: 35.839874, mean_q: -36.476753\n",
            "  991332/10000000: episode: 4932, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: 73.600, mean reward: 0.366 [-8.000, 336.500], mean action: 2.826 [0.000, 8.000], mean observation: 35.840 [0.000, 491.000], loss: 311.476135, mae: 35.831818, mean_q: -36.267918\n",
            "  991533/10000000: episode: 4933, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -993.200, mean reward: -4.941 [-496.600, 25.500], mean action: 3.075 [0.000, 10.000], mean observation: 33.093 [0.002, 481.400], loss: 289.053650, mae: 35.182823, mean_q: -35.751816\n",
            "  991734/10000000: episode: 4934, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -874.200, mean reward: -4.349 [-437.100, 35.400], mean action: 2.836 [0.000, 8.000], mean observation: 30.914 [0.000, 746.900], loss: 238.960678, mae: 35.198132, mean_q: -35.675446\n",
            "  991935/10000000: episode: 4935, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -793.000, mean reward: -3.945 [-396.500, 50.400], mean action: 2.826 [0.000, 10.000], mean observation: 31.427 [0.000, 609.400], loss: 446.099548, mae: 35.458164, mean_q: -35.941849\n",
            "  992136/10000000: episode: 4936, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -105.000, mean reward: -0.522 [-52.500, 307.200], mean action: 2.761 [0.000, 8.000], mean observation: 29.485 [0.001, 583.400], loss: 321.573303, mae: 35.254654, mean_q: -35.625568\n",
            "  992337/10000000: episode: 4937, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -501.400, mean reward: -2.495 [-250.700, 32.400], mean action: 2.209 [0.000, 8.000], mean observation: 32.912 [0.000, 702.100], loss: 292.419678, mae: 35.319965, mean_q: -35.363708\n",
            "  992538/10000000: episode: 4938, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -318.400, mean reward: -1.584 [-159.200, 170.400], mean action: 2.199 [0.000, 8.000], mean observation: 42.791 [0.000, 653.600], loss: 197.247314, mae: 35.247715, mean_q: -35.466366\n",
            "  992739/10000000: episode: 4939, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -497.600, mean reward: -2.476 [-248.800, 46.500], mean action: 2.557 [0.000, 10.000], mean observation: 25.824 [0.000, 509.800], loss: 336.421753, mae: 35.441181, mean_q: -35.918571\n",
            "  992940/10000000: episode: 4940, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -277.000, mean reward: -1.378 [-138.500, 64.800], mean action: 2.512 [0.000, 10.000], mean observation: 37.190 [0.002, 630.900], loss: 165.038116, mae: 35.508228, mean_q: -36.062965\n",
            "  993141/10000000: episode: 4941, duration: 1.696s, episode steps: 201, steps per second: 118, episode reward: 238.400, mean reward: 1.186 [-8.000, 191.400], mean action: 2.577 [0.000, 8.000], mean observation: 37.291 [0.001, 571.000], loss: 319.686829, mae: 35.380970, mean_q: -35.873760\n",
            "  993342/10000000: episode: 4942, duration: 1.692s, episode steps: 201, steps per second: 119, episode reward: -853.600, mean reward: -4.247 [-426.800, 13.500], mean action: 2.821 [0.000, 8.000], mean observation: 35.754 [0.001, 591.000], loss: 300.171753, mae: 36.035915, mean_q: -36.541199\n",
            "  993543/10000000: episode: 4943, duration: 1.718s, episode steps: 201, steps per second: 117, episode reward: -517.800, mean reward: -2.576 [-258.900, 36.000], mean action: 2.771 [0.000, 8.000], mean observation: 29.524 [0.001, 532.400], loss: 213.779419, mae: 36.097816, mean_q: -36.696869\n",
            "  993744/10000000: episode: 4944, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -162.400, mean reward: -0.808 [-81.200, 144.000], mean action: 2.801 [0.000, 8.000], mean observation: 34.446 [0.002, 595.600], loss: 296.437866, mae: 35.887878, mean_q: -36.497272\n",
            "  993945/10000000: episode: 4945, duration: 1.625s, episode steps: 201, steps per second: 124, episode reward: -385.600, mean reward: -1.918 [-192.800, 58.200], mean action: 2.517 [0.000, 8.000], mean observation: 32.286 [0.000, 530.000], loss: 256.215240, mae: 36.143356, mean_q: -36.783432\n",
            "  994146/10000000: episode: 4946, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -122.200, mean reward: -0.608 [-61.100, 123.800], mean action: 2.423 [0.000, 10.000], mean observation: 28.122 [0.002, 504.200], loss: 291.828735, mae: 36.386566, mean_q: -36.977142\n",
            "  994347/10000000: episode: 4947, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: -329.600, mean reward: -1.640 [-164.800, 118.200], mean action: 2.453 [0.000, 8.000], mean observation: 34.099 [0.001, 591.100], loss: 262.051147, mae: 36.285278, mean_q: -36.886715\n",
            "  994548/10000000: episode: 4948, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -473.200, mean reward: -2.354 [-236.600, 77.400], mean action: 2.965 [0.000, 10.000], mean observation: 30.177 [0.003, 545.800], loss: 229.420731, mae: 36.259823, mean_q: -37.212769\n",
            "  994749/10000000: episode: 4949, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -194.200, mean reward: -0.966 [-97.100, 394.800], mean action: 2.900 [0.000, 10.000], mean observation: 38.593 [0.001, 598.700], loss: 201.257233, mae: 36.254002, mean_q: -37.355328\n",
            "  994950/10000000: episode: 4950, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: 135.800, mean reward: 0.676 [-8.000, 287.500], mean action: 2.308 [0.000, 8.000], mean observation: 29.098 [0.002, 308.400], loss: 198.893311, mae: 36.084209, mean_q: -36.691982\n",
            "  995151/10000000: episode: 4951, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -260.600, mean reward: -1.297 [-130.300, 81.000], mean action: 2.209 [0.000, 10.000], mean observation: 30.065 [0.000, 479.300], loss: 247.395599, mae: 36.245514, mean_q: -36.641151\n",
            "  995352/10000000: episode: 4952, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -383.600, mean reward: -1.908 [-191.800, 70.000], mean action: 2.438 [0.000, 10.000], mean observation: 34.145 [0.000, 663.800], loss: 197.673630, mae: 36.336231, mean_q: -36.896889\n",
            "  995553/10000000: episode: 4953, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -218.400, mean reward: -1.087 [-109.200, 70.800], mean action: 1.886 [0.000, 8.000], mean observation: 28.874 [0.001, 477.000], loss: 200.665146, mae: 36.099770, mean_q: -36.524166\n",
            "  995754/10000000: episode: 4954, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -405.000, mean reward: -2.015 [-202.500, 28.600], mean action: 2.109 [0.000, 10.000], mean observation: 35.037 [0.000, 639.500], loss: 287.503723, mae: 36.051876, mean_q: -36.619843\n",
            "  995955/10000000: episode: 4955, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -561.000, mean reward: -2.791 [-280.500, 30.300], mean action: 1.910 [0.000, 10.000], mean observation: 31.808 [0.001, 496.800], loss: 254.791214, mae: 36.338051, mean_q: -36.644405\n",
            "  996156/10000000: episode: 4956, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -276.400, mean reward: -1.375 [-138.200, 60.000], mean action: 1.905 [0.000, 8.000], mean observation: 31.121 [0.001, 619.200], loss: 279.415894, mae: 36.075172, mean_q: -36.317848\n",
            "  996357/10000000: episode: 4957, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -389.000, mean reward: -1.935 [-194.500, 116.100], mean action: 2.697 [0.000, 10.000], mean observation: 36.161 [0.001, 651.100], loss: 268.357300, mae: 36.166142, mean_q: -36.803131\n",
            "  996558/10000000: episode: 4958, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -398.600, mean reward: -1.983 [-199.300, 58.800], mean action: 2.343 [0.000, 8.000], mean observation: 38.484 [0.000, 818.100], loss: 237.010452, mae: 36.215721, mean_q: -36.883289\n",
            "  996759/10000000: episode: 4959, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -446.200, mean reward: -2.220 [-223.100, 60.400], mean action: 1.891 [0.000, 10.000], mean observation: 30.745 [0.003, 540.500], loss: 290.436676, mae: 36.200493, mean_q: -36.571053\n",
            "  996960/10000000: episode: 4960, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -378.000, mean reward: -1.881 [-189.000, 43.800], mean action: 2.050 [0.000, 10.000], mean observation: 33.962 [0.001, 565.900], loss: 170.596222, mae: 35.891602, mean_q: -36.127819\n",
            "  997161/10000000: episode: 4961, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -432.000, mean reward: -2.149 [-216.000, 39.300], mean action: 1.746 [0.000, 8.000], mean observation: 34.280 [0.000, 720.900], loss: 253.205734, mae: 35.968410, mean_q: -36.050396\n",
            "  997362/10000000: episode: 4962, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -409.000, mean reward: -2.035 [-204.500, 55.200], mean action: 1.861 [0.000, 10.000], mean observation: 30.810 [0.002, 624.500], loss: 260.822479, mae: 36.330025, mean_q: -36.334229\n",
            "  997563/10000000: episode: 4963, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -298.600, mean reward: -1.486 [-149.300, 126.000], mean action: 1.846 [0.000, 8.000], mean observation: 34.524 [0.002, 503.100], loss: 231.953064, mae: 36.337494, mean_q: -36.295403\n",
            "  997764/10000000: episode: 4964, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -385.200, mean reward: -1.916 [-192.600, 35.400], mean action: 1.766 [0.000, 8.000], mean observation: 26.712 [0.001, 462.800], loss: 289.488647, mae: 35.922955, mean_q: -35.940807\n",
            "  997965/10000000: episode: 4965, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -184.600, mean reward: -0.918 [-92.300, 115.300], mean action: 2.269 [0.000, 9.000], mean observation: 34.974 [0.000, 476.600], loss: 233.304062, mae: 35.638748, mean_q: -35.950287\n",
            "  998166/10000000: episode: 4966, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -382.000, mean reward: -1.900 [-191.000, 37.000], mean action: 2.005 [0.000, 10.000], mean observation: 39.899 [0.001, 642.700], loss: 252.400970, mae: 35.849079, mean_q: -36.319584\n",
            "  998367/10000000: episode: 4967, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -321.000, mean reward: -1.597 [-160.500, 42.300], mean action: 2.378 [0.000, 8.000], mean observation: 34.926 [0.000, 907.100], loss: 238.905304, mae: 35.681427, mean_q: -36.239685\n",
            "  998568/10000000: episode: 4968, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: 230.600, mean reward: 1.147 [-10.000, 295.500], mean action: 2.294 [0.000, 10.000], mean observation: 30.402 [0.001, 399.700], loss: 256.137756, mae: 35.964134, mean_q: -36.194107\n",
            "  998769/10000000: episode: 4969, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -671.400, mean reward: -3.340 [-335.700, 24.800], mean action: 2.378 [0.000, 10.000], mean observation: 34.528 [0.001, 507.200], loss: 316.345917, mae: 35.633381, mean_q: -35.755322\n",
            "  998970/10000000: episode: 4970, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: 366.600, mean reward: 1.824 [-8.000, 360.500], mean action: 2.423 [0.000, 10.000], mean observation: 30.877 [0.002, 522.800], loss: 227.224075, mae: 35.070217, mean_q: -35.224178\n",
            "  999171/10000000: episode: 4971, duration: 1.473s, episode steps: 201, steps per second: 137, episode reward: -41.200, mean reward: -0.205 [-20.600, 123.300], mean action: 2.507 [0.000, 10.000], mean observation: 29.855 [0.000, 623.900], loss: 198.312424, mae: 35.169537, mean_q: -35.616547\n",
            "  999372/10000000: episode: 4972, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -791.400, mean reward: -3.937 [-395.700, 28.500], mean action: 2.836 [0.000, 10.000], mean observation: 34.175 [0.002, 542.000], loss: 359.177032, mae: 35.117092, mean_q: -35.793533\n",
            "  999573/10000000: episode: 4973, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -339.600, mean reward: -1.690 [-169.800, 63.600], mean action: 3.234 [0.000, 10.000], mean observation: 33.040 [0.000, 493.100], loss: 232.493088, mae: 35.235329, mean_q: -36.051582\n",
            "  999774/10000000: episode: 4974, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: 12.200, mean reward: 0.061 [-10.000, 242.500], mean action: 2.632 [0.000, 10.000], mean observation: 32.424 [0.000, 781.200], loss: 224.695953, mae: 35.397587, mean_q: -36.209785\n",
            "  999975/10000000: episode: 4975, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -415.000, mean reward: -2.065 [-207.500, 38.500], mean action: 2.000 [0.000, 10.000], mean observation: 31.131 [0.001, 492.700], loss: 149.464447, mae: 36.034832, mean_q: -36.610214\n",
            " 1000176/10000000: episode: 4976, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -235.000, mean reward: -1.169 [-117.500, 70.500], mean action: 1.796 [0.000, 10.000], mean observation: 35.630 [0.001, 467.200], loss: 207.911789, mae: 36.308464, mean_q: -36.655037\n",
            " 1000377/10000000: episode: 4977, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -103.000, mean reward: -0.512 [-51.500, 135.000], mean action: 1.781 [0.000, 10.000], mean observation: 33.202 [0.001, 511.400], loss: 299.400726, mae: 36.473537, mean_q: -36.711548\n",
            " 1000578/10000000: episode: 4978, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -200.800, mean reward: -0.999 [-100.400, 117.000], mean action: 2.159 [0.000, 10.000], mean observation: 34.958 [0.000, 764.600], loss: 254.055389, mae: 36.327534, mean_q: -36.695896\n",
            " 1000779/10000000: episode: 4979, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -396.400, mean reward: -1.972 [-198.200, 85.800], mean action: 2.040 [0.000, 10.000], mean observation: 31.074 [0.000, 498.200], loss: 251.405685, mae: 36.465553, mean_q: -36.777634\n",
            " 1000980/10000000: episode: 4980, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -444.200, mean reward: -2.210 [-222.100, 30.000], mean action: 2.289 [0.000, 10.000], mean observation: 26.203 [0.001, 516.900], loss: 150.776550, mae: 36.059574, mean_q: -36.553532\n",
            " 1001181/10000000: episode: 4981, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -634.000, mean reward: -3.154 [-317.000, 37.100], mean action: 2.428 [0.000, 8.000], mean observation: 35.282 [0.000, 598.900], loss: 408.366608, mae: 36.444473, mean_q: -37.089653\n",
            " 1001382/10000000: episode: 4982, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -672.800, mean reward: -3.347 [-336.400, 47.200], mean action: 2.418 [0.000, 10.000], mean observation: 41.202 [0.000, 792.800], loss: 165.186417, mae: 36.443169, mean_q: -36.909534\n",
            " 1001583/10000000: episode: 4983, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: -538.200, mean reward: -2.678 [-269.100, 32.400], mean action: 2.010 [0.000, 8.000], mean observation: 28.178 [0.001, 420.000], loss: 286.140015, mae: 36.383144, mean_q: -36.707878\n",
            " 1001784/10000000: episode: 4984, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -511.800, mean reward: -2.546 [-255.900, 38.500], mean action: 2.254 [0.000, 8.000], mean observation: 41.107 [0.000, 746.700], loss: 256.050476, mae: 35.835896, mean_q: -36.457165\n",
            " 1001985/10000000: episode: 4985, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: 1931.600, mean reward: 9.610 [-8.000, 965.800], mean action: 2.627 [0.000, 8.000], mean observation: 31.189 [0.001, 375.100], loss: 307.531982, mae: 36.119518, mean_q: -36.763847\n",
            " 1002186/10000000: episode: 4986, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -568.000, mean reward: -2.826 [-284.000, 92.400], mean action: 2.488 [0.000, 10.000], mean observation: 32.999 [0.000, 541.500], loss: 249.717850, mae: 36.601460, mean_q: -37.209126\n",
            " 1002387/10000000: episode: 4987, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 67.000, mean reward: 0.333 [-10.000, 234.600], mean action: 2.254 [0.000, 10.000], mean observation: 29.647 [0.001, 519.300], loss: 171.440201, mae: 37.123966, mean_q: -37.716278\n",
            " 1002588/10000000: episode: 4988, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -458.400, mean reward: -2.281 [-229.200, 19.500], mean action: 1.856 [0.000, 8.000], mean observation: 34.904 [0.002, 460.100], loss: 211.617264, mae: 37.453281, mean_q: -37.869728\n",
            " 1002789/10000000: episode: 4989, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: 1225.000, mean reward: 6.095 [-10.000, 612.500], mean action: 1.796 [0.000, 10.000], mean observation: 35.436 [0.000, 579.300], loss: 255.272156, mae: 37.031342, mean_q: -37.147198\n",
            " 1002990/10000000: episode: 4990, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -129.000, mean reward: -0.642 [-64.500, 230.400], mean action: 1.771 [0.000, 10.000], mean observation: 32.296 [0.001, 576.200], loss: 234.233398, mae: 36.725224, mean_q: -36.717712\n",
            " 1003191/10000000: episode: 4991, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -380.200, mean reward: -1.892 [-190.100, 48.300], mean action: 1.786 [0.000, 10.000], mean observation: 28.844 [0.001, 550.200], loss: 279.817688, mae: 36.968269, mean_q: -37.133358\n",
            " 1003392/10000000: episode: 4992, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -480.600, mean reward: -2.391 [-240.300, 70.000], mean action: 1.701 [0.000, 10.000], mean observation: 32.358 [0.001, 548.600], loss: 202.565002, mae: 37.090305, mean_q: -37.245892\n",
            " 1003593/10000000: episode: 4993, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -10.200, mean reward: -0.051 [-10.000, 99.000], mean action: 1.811 [0.000, 10.000], mean observation: 34.336 [0.001, 494.100], loss: 218.455338, mae: 37.298473, mean_q: -37.640873\n",
            " 1003794/10000000: episode: 4994, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -571.200, mean reward: -2.842 [-285.600, 24.400], mean action: 2.075 [0.000, 10.000], mean observation: 34.076 [0.000, 525.700], loss: 281.533630, mae: 37.035305, mean_q: -37.417496\n",
            " 1003995/10000000: episode: 4995, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -310.200, mean reward: -1.543 [-155.100, 56.800], mean action: 1.587 [0.000, 10.000], mean observation: 43.017 [0.000, 803.400], loss: 290.362701, mae: 36.959995, mean_q: -37.004120\n",
            " 1004196/10000000: episode: 4996, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -108.000, mean reward: -0.537 [-54.000, 139.800], mean action: 2.090 [0.000, 8.000], mean observation: 27.933 [0.000, 597.200], loss: 212.148712, mae: 36.433533, mean_q: -36.778351\n",
            " 1004397/10000000: episode: 4997, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -415.800, mean reward: -2.069 [-207.900, 84.800], mean action: 2.199 [0.000, 10.000], mean observation: 30.818 [0.002, 455.800], loss: 300.525055, mae: 36.420528, mean_q: -36.874119\n",
            " 1004598/10000000: episode: 4998, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -388.800, mean reward: -1.934 [-194.400, 30.100], mean action: 2.154 [0.000, 8.000], mean observation: 41.354 [0.001, 545.700], loss: 155.324203, mae: 37.173496, mean_q: -37.966232\n",
            " 1004799/10000000: episode: 4999, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -677.400, mean reward: -3.370 [-338.700, 28.800], mean action: 2.303 [0.000, 10.000], mean observation: 30.277 [0.000, 581.400], loss: 266.931366, mae: 37.385212, mean_q: -38.107422\n",
            " 1005000/10000000: episode: 5000, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -725.800, mean reward: -3.611 [-362.900, 24.600], mean action: 2.433 [0.000, 8.000], mean observation: 34.787 [0.000, 622.400], loss: 271.713867, mae: 37.586182, mean_q: -38.230286\n",
            " 1005201/10000000: episode: 5001, duration: 1.378s, episode steps: 201, steps per second: 146, episode reward: -404.200, mean reward: -2.011 [-202.100, 68.500], mean action: 1.866 [0.000, 8.000], mean observation: 35.801 [0.001, 430.400], loss: 260.888123, mae: 38.112782, mean_q: -38.695484\n",
            " 1005402/10000000: episode: 5002, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: 187.600, mean reward: 0.933 [-10.000, 214.200], mean action: 1.965 [0.000, 10.000], mean observation: 37.071 [0.000, 669.400], loss: 270.608948, mae: 38.034935, mean_q: -38.522232\n",
            " 1005603/10000000: episode: 5003, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -469.400, mean reward: -2.335 [-234.700, 27.600], mean action: 1.960 [0.000, 8.000], mean observation: 33.862 [0.002, 511.500], loss: 266.878662, mae: 37.806122, mean_q: -38.418606\n",
            " 1005804/10000000: episode: 5004, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -288.200, mean reward: -1.434 [-144.100, 90.600], mean action: 2.005 [0.000, 8.000], mean observation: 36.792 [0.000, 663.000], loss: 395.627167, mae: 38.178036, mean_q: -38.669144\n",
            " 1006005/10000000: episode: 5005, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -411.600, mean reward: -2.048 [-205.800, 47.500], mean action: 1.965 [0.000, 10.000], mean observation: 37.164 [0.000, 637.900], loss: 160.187759, mae: 37.837620, mean_q: -38.256924\n",
            " 1006206/10000000: episode: 5006, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -400.400, mean reward: -1.992 [-200.200, 43.000], mean action: 2.080 [0.000, 7.000], mean observation: 33.749 [0.002, 542.900], loss: 278.701660, mae: 37.636856, mean_q: -38.028103\n",
            " 1006407/10000000: episode: 5007, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -568.200, mean reward: -2.827 [-284.100, 51.000], mean action: 2.279 [0.000, 8.000], mean observation: 32.734 [0.001, 581.100], loss: 288.271484, mae: 37.489944, mean_q: -38.013844\n",
            " 1006608/10000000: episode: 5008, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -765.400, mean reward: -3.808 [-382.700, 53.700], mean action: 2.682 [0.000, 10.000], mean observation: 32.372 [0.001, 469.000], loss: 263.046844, mae: 37.341385, mean_q: -38.109016\n",
            " 1006809/10000000: episode: 5009, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -560.000, mean reward: -2.786 [-280.000, 64.400], mean action: 2.652 [0.000, 10.000], mean observation: 36.001 [0.000, 622.800], loss: 358.400421, mae: 37.045841, mean_q: -37.389229\n",
            " 1007010/10000000: episode: 5010, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -825.000, mean reward: -4.104 [-412.500, 13.800], mean action: 2.269 [0.000, 10.000], mean observation: 34.099 [0.001, 446.000], loss: 298.380981, mae: 36.441921, mean_q: -36.669743\n",
            " 1007211/10000000: episode: 5011, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -767.400, mean reward: -3.818 [-383.700, 12.800], mean action: 2.343 [0.000, 10.000], mean observation: 32.420 [0.001, 424.400], loss: 262.513947, mae: 35.798046, mean_q: -35.963524\n",
            " 1007412/10000000: episode: 5012, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -647.600, mean reward: -3.222 [-323.800, 72.600], mean action: 2.418 [0.000, 10.000], mean observation: 31.927 [0.000, 749.600], loss: 235.778137, mae: 36.050369, mean_q: -36.582977\n",
            " 1007613/10000000: episode: 5013, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: 157.000, mean reward: 0.781 [-7.000, 231.000], mean action: 1.930 [0.000, 8.000], mean observation: 30.239 [0.001, 587.600], loss: 250.037338, mae: 36.325851, mean_q: -36.774109\n",
            " 1007814/10000000: episode: 5014, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 2292.800, mean reward: 11.407 [-8.000, 1146.400], mean action: 2.393 [0.000, 8.000], mean observation: 36.704 [0.002, 516.600], loss: 228.517990, mae: 36.271690, mean_q: -36.804657\n",
            " 1008015/10000000: episode: 5015, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -695.000, mean reward: -3.458 [-347.500, 50.000], mean action: 2.537 [0.000, 8.000], mean observation: 26.283 [0.001, 415.200], loss: 250.115601, mae: 36.863235, mean_q: -37.516365\n",
            " 1008216/10000000: episode: 5016, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: 44.600, mean reward: 0.222 [-6.000, 136.600], mean action: 2.264 [0.000, 8.000], mean observation: 36.579 [0.004, 509.100], loss: 331.134247, mae: 37.018845, mean_q: -37.829792\n",
            " 1008417/10000000: episode: 5017, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -362.600, mean reward: -1.804 [-181.300, 72.600], mean action: 2.229 [0.000, 10.000], mean observation: 28.588 [0.000, 590.900], loss: 196.725082, mae: 37.040264, mean_q: -37.729538\n",
            " 1008618/10000000: episode: 5018, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -375.000, mean reward: -1.866 [-187.500, 95.200], mean action: 1.985 [0.000, 9.000], mean observation: 32.391 [0.003, 566.300], loss: 208.983871, mae: 37.031651, mean_q: -37.584049\n",
            " 1008819/10000000: episode: 5019, duration: 1.380s, episode steps: 201, steps per second: 146, episode reward: -639.800, mean reward: -3.183 [-319.900, 76.000], mean action: 2.697 [0.000, 10.000], mean observation: 29.007 [0.000, 486.700], loss: 402.391571, mae: 37.282833, mean_q: -38.058731\n",
            " 1009020/10000000: episode: 5020, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -260.600, mean reward: -1.297 [-130.300, 135.000], mean action: 2.383 [0.000, 9.000], mean observation: 34.745 [0.001, 510.800], loss: 364.886597, mae: 37.174286, mean_q: -37.866333\n",
            " 1009221/10000000: episode: 5021, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: 306.800, mean reward: 1.526 [-10.000, 200.800], mean action: 2.652 [0.000, 10.000], mean observation: 32.006 [0.001, 423.300], loss: 293.194550, mae: 36.636490, mean_q: -37.411346\n",
            " 1009422/10000000: episode: 5022, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: 543.600, mean reward: 2.704 [-10.000, 271.800], mean action: 2.920 [0.000, 10.000], mean observation: 34.171 [0.000, 637.000], loss: 234.144836, mae: 36.914219, mean_q: -38.092426\n",
            " 1009623/10000000: episode: 5023, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -23.600, mean reward: -0.117 [-11.800, 217.200], mean action: 2.657 [0.000, 9.000], mean observation: 31.122 [0.002, 439.600], loss: 279.184235, mae: 37.200752, mean_q: -38.432125\n",
            " 1009824/10000000: episode: 5024, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: 302.600, mean reward: 1.505 [-10.000, 311.400], mean action: 2.940 [0.000, 10.000], mean observation: 30.320 [0.000, 640.400], loss: 398.166931, mae: 37.205944, mean_q: -38.357380\n",
            " 1010025/10000000: episode: 5025, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -162.600, mean reward: -0.809 [-81.300, 253.600], mean action: 2.552 [0.000, 8.000], mean observation: 32.790 [0.001, 512.800], loss: 158.793243, mae: 37.288906, mean_q: -38.530785\n",
            " 1010226/10000000: episode: 5026, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: 200.800, mean reward: 0.999 [-8.000, 100.400], mean action: 2.224 [0.000, 8.000], mean observation: 33.430 [0.000, 558.800], loss: 336.325043, mae: 37.974445, mean_q: -38.661274\n",
            " 1010427/10000000: episode: 5027, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -274.000, mean reward: -1.363 [-137.000, 84.600], mean action: 2.055 [0.000, 10.000], mean observation: 34.054 [0.003, 539.800], loss: 298.091095, mae: 38.143185, mean_q: -38.826527\n",
            " 1010628/10000000: episode: 5028, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -470.800, mean reward: -2.342 [-235.400, 54.800], mean action: 2.184 [0.000, 10.000], mean observation: 35.683 [0.001, 584.600], loss: 185.151077, mae: 38.332642, mean_q: -39.084579\n",
            " 1010829/10000000: episode: 5029, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -654.400, mean reward: -3.256 [-327.200, 24.000], mean action: 2.040 [0.000, 8.000], mean observation: 36.853 [0.000, 455.400], loss: 423.776031, mae: 38.119068, mean_q: -38.606884\n",
            " 1011030/10000000: episode: 5030, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -390.600, mean reward: -1.943 [-195.300, 61.200], mean action: 2.090 [0.000, 10.000], mean observation: 31.969 [0.001, 510.200], loss: 365.368469, mae: 37.745544, mean_q: -38.252216\n",
            " 1011231/10000000: episode: 5031, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -648.600, mean reward: -3.227 [-324.300, 33.000], mean action: 2.502 [0.000, 10.000], mean observation: 31.636 [0.000, 708.200], loss: 282.470428, mae: 37.703938, mean_q: -38.766060\n",
            " 1011432/10000000: episode: 5032, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: 276.200, mean reward: 1.374 [-10.000, 167.500], mean action: 2.483 [0.000, 10.000], mean observation: 36.140 [0.002, 500.200], loss: 313.709045, mae: 37.843906, mean_q: -39.004684\n",
            " 1011633/10000000: episode: 5033, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: 237.200, mean reward: 1.180 [-9.000, 118.600], mean action: 2.239 [0.000, 9.000], mean observation: 36.042 [0.000, 440.500], loss: 410.662628, mae: 37.689423, mean_q: -38.218590\n",
            " 1011834/10000000: episode: 5034, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -291.400, mean reward: -1.450 [-145.700, 35.000], mean action: 2.045 [0.000, 10.000], mean observation: 34.430 [0.000, 566.800], loss: 278.613251, mae: 37.815765, mean_q: -38.219997\n",
            " 1012035/10000000: episode: 5035, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -503.800, mean reward: -2.506 [-251.900, 53.200], mean action: 2.204 [0.000, 7.000], mean observation: 31.096 [0.001, 571.900], loss: 394.789734, mae: 37.306782, mean_q: -38.204578\n",
            " 1012236/10000000: episode: 5036, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -311.000, mean reward: -1.547 [-155.500, 141.000], mean action: 2.338 [0.000, 10.000], mean observation: 34.059 [0.000, 494.100], loss: 265.524139, mae: 37.420563, mean_q: -38.662540\n",
            " 1012437/10000000: episode: 5037, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -682.400, mean reward: -3.395 [-341.200, 51.800], mean action: 2.502 [0.000, 10.000], mean observation: 28.193 [0.002, 465.200], loss: 256.427185, mae: 37.767414, mean_q: -39.075645\n",
            " 1012638/10000000: episode: 5038, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -530.000, mean reward: -2.637 [-265.000, 136.200], mean action: 3.463 [0.000, 10.000], mean observation: 31.720 [0.001, 533.900], loss: 283.403931, mae: 37.879688, mean_q: -39.336315\n",
            " 1012839/10000000: episode: 5039, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -651.800, mean reward: -3.243 [-325.900, 52.800], mean action: 3.184 [0.000, 10.000], mean observation: 33.321 [0.000, 798.300], loss: 434.454651, mae: 38.379826, mean_q: -39.766018\n",
            " 1013040/10000000: episode: 5040, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -259.200, mean reward: -1.290 [-129.600, 144.000], mean action: 2.338 [0.000, 10.000], mean observation: 30.257 [0.001, 434.300], loss: 503.368622, mae: 38.145775, mean_q: -38.990532\n",
            " 1013241/10000000: episode: 5041, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -505.000, mean reward: -2.512 [-252.500, 35.600], mean action: 2.104 [0.000, 9.000], mean observation: 32.581 [0.000, 668.900], loss: 184.590714, mae: 38.197414, mean_q: -39.032482\n",
            " 1013442/10000000: episode: 5042, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: 557.200, mean reward: 2.772 [-8.000, 278.600], mean action: 2.035 [0.000, 8.000], mean observation: 34.252 [0.000, 506.300], loss: 135.018265, mae: 37.888260, mean_q: -38.813572\n",
            " 1013643/10000000: episode: 5043, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -283.800, mean reward: -1.412 [-141.900, 67.500], mean action: 2.060 [0.000, 10.000], mean observation: 29.416 [0.003, 383.900], loss: 205.042191, mae: 38.113972, mean_q: -38.935581\n",
            " 1013844/10000000: episode: 5044, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -681.200, mean reward: -3.389 [-340.600, 40.000], mean action: 2.299 [0.000, 10.000], mean observation: 37.155 [0.000, 605.900], loss: 455.404388, mae: 37.789661, mean_q: -38.645950\n",
            " 1014045/10000000: episode: 5045, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: 367.200, mean reward: 1.827 [-8.000, 328.500], mean action: 2.299 [0.000, 8.000], mean observation: 35.367 [0.000, 650.000], loss: 181.113007, mae: 37.697250, mean_q: -38.522793\n",
            " 1014246/10000000: episode: 5046, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: 817.800, mean reward: 4.069 [-10.000, 540.000], mean action: 2.323 [0.000, 10.000], mean observation: 28.912 [0.002, 432.300], loss: 268.153717, mae: 37.196716, mean_q: -38.103497\n",
            " 1014447/10000000: episode: 5047, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: 231.000, mean reward: 1.149 [-8.000, 161.500], mean action: 2.562 [0.000, 10.000], mean observation: 36.294 [0.003, 420.100], loss: 355.888458, mae: 37.364487, mean_q: -38.425964\n",
            " 1014648/10000000: episode: 5048, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: 95.000, mean reward: 0.473 [-10.000, 458.500], mean action: 2.637 [0.000, 10.000], mean observation: 40.449 [0.001, 510.400], loss: 184.298233, mae: 37.470894, mean_q: -38.674835\n",
            " 1014849/10000000: episode: 5049, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: 101.800, mean reward: 0.506 [-8.000, 187.700], mean action: 2.448 [0.000, 8.000], mean observation: 32.131 [0.000, 508.900], loss: 266.683624, mae: 37.340397, mean_q: -38.551674\n",
            " 1015050/10000000: episode: 5050, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -365.200, mean reward: -1.817 [-182.600, 76.800], mean action: 1.960 [0.000, 8.000], mean observation: 32.582 [0.002, 447.500], loss: 466.617645, mae: 37.358898, mean_q: -37.777901\n",
            " 1015251/10000000: episode: 5051, duration: 1.395s, episode steps: 201, steps per second: 144, episode reward: -131.000, mean reward: -0.652 [-65.500, 76.400], mean action: 1.597 [0.000, 8.000], mean observation: 31.775 [0.001, 618.400], loss: 401.191010, mae: 36.811092, mean_q: -36.788376\n",
            " 1015452/10000000: episode: 5052, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -405.800, mean reward: -2.019 [-202.900, 24.400], mean action: 1.483 [0.000, 10.000], mean observation: 33.126 [0.001, 512.000], loss: 321.369965, mae: 36.455154, mean_q: -36.585712\n",
            " 1015653/10000000: episode: 5053, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -487.400, mean reward: -2.425 [-243.700, 14.400], mean action: 1.692 [0.000, 10.000], mean observation: 32.785 [0.000, 586.000], loss: 433.121490, mae: 36.128296, mean_q: -36.358646\n",
            " 1015854/10000000: episode: 5054, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -415.400, mean reward: -2.067 [-207.700, 32.000], mean action: 1.637 [0.000, 10.000], mean observation: 31.245 [0.001, 423.700], loss: 206.561096, mae: 36.297626, mean_q: -36.152489\n",
            " 1016055/10000000: episode: 5055, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: 386.200, mean reward: 1.921 [-8.000, 216.000], mean action: 1.746 [0.000, 8.000], mean observation: 31.248 [0.001, 459.200], loss: 234.754196, mae: 36.298367, mean_q: -36.456345\n",
            " 1016256/10000000: episode: 5056, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -492.600, mean reward: -2.451 [-246.300, 88.000], mean action: 2.408 [0.000, 10.000], mean observation: 31.573 [0.001, 422.300], loss: 380.127258, mae: 36.358555, mean_q: -37.142605\n",
            " 1016457/10000000: episode: 5057, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: 732.200, mean reward: 3.643 [-7.000, 457.600], mean action: 2.214 [0.000, 7.000], mean observation: 32.291 [0.001, 497.500], loss: 221.187225, mae: 36.377373, mean_q: -37.353226\n",
            " 1016658/10000000: episode: 5058, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -308.200, mean reward: -1.533 [-154.100, 67.500], mean action: 2.398 [0.000, 8.000], mean observation: 31.402 [0.002, 439.900], loss: 260.750397, mae: 36.297867, mean_q: -37.267494\n",
            " 1016859/10000000: episode: 5059, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -765.400, mean reward: -3.808 [-382.700, 31.200], mean action: 2.448 [0.000, 7.000], mean observation: 30.549 [0.002, 527.400], loss: 189.734848, mae: 36.282795, mean_q: -37.436146\n",
            " 1017060/10000000: episode: 5060, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -396.600, mean reward: -1.973 [-198.300, 143.800], mean action: 2.468 [0.000, 10.000], mean observation: 36.649 [0.000, 720.900], loss: 205.553787, mae: 36.428143, mean_q: -37.545300\n",
            " 1017261/10000000: episode: 5061, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -356.600, mean reward: -1.774 [-178.300, 62.000], mean action: 2.587 [0.000, 10.000], mean observation: 31.251 [0.001, 500.800], loss: 213.250778, mae: 36.565037, mean_q: -37.779789\n",
            " 1017462/10000000: episode: 5062, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: 521.600, mean reward: 2.595 [-8.000, 319.000], mean action: 2.109 [0.000, 8.000], mean observation: 31.946 [0.001, 424.400], loss: 334.383514, mae: 36.357258, mean_q: -37.445908\n",
            " 1017663/10000000: episode: 5063, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -346.600, mean reward: -1.724 [-173.300, 73.100], mean action: 1.900 [0.000, 8.000], mean observation: 32.694 [0.000, 441.700], loss: 187.755173, mae: 37.047291, mean_q: -37.886776\n",
            " 1017864/10000000: episode: 5064, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: 1004.400, mean reward: 4.997 [-10.000, 592.000], mean action: 1.925 [0.000, 10.000], mean observation: 27.097 [0.003, 380.000], loss: 243.519318, mae: 36.777039, mean_q: -37.373344\n",
            " 1018065/10000000: episode: 5065, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -485.600, mean reward: -2.416 [-242.800, 32.700], mean action: 2.035 [0.000, 10.000], mean observation: 35.443 [0.002, 537.100], loss: 319.808716, mae: 36.972420, mean_q: -37.560318\n",
            " 1018266/10000000: episode: 5066, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -78.600, mean reward: -0.391 [-39.300, 100.800], mean action: 1.478 [0.000, 8.000], mean observation: 33.800 [0.001, 562.800], loss: 314.858887, mae: 36.667770, mean_q: -36.506008\n",
            " 1018467/10000000: episode: 5067, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -63.600, mean reward: -0.316 [-31.800, 108.800], mean action: 1.965 [0.000, 10.000], mean observation: 30.996 [0.001, 530.600], loss: 301.696259, mae: 35.522251, mean_q: -35.819893\n",
            " 1018668/10000000: episode: 5068, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -447.000, mean reward: -2.224 [-223.500, 71.200], mean action: 2.408 [0.000, 8.000], mean observation: 40.891 [0.000, 693.300], loss: 271.892303, mae: 35.154549, mean_q: -35.939857\n",
            " 1018869/10000000: episode: 5069, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -474.400, mean reward: -2.360 [-237.200, 69.300], mean action: 2.274 [0.000, 8.000], mean observation: 32.854 [0.000, 727.600], loss: 455.444794, mae: 35.186588, mean_q: -35.932682\n",
            " 1019070/10000000: episode: 5070, duration: 1.653s, episode steps: 201, steps per second: 122, episode reward: -523.600, mean reward: -2.605 [-261.800, 52.200], mean action: 2.438 [0.000, 8.000], mean observation: 34.185 [0.002, 453.000], loss: 333.050262, mae: 35.116959, mean_q: -35.711689\n",
            " 1019271/10000000: episode: 5071, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -320.400, mean reward: -1.594 [-160.200, 51.600], mean action: 2.368 [0.000, 10.000], mean observation: 32.880 [0.001, 606.200], loss: 235.829117, mae: 35.145554, mean_q: -35.663029\n",
            " 1019472/10000000: episode: 5072, duration: 1.636s, episode steps: 201, steps per second: 123, episode reward: -530.600, mean reward: -2.640 [-265.300, 66.700], mean action: 2.493 [0.000, 8.000], mean observation: 35.398 [0.003, 593.100], loss: 292.382080, mae: 35.202518, mean_q: -35.572330\n",
            " 1019673/10000000: episode: 5073, duration: 1.571s, episode steps: 201, steps per second: 128, episode reward: -412.800, mean reward: -2.054 [-206.400, 67.000], mean action: 2.174 [0.000, 10.000], mean observation: 30.755 [0.002, 482.900], loss: 212.338638, mae: 35.412346, mean_q: -35.753162\n",
            " 1019874/10000000: episode: 5074, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -550.000, mean reward: -2.736 [-275.000, 64.000], mean action: 2.945 [0.000, 10.000], mean observation: 30.173 [0.001, 697.100], loss: 294.696838, mae: 35.708153, mean_q: -36.117985\n",
            " 1020075/10000000: episode: 5075, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -775.600, mean reward: -3.859 [-387.800, 51.600], mean action: 2.791 [0.000, 10.000], mean observation: 28.928 [0.001, 562.800], loss: 300.492645, mae: 35.914547, mean_q: -36.554710\n",
            " 1020276/10000000: episode: 5076, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -771.600, mean reward: -3.839 [-385.800, 59.000], mean action: 3.259 [0.000, 10.000], mean observation: 30.409 [0.001, 471.400], loss: 334.344604, mae: 35.904915, mean_q: -36.344074\n",
            " 1020477/10000000: episode: 5077, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -1011.800, mean reward: -5.034 [-505.900, 64.500], mean action: 3.214 [0.000, 8.000], mean observation: 35.641 [0.000, 522.200], loss: 189.385849, mae: 35.629684, mean_q: -36.236900\n",
            " 1020678/10000000: episode: 5078, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: 115.400, mean reward: 0.574 [-8.000, 172.200], mean action: 2.891 [0.000, 8.000], mean observation: 29.582 [0.001, 521.600], loss: 319.039307, mae: 36.114418, mean_q: -36.639206\n",
            " 1020879/10000000: episode: 5079, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -114.400, mean reward: -0.569 [-57.200, 206.000], mean action: 2.821 [0.000, 10.000], mean observation: 32.085 [0.000, 527.200], loss: 206.647598, mae: 35.955490, mean_q: -36.370083\n",
            " 1021080/10000000: episode: 5080, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -917.600, mean reward: -4.565 [-458.800, 17.200], mean action: 2.632 [0.000, 10.000], mean observation: 36.477 [0.000, 451.200], loss: 297.643433, mae: 36.133423, mean_q: -36.823887\n",
            " 1021281/10000000: episode: 5081, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -408.800, mean reward: -2.034 [-204.400, 136.500], mean action: 2.493 [0.000, 9.000], mean observation: 30.121 [0.000, 765.000], loss: 332.156647, mae: 36.334484, mean_q: -37.011616\n",
            " 1021482/10000000: episode: 5082, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: 980.200, mean reward: 4.877 [-10.000, 490.100], mean action: 2.726 [0.000, 10.000], mean observation: 34.826 [0.000, 638.500], loss: 345.011932, mae: 36.709682, mean_q: -37.247597\n",
            " 1021683/10000000: episode: 5083, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -144.800, mean reward: -0.720 [-72.400, 135.200], mean action: 2.617 [0.000, 10.000], mean observation: 33.470 [0.000, 534.900], loss: 182.010178, mae: 37.087265, mean_q: -37.958652\n",
            " 1021884/10000000: episode: 5084, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -649.000, mean reward: -3.229 [-324.500, 23.700], mean action: 2.204 [0.000, 8.000], mean observation: 38.874 [0.000, 793.800], loss: 406.265228, mae: 37.189030, mean_q: -37.871418\n",
            " 1022085/10000000: episode: 5085, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -185.800, mean reward: -0.924 [-92.900, 110.400], mean action: 2.358 [0.000, 6.000], mean observation: 36.055 [0.000, 701.500], loss: 277.094299, mae: 37.091900, mean_q: -38.128105\n",
            " 1022286/10000000: episode: 5086, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -884.200, mean reward: -4.399 [-442.100, 15.500], mean action: 2.478 [0.000, 10.000], mean observation: 40.684 [0.000, 628.000], loss: 355.270996, mae: 37.463524, mean_q: -38.457806\n",
            " 1022487/10000000: episode: 5087, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: 1248.000, mean reward: 6.209 [-10.000, 624.000], mean action: 2.512 [0.000, 10.000], mean observation: 33.361 [0.000, 783.800], loss: 229.208252, mae: 37.086952, mean_q: -38.170162\n",
            " 1022688/10000000: episode: 5088, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -95.200, mean reward: -0.474 [-47.600, 76.000], mean action: 2.602 [0.000, 10.000], mean observation: 36.552 [0.000, 642.000], loss: 180.612213, mae: 37.612614, mean_q: -38.967651\n",
            " 1022889/10000000: episode: 5089, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -520.400, mean reward: -2.589 [-260.200, 67.000], mean action: 2.478 [0.000, 8.000], mean observation: 34.188 [0.002, 515.200], loss: 322.563690, mae: 37.484814, mean_q: -38.487663\n",
            " 1023090/10000000: episode: 5090, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -352.600, mean reward: -1.754 [-176.300, 56.000], mean action: 2.204 [0.000, 8.000], mean observation: 35.094 [0.001, 430.300], loss: 268.091003, mae: 37.221588, mean_q: -37.669979\n",
            " 1023291/10000000: episode: 5091, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -237.400, mean reward: -1.181 [-118.700, 136.800], mean action: 1.716 [0.000, 8.000], mean observation: 39.981 [0.000, 673.700], loss: 196.752335, mae: 37.023869, mean_q: -37.429268\n",
            " 1023492/10000000: episode: 5092, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -332.000, mean reward: -1.652 [-166.000, 38.800], mean action: 1.821 [0.000, 8.000], mean observation: 30.716 [0.001, 459.000], loss: 454.565613, mae: 36.282875, mean_q: -36.273296\n",
            " 1023693/10000000: episode: 5093, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -33.600, mean reward: -0.167 [-16.800, 188.000], mean action: 2.323 [0.000, 8.000], mean observation: 37.829 [0.001, 558.800], loss: 184.903381, mae: 35.609406, mean_q: -36.064404\n",
            " 1023894/10000000: episode: 5094, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 507.200, mean reward: 2.523 [-10.000, 328.800], mean action: 2.438 [0.000, 10.000], mean observation: 35.071 [0.000, 933.200], loss: 230.758362, mae: 35.478039, mean_q: -36.012661\n",
            " 1024095/10000000: episode: 5095, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -351.200, mean reward: -1.747 [-175.600, 50.500], mean action: 1.657 [0.000, 7.000], mean observation: 31.687 [0.000, 684.900], loss: 400.387024, mae: 35.124924, mean_q: -35.059338\n",
            " 1024296/10000000: episode: 5096, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: 90.200, mean reward: 0.449 [-7.000, 129.600], mean action: 1.413 [0.000, 7.000], mean observation: 31.790 [0.000, 567.300], loss: 389.564819, mae: 34.832634, mean_q: -34.460247\n",
            " 1024497/10000000: episode: 5097, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -362.800, mean reward: -1.805 [-181.400, 47.700], mean action: 1.721 [0.000, 7.000], mean observation: 30.829 [0.001, 556.700], loss: 210.905319, mae: 34.401859, mean_q: -34.266083\n",
            " 1024698/10000000: episode: 5098, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -563.000, mean reward: -2.801 [-281.500, 64.800], mean action: 2.259 [0.000, 7.000], mean observation: 36.387 [0.000, 444.500], loss: 290.482788, mae: 33.868874, mean_q: -33.969845\n",
            " 1024899/10000000: episode: 5099, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -248.200, mean reward: -1.235 [-124.100, 200.000], mean action: 2.229 [0.000, 8.000], mean observation: 35.228 [0.000, 607.900], loss: 209.624359, mae: 33.554390, mean_q: -33.716198\n",
            " 1025100/10000000: episode: 5100, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -274.400, mean reward: -1.365 [-137.200, 84.300], mean action: 2.050 [0.000, 8.000], mean observation: 36.081 [0.001, 528.400], loss: 477.559540, mae: 33.793407, mean_q: -33.867939\n",
            " 1025301/10000000: episode: 5101, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: 473.000, mean reward: 2.353 [-10.000, 399.000], mean action: 1.886 [0.000, 10.000], mean observation: 31.141 [0.000, 455.800], loss: 317.134827, mae: 33.483200, mean_q: -33.194225\n",
            " 1025502/10000000: episode: 5102, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -357.200, mean reward: -1.777 [-178.600, 96.000], mean action: 2.070 [0.000, 8.000], mean observation: 29.540 [0.000, 818.500], loss: 312.385986, mae: 32.503727, mean_q: -32.398514\n",
            " 1025703/10000000: episode: 5103, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -321.800, mean reward: -1.601 [-160.900, 61.500], mean action: 2.249 [0.000, 10.000], mean observation: 33.953 [0.002, 441.900], loss: 358.014923, mae: 32.366158, mean_q: -32.529480\n",
            " 1025904/10000000: episode: 5104, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -564.000, mean reward: -2.806 [-282.000, 30.000], mean action: 1.761 [0.000, 8.000], mean observation: 29.019 [0.001, 619.000], loss: 263.218140, mae: 32.408825, mean_q: -31.769764\n",
            " 1026105/10000000: episode: 5105, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: 244.400, mean reward: 1.216 [-8.000, 122.200], mean action: 1.562 [0.000, 8.000], mean observation: 33.800 [0.001, 412.100], loss: 155.887787, mae: 32.151646, mean_q: -31.482761\n",
            " 1026306/10000000: episode: 5106, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -426.400, mean reward: -2.121 [-213.200, 14.000], mean action: 1.408 [0.000, 6.000], mean observation: 35.380 [0.002, 402.500], loss: 220.042557, mae: 31.854973, mean_q: -31.046366\n",
            " 1026507/10000000: episode: 5107, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -420.600, mean reward: -2.093 [-210.300, 40.400], mean action: 1.557 [0.000, 8.000], mean observation: 35.408 [0.000, 501.600], loss: 198.390762, mae: 31.478409, mean_q: -30.911255\n",
            " 1026708/10000000: episode: 5108, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -374.400, mean reward: -1.863 [-187.200, 64.800], mean action: 2.000 [0.000, 8.000], mean observation: 34.714 [0.001, 492.600], loss: 410.316895, mae: 30.976450, mean_q: -30.714912\n",
            " 1026909/10000000: episode: 5109, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -719.200, mean reward: -3.578 [-359.600, 35.500], mean action: 2.547 [0.000, 8.000], mean observation: 32.008 [0.000, 590.900], loss: 409.451813, mae: 31.026814, mean_q: -31.084845\n",
            " 1027110/10000000: episode: 5110, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -493.800, mean reward: -2.457 [-246.900, 65.200], mean action: 2.711 [0.000, 10.000], mean observation: 36.469 [0.000, 747.100], loss: 318.458496, mae: 31.692616, mean_q: -31.973808\n",
            " 1027311/10000000: episode: 5111, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -252.800, mean reward: -1.258 [-126.400, 26.400], mean action: 1.502 [0.000, 10.000], mean observation: 30.502 [0.000, 462.000], loss: 412.669769, mae: 31.738651, mean_q: -31.041479\n",
            " 1027512/10000000: episode: 5112, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -189.000, mean reward: -0.940 [-94.500, 73.600], mean action: 1.905 [0.000, 8.000], mean observation: 30.540 [0.000, 534.800], loss: 274.639954, mae: 31.310518, mean_q: -30.473209\n",
            " 1027713/10000000: episode: 5113, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -162.400, mean reward: -0.808 [-81.200, 96.500], mean action: 2.159 [0.000, 10.000], mean observation: 29.382 [0.000, 580.300], loss: 228.168060, mae: 31.005714, mean_q: -30.551207\n",
            " 1027914/10000000: episode: 5114, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -487.400, mean reward: -2.425 [-243.700, 49.500], mean action: 2.030 [0.000, 8.000], mean observation: 38.353 [0.000, 522.100], loss: 301.548340, mae: 30.903057, mean_q: -30.510017\n",
            " 1028115/10000000: episode: 5115, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -71.600, mean reward: -0.356 [-35.800, 93.000], mean action: 2.134 [0.000, 8.000], mean observation: 26.656 [0.000, 769.000], loss: 246.019104, mae: 30.963926, mean_q: -30.491920\n",
            " 1028316/10000000: episode: 5116, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -475.800, mean reward: -2.367 [-237.900, 26.000], mean action: 1.746 [0.000, 10.000], mean observation: 37.353 [0.000, 813.800], loss: 266.595123, mae: 30.915928, mean_q: -30.444141\n",
            " 1028517/10000000: episode: 5117, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -140.400, mean reward: -0.699 [-70.200, 205.000], mean action: 1.736 [0.000, 7.000], mean observation: 33.558 [0.001, 632.400], loss: 199.976761, mae: 30.665033, mean_q: -30.169096\n",
            " 1028718/10000000: episode: 5118, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -260.200, mean reward: -1.295 [-130.100, 137.000], mean action: 1.731 [0.000, 8.000], mean observation: 33.340 [0.003, 531.400], loss: 364.155548, mae: 30.753450, mean_q: -29.876383\n",
            " 1028919/10000000: episode: 5119, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -350.800, mean reward: -1.745 [-175.400, 41.000], mean action: 1.746 [0.000, 8.000], mean observation: 34.340 [0.001, 519.900], loss: 150.191208, mae: 30.579865, mean_q: -30.059235\n",
            " 1029120/10000000: episode: 5120, duration: 1.389s, episode steps: 201, steps per second: 145, episode reward: -210.600, mean reward: -1.048 [-105.300, 142.000], mean action: 1.816 [0.000, 9.000], mean observation: 39.579 [0.001, 518.800], loss: 230.560135, mae: 30.515747, mean_q: -30.043726\n",
            " 1029321/10000000: episode: 5121, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 155.600, mean reward: 0.774 [-8.000, 132.000], mean action: 2.114 [0.000, 8.000], mean observation: 37.252 [0.001, 627.200], loss: 307.148315, mae: 30.566168, mean_q: -30.113201\n",
            " 1029522/10000000: episode: 5122, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -537.600, mean reward: -2.675 [-268.800, 37.900], mean action: 2.393 [0.000, 8.000], mean observation: 38.534 [0.001, 501.700], loss: 258.392944, mae: 29.978874, mean_q: -29.898846\n",
            " 1029723/10000000: episode: 5123, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: 247.800, mean reward: 1.233 [-8.000, 308.500], mean action: 1.980 [0.000, 8.000], mean observation: 32.870 [0.000, 604.500], loss: 187.204254, mae: 30.457966, mean_q: -30.335379\n",
            " 1029924/10000000: episode: 5124, duration: 1.377s, episode steps: 201, steps per second: 146, episode reward: -285.200, mean reward: -1.419 [-142.600, 52.000], mean action: 2.279 [0.000, 8.000], mean observation: 31.920 [0.001, 437.600], loss: 382.308258, mae: 30.770046, mean_q: -30.119888\n",
            " 1030125/10000000: episode: 5125, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -435.400, mean reward: -2.166 [-217.700, 33.000], mean action: 1.756 [0.000, 8.000], mean observation: 31.505 [0.001, 464.600], loss: 238.067703, mae: 30.643288, mean_q: -29.860502\n",
            " 1030326/10000000: episode: 5126, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -212.600, mean reward: -1.058 [-106.300, 86.400], mean action: 2.109 [0.000, 8.000], mean observation: 31.504 [0.001, 531.100], loss: 327.823975, mae: 30.312767, mean_q: -29.835287\n",
            " 1030527/10000000: episode: 5127, duration: 1.368s, episode steps: 201, steps per second: 147, episode reward: -362.800, mean reward: -1.805 [-181.400, 55.500], mean action: 1.607 [0.000, 8.000], mean observation: 36.175 [0.002, 439.400], loss: 372.040955, mae: 29.973289, mean_q: -29.014904\n",
            " 1030728/10000000: episode: 5128, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -425.200, mean reward: -2.115 [-212.600, 49.000], mean action: 2.040 [0.000, 10.000], mean observation: 33.395 [0.000, 784.200], loss: 180.113068, mae: 29.571354, mean_q: -29.080788\n",
            " 1030929/10000000: episode: 5129, duration: 1.391s, episode steps: 201, steps per second: 144, episode reward: -355.400, mean reward: -1.768 [-177.700, 24.500], mean action: 1.622 [0.000, 7.000], mean observation: 38.617 [0.000, 722.600], loss: 436.272156, mae: 29.699841, mean_q: -28.462986\n",
            " 1031130/10000000: episode: 5130, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: -582.600, mean reward: -2.899 [-291.300, 17.200], mean action: 1.806 [0.000, 7.000], mean observation: 30.865 [0.001, 542.200], loss: 242.693329, mae: 29.532845, mean_q: -28.469658\n",
            " 1031331/10000000: episode: 5131, duration: 1.385s, episode steps: 201, steps per second: 145, episode reward: -34.400, mean reward: -0.171 [-17.200, 100.800], mean action: 1.761 [0.000, 8.000], mean observation: 37.761 [0.001, 527.300], loss: 227.938782, mae: 29.915710, mean_q: -29.098793\n",
            " 1031532/10000000: episode: 5132, duration: 1.384s, episode steps: 201, steps per second: 145, episode reward: -643.800, mean reward: -3.203 [-321.900, 24.000], mean action: 2.169 [0.000, 10.000], mean observation: 28.142 [0.000, 529.100], loss: 280.563873, mae: 29.521271, mean_q: -29.061127\n",
            " 1031733/10000000: episode: 5133, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -483.800, mean reward: -2.407 [-241.900, 22.200], mean action: 1.950 [0.000, 10.000], mean observation: 32.162 [0.000, 694.400], loss: 202.079987, mae: 29.828800, mean_q: -29.457785\n",
            " 1031934/10000000: episode: 5134, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -762.200, mean reward: -3.792 [-381.100, 17.500], mean action: 2.229 [0.000, 10.000], mean observation: 40.511 [0.001, 598.300], loss: 407.070648, mae: 29.780264, mean_q: -29.281357\n",
            " 1032135/10000000: episode: 5135, duration: 1.451s, episode steps: 201, steps per second: 138, episode reward: -469.600, mean reward: -2.336 [-234.800, 34.900], mean action: 1.642 [0.000, 7.000], mean observation: 33.133 [0.000, 555.700], loss: 227.484009, mae: 29.492664, mean_q: -28.616707\n",
            " 1032336/10000000: episode: 5136, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -204.800, mean reward: -1.019 [-102.400, 98.000], mean action: 2.010 [0.000, 10.000], mean observation: 28.020 [0.003, 492.800], loss: 322.802063, mae: 29.111441, mean_q: -28.224041\n",
            " 1032537/10000000: episode: 5137, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -290.600, mean reward: -1.446 [-145.300, 94.200], mean action: 2.144 [0.000, 7.000], mean observation: 38.137 [0.000, 669.900], loss: 155.080231, mae: 28.705698, mean_q: -28.616911\n",
            " 1032738/10000000: episode: 5138, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -332.600, mean reward: -1.655 [-166.300, 98.400], mean action: 2.348 [0.000, 10.000], mean observation: 37.184 [0.001, 662.900], loss: 231.006775, mae: 28.848278, mean_q: -28.527897\n",
            " 1032939/10000000: episode: 5139, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -422.400, mean reward: -2.101 [-211.200, 48.500], mean action: 1.731 [0.000, 7.000], mean observation: 34.507 [0.002, 482.600], loss: 117.979630, mae: 29.253473, mean_q: -28.681747\n",
            " 1033140/10000000: episode: 5140, duration: 1.381s, episode steps: 201, steps per second: 146, episode reward: 9.200, mean reward: 0.046 [-7.000, 294.400], mean action: 1.866 [0.000, 7.000], mean observation: 39.012 [0.000, 654.700], loss: 291.449188, mae: 29.828300, mean_q: -29.012133\n",
            " 1033341/10000000: episode: 5141, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -241.400, mean reward: -1.201 [-120.700, 127.500], mean action: 2.259 [0.000, 8.000], mean observation: 34.765 [0.000, 706.400], loss: 304.583984, mae: 29.478779, mean_q: -29.076170\n",
            " 1033542/10000000: episode: 5142, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -418.400, mean reward: -2.082 [-209.200, 203.000], mean action: 2.672 [0.000, 10.000], mean observation: 30.955 [0.001, 480.700], loss: 259.839539, mae: 29.474857, mean_q: -29.425282\n",
            " 1033743/10000000: episode: 5143, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -282.000, mean reward: -1.403 [-141.000, 148.500], mean action: 2.498 [0.000, 8.000], mean observation: 34.892 [0.001, 422.500], loss: 283.941162, mae: 30.094467, mean_q: -30.303070\n",
            " 1033944/10000000: episode: 5144, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: 92.200, mean reward: 0.459 [-8.000, 409.000], mean action: 2.378 [0.000, 8.000], mean observation: 32.275 [0.001, 676.900], loss: 330.935699, mae: 29.973011, mean_q: -30.178711\n",
            " 1034145/10000000: episode: 5145, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -311.000, mean reward: -1.547 [-155.500, 84.500], mean action: 2.632 [0.000, 10.000], mean observation: 26.519 [0.007, 497.200], loss: 287.658630, mae: 29.877060, mean_q: -29.888004\n",
            " 1034346/10000000: episode: 5146, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -114.800, mean reward: -0.571 [-57.400, 159.200], mean action: 1.861 [0.000, 10.000], mean observation: 43.954 [0.000, 714.300], loss: 211.262238, mae: 30.179264, mean_q: -29.953024\n",
            " 1034547/10000000: episode: 5147, duration: 1.391s, episode steps: 201, steps per second: 144, episode reward: -398.600, mean reward: -1.983 [-199.300, 40.800], mean action: 1.652 [0.000, 7.000], mean observation: 33.371 [0.000, 582.800], loss: 196.635178, mae: 30.129572, mean_q: -29.809656\n",
            " 1034748/10000000: episode: 5148, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -180.800, mean reward: -0.900 [-90.400, 60.400], mean action: 1.667 [0.000, 9.000], mean observation: 33.435 [0.000, 566.700], loss: 175.644302, mae: 30.190687, mean_q: -29.762754\n",
            " 1034949/10000000: episode: 5149, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -492.200, mean reward: -2.449 [-246.100, 48.000], mean action: 2.134 [0.000, 7.000], mean observation: 30.152 [0.001, 498.800], loss: 233.761765, mae: 30.040651, mean_q: -30.084305\n",
            " 1035150/10000000: episode: 5150, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -190.200, mean reward: -0.946 [-95.100, 65.100], mean action: 1.891 [0.000, 8.000], mean observation: 32.136 [0.002, 508.500], loss: 162.340698, mae: 30.457798, mean_q: -30.327679\n",
            " 1035351/10000000: episode: 5151, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: 108.400, mean reward: 0.539 [-10.000, 175.500], mean action: 1.960 [0.000, 10.000], mean observation: 31.725 [0.000, 413.800], loss: 234.616196, mae: 30.788429, mean_q: -30.780235\n",
            " 1035552/10000000: episode: 5152, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -666.800, mean reward: -3.317 [-333.400, 33.500], mean action: 2.199 [0.000, 10.000], mean observation: 37.726 [0.000, 451.700], loss: 250.542755, mae: 31.024475, mean_q: -31.368362\n",
            " 1035753/10000000: episode: 5153, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -320.000, mean reward: -1.592 [-160.000, 120.200], mean action: 2.055 [0.000, 10.000], mean observation: 34.841 [0.001, 630.500], loss: 327.031647, mae: 30.786743, mean_q: -30.728817\n",
            " 1035954/10000000: episode: 5154, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -281.600, mean reward: -1.401 [-140.800, 52.400], mean action: 1.682 [0.000, 8.000], mean observation: 31.478 [0.001, 527.300], loss: 207.430344, mae: 30.663820, mean_q: -30.419746\n",
            " 1036155/10000000: episode: 5155, duration: 1.686s, episode steps: 201, steps per second: 119, episode reward: -178.000, mean reward: -0.886 [-89.000, 111.100], mean action: 2.119 [0.000, 10.000], mean observation: 34.905 [0.000, 562.200], loss: 160.326859, mae: 30.550835, mean_q: -30.621902\n",
            " 1036356/10000000: episode: 5156, duration: 1.669s, episode steps: 201, steps per second: 120, episode reward: -59.000, mean reward: -0.294 [-29.500, 138.800], mean action: 1.687 [0.000, 10.000], mean observation: 32.801 [0.002, 637.200], loss: 369.819061, mae: 30.639372, mean_q: -30.320410\n",
            " 1036557/10000000: episode: 5157, duration: 1.682s, episode steps: 201, steps per second: 120, episode reward: -182.400, mean reward: -0.907 [-91.200, 83.400], mean action: 1.856 [0.000, 10.000], mean observation: 26.236 [0.003, 436.400], loss: 434.345062, mae: 30.616920, mean_q: -30.463619\n",
            " 1036758/10000000: episode: 5158, duration: 1.675s, episode steps: 201, steps per second: 120, episode reward: 226.600, mean reward: 1.127 [-10.000, 120.000], mean action: 2.154 [0.000, 10.000], mean observation: 31.398 [0.001, 607.700], loss: 152.630768, mae: 30.530531, mean_q: -30.807312\n",
            " 1036959/10000000: episode: 5159, duration: 1.640s, episode steps: 201, steps per second: 123, episode reward: -423.000, mean reward: -2.104 [-211.500, 100.200], mean action: 2.199 [0.000, 10.000], mean observation: 32.702 [0.001, 462.800], loss: 132.251282, mae: 30.772917, mean_q: -31.124830\n",
            " 1037160/10000000: episode: 5160, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -624.200, mean reward: -3.105 [-312.100, 43.800], mean action: 2.403 [0.000, 10.000], mean observation: 38.006 [0.003, 524.500], loss: 236.750732, mae: 31.455462, mean_q: -31.870506\n",
            " 1037361/10000000: episode: 5161, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: 249.800, mean reward: 1.243 [-10.000, 266.700], mean action: 2.458 [0.000, 10.000], mean observation: 28.555 [0.000, 772.000], loss: 214.182297, mae: 32.155144, mean_q: -32.432053\n",
            " 1037562/10000000: episode: 5162, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -661.000, mean reward: -3.289 [-330.500, 27.000], mean action: 2.055 [0.000, 10.000], mean observation: 33.011 [0.000, 558.800], loss: 178.920654, mae: 31.729424, mean_q: -31.750380\n",
            " 1037763/10000000: episode: 5163, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: 424.400, mean reward: 2.111 [-10.000, 236.800], mean action: 1.781 [0.000, 10.000], mean observation: 33.545 [0.001, 501.100], loss: 245.117920, mae: 31.388821, mean_q: -31.221159\n",
            " 1037964/10000000: episode: 5164, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -5.000, mean reward: -0.025 [-7.000, 56.400], mean action: 1.393 [0.000, 10.000], mean observation: 32.916 [0.001, 645.600], loss: 136.188583, mae: 31.388592, mean_q: -30.923695\n",
            " 1038165/10000000: episode: 5165, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -132.800, mean reward: -0.661 [-66.400, 59.000], mean action: 2.149 [0.000, 10.000], mean observation: 31.990 [0.001, 516.000], loss: 217.420471, mae: 30.829702, mean_q: -30.807091\n",
            " 1038366/10000000: episode: 5166, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -488.200, mean reward: -2.429 [-244.100, 36.500], mean action: 2.015 [0.000, 10.000], mean observation: 36.120 [0.000, 555.400], loss: 228.967422, mae: 30.982344, mean_q: -31.031012\n",
            " 1038567/10000000: episode: 5167, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -473.600, mean reward: -2.356 [-236.800, 34.000], mean action: 1.741 [0.000, 10.000], mean observation: 32.884 [0.000, 553.100], loss: 384.819336, mae: 30.396715, mean_q: -30.084339\n",
            " 1038768/10000000: episode: 5168, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -36.800, mean reward: -0.183 [-18.400, 112.800], mean action: 2.065 [0.000, 10.000], mean observation: 37.874 [0.000, 668.100], loss: 172.949738, mae: 30.555513, mean_q: -30.477888\n",
            " 1038969/10000000: episode: 5169, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -539.400, mean reward: -2.684 [-269.700, 27.000], mean action: 1.970 [0.000, 10.000], mean observation: 29.525 [0.000, 483.800], loss: 205.641205, mae: 30.814798, mean_q: -30.626036\n",
            " 1039170/10000000: episode: 5170, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: 223.000, mean reward: 1.109 [-10.000, 261.600], mean action: 2.353 [0.000, 10.000], mean observation: 28.291 [0.002, 513.300], loss: 291.527588, mae: 30.285271, mean_q: -30.367601\n",
            " 1039371/10000000: episode: 5171, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -326.200, mean reward: -1.623 [-163.100, 29.400], mean action: 1.244 [0.000, 10.000], mean observation: 36.697 [0.000, 606.600], loss: 352.108765, mae: 30.812742, mean_q: -29.666914\n",
            " 1039572/10000000: episode: 5172, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -254.400, mean reward: -1.266 [-127.200, 61.700], mean action: 1.627 [0.000, 7.000], mean observation: 37.789 [0.001, 440.800], loss: 242.792252, mae: 30.206816, mean_q: -29.417900\n",
            " 1039773/10000000: episode: 5173, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -435.800, mean reward: -2.168 [-217.900, 14.000], mean action: 1.438 [0.000, 7.000], mean observation: 32.920 [0.002, 578.200], loss: 418.304169, mae: 29.451666, mean_q: -28.704773\n",
            " 1039974/10000000: episode: 5174, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -444.800, mean reward: -2.213 [-222.400, 20.300], mean action: 1.398 [0.000, 9.000], mean observation: 32.162 [0.001, 478.600], loss: 227.002274, mae: 29.052361, mean_q: -28.085903\n",
            " 1040175/10000000: episode: 5175, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -132.400, mean reward: -0.659 [-66.200, 68.400], mean action: 1.159 [0.000, 10.000], mean observation: 37.207 [0.000, 527.200], loss: 224.061844, mae: 29.042381, mean_q: -28.023918\n",
            " 1040376/10000000: episode: 5176, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: 27.400, mean reward: 0.136 [-7.000, 122.400], mean action: 1.617 [0.000, 7.000], mean observation: 31.770 [0.001, 675.300], loss: 180.020752, mae: 28.863529, mean_q: -28.253763\n",
            " 1040577/10000000: episode: 5177, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -329.000, mean reward: -1.637 [-164.500, 78.400], mean action: 1.771 [0.000, 10.000], mean observation: 30.100 [0.004, 432.400], loss: 215.190109, mae: 29.248581, mean_q: -28.424669\n",
            " 1040778/10000000: episode: 5178, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -25.000, mean reward: -0.124 [-12.500, 161.700], mean action: 1.542 [0.000, 8.000], mean observation: 31.972 [0.000, 527.500], loss: 309.801819, mae: 29.448126, mean_q: -28.630758\n",
            " 1040979/10000000: episode: 5179, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -721.800, mean reward: -3.591 [-360.900, 28.100], mean action: 2.338 [0.000, 10.000], mean observation: 35.852 [0.000, 544.700], loss: 295.556763, mae: 29.335203, mean_q: -29.198172\n",
            " 1041180/10000000: episode: 5180, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: 255.800, mean reward: 1.273 [-10.000, 127.900], mean action: 2.030 [0.000, 10.000], mean observation: 36.024 [0.002, 532.900], loss: 141.940521, mae: 29.627077, mean_q: -29.473740\n",
            " 1041381/10000000: episode: 5181, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -284.400, mean reward: -1.415 [-142.200, 97.200], mean action: 2.323 [0.000, 10.000], mean observation: 33.918 [0.001, 647.400], loss: 234.163162, mae: 29.751820, mean_q: -29.977705\n",
            " 1041582/10000000: episode: 5182, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -403.200, mean reward: -2.006 [-201.600, 68.000], mean action: 1.886 [0.000, 10.000], mean observation: 33.465 [0.001, 602.400], loss: 313.371185, mae: 29.961836, mean_q: -30.048172\n",
            " 1041783/10000000: episode: 5183, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -527.000, mean reward: -2.622 [-263.500, 51.900], mean action: 2.050 [0.000, 10.000], mean observation: 33.284 [0.000, 453.500], loss: 269.110413, mae: 30.029074, mean_q: -30.008333\n",
            " 1041984/10000000: episode: 5184, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -340.200, mean reward: -1.693 [-170.100, 36.600], mean action: 1.945 [0.000, 10.000], mean observation: 28.624 [0.002, 364.300], loss: 214.838791, mae: 30.147655, mean_q: -29.790409\n",
            " 1042185/10000000: episode: 5185, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: 198.800, mean reward: 0.989 [-10.000, 106.200], mean action: 1.975 [0.000, 10.000], mean observation: 29.581 [0.000, 695.400], loss: 284.760132, mae: 30.058126, mean_q: -29.774708\n",
            " 1042386/10000000: episode: 5186, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -206.800, mean reward: -1.029 [-103.400, 117.500], mean action: 2.328 [0.000, 10.000], mean observation: 33.729 [0.000, 660.900], loss: 161.118057, mae: 29.771309, mean_q: -29.812082\n",
            " 1042587/10000000: episode: 5187, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: 42.000, mean reward: 0.209 [-10.000, 190.800], mean action: 2.299 [0.000, 10.000], mean observation: 35.092 [0.000, 525.900], loss: 146.643875, mae: 29.897606, mean_q: -30.183193\n",
            " 1042788/10000000: episode: 5188, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -71.000, mean reward: -0.353 [-35.500, 166.600], mean action: 2.905 [0.000, 10.000], mean observation: 34.857 [0.001, 446.900], loss: 177.694473, mae: 30.067310, mean_q: -30.808914\n",
            " 1042989/10000000: episode: 5189, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -53.200, mean reward: -0.265 [-26.600, 199.500], mean action: 2.463 [0.000, 10.000], mean observation: 32.862 [0.000, 386.400], loss: 214.065140, mae: 30.688883, mean_q: -31.218584\n",
            " 1043190/10000000: episode: 5190, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: 599.800, mean reward: 2.984 [-10.000, 468.300], mean action: 2.383 [0.000, 10.000], mean observation: 30.067 [0.001, 654.600], loss: 222.261978, mae: 30.513769, mean_q: -30.802448\n",
            " 1043391/10000000: episode: 5191, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: 36.600, mean reward: 0.182 [-10.000, 192.000], mean action: 2.498 [0.000, 10.000], mean observation: 36.889 [0.000, 467.400], loss: 220.598587, mae: 31.197081, mean_q: -31.781479\n",
            " 1043592/10000000: episode: 5192, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -240.200, mean reward: -1.195 [-120.100, 134.400], mean action: 1.960 [0.000, 10.000], mean observation: 35.242 [0.000, 697.600], loss: 279.717560, mae: 31.553211, mean_q: -31.953718\n",
            " 1043793/10000000: episode: 5193, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -353.600, mean reward: -1.759 [-176.800, 93.300], mean action: 2.537 [0.000, 10.000], mean observation: 30.915 [0.000, 499.800], loss: 243.314758, mae: 31.363432, mean_q: -31.787998\n",
            " 1043994/10000000: episode: 5194, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -520.600, mean reward: -2.590 [-260.300, 117.000], mean action: 2.776 [0.000, 10.000], mean observation: 31.573 [0.001, 443.500], loss: 142.284424, mae: 31.468662, mean_q: -32.004356\n",
            " 1044195/10000000: episode: 5195, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -277.200, mean reward: -1.379 [-138.600, 93.500], mean action: 2.463 [0.000, 10.000], mean observation: 34.123 [0.001, 470.800], loss: 170.304108, mae: 31.863653, mean_q: -32.259872\n",
            " 1044396/10000000: episode: 5196, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: 17.400, mean reward: 0.087 [-10.000, 196.000], mean action: 2.035 [0.000, 10.000], mean observation: 29.570 [0.002, 427.500], loss: 370.109772, mae: 31.854057, mean_q: -31.874353\n",
            " 1044597/10000000: episode: 5197, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -322.800, mean reward: -1.606 [-161.400, 124.800], mean action: 2.572 [0.000, 10.000], mean observation: 33.115 [0.002, 515.900], loss: 170.093018, mae: 31.402145, mean_q: -31.952129\n",
            " 1044798/10000000: episode: 5198, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -128.000, mean reward: -0.637 [-64.000, 90.500], mean action: 2.010 [0.000, 10.000], mean observation: 34.584 [0.002, 467.700], loss: 204.338638, mae: 31.951059, mean_q: -32.091537\n",
            " 1044999/10000000: episode: 5199, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -358.200, mean reward: -1.782 [-179.100, 60.600], mean action: 2.060 [0.000, 8.000], mean observation: 27.355 [0.002, 434.600], loss: 158.512375, mae: 31.836405, mean_q: -32.197514\n",
            " 1045200/10000000: episode: 5200, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -4.200, mean reward: -0.021 [-10.000, 153.000], mean action: 2.408 [0.000, 10.000], mean observation: 32.524 [0.000, 638.600], loss: 181.766556, mae: 31.745808, mean_q: -32.457321\n",
            " 1045401/10000000: episode: 5201, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -449.600, mean reward: -2.237 [-224.800, 67.800], mean action: 2.154 [0.000, 10.000], mean observation: 34.245 [0.000, 407.100], loss: 168.576904, mae: 31.929464, mean_q: -32.488392\n",
            " 1045602/10000000: episode: 5202, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -438.200, mean reward: -2.180 [-219.100, 87.000], mean action: 1.995 [0.000, 10.000], mean observation: 35.950 [0.000, 530.500], loss: 282.114777, mae: 31.802511, mean_q: -31.908188\n",
            " 1045803/10000000: episode: 5203, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -472.400, mean reward: -2.350 [-236.200, 17.400], mean action: 1.572 [0.000, 10.000], mean observation: 33.612 [0.000, 796.200], loss: 135.959396, mae: 31.301580, mean_q: -31.364563\n",
            " 1046004/10000000: episode: 5204, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: 42.600, mean reward: 0.212 [-10.000, 392.000], mean action: 1.990 [0.000, 10.000], mean observation: 38.156 [0.000, 588.400], loss: 206.072021, mae: 31.486664, mean_q: -31.671776\n",
            " 1046205/10000000: episode: 5205, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -442.200, mean reward: -2.200 [-221.100, 185.000], mean action: 2.692 [0.000, 10.000], mean observation: 29.926 [0.001, 466.300], loss: 129.778259, mae: 31.388371, mean_q: -31.954670\n",
            " 1046406/10000000: episode: 5206, duration: 2.169s, episode steps: 201, steps per second: 93, episode reward: -603.400, mean reward: -3.002 [-301.700, 71.500], mean action: 2.473 [0.000, 10.000], mean observation: 33.400 [0.003, 438.400], loss: 233.706650, mae: 31.636301, mean_q: -32.196915\n",
            " 1046607/10000000: episode: 5207, duration: 1.853s, episode steps: 201, steps per second: 108, episode reward: -768.800, mean reward: -3.825 [-384.400, 34.000], mean action: 2.791 [0.000, 10.000], mean observation: 28.393 [0.002, 545.200], loss: 146.651428, mae: 31.632132, mean_q: -32.531342\n",
            " 1046808/10000000: episode: 5208, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -793.600, mean reward: -3.948 [-396.800, 34.200], mean action: 2.637 [0.000, 10.000], mean observation: 34.574 [0.001, 491.000], loss: 143.794434, mae: 32.181953, mean_q: -33.200729\n",
            " 1047009/10000000: episode: 5209, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: 351.000, mean reward: 1.746 [-10.000, 403.800], mean action: 2.716 [0.000, 10.000], mean observation: 30.581 [0.000, 452.700], loss: 161.054413, mae: 32.506645, mean_q: -33.525787\n",
            " 1047210/10000000: episode: 5210, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -656.600, mean reward: -3.267 [-328.300, 27.600], mean action: 1.940 [0.000, 10.000], mean observation: 34.954 [0.002, 481.400], loss: 163.729980, mae: 32.733685, mean_q: -33.345993\n",
            " 1047411/10000000: episode: 5211, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -251.400, mean reward: -1.251 [-125.700, 170.000], mean action: 2.697 [0.000, 10.000], mean observation: 28.610 [0.000, 746.900], loss: 228.960312, mae: 32.012844, mean_q: -32.781113\n",
            " 1047612/10000000: episode: 5212, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -547.400, mean reward: -2.723 [-273.700, 126.000], mean action: 2.682 [0.000, 10.000], mean observation: 33.728 [0.000, 609.400], loss: 233.250656, mae: 32.119812, mean_q: -32.901585\n",
            " 1047813/10000000: episode: 5213, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: 839.600, mean reward: 4.177 [-10.000, 614.400], mean action: 2.652 [0.000, 10.000], mean observation: 30.856 [0.000, 702.100], loss: 180.416000, mae: 32.154381, mean_q: -32.941669\n",
            " 1048014/10000000: episode: 5214, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -359.000, mean reward: -1.786 [-179.500, 59.000], mean action: 2.507 [0.000, 10.000], mean observation: 34.474 [0.000, 472.400], loss: 179.459747, mae: 32.062672, mean_q: -32.618534\n",
            " 1048215/10000000: episode: 5215, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 427.600, mean reward: 2.127 [-10.000, 298.200], mean action: 1.836 [0.000, 10.000], mean observation: 38.916 [0.000, 653.600], loss: 438.203979, mae: 32.027496, mean_q: -32.238079\n",
            " 1048416/10000000: episode: 5216, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -209.800, mean reward: -1.044 [-104.900, 81.000], mean action: 2.075 [0.000, 10.000], mean observation: 27.138 [0.000, 509.800], loss: 258.971313, mae: 31.861330, mean_q: -31.898054\n",
            " 1048617/10000000: episode: 5217, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -384.200, mean reward: -1.911 [-192.100, 54.600], mean action: 1.786 [0.000, 10.000], mean observation: 39.013 [0.002, 630.900], loss: 193.580688, mae: 31.549458, mean_q: -31.821589\n",
            " 1048818/10000000: episode: 5218, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: 670.800, mean reward: 3.337 [-10.000, 421.500], mean action: 1.995 [0.000, 10.000], mean observation: 33.356 [0.001, 571.000], loss: 145.746292, mae: 32.001411, mean_q: -32.219936\n",
            " 1049019/10000000: episode: 5219, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -647.600, mean reward: -3.222 [-323.800, 33.600], mean action: 2.547 [0.000, 10.000], mean observation: 36.859 [0.001, 591.000], loss: 302.068298, mae: 31.529146, mean_q: -32.004925\n",
            " 1049220/10000000: episode: 5220, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: 313.400, mean reward: 1.559 [-8.000, 206.000], mean action: 2.468 [0.000, 8.000], mean observation: 30.957 [0.001, 595.600], loss: 381.460907, mae: 31.797636, mean_q: -32.059109\n",
            " 1049421/10000000: episode: 5221, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -254.800, mean reward: -1.268 [-127.400, 72.000], mean action: 2.239 [0.000, 10.000], mean observation: 34.263 [0.000, 530.000], loss: 323.005035, mae: 31.857607, mean_q: -32.148907\n",
            " 1049622/10000000: episode: 5222, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: 1275.400, mean reward: 6.345 [-8.000, 637.700], mean action: 2.308 [0.000, 8.000], mean observation: 30.283 [0.001, 453.300], loss: 235.240707, mae: 31.848295, mean_q: -32.043743\n",
            " 1049823/10000000: episode: 5223, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -677.200, mean reward: -3.369 [-338.600, 60.500], mean action: 2.910 [0.000, 10.000], mean observation: 32.062 [0.006, 504.200], loss: 226.951614, mae: 31.650639, mean_q: -31.985792\n",
            " 1050024/10000000: episode: 5224, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -897.400, mean reward: -4.465 [-448.700, 12.700], mean action: 2.692 [0.000, 10.000], mean observation: 31.185 [0.001, 591.100], loss: 211.762360, mae: 31.661020, mean_q: -32.150848\n",
            " 1050225/10000000: episode: 5225, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -174.200, mean reward: -0.867 [-87.100, 176.100], mean action: 2.627 [0.000, 10.000], mean observation: 33.672 [0.001, 578.100], loss: 142.697235, mae: 31.986853, mean_q: -32.600597\n",
            " 1050426/10000000: episode: 5226, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: 536.400, mean reward: 2.669 [-10.000, 526.400], mean action: 1.741 [0.000, 10.000], mean observation: 34.706 [0.001, 598.700], loss: 271.887085, mae: 32.121960, mean_q: -32.046864\n",
            " 1050627/10000000: episode: 5227, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -122.200, mean reward: -0.608 [-61.100, 67.000], mean action: 1.791 [0.000, 10.000], mean observation: 28.307 [0.003, 308.400], loss: 140.751770, mae: 31.856667, mean_q: -31.627769\n",
            " 1050828/10000000: episode: 5228, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: 427.800, mean reward: 2.128 [-10.000, 308.000], mean action: 2.060 [0.000, 10.000], mean observation: 32.839 [0.000, 540.000], loss: 180.832520, mae: 31.442348, mean_q: -31.631208\n",
            " 1051029/10000000: episode: 5229, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -36.600, mean reward: -0.182 [-18.300, 236.000], mean action: 2.915 [0.000, 10.000], mean observation: 29.911 [0.000, 663.800], loss: 273.816711, mae: 30.968388, mean_q: -31.774691\n",
            " 1051230/10000000: episode: 5230, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -780.200, mean reward: -3.882 [-390.100, 56.000], mean action: 2.692 [0.000, 8.000], mean observation: 31.491 [0.002, 477.000], loss: 346.605103, mae: 31.058796, mean_q: -31.477058\n",
            " 1051431/10000000: episode: 5231, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -491.600, mean reward: -2.446 [-245.800, 100.100], mean action: 2.169 [0.000, 10.000], mean observation: 34.455 [0.000, 639.500], loss: 277.949097, mae: 30.713469, mean_q: -30.943846\n",
            " 1051632/10000000: episode: 5232, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -30.400, mean reward: -0.151 [-15.200, 121.200], mean action: 1.940 [0.000, 10.000], mean observation: 36.270 [0.001, 619.200], loss: 285.369446, mae: 30.463959, mean_q: -30.322435\n",
            " 1051833/10000000: episode: 5233, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -26.400, mean reward: -0.131 [-13.200, 183.000], mean action: 1.811 [0.000, 8.000], mean observation: 29.686 [0.001, 651.100], loss: 210.759293, mae: 29.745321, mean_q: -29.439394\n",
            " 1052034/10000000: episode: 5234, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -755.600, mean reward: -3.759 [-377.800, 42.500], mean action: 2.507 [0.000, 10.000], mean observation: 38.144 [0.000, 818.100], loss: 238.715912, mae: 29.223089, mean_q: -29.386044\n",
            " 1052235/10000000: episode: 5235, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -251.400, mean reward: -1.251 [-125.700, 201.600], mean action: 2.706 [0.000, 10.000], mean observation: 32.580 [0.002, 429.000], loss: 232.408157, mae: 28.991388, mean_q: -29.358593\n",
            " 1052436/10000000: episode: 5236, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -372.400, mean reward: -1.853 [-186.200, 143.500], mean action: 2.169 [0.000, 10.000], mean observation: 33.976 [0.001, 565.900], loss: 161.502853, mae: 29.315208, mean_q: -29.381924\n",
            " 1052637/10000000: episode: 5237, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -635.600, mean reward: -3.162 [-317.800, 37.000], mean action: 2.005 [0.000, 10.000], mean observation: 33.448 [0.002, 446.700], loss: 188.640503, mae: 29.871372, mean_q: -29.831779\n",
            " 1052838/10000000: episode: 5238, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -654.600, mean reward: -3.257 [-327.300, 41.000], mean action: 2.289 [0.000, 10.000], mean observation: 33.003 [0.000, 720.900], loss: 297.495758, mae: 29.655279, mean_q: -29.552279\n",
            " 1053039/10000000: episode: 5239, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -116.600, mean reward: -0.580 [-58.300, 205.600], mean action: 2.980 [0.000, 10.000], mean observation: 31.251 [0.002, 624.500], loss: 286.386200, mae: 29.575205, mean_q: -29.518127\n",
            " 1053240/10000000: episode: 5240, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -170.800, mean reward: -0.850 [-85.400, 126.000], mean action: 2.234 [0.000, 10.000], mean observation: 34.541 [0.003, 497.700], loss: 341.929962, mae: 29.469179, mean_q: -28.839661\n",
            " 1053441/10000000: episode: 5241, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -445.400, mean reward: -2.216 [-222.700, 57.000], mean action: 2.015 [0.000, 10.000], mean observation: 24.833 [0.001, 462.800], loss: 157.191589, mae: 29.150915, mean_q: -28.432262\n",
            " 1053642/10000000: episode: 5242, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -105.400, mean reward: -0.524 [-52.700, 93.500], mean action: 1.652 [0.000, 10.000], mean observation: 38.325 [0.000, 476.600], loss: 180.018387, mae: 28.875383, mean_q: -28.002167\n",
            " 1053843/10000000: episode: 5243, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -397.600, mean reward: -1.978 [-198.800, 23.100], mean action: 1.632 [0.000, 8.000], mean observation: 37.680 [0.000, 642.700], loss: 312.673462, mae: 29.073080, mean_q: -28.337872\n",
            " 1054044/10000000: episode: 5244, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: 145.200, mean reward: 0.722 [-10.000, 153.600], mean action: 1.642 [0.000, 10.000], mean observation: 35.197 [0.000, 907.100], loss: 196.736282, mae: 28.846375, mean_q: -28.072680\n",
            " 1054245/10000000: episode: 5245, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: 598.000, mean reward: 2.975 [-7.000, 492.500], mean action: 1.502 [0.000, 7.000], mean observation: 31.724 [0.004, 399.700], loss: 153.805679, mae: 28.848482, mean_q: -28.392389\n",
            " 1054446/10000000: episode: 5246, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -373.800, mean reward: -1.860 [-186.900, 127.800], mean action: 2.323 [0.000, 10.000], mean observation: 34.079 [0.001, 522.800], loss: 147.368790, mae: 29.197533, mean_q: -29.067919\n",
            " 1054647/10000000: episode: 5247, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 30.600, mean reward: 0.152 [-10.000, 288.400], mean action: 2.050 [0.000, 10.000], mean observation: 30.302 [0.000, 476.000], loss: 293.360565, mae: 29.141577, mean_q: -28.938841\n",
            " 1054848/10000000: episode: 5248, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: 35.000, mean reward: 0.174 [-10.000, 84.000], mean action: 1.751 [0.000, 10.000], mean observation: 30.160 [0.001, 623.900], loss: 225.241333, mae: 29.227495, mean_q: -28.469772\n",
            " 1055049/10000000: episode: 5249, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -451.200, mean reward: -2.245 [-225.600, 22.800], mean action: 1.701 [0.000, 10.000], mean observation: 30.968 [0.002, 542.000], loss: 333.865906, mae: 29.105904, mean_q: -28.286486\n",
            " 1055250/10000000: episode: 5250, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: 1030.200, mean reward: 5.125 [-10.000, 563.200], mean action: 2.692 [0.000, 10.000], mean observation: 39.915 [0.000, 515.500], loss: 326.310181, mae: 28.240129, mean_q: -28.259884\n",
            " 1055451/10000000: episode: 5251, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: 33.200, mean reward: 0.165 [-10.000, 291.000], mean action: 3.159 [0.000, 10.000], mean observation: 27.549 [0.000, 781.200], loss: 186.076843, mae: 27.985342, mean_q: -28.485895\n",
            " 1055652/10000000: episode: 5252, duration: 1.569s, episode steps: 201, steps per second: 128, episode reward: 10.800, mean reward: 0.054 [-10.000, 282.000], mean action: 3.050 [0.000, 10.000], mean observation: 31.792 [0.001, 492.700], loss: 200.534470, mae: 28.468653, mean_q: -29.088158\n",
            " 1055853/10000000: episode: 5253, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -409.600, mean reward: -2.038 [-204.800, 84.000], mean action: 2.726 [0.000, 10.000], mean observation: 35.686 [0.002, 467.200], loss: 335.934052, mae: 28.466492, mean_q: -28.826910\n",
            " 1056054/10000000: episode: 5254, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: 548.000, mean reward: 2.726 [-10.000, 337.500], mean action: 2.567 [0.000, 10.000], mean observation: 33.508 [0.000, 511.400], loss: 266.879578, mae: 28.368496, mean_q: -28.482315\n",
            " 1056255/10000000: episode: 5255, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -878.800, mean reward: -4.372 [-439.400, 68.000], mean action: 3.537 [0.000, 10.000], mean observation: 34.238 [0.000, 764.600], loss: 216.491562, mae: 27.620855, mean_q: -28.443760\n",
            " 1056456/10000000: episode: 5256, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -472.800, mean reward: -2.352 [-236.400, 125.600], mean action: 3.363 [0.000, 10.000], mean observation: 28.763 [0.004, 498.200], loss: 174.413803, mae: 28.081785, mean_q: -28.761681\n",
            " 1056657/10000000: episode: 5257, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -559.600, mean reward: -2.784 [-279.800, 74.200], mean action: 2.587 [0.000, 10.000], mean observation: 31.744 [0.000, 598.900], loss: 251.057388, mae: 27.845835, mean_q: -28.131968\n",
            " 1056858/10000000: episode: 5258, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -832.400, mean reward: -4.141 [-416.200, 25.400], mean action: 2.527 [0.000, 10.000], mean observation: 34.530 [0.001, 508.700], loss: 163.722733, mae: 28.344835, mean_q: -28.617178\n",
            " 1057059/10000000: episode: 5259, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -253.200, mean reward: -1.260 [-126.600, 62.600], mean action: 1.900 [0.000, 10.000], mean observation: 37.934 [0.000, 792.800], loss: 113.931259, mae: 28.723482, mean_q: -28.768326\n",
            " 1057260/10000000: episode: 5260, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -421.000, mean reward: -2.095 [-210.500, 148.400], mean action: 2.498 [0.000, 10.000], mean observation: 31.929 [0.002, 455.000], loss: 217.522308, mae: 28.572906, mean_q: -28.542812\n",
            " 1057461/10000000: episode: 5261, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -405.800, mean reward: -2.019 [-202.900, 53.200], mean action: 2.368 [0.000, 10.000], mean observation: 38.003 [0.000, 746.700], loss: 244.445724, mae: 28.193102, mean_q: -28.066734\n",
            " 1057662/10000000: episode: 5262, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -131.400, mean reward: -0.654 [-65.700, 306.600], mean action: 2.701 [0.000, 10.000], mean observation: 33.243 [0.001, 541.500], loss: 209.028214, mae: 27.781399, mean_q: -27.752165\n",
            " 1057863/10000000: episode: 5263, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: -128.800, mean reward: -0.641 [-64.400, 156.400], mean action: 2.383 [0.000, 10.000], mean observation: 32.098 [0.000, 516.300], loss: 210.014069, mae: 27.951517, mean_q: -27.921322\n",
            " 1058064/10000000: episode: 5264, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -86.400, mean reward: -0.430 [-43.200, 128.000], mean action: 2.100 [0.000, 10.000], mean observation: 31.447 [0.001, 519.300], loss: 323.719849, mae: 27.711283, mean_q: -27.716394\n",
            " 1058265/10000000: episode: 5265, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -570.200, mean reward: -2.837 [-285.100, 102.600], mean action: 2.294 [0.000, 10.000], mean observation: 35.721 [0.003, 566.800], loss: 260.774323, mae: 27.807068, mean_q: -27.788671\n",
            " 1058466/10000000: episode: 5266, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -159.400, mean reward: -0.793 [-79.700, 174.800], mean action: 2.622 [0.000, 10.000], mean observation: 32.772 [0.000, 579.300], loss: 147.279099, mae: 27.494627, mean_q: -27.483574\n",
            " 1058667/10000000: episode: 5267, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -292.000, mean reward: -1.453 [-146.000, 192.000], mean action: 2.368 [0.000, 10.000], mean observation: 28.876 [0.001, 576.200], loss: 210.736282, mae: 27.644730, mean_q: -27.189682\n",
            " 1058868/10000000: episode: 5268, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -435.000, mean reward: -2.164 [-217.500, 56.400], mean action: 1.965 [0.000, 7.000], mean observation: 32.560 [0.001, 550.200], loss: 194.032059, mae: 27.938671, mean_q: -27.481859\n",
            " 1059069/10000000: episode: 5269, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -123.000, mean reward: -0.612 [-61.500, 99.000], mean action: 2.204 [0.000, 10.000], mean observation: 30.932 [0.001, 493.400], loss: 214.363647, mae: 27.661676, mean_q: -27.350658\n",
            " 1059270/10000000: episode: 5270, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -52.200, mean reward: -0.260 [-26.100, 158.400], mean action: 1.905 [0.000, 10.000], mean observation: 34.135 [0.001, 494.100], loss: 200.553574, mae: 27.826134, mean_q: -27.241964\n",
            " 1059471/10000000: episode: 5271, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -392.400, mean reward: -1.952 [-196.200, 38.000], mean action: 1.756 [0.000, 10.000], mean observation: 38.832 [0.000, 525.700], loss: 161.283798, mae: 27.447960, mean_q: -26.696056\n",
            " 1059672/10000000: episode: 5272, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: 116.800, mean reward: 0.581 [-7.000, 93.200], mean action: 1.930 [0.000, 7.000], mean observation: 37.148 [0.000, 803.400], loss: 197.470825, mae: 27.437515, mean_q: -27.218668\n",
            " 1059873/10000000: episode: 5273, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -500.800, mean reward: -2.492 [-250.400, 44.500], mean action: 2.000 [0.000, 10.000], mean observation: 32.579 [0.000, 597.200], loss: 182.227463, mae: 27.533407, mean_q: -27.221491\n",
            " 1060074/10000000: episode: 5274, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: 484.800, mean reward: 2.412 [-10.000, 424.000], mean action: 2.507 [0.000, 10.000], mean observation: 31.565 [0.002, 525.700], loss: 150.243332, mae: 27.716272, mean_q: -27.509731\n",
            " 1060275/10000000: episode: 5275, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -602.000, mean reward: -2.995 [-301.000, 29.600], mean action: 2.229 [0.000, 10.000], mean observation: 35.799 [0.001, 545.700], loss: 230.994949, mae: 27.868223, mean_q: -27.293264\n",
            " 1060476/10000000: episode: 5276, duration: 1.676s, episode steps: 201, steps per second: 120, episode reward: -714.000, mean reward: -3.552 [-357.000, 53.200], mean action: 2.423 [0.000, 7.000], mean observation: 31.969 [0.000, 622.400], loss: 300.956116, mae: 27.324320, mean_q: -26.928516\n",
            " 1060677/10000000: episode: 5277, duration: 1.707s, episode steps: 201, steps per second: 118, episode reward: -278.800, mean reward: -1.387 [-139.400, 269.500], mean action: 3.109 [0.000, 10.000], mean observation: 38.775 [0.000, 592.700], loss: 255.144302, mae: 27.081842, mean_q: -27.075312\n",
            " 1060878/10000000: episode: 5278, duration: 1.687s, episode steps: 201, steps per second: 119, episode reward: -123.400, mean reward: -0.614 [-61.700, 195.500], mean action: 2.592 [0.000, 10.000], mean observation: 35.044 [0.000, 669.400], loss: 188.850357, mae: 27.223124, mean_q: -26.883780\n",
            " 1061079/10000000: episode: 5279, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: 117.800, mean reward: 0.586 [-10.000, 204.400], mean action: 2.194 [0.000, 10.000], mean observation: 33.115 [0.000, 556.200], loss: 220.310776, mae: 27.287672, mean_q: -26.886797\n",
            " 1061280/10000000: episode: 5280, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -381.600, mean reward: -1.899 [-190.800, 90.600], mean action: 2.801 [0.000, 10.000], mean observation: 35.381 [0.002, 511.500], loss: 234.682999, mae: 27.561213, mean_q: -27.542490\n",
            " 1061481/10000000: episode: 5281, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -213.600, mean reward: -1.063 [-106.800, 156.600], mean action: 2.985 [0.000, 10.000], mean observation: 41.598 [0.000, 663.000], loss: 275.433960, mae: 27.166910, mean_q: -27.160599\n",
            " 1061682/10000000: episode: 5282, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -635.400, mean reward: -3.161 [-317.700, 66.500], mean action: 2.527 [0.000, 10.000], mean observation: 32.072 [0.000, 637.900], loss: 211.262634, mae: 27.414125, mean_q: -27.129864\n",
            " 1061883/10000000: episode: 5283, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: 18.400, mean reward: 0.092 [-10.000, 123.000], mean action: 2.453 [0.000, 10.000], mean observation: 33.670 [0.001, 459.000], loss: 160.787628, mae: 27.586437, mean_q: -27.318268\n",
            " 1062084/10000000: episode: 5284, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -702.800, mean reward: -3.497 [-351.400, 59.500], mean action: 2.527 [0.000, 10.000], mean observation: 32.852 [0.001, 581.100], loss: 191.134720, mae: 27.772081, mean_q: -27.603617\n",
            " 1062285/10000000: episode: 5285, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -467.000, mean reward: -2.323 [-233.500, 89.500], mean action: 2.766 [0.000, 10.000], mean observation: 32.510 [0.001, 622.800], loss: 148.755768, mae: 27.973288, mean_q: -27.902630\n",
            " 1062486/10000000: episode: 5286, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -645.400, mean reward: -3.211 [-322.700, 70.000], mean action: 2.448 [0.000, 10.000], mean observation: 32.894 [0.000, 570.400], loss: 271.606842, mae: 28.225742, mean_q: -28.082775\n",
            " 1062687/10000000: episode: 5287, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -550.400, mean reward: -2.738 [-275.200, 64.000], mean action: 2.090 [0.000, 10.000], mean observation: 36.106 [0.001, 446.000], loss: 293.327942, mae: 27.939026, mean_q: -27.427902\n",
            " 1062888/10000000: episode: 5288, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -626.000, mean reward: -3.114 [-313.000, 79.800], mean action: 2.582 [0.000, 10.000], mean observation: 33.100 [0.002, 498.600], loss: 183.798462, mae: 27.682096, mean_q: -27.677826\n",
            " 1063089/10000000: episode: 5289, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -176.800, mean reward: -0.880 [-88.400, 308.000], mean action: 2.980 [0.000, 10.000], mean observation: 29.436 [0.000, 749.600], loss: 208.798935, mae: 27.974691, mean_q: -28.253017\n",
            " 1063290/10000000: episode: 5290, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -864.400, mean reward: -4.300 [-432.200, 40.500], mean action: 3.299 [0.000, 10.000], mean observation: 34.911 [0.001, 587.600], loss: 276.956207, mae: 28.036455, mean_q: -28.333179\n",
            " 1063491/10000000: episode: 5291, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: 243.800, mean reward: 1.213 [-10.000, 404.200], mean action: 3.831 [0.000, 10.000], mean observation: 33.323 [0.001, 516.600], loss: 298.790741, mae: 28.301695, mean_q: -28.631138\n",
            " 1063692/10000000: episode: 5292, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -653.600, mean reward: -3.252 [-326.800, 98.000], mean action: 3.289 [0.000, 10.000], mean observation: 27.684 [0.003, 415.200], loss: 128.748062, mae: 28.274094, mean_q: -28.264793\n",
            " 1063893/10000000: episode: 5293, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -450.600, mean reward: -2.242 [-225.300, 68.300], mean action: 3.398 [0.000, 10.000], mean observation: 34.805 [0.004, 509.100], loss: 238.994873, mae: 28.358704, mean_q: -28.351217\n",
            " 1064094/10000000: episode: 5294, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -260.000, mean reward: -1.294 [-130.000, 95.200], mean action: 3.527 [0.000, 10.000], mean observation: 34.181 [0.000, 590.900], loss: 139.393723, mae: 28.432896, mean_q: -28.756540\n",
            " 1064295/10000000: episode: 5295, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -769.200, mean reward: -3.827 [-384.600, 97.000], mean action: 3.433 [0.000, 10.000], mean observation: 25.914 [0.000, 446.200], loss: 234.650345, mae: 28.458649, mean_q: -28.707132\n",
            " 1064496/10000000: episode: 5296, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -200.400, mean reward: -0.997 [-100.200, 96.600], mean action: 3.020 [0.000, 10.000], mean observation: 33.518 [0.001, 504.300], loss: 228.165344, mae: 28.785538, mean_q: -29.057962\n",
            " 1064697/10000000: episode: 5297, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: 90.000, mean reward: 0.448 [-10.000, 157.500], mean action: 2.692 [0.000, 10.000], mean observation: 30.954 [0.003, 510.800], loss: 163.784958, mae: 28.613716, mean_q: -28.678789\n",
            " 1064898/10000000: episode: 5298, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -404.000, mean reward: -2.010 [-202.000, 100.400], mean action: 3.080 [0.000, 10.000], mean observation: 32.374 [0.001, 487.500], loss: 188.800339, mae: 28.725077, mean_q: -28.790253\n",
            " 1065099/10000000: episode: 5299, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: 463.600, mean reward: 2.306 [-10.000, 262.000], mean action: 3.065 [0.000, 10.000], mean observation: 36.387 [0.000, 637.000], loss: 142.827530, mae: 28.747950, mean_q: -28.875544\n",
            " 1065300/10000000: episode: 5300, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -478.600, mean reward: -2.381 [-239.300, 46.000], mean action: 2.746 [0.000, 10.000], mean observation: 30.231 [0.000, 640.400], loss: 255.670090, mae: 29.211132, mean_q: -29.319170\n",
            " 1065501/10000000: episode: 5301, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 155.600, mean reward: 0.774 [-10.000, 126.800], mean action: 2.726 [0.000, 10.000], mean observation: 30.277 [0.001, 413.700], loss: 133.675049, mae: 29.145683, mean_q: -29.417191\n",
            " 1065702/10000000: episode: 5302, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -569.000, mean reward: -2.831 [-284.500, 87.000], mean action: 3.597 [0.000, 10.000], mean observation: 32.126 [0.001, 512.800], loss: 243.500977, mae: 29.062864, mean_q: -29.287901\n",
            " 1065903/10000000: episode: 5303, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: 167.200, mean reward: 0.832 [-10.000, 134.400], mean action: 3.910 [0.000, 10.000], mean observation: 30.453 [0.000, 558.800], loss: 233.821228, mae: 29.217541, mean_q: -29.306124\n",
            " 1066104/10000000: episode: 5304, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -709.200, mean reward: -3.528 [-354.600, 58.000], mean action: 3.408 [0.000, 10.000], mean observation: 38.544 [0.001, 539.800], loss: 207.630905, mae: 29.172443, mean_q: -29.042984\n",
            " 1066305/10000000: episode: 5305, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -694.400, mean reward: -3.455 [-347.200, 82.200], mean action: 3.388 [0.000, 10.000], mean observation: 35.391 [0.000, 584.600], loss: 172.297501, mae: 29.285515, mean_q: -29.034145\n",
            " 1066506/10000000: episode: 5306, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -694.400, mean reward: -3.455 [-347.200, 153.000], mean action: 3.433 [0.000, 10.000], mean observation: 37.959 [0.002, 510.200], loss: 165.199478, mae: 28.967714, mean_q: -28.666552\n",
            " 1066707/10000000: episode: 5307, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -594.800, mean reward: -2.959 [-297.400, 63.000], mean action: 3.035 [0.000, 10.000], mean observation: 28.982 [0.001, 486.800], loss: 206.897736, mae: 28.998066, mean_q: -28.821962\n",
            " 1066908/10000000: episode: 5308, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: 125.600, mean reward: 0.625 [-10.000, 106.300], mean action: 1.746 [0.000, 10.000], mean observation: 31.947 [0.000, 708.200], loss: 238.962692, mae: 29.036669, mean_q: -28.638298\n",
            " 1067109/10000000: episode: 5309, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -565.800, mean reward: -2.815 [-282.900, 81.900], mean action: 2.716 [0.000, 10.000], mean observation: 38.842 [0.002, 500.200], loss: 225.249786, mae: 28.839426, mean_q: -28.518778\n",
            " 1067310/10000000: episode: 5310, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: 6370.600, mean reward: 31.695 [-10.000, 3245.000], mean action: 3.234 [0.000, 10.000], mean observation: 34.948 [0.000, 440.500], loss: 125.193932, mae: 28.518894, mean_q: -28.237450\n",
            " 1067511/10000000: episode: 5311, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -295.800, mean reward: -1.472 [-147.900, 162.500], mean action: 3.055 [0.000, 10.000], mean observation: 35.641 [0.000, 566.800], loss: 1068.015747, mae: 28.010464, mean_q: -27.561283\n",
            " 1067712/10000000: episode: 5312, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -630.800, mean reward: -3.138 [-315.400, 85.400], mean action: 3.507 [0.000, 10.000], mean observation: 29.319 [0.001, 571.900], loss: 255.989349, mae: 27.187685, mean_q: -27.101608\n",
            " 1067913/10000000: episode: 5313, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -759.800, mean reward: -3.780 [-379.900, 183.000], mean action: 3.418 [0.000, 10.000], mean observation: 31.884 [0.000, 494.100], loss: 223.549316, mae: 27.071541, mean_q: -27.071581\n",
            " 1068114/10000000: episode: 5314, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -572.000, mean reward: -2.846 [-286.000, 107.600], mean action: 2.871 [0.000, 10.000], mean observation: 30.899 [0.001, 465.200], loss: 264.679962, mae: 26.963104, mean_q: -26.811615\n",
            " 1068315/10000000: episode: 5315, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -93.200, mean reward: -0.464 [-46.600, 158.900], mean action: 2.567 [0.000, 10.000], mean observation: 32.045 [0.000, 798.300], loss: 154.327499, mae: 27.083183, mean_q: -26.965685\n",
            " 1068516/10000000: episode: 5316, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -461.600, mean reward: -2.297 [-230.800, 75.600], mean action: 2.179 [0.000, 10.000], mean observation: 33.815 [0.001, 556.200], loss: 294.649384, mae: 27.920286, mean_q: -27.601660\n",
            " 1068717/10000000: episode: 5317, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -572.800, mean reward: -2.850 [-286.400, 115.200], mean action: 2.443 [0.000, 10.000], mean observation: 26.702 [0.002, 434.300], loss: 229.819733, mae: 28.399420, mean_q: -28.234112\n",
            " 1068918/10000000: episode: 5318, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -837.800, mean reward: -4.168 [-418.900, 58.100], mean action: 2.866 [0.000, 10.000], mean observation: 34.431 [0.000, 668.900], loss: 1829.455200, mae: 27.797981, mean_q: -27.638603\n",
            " 1069119/10000000: episode: 5319, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: 544.200, mean reward: 2.707 [-10.000, 375.900], mean action: 4.015 [0.000, 10.000], mean observation: 32.815 [0.000, 506.300], loss: 228.787781, mae: 27.575399, mean_q: -27.788654\n",
            " 1069320/10000000: episode: 5320, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -230.400, mean reward: -1.146 [-115.200, 151.800], mean action: 3.766 [0.000, 10.000], mean observation: 32.101 [0.000, 531.100], loss: 197.128555, mae: 27.788532, mean_q: -28.025381\n",
            " 1069521/10000000: episode: 5321, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -633.400, mean reward: -3.151 [-316.700, 133.000], mean action: 2.955 [0.000, 10.000], mean observation: 37.975 [0.000, 650.000], loss: 232.952393, mae: 28.240944, mean_q: -28.473186\n",
            " 1069722/10000000: episode: 5322, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -1184.400, mean reward: -5.893 [-592.200, 44.000], mean action: 3.662 [0.000, 10.000], mean observation: 35.777 [0.000, 574.000], loss: 2640.572998, mae: 28.044329, mean_q: -28.002844\n",
            " 1069923/10000000: episode: 5323, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -802.400, mean reward: -3.992 [-401.200, 168.000], mean action: 4.831 [0.000, 10.000], mean observation: 26.625 [0.002, 420.100], loss: 219.964584, mae: 27.445269, mean_q: -27.373386\n",
            " 1070124/10000000: episode: 5324, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: 585.200, mean reward: 2.911 [-10.000, 363.000], mean action: 3.891 [0.000, 10.000], mean observation: 40.205 [0.002, 470.300], loss: 197.378418, mae: 27.532007, mean_q: -27.550552\n",
            " 1070325/10000000: episode: 5325, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -301.800, mean reward: -1.501 [-150.900, 145.000], mean action: 3.682 [0.000, 10.000], mean observation: 36.108 [0.001, 510.400], loss: 1035.844360, mae: 27.296190, mean_q: -26.976101\n",
            " 1070526/10000000: episode: 5326, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: 582.600, mean reward: 2.899 [-10.000, 750.800], mean action: 3.517 [0.000, 10.000], mean observation: 34.001 [0.000, 508.900], loss: 318.550415, mae: 26.933281, mean_q: -26.644415\n",
            " 1070727/10000000: episode: 5327, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -658.400, mean reward: -3.276 [-329.200, 192.000], mean action: 3.582 [0.000, 10.000], mean observation: 31.459 [0.002, 472.800], loss: 213.419693, mae: 26.838181, mean_q: -26.663870\n",
            " 1070928/10000000: episode: 5328, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -613.200, mean reward: -3.051 [-306.600, 138.000], mean action: 3.149 [0.000, 10.000], mean observation: 29.213 [0.001, 618.400], loss: 242.011795, mae: 26.694967, mean_q: -26.428410\n",
            " 1071129/10000000: episode: 5329, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -230.200, mean reward: -1.145 [-115.100, 138.900], mean action: 3.274 [0.000, 10.000], mean observation: 33.932 [0.001, 512.000], loss: 265.138611, mae: 27.069805, mean_q: -26.759266\n",
            " 1071330/10000000: episode: 5330, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -13.200, mean reward: -0.066 [-10.000, 276.000], mean action: 3.468 [0.000, 10.000], mean observation: 33.506 [0.000, 586.000], loss: 965.488953, mae: 27.043318, mean_q: -26.766693\n",
            " 1071531/10000000: episode: 5331, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -1408.000, mean reward: -7.005 [-704.000, 40.800], mean action: 4.652 [0.000, 10.000], mean observation: 32.225 [0.001, 431.000], loss: 248.472443, mae: 26.931179, mean_q: -26.880573\n",
            " 1071732/10000000: episode: 5332, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: 1193.200, mean reward: 5.936 [-10.000, 775.000], mean action: 3.970 [0.000, 10.000], mean observation: 29.241 [0.001, 459.200], loss: 187.434021, mae: 26.856297, mean_q: -26.965847\n",
            " 1071933/10000000: episode: 5333, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -1087.600, mean reward: -5.411 [-543.800, 39.000], mean action: 3.438 [0.000, 10.000], mean observation: 33.625 [0.001, 422.300], loss: 208.723602, mae: 27.272144, mean_q: -27.263823\n",
            " 1072134/10000000: episode: 5334, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: 464.400, mean reward: 2.310 [-10.000, 343.200], mean action: 3.303 [0.000, 10.000], mean observation: 33.254 [0.001, 497.500], loss: 237.419373, mae: 27.296263, mean_q: -27.417000\n",
            " 1072335/10000000: episode: 5335, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -569.600, mean reward: -2.834 [-284.800, 119.000], mean action: 2.791 [0.000, 10.000], mean observation: 29.005 [0.002, 430.700], loss: 206.694107, mae: 27.395473, mean_q: -27.460093\n",
            " 1072536/10000000: episode: 5336, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -491.400, mean reward: -2.445 [-245.700, 135.500], mean action: 2.796 [0.000, 10.000], mean observation: 31.462 [0.002, 527.400], loss: 287.429504, mae: 27.361645, mean_q: -27.263035\n",
            " 1072737/10000000: episode: 5337, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: 58.200, mean reward: 0.290 [-10.000, 431.400], mean action: 2.801 [0.000, 10.000], mean observation: 36.259 [0.000, 720.900], loss: 1139.510132, mae: 27.202902, mean_q: -26.939434\n",
            " 1072938/10000000: episode: 5338, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -646.000, mean reward: -3.214 [-323.000, 73.800], mean action: 3.124 [0.000, 10.000], mean observation: 31.389 [0.001, 500.800], loss: 226.152832, mae: 27.060390, mean_q: -27.089371\n",
            " 1073139/10000000: episode: 5339, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: 12.000, mean reward: 0.060 [-10.000, 255.200], mean action: 3.025 [0.000, 10.000], mean observation: 31.516 [0.001, 424.400], loss: 1039.936401, mae: 26.940060, mean_q: -26.693748\n",
            " 1073340/10000000: episode: 5340, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: 2111.200, mean reward: 10.503 [-10.000, 1055.600], mean action: 2.975 [0.000, 10.000], mean observation: 32.500 [0.000, 441.700], loss: 158.073624, mae: 26.806990, mean_q: -26.524937\n",
            " 1073541/10000000: episode: 5341, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -85.800, mean reward: -0.427 [-42.900, 149.100], mean action: 2.846 [0.000, 10.000], mean observation: 29.666 [0.002, 387.100], loss: 207.980988, mae: 26.903036, mean_q: -26.493320\n",
            " 1073742/10000000: episode: 5342, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -386.400, mean reward: -1.922 [-193.200, 105.000], mean action: 3.229 [0.000, 10.000], mean observation: 32.916 [0.002, 537.100], loss: 232.397781, mae: 26.754513, mean_q: -26.408800\n",
            " 1073943/10000000: episode: 5343, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: 308.000, mean reward: 1.532 [-10.000, 504.000], mean action: 3.597 [0.000, 10.000], mean observation: 35.387 [0.001, 562.800], loss: 1034.843628, mae: 26.173265, mean_q: -25.888725\n",
            " 1074144/10000000: episode: 5344, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: 1105.400, mean reward: 5.500 [-10.000, 652.800], mean action: 3.731 [0.000, 10.000], mean observation: 30.365 [0.000, 693.300], loss: 1083.487793, mae: 26.380529, mean_q: -26.169645\n",
            " 1074345/10000000: episode: 5345, duration: 1.483s, episode steps: 201, steps per second: 135, episode reward: -1089.400, mean reward: -5.420 [-544.700, 105.000], mean action: 3.831 [0.000, 10.000], mean observation: 40.577 [0.001, 600.600], loss: 347.610535, mae: 26.198782, mean_q: -25.869106\n",
            " 1074546/10000000: episode: 5346, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -331.800, mean reward: -1.651 [-165.900, 231.000], mean action: 3.353 [0.000, 10.000], mean observation: 35.304 [0.000, 727.600], loss: 218.049973, mae: 26.507572, mean_q: -26.237572\n",
            " 1074747/10000000: episode: 5347, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -100.000, mean reward: -0.498 [-50.000, 172.000], mean action: 2.716 [0.000, 10.000], mean observation: 34.349 [0.001, 606.200], loss: 245.531555, mae: 26.824238, mean_q: -26.630049\n",
            " 1074948/10000000: episode: 5348, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -847.400, mean reward: -4.216 [-423.700, 68.800], mean action: 3.383 [0.000, 10.000], mean observation: 30.994 [0.001, 597.900], loss: 340.802399, mae: 26.646965, mean_q: -26.532051\n",
            " 1075149/10000000: episode: 5349, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -610.200, mean reward: -3.036 [-305.100, 82.600], mean action: 3.075 [0.000, 10.000], mean observation: 34.300 [0.003, 593.100], loss: 320.386414, mae: 26.476534, mean_q: -26.282688\n",
            " 1075350/10000000: episode: 5350, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -623.000, mean reward: -3.100 [-311.500, 53.500], mean action: 2.761 [0.000, 10.000], mean observation: 28.976 [0.002, 435.000], loss: 235.566269, mae: 26.678701, mean_q: -26.503658\n",
            " 1075551/10000000: episode: 5351, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -693.400, mean reward: -3.450 [-346.700, 121.000], mean action: 3.418 [0.000, 10.000], mean observation: 31.773 [0.001, 697.100], loss: 253.683945, mae: 26.794247, mean_q: -26.724154\n",
            " 1075752/10000000: episode: 5352, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -730.000, mean reward: -3.632 [-365.000, 63.600], mean action: 3.498 [0.000, 10.000], mean observation: 30.631 [0.001, 562.800], loss: 222.281479, mae: 26.791334, mean_q: -27.065695\n",
            " 1075953/10000000: episode: 5353, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -1052.400, mean reward: -5.236 [-526.200, 28.800], mean action: 3.050 [0.000, 10.000], mean observation: 31.387 [0.000, 458.900], loss: 296.445404, mae: 27.358833, mean_q: -27.607138\n",
            " 1076154/10000000: episode: 5354, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -632.400, mean reward: -3.146 [-316.200, 52.400], mean action: 3.388 [0.000, 10.000], mean observation: 33.574 [0.002, 522.200], loss: 259.858429, mae: 27.636087, mean_q: -28.055042\n",
            " 1076355/10000000: episode: 5355, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: 252.000, mean reward: 1.254 [-10.000, 203.000], mean action: 2.721 [0.000, 10.000], mean observation: 29.810 [0.001, 521.600], loss: 1200.395630, mae: 28.064205, mean_q: -28.182034\n",
            " 1076556/10000000: episode: 5356, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -123.000, mean reward: -0.612 [-61.500, 206.000], mean action: 3.060 [0.000, 10.000], mean observation: 33.305 [0.000, 527.200], loss: 385.075562, mae: 27.513248, mean_q: -27.507584\n",
            " 1076757/10000000: episode: 5357, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -860.800, mean reward: -4.283 [-430.400, 34.400], mean action: 2.861 [0.000, 10.000], mean observation: 35.082 [0.002, 451.200], loss: 342.310242, mae: 27.672148, mean_q: -27.710608\n",
            " 1076958/10000000: episode: 5358, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -484.600, mean reward: -2.411 [-242.300, 106.000], mean action: 2.980 [0.000, 10.000], mean observation: 31.361 [0.000, 765.000], loss: 482.813477, mae: 27.824963, mean_q: -27.961666\n",
            " 1077159/10000000: episode: 5359, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: 227.800, mean reward: 1.133 [-10.000, 291.600], mean action: 2.920 [0.000, 10.000], mean observation: 36.509 [0.002, 439.000], loss: 272.947388, mae: 27.494690, mean_q: -27.419325\n",
            " 1077360/10000000: episode: 5360, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -490.000, mean reward: -2.438 [-245.000, 108.400], mean action: 2.985 [0.000, 10.000], mean observation: 36.468 [0.000, 793.800], loss: 1027.989136, mae: 27.292170, mean_q: -27.328436\n",
            " 1077561/10000000: episode: 5361, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -23.200, mean reward: -0.115 [-11.600, 276.000], mean action: 3.274 [0.000, 10.000], mean observation: 34.478 [0.000, 633.800], loss: 240.471222, mae: 27.115078, mean_q: -27.226885\n",
            " 1077762/10000000: episode: 5362, duration: 1.660s, episode steps: 201, steps per second: 121, episode reward: -1047.600, mean reward: -5.212 [-523.800, 10.400], mean action: 2.836 [0.000, 10.000], mean observation: 35.802 [0.000, 701.500], loss: 320.080719, mae: 27.632006, mean_q: -27.808937\n",
            " 1077963/10000000: episode: 5363, duration: 1.708s, episode steps: 201, steps per second: 118, episode reward: -851.200, mean reward: -4.235 [-425.600, 77.600], mean action: 3.119 [0.000, 10.000], mean observation: 39.965 [0.002, 542.400], loss: 291.433899, mae: 27.387938, mean_q: -27.407211\n",
            " 1078164/10000000: episode: 5364, duration: 1.641s, episode steps: 201, steps per second: 122, episode reward: 1805.400, mean reward: 8.982 [-10.000, 902.700], mean action: 3.104 [0.000, 10.000], mean observation: 31.760 [0.000, 783.800], loss: 341.583984, mae: 27.225554, mean_q: -27.114389\n",
            " 1078365/10000000: episode: 5365, duration: 1.650s, episode steps: 201, steps per second: 122, episode reward: -422.600, mean reward: -2.102 [-211.300, 82.800], mean action: 3.010 [0.000, 10.000], mean observation: 38.546 [0.000, 620.700], loss: 245.830811, mae: 27.215048, mean_q: -26.991886\n",
            " 1078566/10000000: episode: 5366, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: -389.600, mean reward: -1.938 [-194.800, 82.800], mean action: 2.517 [0.000, 10.000], mean observation: 36.638 [0.002, 515.200], loss: 258.607269, mae: 27.453957, mean_q: -27.261494\n",
            " 1078767/10000000: episode: 5367, duration: 1.679s, episode steps: 201, steps per second: 120, episode reward: -214.200, mean reward: -1.066 [-107.100, 241.200], mean action: 3.204 [0.000, 10.000], mean observation: 34.171 [0.000, 673.700], loss: 1074.223389, mae: 27.283770, mean_q: -27.219833\n",
            " 1078968/10000000: episode: 5368, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -783.400, mean reward: -3.898 [-391.700, 60.000], mean action: 3.303 [0.000, 10.000], mean observation: 37.419 [0.002, 446.200], loss: 326.060547, mae: 27.358610, mean_q: -27.239994\n",
            " 1079169/10000000: episode: 5369, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -521.800, mean reward: -2.596 [-260.900, 136.000], mean action: 3.199 [0.000, 10.000], mean observation: 33.553 [0.001, 459.000], loss: 1102.022461, mae: 27.484890, mean_q: -27.455311\n",
            " 1079370/10000000: episode: 5370, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: 80.000, mean reward: 0.398 [-10.000, 564.000], mean action: 4.045 [0.000, 10.000], mean observation: 37.039 [0.001, 558.800], loss: 280.425751, mae: 27.341433, mean_q: -27.474138\n",
            " 1079571/10000000: episode: 5371, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: 1421.000, mean reward: 7.070 [-10.000, 871.000], mean action: 4.244 [0.000, 10.000], mean observation: 32.975 [0.000, 933.200], loss: 206.616104, mae: 27.487242, mean_q: -27.551315\n",
            " 1079772/10000000: episode: 5372, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: -318.600, mean reward: -1.585 [-159.300, 95.200], mean action: 3.274 [0.000, 10.000], mean observation: 32.885 [0.000, 684.900], loss: 247.635803, mae: 27.819365, mean_q: -27.932024\n",
            " 1079973/10000000: episode: 5373, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -687.000, mean reward: -3.418 [-343.500, 64.800], mean action: 2.423 [0.000, 10.000], mean observation: 30.161 [0.002, 507.800], loss: 446.649170, mae: 27.825407, mean_q: -27.626982\n",
            " 1080174/10000000: episode: 5374, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -270.400, mean reward: -1.345 [-135.200, 73.000], mean action: 2.537 [0.000, 10.000], mean observation: 33.858 [0.000, 556.700], loss: 186.398102, mae: 27.790314, mean_q: -27.690886\n",
            " 1080375/10000000: episode: 5375, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -961.000, mean reward: -4.781 [-480.500, 50.000], mean action: 2.756 [0.000, 10.000], mean observation: 34.589 [0.001, 444.500], loss: 1060.660767, mae: 27.834461, mean_q: -27.521431\n",
            " 1080576/10000000: episode: 5376, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -866.400, mean reward: -4.310 [-433.200, 112.400], mean action: 3.184 [0.000, 10.000], mean observation: 35.892 [0.000, 607.900], loss: 1052.304321, mae: 27.629189, mean_q: -27.649548\n",
            " 1080777/10000000: episode: 5377, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -299.800, mean reward: -1.492 [-149.900, 132.000], mean action: 3.627 [0.000, 10.000], mean observation: 36.276 [0.000, 528.400], loss: 326.981110, mae: 27.544621, mean_q: -27.763016\n",
            " 1080978/10000000: episode: 5378, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -719.400, mean reward: -3.579 [-359.700, 175.200], mean action: 3.343 [0.000, 10.000], mean observation: 28.812 [0.001, 455.800], loss: 327.913971, mae: 27.441906, mean_q: -27.594355\n",
            " 1081179/10000000: episode: 5379, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: 293.000, mean reward: 1.458 [-10.000, 192.000], mean action: 3.557 [0.000, 10.000], mean observation: 31.512 [0.000, 818.500], loss: 1022.646301, mae: 27.332952, mean_q: -27.507690\n",
            " 1081380/10000000: episode: 5380, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -780.800, mean reward: -3.885 [-390.400, 205.000], mean action: 3.687 [0.000, 10.000], mean observation: 32.032 [0.002, 441.900], loss: 332.710693, mae: 27.406809, mean_q: -27.628649\n",
            " 1081581/10000000: episode: 5381, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -958.600, mean reward: -4.769 [-479.300, 57.500], mean action: 3.512 [0.000, 10.000], mean observation: 32.505 [0.001, 619.000], loss: 319.080841, mae: 27.115707, mean_q: -27.174810\n",
            " 1081782/10000000: episode: 5382, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -248.200, mean reward: -1.235 [-124.100, 143.600], mean action: 3.687 [0.000, 10.000], mean observation: 33.845 [0.001, 412.100], loss: 1051.020630, mae: 27.251942, mean_q: -27.312542\n",
            " 1081983/10000000: episode: 5383, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -482.200, mean reward: -2.399 [-241.100, 80.400], mean action: 2.975 [0.000, 10.000], mean observation: 33.614 [0.000, 500.900], loss: 245.330566, mae: 27.329889, mean_q: -27.385431\n",
            " 1082184/10000000: episode: 5384, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -292.000, mean reward: -1.453 [-146.000, 80.800], mean action: 2.796 [0.000, 10.000], mean observation: 38.175 [0.001, 501.600], loss: 373.607147, mae: 27.214649, mean_q: -26.783743\n",
            " 1082385/10000000: episode: 5385, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -331.200, mean reward: -1.648 [-165.600, 109.000], mean action: 2.900 [0.000, 10.000], mean observation: 32.145 [0.001, 590.900], loss: 197.470001, mae: 27.010336, mean_q: -26.871216\n",
            " 1082586/10000000: episode: 5386, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -431.400, mean reward: -2.146 [-215.700, 68.400], mean action: 2.453 [0.000, 10.000], mean observation: 29.502 [0.000, 567.300], loss: 307.306488, mae: 27.141113, mean_q: -26.934212\n",
            " 1082787/10000000: episode: 5387, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -203.800, mean reward: -1.014 [-101.900, 101.400], mean action: 2.811 [0.000, 10.000], mean observation: 37.526 [0.000, 747.100], loss: 330.451538, mae: 27.249208, mean_q: -27.259844\n",
            " 1082988/10000000: episode: 5388, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -612.800, mean reward: -3.049 [-306.400, 66.800], mean action: 3.104 [0.000, 10.000], mean observation: 30.638 [0.000, 534.800], loss: 241.957443, mae: 27.327299, mean_q: -27.700827\n",
            " 1083189/10000000: episode: 5389, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: 221.000, mean reward: 1.100 [-10.000, 165.000], mean action: 2.687 [0.000, 10.000], mean observation: 29.751 [0.000, 478.800], loss: 408.664917, mae: 27.551394, mean_q: -27.555601\n",
            " 1083390/10000000: episode: 5390, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -516.800, mean reward: -2.571 [-258.400, 33.000], mean action: 2.060 [0.000, 10.000], mean observation: 33.306 [0.000, 580.300], loss: 432.393372, mae: 27.843702, mean_q: -27.716860\n",
            " 1083591/10000000: episode: 5391, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -457.200, mean reward: -2.275 [-228.600, 78.000], mean action: 2.537 [0.000, 10.000], mean observation: 34.772 [0.000, 589.600], loss: 288.605713, mae: 27.579021, mean_q: -27.602503\n",
            " 1083792/10000000: episode: 5392, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -194.200, mean reward: -0.966 [-97.100, 73.600], mean action: 2.512 [0.000, 10.000], mean observation: 29.675 [0.000, 813.800], loss: 262.216156, mae: 27.728968, mean_q: -27.813433\n",
            " 1083993/10000000: episode: 5393, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -709.200, mean reward: -3.528 [-354.600, 60.800], mean action: 2.711 [0.000, 10.000], mean observation: 37.590 [0.000, 621.900], loss: 368.456085, mae: 27.828657, mean_q: -27.934269\n",
            " 1084194/10000000: episode: 5394, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -926.200, mean reward: -4.608 [-463.100, 41.000], mean action: 2.900 [0.000, 10.000], mean observation: 30.580 [0.002, 632.400], loss: 207.608810, mae: 28.108555, mean_q: -28.529366\n",
            " 1084395/10000000: episode: 5395, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -728.600, mean reward: -3.625 [-364.300, 99.600], mean action: 3.000 [0.000, 10.000], mean observation: 34.609 [0.003, 531.400], loss: 446.538879, mae: 28.148136, mean_q: -28.489433\n",
            " 1084596/10000000: episode: 5396, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -805.200, mean reward: -4.006 [-402.600, 82.000], mean action: 3.015 [0.000, 10.000], mean observation: 34.802 [0.001, 519.900], loss: 294.826172, mae: 28.284063, mean_q: -28.697304\n",
            " 1084797/10000000: episode: 5397, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: 172.200, mean reward: 0.857 [-10.000, 355.000], mean action: 2.781 [0.000, 10.000], mean observation: 37.517 [0.001, 447.800], loss: 268.399170, mae: 28.680969, mean_q: -29.078598\n",
            " 1084998/10000000: episode: 5398, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -145.400, mean reward: -0.723 [-72.700, 158.400], mean action: 2.960 [0.000, 10.000], mean observation: 40.446 [0.001, 627.200], loss: 1109.994507, mae: 28.687933, mean_q: -29.290396\n",
            " 1085199/10000000: episode: 5399, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: 218.800, mean reward: 1.089 [-7.000, 308.500], mean action: 2.612 [0.000, 7.000], mean observation: 39.028 [0.003, 501.700], loss: 312.862183, mae: 28.841583, mean_q: -29.429226\n",
            " 1085400/10000000: episode: 5400, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -509.000, mean reward: -2.532 [-254.500, 115.800], mean action: 3.249 [0.000, 10.000], mean observation: 29.303 [0.000, 604.500], loss: 254.168182, mae: 28.938673, mean_q: -29.614552\n",
            " 1085601/10000000: episode: 5401, duration: 1.517s, episode steps: 201, steps per second: 133, episode reward: -744.600, mean reward: -3.704 [-372.300, 107.400], mean action: 3.119 [0.000, 10.000], mean observation: 32.017 [0.001, 437.600], loss: 1081.445312, mae: 29.091976, mean_q: -29.685848\n",
            " 1085802/10000000: episode: 5402, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -655.600, mean reward: -3.262 [-327.800, 45.500], mean action: 2.950 [0.000, 10.000], mean observation: 29.865 [0.001, 464.600], loss: 274.572113, mae: 29.481943, mean_q: -29.965242\n",
            " 1086003/10000000: episode: 5403, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: 436.400, mean reward: 2.171 [-10.000, 218.200], mean action: 3.085 [0.000, 10.000], mean observation: 34.221 [0.001, 531.100], loss: 297.908264, mae: 29.102079, mean_q: -29.486620\n",
            " 1086204/10000000: episode: 5404, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -396.600, mean reward: -1.973 [-198.300, 63.200], mean action: 3.065 [0.000, 10.000], mean observation: 38.287 [0.000, 467.700], loss: 1202.442383, mae: 28.637136, mean_q: -28.371059\n",
            " 1086405/10000000: episode: 5405, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -712.000, mean reward: -3.542 [-356.000, 78.400], mean action: 2.741 [0.000, 10.000], mean observation: 35.554 [0.000, 784.200], loss: 204.758499, mae: 28.340786, mean_q: -28.137209\n",
            " 1086606/10000000: episode: 5406, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -265.600, mean reward: -1.321 [-132.800, 137.200], mean action: 3.174 [0.000, 10.000], mean observation: 33.602 [0.000, 507.700], loss: 1220.721436, mae: 28.192978, mean_q: -28.153038\n",
            " 1086807/10000000: episode: 5407, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -1043.200, mean reward: -5.190 [-521.600, 62.500], mean action: 3.781 [0.000, 10.000], mean observation: 31.757 [0.001, 542.200], loss: 241.081314, mae: 27.589090, mean_q: -28.293600\n",
            " 1087008/10000000: episode: 5408, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: 1552.000, mean reward: 7.721 [-10.000, 875.500], mean action: 3.328 [0.000, 10.000], mean observation: 34.212 [0.001, 527.300], loss: 193.875687, mae: 28.300110, mean_q: -29.005661\n",
            " 1087209/10000000: episode: 5409, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -920.200, mean reward: -4.578 [-460.100, 34.200], mean action: 2.761 [0.000, 10.000], mean observation: 29.953 [0.000, 542.400], loss: 415.265076, mae: 28.588839, mean_q: -29.073214\n",
            " 1087410/10000000: episode: 5410, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -691.400, mean reward: -3.440 [-345.700, 154.000], mean action: 3.219 [0.000, 10.000], mean observation: 32.821 [0.000, 694.400], loss: 292.370728, mae: 28.760494, mean_q: -29.390469\n",
            " 1087611/10000000: episode: 5411, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -1231.200, mean reward: -6.125 [-615.600, 6.000], mean action: 3.189 [0.000, 10.000], mean observation: 43.930 [0.000, 598.300], loss: 1121.513184, mae: 29.113518, mean_q: -29.689381\n",
            " 1087812/10000000: episode: 5412, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -855.400, mean reward: -4.256 [-427.700, 74.000], mean action: 3.572 [0.000, 10.000], mean observation: 30.357 [0.001, 555.700], loss: 1150.918335, mae: 29.045923, mean_q: -29.838644\n",
            " 1088013/10000000: episode: 5413, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -260.000, mean reward: -1.294 [-130.000, 282.600], mean action: 4.318 [0.000, 10.000], mean observation: 28.973 [0.003, 492.800], loss: 292.926697, mae: 29.145775, mean_q: -30.122520\n",
            " 1088214/10000000: episode: 5414, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -1096.600, mean reward: -5.456 [-548.300, 34.000], mean action: 3.363 [0.000, 10.000], mean observation: 41.276 [0.000, 669.900], loss: 1214.566650, mae: 29.850950, mean_q: -30.604109\n",
            " 1088415/10000000: episode: 5415, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -654.400, mean reward: -3.256 [-327.200, 158.200], mean action: 3.657 [0.000, 10.000], mean observation: 33.569 [0.002, 547.800], loss: 222.143188, mae: 29.583416, mean_q: -30.432123\n",
            " 1088616/10000000: episode: 5416, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -465.000, mean reward: -2.313 [-232.500, 88.400], mean action: 2.572 [0.000, 10.000], mean observation: 36.244 [0.001, 482.600], loss: 1107.015869, mae: 30.070002, mean_q: -30.737873\n",
            " 1088817/10000000: episode: 5417, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 229.200, mean reward: 1.140 [-10.000, 185.000], mean action: 2.557 [0.000, 10.000], mean observation: 37.653 [0.000, 654.700], loss: 383.908569, mae: 30.318228, mean_q: -30.686649\n",
            " 1089018/10000000: episode: 5418, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -216.800, mean reward: -1.079 [-108.400, 117.200], mean action: 2.836 [0.000, 10.000], mean observation: 33.730 [0.000, 706.400], loss: 328.695190, mae: 30.086227, mean_q: -30.581331\n",
            " 1089219/10000000: episode: 5419, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -834.600, mean reward: -4.152 [-417.300, 49.200], mean action: 3.045 [0.000, 10.000], mean observation: 30.972 [0.001, 480.700], loss: 456.018280, mae: 29.914043, mean_q: -30.368801\n",
            " 1089420/10000000: episode: 5420, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: -303.200, mean reward: -1.508 [-151.600, 148.500], mean action: 2.930 [0.000, 10.000], mean observation: 35.786 [0.001, 463.400], loss: 1082.796143, mae: 29.711479, mean_q: -30.394484\n",
            " 1089621/10000000: episode: 5421, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -58.000, mean reward: -0.289 [-29.000, 409.000], mean action: 3.587 [0.000, 10.000], mean observation: 29.598 [0.001, 676.900], loss: 1941.798828, mae: 29.615707, mean_q: -30.276028\n",
            " 1089822/10000000: episode: 5422, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: 70.800, mean reward: 0.352 [-10.000, 169.000], mean action: 3.721 [0.000, 10.000], mean observation: 32.018 [0.001, 714.300], loss: 209.368271, mae: 29.654467, mean_q: -30.495617\n",
            " 1090023/10000000: episode: 5423, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -532.200, mean reward: -2.648 [-266.100, 119.400], mean action: 3.284 [0.000, 10.000], mean observation: 42.543 [0.000, 601.500], loss: 206.214798, mae: 30.061321, mean_q: -30.870739\n",
            " 1090224/10000000: episode: 5424, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -758.000, mean reward: -3.771 [-379.000, 41.600], mean action: 2.935 [0.000, 10.000], mean observation: 32.635 [0.000, 582.800], loss: 266.072021, mae: 30.119104, mean_q: -30.766796\n",
            " 1090425/10000000: episode: 5425, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -404.400, mean reward: -2.012 [-202.200, 40.000], mean action: 2.751 [0.000, 10.000], mean observation: 32.613 [0.000, 566.700], loss: 323.210541, mae: 30.039894, mean_q: -30.218962\n",
            " 1090626/10000000: episode: 5426, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: 124.400, mean reward: 0.619 [-10.000, 130.200], mean action: 2.781 [0.000, 10.000], mean observation: 29.415 [0.001, 498.800], loss: 409.172180, mae: 29.474939, mean_q: -29.761204\n",
            " 1090827/10000000: episode: 5427, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: 264.400, mean reward: 1.315 [-10.000, 229.200], mean action: 2.776 [0.000, 10.000], mean observation: 33.264 [0.002, 508.500], loss: 371.361389, mae: 29.530174, mean_q: -30.346943\n",
            " 1091028/10000000: episode: 5428, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -379.800, mean reward: -1.890 [-189.900, 93.100], mean action: 2.577 [0.000, 10.000], mean observation: 32.966 [0.000, 389.300], loss: 238.893661, mae: 29.740244, mean_q: -30.500446\n",
            " 1091229/10000000: episode: 5429, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -955.200, mean reward: -4.752 [-477.600, 40.200], mean action: 2.866 [0.000, 10.000], mean observation: 37.201 [0.000, 630.500], loss: 329.823761, mae: 29.960077, mean_q: -30.924442\n",
            " 1091430/10000000: episode: 5430, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: 213.000, mean reward: 1.060 [-10.000, 360.600], mean action: 2.940 [0.000, 10.000], mean observation: 31.087 [0.001, 518.600], loss: 326.084259, mae: 30.483009, mean_q: -31.250597\n",
            " 1091631/10000000: episode: 5431, duration: 1.572s, episode steps: 201, steps per second: 128, episode reward: 362.400, mean reward: 1.803 [-10.000, 264.200], mean action: 2.572 [0.000, 10.000], mean observation: 36.054 [0.000, 562.200], loss: 397.049591, mae: 30.962147, mean_q: -31.693218\n",
            " 1091832/10000000: episode: 5432, duration: 1.561s, episode steps: 201, steps per second: 129, episode reward: -500.200, mean reward: -2.489 [-250.100, 133.700], mean action: 2.746 [0.000, 10.000], mean observation: 30.950 [0.002, 468.200], loss: 393.014893, mae: 30.916374, mean_q: -31.692524\n",
            " 1092033/10000000: episode: 5433, duration: 1.541s, episode steps: 201, steps per second: 130, episode reward: 2605.600, mean reward: 12.963 [-10.000, 1302.800], mean action: 2.597 [0.000, 10.000], mean observation: 33.663 [0.002, 637.200], loss: 1315.027100, mae: 31.188841, mean_q: -31.974691\n",
            " 1092234/10000000: episode: 5434, duration: 1.605s, episode steps: 201, steps per second: 125, episode reward: -267.000, mean reward: -1.328 [-133.500, 115.500], mean action: 3.647 [0.000, 10.000], mean observation: 28.452 [0.003, 515.700], loss: 355.125397, mae: 30.863119, mean_q: -31.617950\n",
            " 1092435/10000000: episode: 5435, duration: 1.589s, episode steps: 201, steps per second: 127, episode reward: 430.200, mean reward: 2.140 [-10.000, 215.100], mean action: 2.791 [0.000, 10.000], mean observation: 28.934 [0.001, 607.700], loss: 329.556671, mae: 30.839951, mean_q: -31.191807\n",
            " 1092636/10000000: episode: 5436, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -418.200, mean reward: -2.081 [-209.100, 50.100], mean action: 1.955 [0.000, 10.000], mean observation: 33.958 [0.001, 462.800], loss: 291.921021, mae: 31.159447, mean_q: -30.965792\n",
            " 1092837/10000000: episode: 5437, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -500.200, mean reward: -2.489 [-250.100, 126.000], mean action: 2.647 [0.000, 10.000], mean observation: 38.778 [0.003, 524.500], loss: 1267.688110, mae: 30.806509, mean_q: -31.349644\n",
            " 1093038/10000000: episode: 5438, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -390.400, mean reward: -1.942 [-195.200, 78.300], mean action: 3.557 [0.000, 10.000], mean observation: 26.686 [0.000, 772.000], loss: 631.125305, mae: 30.691032, mean_q: -31.620710\n",
            " 1093239/10000000: episode: 5439, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -1066.400, mean reward: -5.305 [-533.200, 26.800], mean action: 3.095 [0.000, 10.000], mean observation: 33.429 [0.000, 558.800], loss: 442.917999, mae: 30.833164, mean_q: -31.851669\n",
            " 1093440/10000000: episode: 5440, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: 1141.600, mean reward: 5.680 [-10.000, 592.000], mean action: 3.090 [0.000, 10.000], mean observation: 34.263 [0.001, 645.600], loss: 271.972687, mae: 31.292309, mean_q: -32.408314\n",
            " 1093641/10000000: episode: 5441, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -682.600, mean reward: -3.396 [-341.300, 47.600], mean action: 2.687 [0.000, 10.000], mean observation: 34.924 [0.001, 532.400], loss: 380.152374, mae: 32.009861, mean_q: -33.147537\n",
            " 1093842/10000000: episode: 5442, duration: 1.544s, episode steps: 201, steps per second: 130, episode reward: -795.600, mean reward: -3.958 [-397.800, 47.200], mean action: 3.279 [0.000, 10.000], mean observation: 30.162 [0.000, 516.000], loss: 429.570709, mae: 31.849508, mean_q: -33.056324\n",
            " 1094043/10000000: episode: 5443, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: 147.600, mean reward: 0.734 [-10.000, 163.000], mean action: 3.229 [0.000, 10.000], mean observation: 37.541 [0.000, 555.400], loss: 1128.201294, mae: 31.869286, mean_q: -33.161102\n",
            " 1094244/10000000: episode: 5444, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -817.800, mean reward: -4.069 [-408.900, 81.500], mean action: 3.274 [0.000, 10.000], mean observation: 32.447 [0.002, 464.000], loss: 464.802429, mae: 31.964460, mean_q: -33.262806\n",
            " 1094445/10000000: episode: 5445, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -572.600, mean reward: -2.849 [-286.300, 106.200], mean action: 3.030 [0.000, 10.000], mean observation: 37.111 [0.000, 668.100], loss: 571.662476, mae: 32.430538, mean_q: -33.635918\n",
            " 1094646/10000000: episode: 5446, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: -918.400, mean reward: -4.569 [-459.200, 36.000], mean action: 3.313 [0.000, 10.000], mean observation: 28.234 [0.000, 389.300], loss: 531.185730, mae: 32.803261, mean_q: -33.655502\n",
            " 1094847/10000000: episode: 5447, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: 48.600, mean reward: 0.242 [-10.000, 261.600], mean action: 3.189 [0.000, 10.000], mean observation: 31.300 [0.001, 587.900], loss: 606.769897, mae: 32.463047, mean_q: -32.804512\n",
            " 1095048/10000000: episode: 5448, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -643.200, mean reward: -3.200 [-321.600, 111.000], mean action: 2.975 [0.000, 10.000], mean observation: 36.952 [0.000, 606.600], loss: 1279.114868, mae: 31.372250, mean_q: -31.653873\n",
            " 1095249/10000000: episode: 5449, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -847.200, mean reward: -4.215 [-423.600, 11.000], mean action: 2.338 [0.000, 10.000], mean observation: 35.283 [0.001, 426.200], loss: 553.077881, mae: 31.590811, mean_q: -32.181168\n",
            " 1095450/10000000: episode: 5450, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: 171.400, mean reward: 0.853 [-10.000, 372.000], mean action: 2.124 [0.000, 10.000], mean observation: 32.455 [0.002, 578.200], loss: 453.956696, mae: 31.686392, mean_q: -32.317650\n",
            " 1095651/10000000: episode: 5451, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -596.800, mean reward: -2.969 [-298.400, 32.400], mean action: 2.289 [0.000, 10.000], mean observation: 33.058 [0.000, 497.600], loss: 481.049835, mae: 32.258671, mean_q: -32.699890\n",
            " 1095852/10000000: episode: 5452, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -323.000, mean reward: -1.607 [-161.500, 46.000], mean action: 1.905 [0.000, 10.000], mean observation: 36.105 [0.001, 527.200], loss: 465.521576, mae: 32.435974, mean_q: -32.834602\n",
            " 1096053/10000000: episode: 5453, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -375.600, mean reward: -1.869 [-187.800, 40.800], mean action: 1.851 [0.000, 9.000], mean observation: 31.751 [0.001, 675.300], loss: 287.627930, mae: 31.975046, mean_q: -32.139778\n",
            " 1096254/10000000: episode: 5454, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -503.200, mean reward: -2.503 [-251.600, 78.400], mean action: 2.060 [0.000, 10.000], mean observation: 32.488 [0.000, 527.500], loss: 454.867920, mae: 31.992071, mean_q: -32.245064\n",
            " 1096455/10000000: episode: 5455, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -181.600, mean reward: -0.903 [-90.800, 53.400], mean action: 2.124 [0.000, 10.000], mean observation: 29.162 [0.001, 466.900], loss: 1319.588257, mae: 32.006264, mean_q: -32.209026\n",
            " 1096656/10000000: episode: 5456, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: 328.800, mean reward: 1.636 [-10.000, 193.200], mean action: 2.607 [0.000, 10.000], mean observation: 33.672 [0.000, 544.700], loss: 1142.331665, mae: 31.676174, mean_q: -32.211033\n",
            " 1096857/10000000: episode: 5457, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -115.400, mean reward: -0.574 [-57.700, 112.000], mean action: 2.343 [0.000, 10.000], mean observation: 37.535 [0.002, 532.900], loss: 339.604858, mae: 31.697227, mean_q: -32.422855\n",
            " 1097058/10000000: episode: 5458, duration: 1.559s, episode steps: 201, steps per second: 129, episode reward: -625.200, mean reward: -3.110 [-312.600, 43.200], mean action: 2.512 [0.000, 10.000], mean observation: 33.676 [0.001, 647.400], loss: 454.784760, mae: 31.673656, mean_q: -32.320210\n",
            " 1097259/10000000: episode: 5459, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -600.200, mean reward: -2.986 [-300.100, 103.800], mean action: 2.811 [0.000, 10.000], mean observation: 33.188 [0.000, 602.400], loss: 638.513245, mae: 31.174917, mean_q: -31.832664\n",
            " 1097460/10000000: episode: 5460, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -1096.200, mean reward: -5.454 [-548.100, 1.800], mean action: 2.761 [0.000, 10.000], mean observation: 35.540 [0.002, 453.500], loss: 319.820923, mae: 30.882078, mean_q: -31.517614\n",
            " 1097661/10000000: episode: 5461, duration: 1.526s, episode steps: 201, steps per second: 132, episode reward: -292.200, mean reward: -1.454 [-146.100, 93.600], mean action: 3.124 [0.000, 10.000], mean observation: 27.873 [0.002, 362.700], loss: 412.865082, mae: 31.303579, mean_q: -32.066963\n",
            " 1097862/10000000: episode: 5462, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: 729.800, mean reward: 3.631 [-9.000, 364.900], mean action: 2.294 [0.000, 10.000], mean observation: 31.326 [0.000, 695.400], loss: 1285.846436, mae: 31.357111, mean_q: -31.689589\n",
            " 1098063/10000000: episode: 5463, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -467.800, mean reward: -2.327 [-233.900, 34.200], mean action: 2.348 [0.000, 10.000], mean observation: 34.271 [0.000, 660.900], loss: 377.312958, mae: 31.447615, mean_q: -31.880774\n",
            " 1098264/10000000: episode: 5464, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -531.400, mean reward: -2.644 [-265.700, 34.200], mean action: 2.254 [0.000, 10.000], mean observation: 33.259 [0.000, 525.900], loss: 337.883362, mae: 31.965353, mean_q: -32.617504\n",
            " 1098465/10000000: episode: 5465, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: 55.400, mean reward: 0.276 [-10.000, 249.900], mean action: 2.343 [0.000, 10.000], mean observation: 33.772 [0.001, 446.900], loss: 372.944977, mae: 32.176796, mean_q: -32.933228\n",
            " 1098666/10000000: episode: 5466, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: 181.400, mean reward: 0.902 [-10.000, 240.000], mean action: 2.557 [0.000, 10.000], mean observation: 35.895 [0.000, 410.900], loss: 1222.994995, mae: 32.010082, mean_q: -32.472393\n",
            " 1098867/10000000: episode: 5467, duration: 1.506s, episode steps: 201, steps per second: 134, episode reward: -471.600, mean reward: -2.346 [-235.800, 96.000], mean action: 2.701 [0.000, 10.000], mean observation: 27.714 [0.001, 654.600], loss: 616.964233, mae: 31.843351, mean_q: -32.254559\n",
            " 1099068/10000000: episode: 5468, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -447.800, mean reward: -2.228 [-223.900, 72.200], mean action: 1.980 [0.000, 10.000], mean observation: 34.554 [0.000, 467.400], loss: 427.008057, mae: 31.285084, mean_q: -31.665556\n",
            " 1099269/10000000: episode: 5469, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -545.800, mean reward: -2.715 [-272.900, 31.100], mean action: 1.866 [0.000, 10.000], mean observation: 37.821 [0.000, 697.600], loss: 446.087555, mae: 31.243025, mean_q: -31.540953\n",
            " 1099470/10000000: episode: 5470, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -538.200, mean reward: -2.678 [-269.100, 91.600], mean action: 2.274 [0.000, 10.000], mean observation: 32.150 [0.000, 499.800], loss: 372.606079, mae: 31.220766, mean_q: -31.477421\n",
            " 1099671/10000000: episode: 5471, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: 380.400, mean reward: 1.893 [-10.000, 267.000], mean action: 2.264 [0.000, 10.000], mean observation: 31.434 [0.001, 443.500], loss: 344.779999, mae: 31.266972, mean_q: -31.439939\n",
            " 1099872/10000000: episode: 5472, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -327.600, mean reward: -1.630 [-163.800, 176.400], mean action: 2.413 [0.000, 10.000], mean observation: 31.683 [0.001, 470.800], loss: 1204.325684, mae: 31.217113, mean_q: -31.170856\n",
            " 1100073/10000000: episode: 5473, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -257.400, mean reward: -1.281 [-128.700, 235.200], mean action: 2.552 [0.000, 10.000], mean observation: 33.148 [0.002, 509.500], loss: 334.358551, mae: 31.127192, mean_q: -31.295904\n",
            " 1100274/10000000: episode: 5474, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -128.600, mean reward: -0.640 [-64.300, 216.000], mean action: 3.085 [0.000, 10.000], mean observation: 30.431 [0.002, 515.900], loss: 1083.771973, mae: 30.954090, mean_q: -31.097691\n",
            " 1100475/10000000: episode: 5475, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: 1379.400, mean reward: 6.863 [-10.000, 903.000], mean action: 3.562 [0.000, 10.000], mean observation: 35.138 [0.002, 467.700], loss: 1256.954346, mae: 30.240164, mean_q: -30.306820\n",
            " 1100676/10000000: episode: 5476, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -779.600, mean reward: -3.879 [-389.800, 43.600], mean action: 3.627 [0.000, 10.000], mean observation: 25.100 [0.002, 434.600], loss: 449.368164, mae: 29.751677, mean_q: -30.096003\n",
            " 1100877/10000000: episode: 5477, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -950.600, mean reward: -4.729 [-475.300, 193.000], mean action: 3.622 [0.000, 10.000], mean observation: 36.433 [0.000, 638.600], loss: 1177.775024, mae: 29.865307, mean_q: -30.324884\n",
            " 1101078/10000000: episode: 5478, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -175.000, mean reward: -0.871 [-87.500, 199.800], mean action: 3.622 [0.000, 10.000], mean observation: 31.957 [0.000, 407.100], loss: 291.312317, mae: 29.952814, mean_q: -30.494389\n",
            " 1101279/10000000: episode: 5479, duration: 1.706s, episode steps: 201, steps per second: 118, episode reward: 547.000, mean reward: 2.721 [-10.000, 468.000], mean action: 3.333 [0.000, 10.000], mean observation: 34.481 [0.002, 530.500], loss: 363.089478, mae: 30.100687, mean_q: -30.521940\n",
            " 1101480/10000000: episode: 5480, duration: 1.688s, episode steps: 201, steps per second: 119, episode reward: -732.000, mean reward: -3.642 [-366.000, 40.800], mean action: 2.960 [0.000, 10.000], mean observation: 35.415 [0.000, 796.200], loss: 382.064667, mae: 30.195469, mean_q: -30.520744\n",
            " 1101681/10000000: episode: 5481, duration: 1.755s, episode steps: 201, steps per second: 115, episode reward: 678.600, mean reward: 3.376 [-10.000, 339.300], mean action: 2.657 [0.000, 10.000], mean observation: 37.039 [0.000, 588.400], loss: 518.172180, mae: 30.178305, mean_q: -30.512897\n",
            " 1101882/10000000: episode: 5482, duration: 1.604s, episode steps: 201, steps per second: 125, episode reward: -803.400, mean reward: -3.997 [-401.700, 37.600], mean action: 2.547 [0.000, 10.000], mean observation: 29.798 [0.001, 466.300], loss: 410.456299, mae: 30.346159, mean_q: -30.709551\n",
            " 1102083/10000000: episode: 5483, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -222.800, mean reward: -1.108 [-111.400, 147.400], mean action: 2.806 [0.000, 10.000], mean observation: 35.158 [0.003, 438.400], loss: 515.134705, mae: 30.369297, mean_q: -30.523174\n",
            " 1102284/10000000: episode: 5484, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -387.200, mean reward: -1.926 [-193.600, 166.500], mean action: 2.403 [0.000, 10.000], mean observation: 26.002 [0.002, 545.200], loss: 334.279449, mae: 30.418804, mean_q: -30.825287\n",
            " 1102485/10000000: episode: 5485, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -585.000, mean reward: -2.910 [-292.500, 68.400], mean action: 2.617 [0.000, 10.000], mean observation: 36.433 [0.001, 491.000], loss: 636.191101, mae: 30.545408, mean_q: -30.599726\n",
            " 1102686/10000000: episode: 5486, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 201.600, mean reward: 1.003 [-10.000, 166.000], mean action: 2.637 [0.000, 10.000], mean observation: 32.461 [0.000, 481.400], loss: 305.801849, mae: 30.953484, mean_q: -31.174839\n",
            " 1102887/10000000: episode: 5487, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -875.200, mean reward: -4.354 [-437.600, 34.000], mean action: 2.637 [0.000, 10.000], mean observation: 30.579 [0.002, 446.500], loss: 455.253723, mae: 31.372532, mean_q: -31.496784\n",
            " 1103088/10000000: episode: 5488, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -240.000, mean reward: -1.194 [-120.000, 154.700], mean action: 2.358 [0.000, 10.000], mean observation: 32.958 [0.000, 746.900], loss: 345.953827, mae: 31.511709, mean_q: -31.695692\n",
            " 1103289/10000000: episode: 5489, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -44.800, mean reward: -0.223 [-22.400, 153.600], mean action: 1.776 [0.000, 10.000], mean observation: 29.508 [0.000, 583.400], loss: 417.737488, mae: 31.513367, mean_q: -31.287918\n",
            " 1103490/10000000: episode: 5490, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -589.400, mean reward: -2.932 [-294.700, 57.200], mean action: 2.104 [0.000, 10.000], mean observation: 33.061 [0.000, 702.100], loss: 427.943237, mae: 31.625805, mean_q: -31.628197\n",
            " 1103691/10000000: episode: 5491, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -627.000, mean reward: -3.119 [-313.500, 42.600], mean action: 2.323 [0.000, 8.000], mean observation: 35.986 [0.000, 472.400], loss: 466.777649, mae: 31.711023, mean_q: -31.837790\n",
            " 1103892/10000000: episode: 5492, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -268.400, mean reward: -1.335 [-134.200, 60.000], mean action: 2.791 [0.000, 10.000], mean observation: 34.250 [0.000, 653.600], loss: 471.389038, mae: 31.624748, mean_q: -31.727718\n",
            " 1104093/10000000: episode: 5493, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -286.000, mean reward: -1.423 [-143.000, 98.000], mean action: 2.154 [0.000, 10.000], mean observation: 31.406 [0.000, 509.800], loss: 435.608826, mae: 31.843824, mean_q: -31.900234\n",
            " 1104294/10000000: episode: 5494, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -143.200, mean reward: -0.712 [-71.600, 127.600], mean action: 2.234 [0.000, 10.000], mean observation: 40.419 [0.002, 630.900], loss: 377.378662, mae: 31.961700, mean_q: -31.883854\n",
            " 1104495/10000000: episode: 5495, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: 7.000, mean reward: 0.035 [-7.000, 337.200], mean action: 2.085 [0.000, 7.000], mean observation: 34.961 [0.001, 591.000], loss: 271.127258, mae: 31.911406, mean_q: -31.734058\n",
            " 1104696/10000000: episode: 5496, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -223.400, mean reward: -1.111 [-111.700, 82.800], mean action: 1.826 [0.000, 7.000], mean observation: 32.504 [0.001, 532.400], loss: 363.860748, mae: 32.013275, mean_q: -31.659151\n",
            " 1104897/10000000: episode: 5497, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -399.800, mean reward: -1.989 [-199.900, 32.800], mean action: 1.766 [0.000, 10.000], mean observation: 33.337 [0.001, 595.600], loss: 321.300781, mae: 31.776747, mean_q: -31.367596\n",
            " 1105098/10000000: episode: 5498, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: 133.600, mean reward: 0.665 [-10.000, 299.600], mean action: 2.552 [0.000, 10.000], mean observation: 32.907 [0.000, 530.000], loss: 554.962158, mae: 30.851719, mean_q: -30.851183\n",
            " 1105299/10000000: episode: 5499, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -500.800, mean reward: -2.492 [-250.400, 52.800], mean action: 2.035 [0.000, 7.000], mean observation: 29.751 [0.002, 453.300], loss: 221.102371, mae: 31.401941, mean_q: -31.176483\n",
            " 1105500/10000000: episode: 5500, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -264.800, mean reward: -1.317 [-132.400, 50.800], mean action: 2.045 [0.000, 10.000], mean observation: 33.988 [0.001, 504.200], loss: 1164.014526, mae: 31.292501, mean_q: -30.934551\n",
            " 1105701/10000000: episode: 5501, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -573.800, mean reward: -2.855 [-286.900, 29.500], mean action: 2.502 [0.000, 10.000], mean observation: 27.175 [0.003, 591.100], loss: 505.569977, mae: 30.947683, mean_q: -31.092958\n",
            " 1105902/10000000: episode: 5502, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -494.600, mean reward: -2.461 [-247.300, 58.700], mean action: 2.433 [0.000, 10.000], mean observation: 37.561 [0.001, 578.100], loss: 520.792542, mae: 30.649965, mean_q: -30.795412\n",
            " 1106103/10000000: episode: 5503, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -266.400, mean reward: -1.325 [-133.200, 131.600], mean action: 2.020 [0.000, 10.000], mean observation: 30.452 [0.001, 598.700], loss: 1248.295044, mae: 31.007643, mean_q: -30.842150\n",
            " 1106304/10000000: episode: 5504, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -306.800, mean reward: -1.526 [-153.400, 51.000], mean action: 1.796 [0.000, 10.000], mean observation: 31.597 [0.000, 305.100], loss: 433.734375, mae: 31.048084, mean_q: -30.806728\n",
            " 1106505/10000000: episode: 5505, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: -321.600, mean reward: -1.600 [-160.800, 86.400], mean action: 2.423 [0.000, 10.000], mean observation: 32.214 [0.000, 540.000], loss: 483.848877, mae: 30.652218, mean_q: -30.569267\n",
            " 1106706/10000000: episode: 5506, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -385.000, mean reward: -1.915 [-192.500, 35.000], mean action: 2.532 [0.000, 10.000], mean observation: 30.879 [0.000, 663.800], loss: 435.512817, mae: 30.670626, mean_q: -30.636078\n",
            " 1106907/10000000: episode: 5507, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: -652.200, mean reward: -3.245 [-326.100, 73.800], mean action: 2.900 [0.000, 10.000], mean observation: 31.295 [0.000, 423.600], loss: 326.319214, mae: 30.546700, mean_q: -30.630068\n",
            " 1107108/10000000: episode: 5508, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -654.600, mean reward: -3.257 [-327.300, 68.500], mean action: 3.483 [0.000, 10.000], mean observation: 34.234 [0.000, 639.500], loss: 1034.600586, mae: 30.583469, mean_q: -30.921555\n",
            " 1107309/10000000: episode: 5509, duration: 1.531s, episode steps: 201, steps per second: 131, episode reward: -998.200, mean reward: -4.966 [-499.100, 200.000], mean action: 4.343 [0.000, 10.000], mean observation: 33.359 [0.001, 619.200], loss: 319.901184, mae: 30.662930, mean_q: -31.065027\n",
            " 1107510/10000000: episode: 5510, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -925.000, mean reward: -4.602 [-462.500, 61.000], mean action: 3.577 [0.000, 10.000], mean observation: 34.029 [0.001, 651.100], loss: 257.839630, mae: 31.128820, mean_q: -31.571907\n",
            " 1107711/10000000: episode: 5511, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -458.000, mean reward: -2.279 [-229.000, 154.800], mean action: 3.199 [0.000, 10.000], mean observation: 36.835 [0.000, 818.100], loss: 280.826172, mae: 31.651918, mean_q: -32.331497\n",
            " 1107912/10000000: episode: 5512, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -485.000, mean reward: -2.413 [-242.500, 50.400], mean action: 2.542 [0.000, 10.000], mean observation: 33.181 [0.003, 540.500], loss: 314.083069, mae: 31.911331, mean_q: -32.311047\n",
            " 1108113/10000000: episode: 5513, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -673.400, mean reward: -3.350 [-336.700, 43.800], mean action: 2.572 [0.000, 10.000], mean observation: 35.502 [0.001, 565.900], loss: 365.660919, mae: 32.318363, mean_q: -32.624916\n",
            " 1108314/10000000: episode: 5514, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -752.400, mean reward: -3.743 [-376.200, 20.000], mean action: 2.244 [0.000, 10.000], mean observation: 31.605 [0.002, 412.300], loss: 1192.690308, mae: 32.312115, mean_q: -32.720146\n",
            " 1108515/10000000: episode: 5515, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -568.000, mean reward: -2.826 [-284.000, 54.500], mean action: 2.164 [0.000, 10.000], mean observation: 31.306 [0.000, 720.900], loss: 238.218475, mae: 32.860905, mean_q: -33.398983\n",
            " 1108716/10000000: episode: 5516, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -610.800, mean reward: -3.039 [-305.400, 110.400], mean action: 2.736 [0.000, 10.000], mean observation: 33.354 [0.002, 624.500], loss: 295.339355, mae: 33.182587, mean_q: -33.645981\n",
            " 1108917/10000000: episode: 5517, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -366.800, mean reward: -1.825 [-183.400, 77.000], mean action: 2.025 [0.000, 10.000], mean observation: 32.480 [0.003, 497.700], loss: 367.110046, mae: 33.830605, mean_q: -34.030766\n",
            " 1109118/10000000: episode: 5518, duration: 1.563s, episode steps: 201, steps per second: 129, episode reward: -243.400, mean reward: -1.211 [-121.700, 47.200], mean action: 1.562 [0.000, 10.000], mean observation: 27.285 [0.001, 419.900], loss: 319.205566, mae: 33.956165, mean_q: -33.837975\n",
            " 1109319/10000000: episode: 5519, duration: 1.523s, episode steps: 201, steps per second: 132, episode reward: -70.600, mean reward: -0.351 [-35.300, 83.400], mean action: 1.194 [0.000, 10.000], mean observation: 38.917 [0.000, 615.100], loss: 1240.138916, mae: 33.905682, mean_q: -33.709881\n",
            " 1109520/10000000: episode: 5520, duration: 1.556s, episode steps: 201, steps per second: 129, episode reward: -424.000, mean reward: -2.109 [-212.000, 60.600], mean action: 1.781 [0.000, 8.000], mean observation: 34.872 [0.000, 642.700], loss: 535.436707, mae: 33.577785, mean_q: -33.766747\n",
            " 1109721/10000000: episode: 5521, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: 14.400, mean reward: 0.072 [-10.000, 153.600], mean action: 2.189 [0.000, 10.000], mean observation: 35.316 [0.000, 907.100], loss: 607.596252, mae: 33.394405, mean_q: -33.474625\n",
            " 1109922/10000000: episode: 5522, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -487.200, mean reward: -2.424 [-243.600, 33.200], mean action: 2.139 [0.000, 10.000], mean observation: 32.661 [0.001, 507.200], loss: 387.090485, mae: 32.794582, mean_q: -33.011772\n",
            " 1110123/10000000: episode: 5523, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -615.800, mean reward: -3.064 [-307.900, 85.200], mean action: 2.403 [0.000, 10.000], mean observation: 34.951 [0.002, 522.800], loss: 1231.947266, mae: 33.213242, mean_q: -33.324856\n",
            " 1110324/10000000: episode: 5524, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -70.800, mean reward: -0.352 [-35.400, 199.000], mean action: 2.960 [0.000, 10.000], mean observation: 29.980 [0.000, 623.900], loss: 1300.613159, mae: 32.928680, mean_q: -33.368595\n",
            " 1110525/10000000: episode: 5525, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -202.400, mean reward: -1.007 [-101.200, 106.500], mean action: 2.816 [0.000, 10.000], mean observation: 29.211 [0.001, 473.800], loss: 400.365753, mae: 32.967476, mean_q: -33.681595\n",
            " 1110726/10000000: episode: 5526, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -183.800, mean reward: -0.914 [-91.900, 140.800], mean action: 2.383 [0.000, 10.000], mean observation: 35.564 [0.000, 542.000], loss: 1182.824585, mae: 33.402775, mean_q: -33.947689\n",
            " 1110927/10000000: episode: 5527, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -138.400, mean reward: -0.689 [-69.200, 155.600], mean action: 2.134 [0.000, 10.000], mean observation: 36.713 [0.000, 781.200], loss: 216.874619, mae: 32.831139, mean_q: -33.472744\n",
            " 1111128/10000000: episode: 5528, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: 612.600, mean reward: 3.048 [-10.000, 388.000], mean action: 2.463 [0.000, 10.000], mean observation: 25.480 [0.001, 492.700], loss: 1129.157959, mae: 33.012379, mean_q: -33.668652\n",
            " 1111329/10000000: episode: 5529, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -223.800, mean reward: -1.113 [-111.900, 104.000], mean action: 2.552 [0.000, 10.000], mean observation: 33.377 [0.001, 488.900], loss: 530.315979, mae: 33.109180, mean_q: -33.676647\n",
            " 1111530/10000000: episode: 5530, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -417.600, mean reward: -2.078 [-208.800, 47.200], mean action: 2.313 [0.000, 10.000], mean observation: 33.116 [0.002, 467.200], loss: 369.286621, mae: 33.167931, mean_q: -33.942333\n",
            " 1111731/10000000: episode: 5531, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -578.400, mean reward: -2.878 [-289.200, 81.200], mean action: 2.816 [0.000, 10.000], mean observation: 34.712 [0.000, 511.400], loss: 463.767944, mae: 33.594730, mean_q: -34.471851\n",
            " 1111932/10000000: episode: 5532, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -248.000, mean reward: -1.234 [-124.000, 89.000], mean action: 2.692 [0.000, 10.000], mean observation: 36.122 [0.000, 764.600], loss: 1246.221558, mae: 33.827187, mean_q: -34.801147\n",
            " 1112133/10000000: episode: 5533, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -402.800, mean reward: -2.004 [-201.400, 200.200], mean action: 3.179 [0.000, 10.000], mean observation: 27.059 [0.004, 516.900], loss: 362.623199, mae: 34.035870, mean_q: -35.101913\n",
            " 1112334/10000000: episode: 5534, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: -538.200, mean reward: -2.678 [-269.100, 61.200], mean action: 2.995 [0.000, 10.000], mean observation: 31.793 [0.000, 598.900], loss: 302.386322, mae: 34.428654, mean_q: -35.499149\n",
            " 1112535/10000000: episode: 5535, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -698.800, mean reward: -3.477 [-349.400, 27.000], mean action: 2.632 [0.000, 10.000], mean observation: 38.336 [0.002, 508.700], loss: 297.803864, mae: 34.795452, mean_q: -35.759521\n",
            " 1112736/10000000: episode: 5536, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -784.600, mean reward: -3.903 [-392.300, 62.600], mean action: 2.612 [0.000, 10.000], mean observation: 35.720 [0.000, 792.800], loss: 1214.713257, mae: 34.602337, mean_q: -35.429745\n",
            " 1112937/10000000: episode: 5537, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -595.800, mean reward: -2.964 [-297.900, 64.000], mean action: 3.075 [0.000, 10.000], mean observation: 31.948 [0.000, 746.700], loss: 442.490997, mae: 34.543156, mean_q: -35.568817\n",
            " 1113138/10000000: episode: 5538, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -149.200, mean reward: -0.742 [-74.600, 175.200], mean action: 2.537 [0.000, 10.000], mean observation: 35.936 [0.001, 593.600], loss: 477.316040, mae: 35.136101, mean_q: -36.228619\n",
            " 1113339/10000000: episode: 5539, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: 75.600, mean reward: 0.376 [-10.000, 149.400], mean action: 2.522 [0.000, 10.000], mean observation: 36.653 [0.001, 541.500], loss: 202.418137, mae: 34.894627, mean_q: -35.865059\n",
            " 1113540/10000000: episode: 5540, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: 56.400, mean reward: 0.281 [-10.000, 312.800], mean action: 2.323 [0.000, 10.000], mean observation: 31.620 [0.000, 519.300], loss: 286.003113, mae: 35.064121, mean_q: -36.106003\n",
            " 1113741/10000000: episode: 5541, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -559.400, mean reward: -2.783 [-279.700, 52.000], mean action: 2.552 [0.000, 10.000], mean observation: 28.593 [0.001, 460.100], loss: 477.901093, mae: 35.075256, mean_q: -36.042347\n",
            " 1113942/10000000: episode: 5542, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -291.000, mean reward: -1.448 [-145.500, 225.800], mean action: 3.020 [0.000, 10.000], mean observation: 34.544 [0.003, 566.800], loss: 1159.463867, mae: 34.088085, mean_q: -34.815334\n",
            " 1114143/10000000: episode: 5543, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: 251.800, mean reward: 1.253 [-10.000, 524.400], mean action: 3.925 [0.000, 10.000], mean observation: 35.413 [0.000, 579.300], loss: 3024.968994, mae: 33.559292, mean_q: -34.136471\n",
            " 1114344/10000000: episode: 5544, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -648.400, mean reward: -3.226 [-324.200, 192.000], mean action: 4.005 [0.000, 10.000], mean observation: 27.047 [0.001, 402.600], loss: 1300.708008, mae: 32.915390, mean_q: -33.457535\n",
            " 1114545/10000000: episode: 5545, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -605.400, mean reward: -3.012 [-302.700, 141.000], mean action: 4.363 [0.000, 10.000], mean observation: 31.902 [0.001, 550.200], loss: 1398.896606, mae: 32.681873, mean_q: -33.237747\n",
            " 1114746/10000000: episode: 5546, duration: 1.570s, episode steps: 201, steps per second: 128, episode reward: -958.000, mean reward: -4.766 [-479.000, 118.800], mean action: 4.597 [0.000, 10.000], mean observation: 34.830 [0.001, 494.100], loss: 408.169708, mae: 32.356091, mean_q: -33.004673\n",
            " 1114947/10000000: episode: 5547, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -544.400, mean reward: -2.708 [-272.200, 148.800], mean action: 4.408 [0.000, 10.000], mean observation: 31.982 [0.001, 446.100], loss: 387.299713, mae: 32.382347, mean_q: -33.069313\n",
            " 1115148/10000000: episode: 5548, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -1175.200, mean reward: -5.847 [-587.600, 38.000], mean action: 3.756 [0.000, 10.000], mean observation: 41.069 [0.000, 593.600], loss: 507.633759, mae: 32.557278, mean_q: -33.114086\n",
            " 1115349/10000000: episode: 5549, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -261.200, mean reward: -1.300 [-130.600, 186.400], mean action: 3.124 [0.000, 10.000], mean observation: 33.768 [0.000, 803.400], loss: 281.771881, mae: 32.796730, mean_q: -33.490986\n",
            " 1115550/10000000: episode: 5550, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -682.600, mean reward: -3.396 [-341.300, 52.000], mean action: 2.537 [0.000, 10.000], mean observation: 34.116 [0.001, 455.800], loss: 361.916656, mae: 33.458580, mean_q: -34.079773\n",
            " 1115751/10000000: episode: 5551, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -706.400, mean reward: -3.514 [-353.200, 31.200], mean action: 2.776 [0.000, 10.000], mean observation: 33.169 [0.002, 525.700], loss: 1314.219360, mae: 33.911846, mean_q: -34.677670\n",
            " 1115952/10000000: episode: 5552, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -760.000, mean reward: -3.781 [-380.000, 43.000], mean action: 2.826 [0.000, 10.000], mean observation: 32.211 [0.001, 545.700], loss: 245.938904, mae: 33.967945, mean_q: -35.235897\n",
            " 1116153/10000000: episode: 5553, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -981.400, mean reward: -4.883 [-490.700, 76.000], mean action: 3.711 [0.000, 10.000], mean observation: 35.488 [0.000, 622.400], loss: 1235.029785, mae: 34.089348, mean_q: -35.235477\n",
            " 1116354/10000000: episode: 5554, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: 100.600, mean reward: 0.500 [-10.000, 385.000], mean action: 3.229 [0.000, 10.000], mean observation: 34.602 [0.000, 520.500], loss: 555.393555, mae: 34.321323, mean_q: -35.219021\n",
            " 1116555/10000000: episode: 5555, duration: 1.551s, episode steps: 201, steps per second: 130, episode reward: -231.400, mean reward: -1.151 [-115.700, 142.800], mean action: 2.657 [0.000, 10.000], mean observation: 36.531 [0.000, 669.400], loss: 453.945496, mae: 34.161770, mean_q: -34.973595\n",
            " 1116756/10000000: episode: 5556, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: -1147.400, mean reward: -5.708 [-573.700, 51.100], mean action: 4.015 [0.000, 10.000], mean observation: 34.238 [0.000, 556.200], loss: 1953.395874, mae: 33.583275, mean_q: -34.364662\n",
            " 1116957/10000000: episode: 5557, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -994.000, mean reward: -4.945 [-497.000, 98.000], mean action: 4.418 [0.000, 10.000], mean observation: 38.722 [0.000, 663.000], loss: 1430.138184, mae: 33.082462, mean_q: -33.487850\n",
            " 1117158/10000000: episode: 5558, duration: 1.542s, episode steps: 201, steps per second: 130, episode reward: 733.000, mean reward: 3.647 [-10.000, 522.000], mean action: 2.667 [0.000, 10.000], mean observation: 38.338 [0.002, 515.300], loss: 392.951813, mae: 32.586403, mean_q: -32.651417\n",
            " 1117359/10000000: episode: 5559, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -903.000, mean reward: -4.493 [-451.500, 38.000], mean action: 2.682 [0.000, 10.000], mean observation: 29.796 [0.000, 637.900], loss: 238.105240, mae: 32.661457, mean_q: -32.809494\n",
            " 1117560/10000000: episode: 5560, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: -659.200, mean reward: -3.280 [-329.600, 27.200], mean action: 2.139 [0.000, 10.000], mean observation: 33.641 [0.001, 459.000], loss: 517.498901, mae: 32.482689, mean_q: -32.681255\n",
            " 1117761/10000000: episode: 5561, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -402.400, mean reward: -2.002 [-201.200, 85.000], mean action: 2.124 [0.000, 10.000], mean observation: 35.374 [0.001, 581.100], loss: 454.336334, mae: 32.493904, mean_q: -32.206451\n",
            " 1117962/10000000: episode: 5562, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -295.200, mean reward: -1.469 [-147.600, 64.400], mean action: 1.443 [0.000, 10.000], mean observation: 33.119 [0.001, 622.800], loss: 504.537323, mae: 32.108429, mean_q: -31.236052\n",
            " 1118163/10000000: episode: 5563, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -520.800, mean reward: -2.591 [-260.400, 28.000], mean action: 1.602 [0.000, 10.000], mean observation: 30.563 [0.000, 570.400], loss: 337.130280, mae: 31.696890, mean_q: -31.093279\n",
            " 1118364/10000000: episode: 5564, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -143.000, mean reward: -0.711 [-71.500, 68.400], mean action: 1.657 [0.000, 10.000], mean observation: 36.965 [0.001, 446.000], loss: 419.816864, mae: 32.040188, mean_q: -31.236620\n",
            " 1118565/10000000: episode: 5565, duration: 1.517s, episode steps: 201, steps per second: 132, episode reward: -349.000, mean reward: -1.736 [-174.500, 164.800], mean action: 2.274 [0.000, 10.000], mean observation: 34.314 [0.000, 749.600], loss: 607.889709, mae: 30.880787, mean_q: -30.974255\n",
            " 1118766/10000000: episode: 5566, duration: 1.525s, episode steps: 201, steps per second: 132, episode reward: 72.400, mean reward: 0.360 [-10.000, 462.000], mean action: 2.891 [0.000, 10.000], mean observation: 26.770 [0.002, 368.100], loss: 422.367523, mae: 31.214376, mean_q: -31.507784\n",
            " 1118967/10000000: episode: 5567, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: -166.600, mean reward: -0.829 [-83.300, 136.400], mean action: 2.507 [0.000, 10.000], mean observation: 34.882 [0.001, 587.600], loss: 420.130554, mae: 31.442713, mean_q: -31.748550\n",
            " 1119168/10000000: episode: 5568, duration: 1.659s, episode steps: 201, steps per second: 121, episode reward: -354.400, mean reward: -1.763 [-177.200, 202.100], mean action: 2.741 [0.000, 10.000], mean observation: 31.277 [0.001, 512.300], loss: 314.430664, mae: 31.460953, mean_q: -31.830822\n",
            " 1119369/10000000: episode: 5569, duration: 1.637s, episode steps: 201, steps per second: 123, episode reward: -76.800, mean reward: -0.382 [-38.400, 204.900], mean action: 2.662 [0.000, 10.000], mean observation: 33.586 [0.003, 415.200], loss: 466.880646, mae: 31.552225, mean_q: -31.752594\n",
            " 1119570/10000000: episode: 5570, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: -591.600, mean reward: -2.943 [-295.800, 47.600], mean action: 2.234 [0.000, 10.000], mean observation: 32.523 [0.000, 509.100], loss: 463.490753, mae: 31.418974, mean_q: -31.238373\n",
            " 1119771/10000000: episode: 5571, duration: 1.634s, episode steps: 201, steps per second: 123, episode reward: -217.400, mean reward: -1.082 [-108.700, 77.600], mean action: 1.652 [0.000, 10.000], mean observation: 33.636 [0.001, 590.900], loss: 438.206696, mae: 31.660076, mean_q: -31.632202\n",
            " 1119972/10000000: episode: 5572, duration: 1.708s, episode steps: 201, steps per second: 118, episode reward: -614.400, mean reward: -3.057 [-307.200, 32.700], mean action: 2.219 [0.000, 10.000], mean observation: 23.405 [0.000, 446.200], loss: 476.454742, mae: 31.789783, mean_q: -31.707184\n",
            " 1120173/10000000: episode: 5573, duration: 1.537s, episode steps: 201, steps per second: 131, episode reward: -329.000, mean reward: -1.637 [-164.500, 46.800], mean action: 1.667 [0.000, 10.000], mean observation: 36.036 [0.001, 504.300], loss: 468.806763, mae: 32.137894, mean_q: -31.830317\n",
            " 1120374/10000000: episode: 5574, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: 246.800, mean reward: 1.228 [-8.000, 123.400], mean action: 1.587 [0.000, 8.000], mean observation: 31.492 [0.001, 510.800], loss: 221.105988, mae: 32.561321, mean_q: -32.470673\n",
            " 1120575/10000000: episode: 5575, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -238.600, mean reward: -1.187 [-119.300, 45.600], mean action: 1.816 [0.000, 10.000], mean observation: 35.154 [0.002, 487.500], loss: 247.805328, mae: 32.647800, mean_q: -32.596363\n",
            " 1120776/10000000: episode: 5576, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: 210.600, mean reward: 1.048 [-10.000, 153.600], mean action: 1.632 [0.000, 10.000], mean observation: 33.837 [0.000, 637.000], loss: 281.341949, mae: 32.566418, mean_q: -32.329365\n",
            " 1120977/10000000: episode: 5577, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -319.000, mean reward: -1.587 [-159.500, 51.900], mean action: 1.905 [0.000, 10.000], mean observation: 30.629 [0.000, 640.400], loss: 330.055664, mae: 33.348667, mean_q: -33.356007\n",
            " 1121178/10000000: episode: 5578, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: 40.400, mean reward: 0.201 [-8.000, 81.600], mean action: 1.682 [0.000, 8.000], mean observation: 28.221 [0.001, 413.700], loss: 534.534912, mae: 33.112385, mean_q: -33.046379\n",
            " 1121379/10000000: episode: 5579, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: -426.600, mean reward: -2.122 [-213.300, 38.400], mean action: 2.104 [0.000, 8.000], mean observation: 34.388 [0.001, 512.800], loss: 376.323456, mae: 32.824429, mean_q: -32.945576\n",
            " 1121580/10000000: episode: 5580, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -400.000, mean reward: -1.990 [-200.000, 77.400], mean action: 2.184 [0.000, 10.000], mean observation: 31.246 [0.000, 558.800], loss: 454.433228, mae: 32.409790, mean_q: -32.580696\n",
            " 1121781/10000000: episode: 5581, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -330.200, mean reward: -1.643 [-165.100, 85.400], mean action: 2.075 [0.000, 10.000], mean observation: 40.472 [0.001, 584.600], loss: 477.528076, mae: 32.001671, mean_q: -32.141136\n",
            " 1121982/10000000: episode: 5582, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -661.600, mean reward: -3.292 [-330.800, 109.600], mean action: 2.483 [0.000, 10.000], mean observation: 33.822 [0.000, 455.400], loss: 407.559631, mae: 31.711254, mean_q: -31.952488\n",
            " 1122183/10000000: episode: 5583, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -609.000, mean reward: -3.030 [-304.500, 122.400], mean action: 2.766 [0.000, 10.000], mean observation: 34.297 [0.002, 510.200], loss: 279.472687, mae: 32.057167, mean_q: -32.288933\n",
            " 1122384/10000000: episode: 5584, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -925.000, mean reward: -4.602 [-462.500, 55.200], mean action: 3.184 [0.000, 10.000], mean observation: 29.056 [0.001, 486.800], loss: 517.023071, mae: 32.396851, mean_q: -32.828575\n",
            " 1122585/10000000: episode: 5585, duration: 1.549s, episode steps: 201, steps per second: 130, episode reward: 1094.800, mean reward: 5.447 [-10.000, 744.100], mean action: 3.204 [0.000, 10.000], mean observation: 34.943 [0.000, 708.200], loss: 582.492249, mae: 32.807487, mean_q: -33.220901\n",
            " 1122786/10000000: episode: 5586, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -232.400, mean reward: -1.156 [-116.200, 122.500], mean action: 2.756 [0.000, 10.000], mean observation: 37.650 [0.002, 500.200], loss: 393.718506, mae: 32.643391, mean_q: -32.817940\n",
            " 1122987/10000000: episode: 5587, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: 276.200, mean reward: 1.374 [-10.000, 324.500], mean action: 1.955 [0.000, 10.000], mean observation: 35.661 [0.000, 440.500], loss: 328.116241, mae: 32.807133, mean_q: -32.696964\n",
            " 1123188/10000000: episode: 5588, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: 198.400, mean reward: 0.987 [-10.000, 325.000], mean action: 1.945 [0.000, 10.000], mean observation: 34.156 [0.000, 566.800], loss: 357.042053, mae: 32.454861, mean_q: -32.056755\n",
            " 1123389/10000000: episode: 5589, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -535.600, mean reward: -2.665 [-267.800, 73.200], mean action: 2.154 [0.000, 10.000], mean observation: 32.645 [0.001, 571.900], loss: 237.975052, mae: 31.960850, mean_q: -31.814278\n",
            " 1123590/10000000: episode: 5590, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -609.600, mean reward: -3.033 [-304.800, 21.000], mean action: 2.234 [0.000, 10.000], mean observation: 29.960 [0.000, 490.900], loss: 355.770996, mae: 31.948355, mean_q: -31.802298\n",
            " 1123791/10000000: episode: 5591, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: 295.600, mean reward: 1.471 [-10.000, 376.600], mean action: 2.532 [0.000, 10.000], mean observation: 28.302 [0.001, 457.300], loss: 470.480499, mae: 32.034016, mean_q: -32.528774\n",
            " 1123992/10000000: episode: 5592, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -669.000, mean reward: -3.328 [-334.500, 134.400], mean action: 2.662 [0.000, 10.000], mean observation: 32.333 [0.000, 798.300], loss: 282.382507, mae: 32.579300, mean_q: -33.139488\n",
            " 1124193/10000000: episode: 5593, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -719.600, mean reward: -3.580 [-359.800, 46.000], mean action: 2.736 [0.000, 10.000], mean observation: 35.186 [0.001, 556.200], loss: 507.045929, mae: 33.222546, mean_q: -33.945419\n",
            " 1124394/10000000: episode: 5594, duration: 1.603s, episode steps: 201, steps per second: 125, episode reward: -839.600, mean reward: -4.177 [-419.800, 28.800], mean action: 2.657 [0.000, 10.000], mean observation: 29.905 [0.000, 481.500], loss: 288.956085, mae: 33.934620, mean_q: -34.655945\n",
            " 1124595/10000000: episode: 5595, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -280.400, mean reward: -1.395 [-140.200, 88.400], mean action: 3.005 [0.000, 10.000], mean observation: 31.309 [0.000, 668.900], loss: 403.923492, mae: 34.253845, mean_q: -35.429806\n",
            " 1124796/10000000: episode: 5596, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -90.800, mean reward: -0.452 [-45.400, 112.500], mean action: 2.289 [0.000, 10.000], mean observation: 31.961 [0.000, 506.300], loss: 347.688599, mae: 35.224747, mean_q: -36.092426\n",
            " 1124997/10000000: episode: 5597, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -245.200, mean reward: -1.220 [-122.600, 56.500], mean action: 1.741 [0.000, 8.000], mean observation: 36.546 [0.000, 605.900], loss: 597.320923, mae: 35.181683, mean_q: -35.853600\n",
            " 1125198/10000000: episode: 5598, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -229.200, mean reward: -1.140 [-114.600, 79.800], mean action: 2.826 [0.000, 10.000], mean observation: 35.331 [0.000, 650.000], loss: 576.505188, mae: 34.422829, mean_q: -35.243053\n",
            " 1125399/10000000: episode: 5599, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 509.200, mean reward: 2.533 [-10.000, 450.000], mean action: 2.831 [0.000, 10.000], mean observation: 33.829 [0.000, 574.000], loss: 356.287964, mae: 33.898006, mean_q: -34.753601\n",
            " 1125600/10000000: episode: 5600, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -271.400, mean reward: -1.350 [-135.700, 193.800], mean action: 2.249 [0.000, 7.000], mean observation: 31.444 [0.002, 420.100], loss: 334.997345, mae: 34.195095, mean_q: -35.172157\n",
            " 1125801/10000000: episode: 5601, duration: 1.540s, episode steps: 201, steps per second: 130, episode reward: -288.400, mean reward: -1.435 [-144.200, 108.900], mean action: 2.547 [0.000, 10.000], mean observation: 38.661 [0.001, 495.500], loss: 308.813751, mae: 34.723709, mean_q: -35.838188\n",
            " 1126002/10000000: episode: 5602, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -523.800, mean reward: -2.606 [-261.900, 87.000], mean action: 2.313 [0.000, 10.000], mean observation: 33.280 [0.001, 510.400], loss: 389.077393, mae: 35.215229, mean_q: -36.210350\n",
            " 1126203/10000000: episode: 5603, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: 608.400, mean reward: 3.027 [-10.000, 337.500], mean action: 2.333 [0.000, 10.000], mean observation: 36.349 [0.000, 508.900], loss: 329.602753, mae: 35.826302, mean_q: -36.847813\n",
            " 1126404/10000000: episode: 5604, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -55.000, mean reward: -0.274 [-27.500, 133.700], mean action: 2.154 [0.000, 10.000], mean observation: 25.915 [0.003, 472.800], loss: 376.496002, mae: 35.799980, mean_q: -36.723957\n",
            " 1126605/10000000: episode: 5605, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -168.600, mean reward: -0.839 [-84.300, 140.000], mean action: 1.821 [0.000, 10.000], mean observation: 35.519 [0.001, 618.400], loss: 452.404236, mae: 35.858578, mean_q: -36.417088\n",
            " 1126806/10000000: episode: 5606, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -295.800, mean reward: -1.472 [-147.900, 82.200], mean action: 1.985 [0.000, 10.000], mean observation: 32.406 [0.001, 512.000], loss: 382.180206, mae: 35.919975, mean_q: -36.582294\n",
            " 1127007/10000000: episode: 5607, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -319.400, mean reward: -1.589 [-159.700, 54.800], mean action: 1.517 [0.000, 8.000], mean observation: 32.316 [0.000, 586.000], loss: 301.884613, mae: 36.530056, mean_q: -37.062939\n",
            " 1127208/10000000: episode: 5608, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -80.200, mean reward: -0.399 [-40.100, 155.000], mean action: 1.642 [0.000, 7.000], mean observation: 30.275 [0.002, 431.000], loss: 323.049438, mae: 36.015099, mean_q: -36.423824\n",
            " 1127409/10000000: episode: 5609, duration: 1.809s, episode steps: 201, steps per second: 111, episode reward: -213.400, mean reward: -1.062 [-106.700, 88.000], mean action: 1.995 [0.000, 8.000], mean observation: 30.279 [0.001, 459.200], loss: 316.438721, mae: 35.583244, mean_q: -35.948318\n",
            " 1127610/10000000: episode: 5610, duration: 2.104s, episode steps: 201, steps per second: 96, episode reward: 145.000, mean reward: 0.721 [-8.000, 228.800], mean action: 1.821 [0.000, 8.000], mean observation: 34.594 [0.003, 422.300], loss: 151.490784, mae: 35.598309, mean_q: -36.067684\n",
            " 1127811/10000000: episode: 5611, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -137.400, mean reward: -0.684 [-68.700, 87.600], mean action: 1.577 [0.000, 10.000], mean observation: 32.262 [0.001, 497.500], loss: 518.835754, mae: 35.863758, mean_q: -36.219273\n",
            " 1128012/10000000: episode: 5612, duration: 1.508s, episode steps: 201, steps per second: 133, episode reward: -527.400, mean reward: -2.624 [-263.700, 33.000], mean action: 2.189 [0.000, 10.000], mean observation: 29.180 [0.002, 430.700], loss: 238.212234, mae: 35.732155, mean_q: -36.323879\n",
            " 1128213/10000000: episode: 5613, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -14.400, mean reward: -0.072 [-10.000, 135.500], mean action: 1.980 [0.000, 10.000], mean observation: 34.200 [0.000, 552.400], loss: 237.824066, mae: 35.928341, mean_q: -36.466496\n",
            " 1128414/10000000: episode: 5614, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -365.200, mean reward: -1.817 [-182.600, 21.600], mean action: 1.483 [0.000, 6.000], mean observation: 34.300 [0.000, 720.900], loss: 399.629456, mae: 36.412395, mean_q: -36.435688\n",
            " 1128615/10000000: episode: 5615, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -171.600, mean reward: -0.854 [-85.800, 116.800], mean action: 1.592 [0.000, 8.000], mean observation: 30.428 [0.001, 500.800], loss: 259.171722, mae: 35.634750, mean_q: -35.977554\n",
            " 1128816/10000000: episode: 5616, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: 141.600, mean reward: 0.704 [-10.000, 191.400], mean action: 2.075 [0.000, 10.000], mean observation: 31.874 [0.001, 418.500], loss: 323.600616, mae: 35.704273, mean_q: -36.159424\n",
            " 1129017/10000000: episode: 5617, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: 212.200, mean reward: 1.056 [-10.000, 118.400], mean action: 1.851 [0.000, 10.000], mean observation: 31.807 [0.000, 441.700], loss: 311.518066, mae: 35.201744, mean_q: -35.696346\n",
            " 1129218/10000000: episode: 5618, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -403.600, mean reward: -2.008 [-201.800, 52.500], mean action: 1.821 [0.000, 10.000], mean observation: 28.952 [0.002, 387.100], loss: 318.254974, mae: 35.339291, mean_q: -35.641190\n",
            " 1129419/10000000: episode: 5619, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -119.000, mean reward: -0.592 [-59.500, 113.400], mean action: 1.443 [0.000, 10.000], mean observation: 36.103 [0.002, 543.500], loss: 277.107269, mae: 35.892246, mean_q: -35.492878\n",
            " 1129620/10000000: episode: 5620, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -422.400, mean reward: -2.101 [-211.200, 4.800], mean action: 1.179 [0.000, 7.000], mean observation: 32.799 [0.001, 562.800], loss: 247.585953, mae: 35.563725, mean_q: -35.021538\n",
            " 1129821/10000000: episode: 5621, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: 310.000, mean reward: 1.542 [-7.000, 217.600], mean action: 1.448 [0.000, 7.000], mean observation: 35.241 [0.000, 693.300], loss: 268.316162, mae: 35.136127, mean_q: -34.468700\n",
            " 1130022/10000000: episode: 5622, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -34.800, mean reward: -0.173 [-17.400, 69.300], mean action: 1.005 [0.000, 10.000], mean observation: 36.671 [0.000, 727.600], loss: 293.560272, mae: 35.004696, mean_q: -33.927593\n",
            " 1130223/10000000: episode: 5623, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -495.200, mean reward: -2.464 [-247.600, 16.600], mean action: 1.582 [0.000, 7.000], mean observation: 35.676 [0.000, 479.600], loss: 276.132477, mae: 34.659767, mean_q: -34.226311\n",
            " 1130424/10000000: episode: 5624, duration: 1.536s, episode steps: 201, steps per second: 131, episode reward: 704.800, mean reward: 3.506 [-10.000, 352.400], mean action: 1.687 [0.000, 10.000], mean observation: 34.113 [0.001, 606.200], loss: 259.319305, mae: 34.573944, mean_q: -34.481602\n",
            " 1130625/10000000: episode: 5625, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -454.000, mean reward: -2.259 [-227.000, 68.800], mean action: 1.896 [0.000, 10.000], mean observation: 32.356 [0.001, 597.900], loss: 261.915466, mae: 34.802807, mean_q: -34.520084\n",
            " 1130826/10000000: episode: 5626, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -134.000, mean reward: -0.667 [-67.000, 135.100], mean action: 1.552 [0.000, 8.000], mean observation: 32.313 [0.004, 482.900], loss: 244.735947, mae: 34.715294, mean_q: -34.646530\n",
            " 1131027/10000000: episode: 5627, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -61.000, mean reward: -0.303 [-30.500, 74.900], mean action: 2.050 [0.000, 10.000], mean observation: 26.932 [0.002, 435.000], loss: 312.859467, mae: 34.246906, mean_q: -34.269218\n",
            " 1131228/10000000: episode: 5628, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -673.200, mean reward: -3.349 [-336.600, 18.000], mean action: 1.940 [0.000, 10.000], mean observation: 33.512 [0.001, 697.100], loss: 194.987213, mae: 33.975368, mean_q: -34.320763\n",
            " 1131429/10000000: episode: 5629, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -656.600, mean reward: -3.267 [-328.300, 8.400], mean action: 1.821 [0.000, 7.000], mean observation: 31.066 [0.001, 562.800], loss: 350.693420, mae: 33.810356, mean_q: -34.177544\n",
            " 1131630/10000000: episode: 5630, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -783.800, mean reward: -3.900 [-391.900, 10.200], mean action: 2.129 [0.000, 10.000], mean observation: 33.700 [0.000, 458.900], loss: 324.477386, mae: 34.288151, mean_q: -34.496407\n",
            " 1131831/10000000: episode: 5631, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -494.800, mean reward: -2.462 [-247.400, 64.500], mean action: 2.244 [0.000, 10.000], mean observation: 33.137 [0.002, 522.200], loss: 413.705383, mae: 34.435936, mean_q: -34.506931\n",
            " 1132032/10000000: episode: 5632, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: 68.800, mean reward: 0.342 [-10.000, 94.400], mean action: 1.816 [0.000, 10.000], mean observation: 29.410 [0.000, 527.200], loss: 233.978531, mae: 34.815880, mean_q: -34.897263\n",
            " 1132233/10000000: episode: 5633, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -390.800, mean reward: -1.944 [-195.400, 92.000], mean action: 1.945 [0.000, 10.000], mean observation: 34.734 [0.000, 426.600], loss: 315.756134, mae: 34.221867, mean_q: -34.509945\n",
            " 1132434/10000000: episode: 5634, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -408.800, mean reward: -2.034 [-204.400, 26.600], mean action: 1.393 [0.000, 10.000], mean observation: 32.383 [0.003, 451.200], loss: 339.627594, mae: 34.230270, mean_q: -33.902466\n",
            " 1132635/10000000: episode: 5635, duration: 1.564s, episode steps: 201, steps per second: 129, episode reward: 494.600, mean reward: 2.461 [-10.000, 273.000], mean action: 1.871 [0.000, 10.000], mean observation: 32.464 [0.000, 765.000], loss: 399.873047, mae: 33.450314, mean_q: -33.427410\n",
            " 1132836/10000000: episode: 5636, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: 2649.600, mean reward: 13.182 [-10.000, 1324.800], mean action: 1.786 [0.000, 10.000], mean observation: 33.219 [0.002, 439.000], loss: 393.903107, mae: 33.218292, mean_q: -33.363979\n",
            " 1133037/10000000: episode: 5637, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: 141.600, mean reward: 0.704 [-10.000, 218.000], mean action: 1.856 [0.000, 10.000], mean observation: 39.729 [0.000, 793.800], loss: 302.975800, mae: 33.495510, mean_q: -33.402328\n",
            " 1133238/10000000: episode: 5638, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -129.800, mean reward: -0.646 [-64.900, 47.400], mean action: 1.687 [0.000, 10.000], mean observation: 33.987 [0.000, 633.800], loss: 406.748840, mae: 33.654621, mean_q: -33.355839\n",
            " 1133439/10000000: episode: 5639, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -502.800, mean reward: -2.501 [-251.400, 16.300], mean action: 1.562 [0.000, 10.000], mean observation: 38.551 [0.000, 701.500], loss: 204.633972, mae: 33.664448, mean_q: -33.577805\n",
            " 1133640/10000000: episode: 5640, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: 1013.400, mean reward: 5.042 [-10.000, 506.700], mean action: 1.483 [0.000, 10.000], mean observation: 35.551 [0.002, 542.400], loss: 353.449005, mae: 33.236195, mean_q: -32.918140\n",
            " 1133841/10000000: episode: 5641, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: 786.800, mean reward: 3.914 [-8.000, 393.400], mean action: 1.333 [0.000, 10.000], mean observation: 36.518 [0.000, 783.800], loss: 186.501694, mae: 33.220093, mean_q: -32.892616\n",
            " 1134042/10000000: episode: 5642, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -378.800, mean reward: -1.885 [-189.400, 33.000], mean action: 1.721 [0.000, 10.000], mean observation: 36.859 [0.002, 515.200], loss: 320.456146, mae: 33.290180, mean_q: -32.741962\n",
            " 1134243/10000000: episode: 5643, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -21.800, mean reward: -0.108 [-10.900, 56.000], mean action: 1.209 [0.000, 10.000], mean observation: 34.864 [0.003, 430.300], loss: 301.780334, mae: 32.932903, mean_q: -32.161556\n",
            " 1134444/10000000: episode: 5644, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -217.200, mean reward: -1.081 [-108.600, 68.400], mean action: 1.647 [0.000, 7.000], mean observation: 37.512 [0.000, 673.700], loss: 393.209656, mae: 32.540249, mean_q: -32.142380\n",
            " 1134645/10000000: episode: 5645, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -364.600, mean reward: -1.814 [-182.300, 51.800], mean action: 1.706 [0.000, 10.000], mean observation: 33.915 [0.002, 446.200], loss: 410.952271, mae: 32.249603, mean_q: -32.078716\n",
            " 1134846/10000000: episode: 5646, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -62.000, mean reward: -0.308 [-31.000, 118.300], mean action: 1.592 [0.000, 10.000], mean observation: 36.581 [0.001, 459.000], loss: 303.236816, mae: 32.141537, mean_q: -31.304529\n",
            " 1135047/10000000: episode: 5647, duration: 1.451s, episode steps: 201, steps per second: 138, episode reward: -230.600, mean reward: -1.147 [-115.300, 94.000], mean action: 1.507 [0.000, 10.000], mean observation: 34.243 [0.001, 558.800], loss: 309.989288, mae: 31.808361, mean_q: -30.901396\n",
            " 1135248/10000000: episode: 5648, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: 620.000, mean reward: 3.085 [-8.000, 310.000], mean action: 1.189 [0.000, 8.000], mean observation: 33.682 [0.000, 933.200], loss: 228.520477, mae: 31.444733, mean_q: -30.697147\n",
            " 1135449/10000000: episode: 5649, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -13.800, mean reward: -0.069 [-7.000, 84.000], mean action: 1.348 [0.000, 7.000], mean observation: 31.490 [0.000, 684.900], loss: 353.062347, mae: 31.102043, mean_q: -30.320383\n",
            " 1135650/10000000: episode: 5650, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -447.200, mean reward: -2.225 [-223.600, 48.600], mean action: 1.771 [0.000, 10.000], mean observation: 31.260 [0.004, 507.800], loss: 204.409027, mae: 30.132404, mean_q: -29.989902\n",
            " 1135851/10000000: episode: 5651, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -189.400, mean reward: -0.942 [-94.700, 143.100], mean action: 2.249 [0.000, 10.000], mean observation: 33.324 [0.000, 556.700], loss: 334.978455, mae: 30.004644, mean_q: -30.281649\n",
            " 1136052/10000000: episode: 5652, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -726.600, mean reward: -3.615 [-363.300, 100.000], mean action: 2.562 [0.000, 10.000], mean observation: 38.122 [0.000, 607.900], loss: 225.879166, mae: 30.448666, mean_q: -30.792622\n",
            " 1136253/10000000: episode: 5653, duration: 1.431s, episode steps: 201, steps per second: 141, episode reward: -237.600, mean reward: -1.182 [-118.800, 140.500], mean action: 1.811 [0.000, 10.000], mean observation: 32.935 [0.001, 423.500], loss: 215.658615, mae: 30.948719, mean_q: -30.414091\n",
            " 1136454/10000000: episode: 5654, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -320.800, mean reward: -1.596 [-160.400, 30.600], mean action: 1.517 [0.000, 10.000], mean observation: 35.612 [0.000, 528.400], loss: 185.146423, mae: 30.556692, mean_q: -29.972574\n",
            " 1136655/10000000: episode: 5655, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -208.200, mean reward: -1.036 [-104.100, 70.800], mean action: 2.363 [0.000, 10.000], mean observation: 29.168 [0.003, 464.200], loss: 290.974487, mae: 30.058365, mean_q: -30.562141\n",
            " 1136856/10000000: episode: 5656, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -516.000, mean reward: -2.567 [-258.000, 61.500], mean action: 2.458 [0.000, 10.000], mean observation: 31.768 [0.000, 818.500], loss: 259.006195, mae: 30.484091, mean_q: -30.708858\n",
            " 1137057/10000000: episode: 5657, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -408.000, mean reward: -2.030 [-204.000, 30.000], mean action: 1.363 [0.000, 10.000], mean observation: 29.852 [0.002, 449.500], loss: 415.757233, mae: 30.470526, mean_q: -30.060810\n",
            " 1137258/10000000: episode: 5658, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -161.800, mean reward: -0.805 [-80.900, 115.000], mean action: 2.000 [0.000, 10.000], mean observation: 35.079 [0.001, 619.000], loss: 392.890289, mae: 29.900864, mean_q: -29.687632\n",
            " 1137459/10000000: episode: 5659, duration: 1.514s, episode steps: 201, steps per second: 133, episode reward: -673.000, mean reward: -3.348 [-336.500, 17.200], mean action: 1.891 [0.000, 10.000], mean observation: 35.600 [0.002, 402.500], loss: 222.976257, mae: 29.702780, mean_q: -29.468410\n",
            " 1137660/10000000: episode: 5660, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -228.000, mean reward: -1.134 [-114.000, 66.500], mean action: 1.801 [0.000, 10.000], mean observation: 29.909 [0.000, 500.900], loss: 213.580353, mae: 30.474808, mean_q: -30.487431\n",
            " 1137861/10000000: episode: 5661, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -425.800, mean reward: -2.118 [-212.900, 17.000], mean action: 1.418 [0.000, 7.000], mean observation: 40.382 [0.001, 501.600], loss: 283.188293, mae: 31.218475, mean_q: -30.782885\n",
            " 1138062/10000000: episode: 5662, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -360.800, mean reward: -1.795 [-180.400, 43.600], mean action: 1.478 [0.000, 8.000], mean observation: 31.682 [0.001, 590.900], loss: 318.693939, mae: 31.178818, mean_q: -30.557793\n",
            " 1138263/10000000: episode: 5663, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: 25.800, mean reward: 0.128 [-8.000, 98.900], mean action: 1.403 [0.000, 8.000], mean observation: 32.054 [0.000, 747.100], loss: 533.687317, mae: 30.568981, mean_q: -30.152456\n",
            " 1138464/10000000: episode: 5664, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -470.800, mean reward: -2.342 [-235.400, 23.000], mean action: 1.940 [0.000, 10.000], mean observation: 36.010 [0.001, 502.600], loss: 321.943848, mae: 30.198092, mean_q: -30.202049\n",
            " 1138665/10000000: episode: 5665, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -211.400, mean reward: -1.052 [-105.700, 86.400], mean action: 2.119 [0.000, 10.000], mean observation: 28.754 [0.000, 534.800], loss: 283.623016, mae: 30.488722, mean_q: -30.297934\n",
            " 1138866/10000000: episode: 5666, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -484.000, mean reward: -2.408 [-242.000, 56.400], mean action: 1.791 [0.000, 8.000], mean observation: 29.761 [0.000, 580.300], loss: 220.161926, mae: 29.905434, mean_q: -29.520365\n",
            " 1139067/10000000: episode: 5667, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -502.200, mean reward: -2.499 [-251.100, 16.500], mean action: 1.542 [0.000, 7.000], mean observation: 36.275 [0.000, 522.100], loss: 308.006775, mae: 29.819672, mean_q: -29.075974\n",
            " 1139268/10000000: episode: 5668, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -94.600, mean reward: -0.471 [-47.300, 55.200], mean action: 1.871 [0.000, 10.000], mean observation: 28.508 [0.001, 589.600], loss: 324.088959, mae: 28.944569, mean_q: -28.183079\n",
            " 1139469/10000000: episode: 5669, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -415.600, mean reward: -2.068 [-207.800, 25.200], mean action: 1.667 [0.000, 10.000], mean observation: 34.283 [0.000, 813.800], loss: 257.178009, mae: 29.249155, mean_q: -28.654345\n",
            " 1139670/10000000: episode: 5670, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -228.000, mean reward: -1.134 [-114.000, 144.000], mean action: 1.607 [0.000, 8.000], mean observation: 38.513 [0.000, 621.900], loss: 319.296722, mae: 28.903208, mean_q: -28.020206\n",
            " 1139871/10000000: episode: 5671, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -265.600, mean reward: -1.321 [-132.800, 123.600], mean action: 1.736 [0.000, 10.000], mean observation: 29.883 [0.002, 632.400], loss: 261.753937, mae: 28.148767, mean_q: -27.241026\n",
            " 1140072/10000000: episode: 5672, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -437.600, mean reward: -2.177 [-218.800, 116.200], mean action: 2.114 [0.000, 10.000], mean observation: 34.360 [0.002, 519.900], loss: 468.316162, mae: 28.167362, mean_q: -27.634562\n",
            " 1140273/10000000: episode: 5673, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -171.400, mean reward: -0.853 [-85.700, 106.500], mean action: 2.040 [0.000, 10.000], mean observation: 35.091 [0.001, 518.800], loss: 448.427612, mae: 28.083738, mean_q: -27.730204\n",
            " 1140474/10000000: episode: 5674, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -591.400, mean reward: -2.942 [-295.700, 18.300], mean action: 2.159 [0.000, 10.000], mean observation: 40.338 [0.001, 490.400], loss: 293.001190, mae: 28.463547, mean_q: -28.163309\n",
            " 1140675/10000000: episode: 5675, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: 126.600, mean reward: 0.630 [-10.000, 149.100], mean action: 2.463 [0.000, 10.000], mean observation: 36.478 [0.001, 627.200], loss: 343.485840, mae: 28.214666, mean_q: -28.131899\n",
            " 1140876/10000000: episode: 5676, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: 296.000, mean reward: 1.473 [-8.000, 179.900], mean action: 2.443 [0.000, 10.000], mean observation: 38.928 [0.001, 501.700], loss: 449.254120, mae: 28.156977, mean_q: -27.583937\n",
            " 1141077/10000000: episode: 5677, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -263.600, mean reward: -1.311 [-131.800, 102.600], mean action: 2.308 [0.000, 10.000], mean observation: 32.383 [0.000, 604.500], loss: 499.050751, mae: 27.828688, mean_q: -27.171034\n",
            " 1141278/10000000: episode: 5678, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: -245.200, mean reward: -1.220 [-122.600, 125.300], mean action: 2.736 [0.000, 10.000], mean observation: 31.335 [0.001, 464.600], loss: 257.998383, mae: 27.484997, mean_q: -26.963669\n",
            " 1141479/10000000: episode: 5679, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: 18.800, mean reward: 0.094 [-10.000, 99.400], mean action: 2.478 [0.000, 10.000], mean observation: 27.590 [0.001, 531.100], loss: 325.227325, mae: 27.453367, mean_q: -26.815027\n",
            " 1141680/10000000: episode: 5680, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -275.600, mean reward: -1.371 [-137.800, 50.400], mean action: 2.368 [0.000, 10.000], mean observation: 36.630 [0.001, 507.400], loss: 442.842346, mae: 27.496544, mean_q: -26.791107\n",
            " 1141881/10000000: episode: 5681, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -345.000, mean reward: -1.716 [-172.500, 92.400], mean action: 2.368 [0.000, 10.000], mean observation: 33.406 [0.000, 467.700], loss: 326.418213, mae: 27.175213, mean_q: -26.544380\n",
            " 1142082/10000000: episode: 5682, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -542.000, mean reward: -2.697 [-271.000, 22.500], mean action: 1.920 [0.000, 10.000], mean observation: 39.453 [0.000, 784.200], loss: 198.636856, mae: 27.288324, mean_q: -26.390884\n",
            " 1142283/10000000: episode: 5683, duration: 1.640s, episode steps: 201, steps per second: 123, episode reward: 402.200, mean reward: 2.001 [-10.000, 471.100], mean action: 2.214 [0.000, 10.000], mean observation: 30.964 [0.000, 507.700], loss: 186.315765, mae: 27.552902, mean_q: -26.919888\n",
            " 1142484/10000000: episode: 5684, duration: 1.664s, episode steps: 201, steps per second: 121, episode reward: -246.000, mean reward: -1.224 [-123.000, 175.100], mean action: 1.846 [0.000, 8.000], mean observation: 34.551 [0.001, 542.200], loss: 280.757904, mae: 27.718294, mean_q: -26.648848\n",
            " 1142685/10000000: episode: 5685, duration: 1.765s, episode steps: 201, steps per second: 114, episode reward: -380.000, mean reward: -1.891 [-190.000, 43.500], mean action: 1.786 [0.000, 7.000], mean observation: 30.572 [0.000, 527.300], loss: 349.889465, mae: 27.723734, mean_q: -26.546335\n",
            " 1142886/10000000: episode: 5686, duration: 1.562s, episode steps: 201, steps per second: 129, episode reward: -595.800, mean reward: -2.964 [-297.900, 10.300], mean action: 1.716 [0.000, 10.000], mean observation: 31.611 [0.001, 542.400], loss: 293.956146, mae: 27.553093, mean_q: -26.372837\n",
            " 1143087/10000000: episode: 5687, duration: 1.538s, episode steps: 201, steps per second: 131, episode reward: -549.000, mean reward: -2.731 [-274.500, 46.200], mean action: 1.826 [0.000, 7.000], mean observation: 38.431 [0.000, 694.400], loss: 462.211182, mae: 27.189240, mean_q: -26.176199\n",
            " 1143288/10000000: episode: 5688, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -506.800, mean reward: -2.521 [-253.400, 34.900], mean action: 1.736 [0.000, 8.000], mean observation: 36.825 [0.000, 598.300], loss: 369.516876, mae: 27.063261, mean_q: -25.886116\n",
            " 1143489/10000000: episode: 5689, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -378.800, mean reward: -1.885 [-189.400, 73.500], mean action: 1.746 [0.000, 7.000], mean observation: 29.769 [0.001, 555.700], loss: 243.806595, mae: 27.250397, mean_q: -26.111149\n",
            " 1143690/10000000: episode: 5690, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -326.600, mean reward: -1.625 [-163.300, 94.200], mean action: 2.104 [0.000, 10.000], mean observation: 31.874 [0.000, 669.900], loss: 424.605713, mae: 27.422043, mean_q: -26.444973\n",
            " 1143891/10000000: episode: 5691, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -450.400, mean reward: -2.241 [-225.200, 36.300], mean action: 2.000 [0.000, 7.000], mean observation: 43.961 [0.001, 662.900], loss: 244.786591, mae: 27.275007, mean_q: -26.777485\n",
            " 1144092/10000000: episode: 5692, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: 446.000, mean reward: 2.219 [-10.000, 237.300], mean action: 2.129 [0.000, 10.000], mean observation: 32.341 [0.002, 441.800], loss: 231.372818, mae: 28.052214, mean_q: -27.358982\n",
            " 1144293/10000000: episode: 5693, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: 783.600, mean reward: 3.899 [-10.000, 391.800], mean action: 2.020 [0.000, 10.000], mean observation: 37.020 [0.001, 482.600], loss: 187.713943, mae: 28.492781, mean_q: -27.528942\n",
            " 1144494/10000000: episode: 5694, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -175.200, mean reward: -0.872 [-87.600, 78.000], mean action: 1.866 [0.000, 7.000], mean observation: 34.360 [0.000, 654.700], loss: 281.361542, mae: 28.865423, mean_q: -28.013954\n",
            " 1144695/10000000: episode: 5695, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -428.600, mean reward: -2.132 [-214.300, 41.000], mean action: 1.776 [0.000, 10.000], mean observation: 32.882 [0.000, 706.400], loss: 269.159119, mae: 28.384567, mean_q: -27.399103\n",
            " 1144896/10000000: episode: 5696, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -391.200, mean reward: -1.946 [-195.600, 103.200], mean action: 1.562 [0.000, 10.000], mean observation: 33.187 [0.001, 480.700], loss: 299.324890, mae: 28.007351, mean_q: -26.684813\n",
            " 1145097/10000000: episode: 5697, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: 382.400, mean reward: 1.902 [-10.000, 297.000], mean action: 2.055 [0.000, 10.000], mean observation: 32.490 [0.001, 463.400], loss: 358.866211, mae: 27.306559, mean_q: -26.479055\n",
            " 1145298/10000000: episode: 5698, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: -436.000, mean reward: -2.169 [-218.000, 31.200], mean action: 2.259 [0.000, 10.000], mean observation: 31.240 [0.001, 676.900], loss: 232.782532, mae: 26.648989, mean_q: -25.949549\n",
            " 1145499/10000000: episode: 5699, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -442.400, mean reward: -2.201 [-221.200, 90.400], mean action: 2.289 [0.000, 10.000], mean observation: 37.465 [0.001, 714.300], loss: 267.334930, mae: 27.221884, mean_q: -26.793341\n",
            " 1145700/10000000: episode: 5700, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -534.200, mean reward: -2.658 [-267.100, 82.000], mean action: 2.473 [0.000, 10.000], mean observation: 37.451 [0.000, 601.500], loss: 227.560303, mae: 26.663857, mean_q: -26.412037\n",
            " 1145901/10000000: episode: 5701, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: 96.200, mean reward: 0.479 [-10.000, 198.600], mean action: 2.701 [0.000, 10.000], mean observation: 32.804 [0.001, 582.800], loss: 174.040268, mae: 26.679844, mean_q: -26.454882\n",
            " 1146102/10000000: episode: 5702, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -336.000, mean reward: -1.672 [-168.000, 46.500], mean action: 2.209 [0.000, 10.000], mean observation: 36.674 [0.000, 566.700], loss: 333.347412, mae: 26.694347, mean_q: -26.196190\n",
            " 1146303/10000000: episode: 5703, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: 47.400, mean reward: 0.236 [-10.000, 151.900], mean action: 2.592 [0.000, 10.000], mean observation: 28.116 [0.002, 508.500], loss: 254.112762, mae: 26.963776, mean_q: -26.520212\n",
            " 1146504/10000000: episode: 5704, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: 122.600, mean reward: 0.610 [-10.000, 229.200], mean action: 2.891 [0.000, 10.000], mean observation: 30.546 [0.004, 499.200], loss: 233.854172, mae: 26.666258, mean_q: -26.172726\n",
            " 1146705/10000000: episode: 5705, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -826.200, mean reward: -4.110 [-413.100, 37.200], mean action: 2.920 [0.000, 10.000], mean observation: 33.914 [0.000, 376.900], loss: 281.812408, mae: 26.958294, mean_q: -26.431030\n",
            " 1146906/10000000: episode: 5706, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -1045.400, mean reward: -5.201 [-522.700, 26.800], mean action: 3.030 [0.000, 10.000], mean observation: 36.593 [0.001, 630.500], loss: 362.568359, mae: 26.937910, mean_q: -26.560318\n",
            " 1147107/10000000: episode: 5707, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 1148.200, mean reward: 5.712 [-10.000, 792.600], mean action: 2.955 [0.000, 10.000], mean observation: 30.548 [0.001, 518.600], loss: 379.518005, mae: 26.751419, mean_q: -26.351786\n",
            " 1147308/10000000: episode: 5708, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 429.400, mean reward: 2.136 [-10.000, 555.500], mean action: 2.756 [0.000, 10.000], mean observation: 36.705 [0.000, 562.200], loss: 196.663589, mae: 27.013985, mean_q: -26.837725\n",
            " 1147509/10000000: episode: 5709, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -7.000, mean reward: -0.035 [-10.000, 269.200], mean action: 2.517 [0.000, 10.000], mean observation: 31.152 [0.002, 536.400], loss: 294.541290, mae: 27.716953, mean_q: -27.537960\n",
            " 1147710/10000000: episode: 5710, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -658.400, mean reward: -3.276 [-329.200, 20.000], mean action: 2.075 [0.000, 10.000], mean observation: 33.432 [0.002, 637.200], loss: 343.248627, mae: 28.109177, mean_q: -27.222757\n",
            " 1147911/10000000: episode: 5711, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: 239.600, mean reward: 1.192 [-10.000, 166.800], mean action: 1.965 [0.000, 10.000], mean observation: 27.359 [0.002, 607.700], loss: 358.220551, mae: 27.929235, mean_q: -27.206400\n",
            " 1148112/10000000: episode: 5712, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -179.600, mean reward: -0.894 [-89.800, 48.000], mean action: 1.428 [0.000, 10.000], mean observation: 29.179 [0.001, 497.800], loss: 184.736343, mae: 28.264980, mean_q: -26.998898\n",
            " 1148313/10000000: episode: 5713, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -342.200, mean reward: -1.702 [-171.100, 33.400], mean action: 1.393 [0.000, 8.000], mean observation: 35.578 [0.001, 462.800], loss: 291.376617, mae: 28.478340, mean_q: -26.765991\n",
            " 1148514/10000000: episode: 5714, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -175.200, mean reward: -0.872 [-87.600, 36.000], mean action: 1.537 [0.000, 10.000], mean observation: 38.695 [0.000, 772.000], loss: 169.314896, mae: 28.352638, mean_q: -26.806581\n",
            " 1148715/10000000: episode: 5715, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: 4.000, mean reward: 0.020 [-7.000, 76.200], mean action: 1.488 [0.000, 7.000], mean observation: 28.362 [0.000, 471.500], loss: 194.446518, mae: 28.533548, mean_q: -27.091211\n",
            " 1148916/10000000: episode: 5716, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -446.800, mean reward: -2.223 [-223.400, 26.800], mean action: 1.746 [0.000, 8.000], mean observation: 32.512 [0.000, 558.800], loss: 225.973648, mae: 28.294037, mean_q: -27.494040\n",
            " 1149117/10000000: episode: 5717, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: 1145.600, mean reward: 5.700 [-7.000, 572.800], mean action: 1.607 [0.000, 7.000], mean observation: 34.680 [0.001, 645.600], loss: 172.063049, mae: 28.237934, mean_q: -27.453907\n",
            " 1149318/10000000: episode: 5718, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -579.600, mean reward: -2.884 [-289.800, 33.600], mean action: 1.940 [0.000, 10.000], mean observation: 32.248 [0.002, 487.300], loss: 215.966415, mae: 28.248728, mean_q: -27.183676\n",
            " 1149519/10000000: episode: 5719, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: 227.400, mean reward: 1.131 [-10.000, 113.700], mean action: 1.771 [0.000, 10.000], mean observation: 31.950 [0.000, 555.400], loss: 223.584732, mae: 27.572357, mean_q: -26.559572\n",
            " 1149720/10000000: episode: 5720, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -429.800, mean reward: -2.138 [-214.900, 22.000], mean action: 1.771 [0.000, 10.000], mean observation: 36.304 [0.000, 553.100], loss: 289.528015, mae: 27.497902, mean_q: -26.747227\n",
            " 1149921/10000000: episode: 5721, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -212.800, mean reward: -1.059 [-106.400, 97.800], mean action: 1.617 [0.000, 8.000], mean observation: 33.677 [0.000, 668.100], loss: 400.239655, mae: 27.601341, mean_q: -26.300726\n",
            " 1150122/10000000: episode: 5722, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -375.800, mean reward: -1.870 [-187.900, 37.600], mean action: 1.721 [0.000, 10.000], mean observation: 34.731 [0.000, 489.400], loss: 220.358383, mae: 27.225105, mean_q: -26.004377\n",
            " 1150323/10000000: episode: 5723, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -545.000, mean reward: -2.711 [-272.500, 19.000], mean action: 1.905 [0.000, 10.000], mean observation: 28.989 [0.000, 513.300], loss: 170.698105, mae: 27.062510, mean_q: -25.820089\n",
            " 1150524/10000000: episode: 5724, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -500.800, mean reward: -2.492 [-250.400, 39.200], mean action: 1.622 [0.000, 10.000], mean observation: 33.230 [0.000, 587.900], loss: 370.169128, mae: 27.333729, mean_q: -26.227938\n",
            " 1150725/10000000: episode: 5725, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -439.600, mean reward: -2.187 [-219.800, 66.600], mean action: 1.701 [0.000, 10.000], mean observation: 36.637 [0.000, 606.600], loss: 229.024078, mae: 27.031309, mean_q: -26.324772\n",
            " 1150926/10000000: episode: 5726, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -431.000, mean reward: -2.144 [-215.500, 61.700], mean action: 1.925 [0.000, 10.000], mean observation: 34.595 [0.001, 578.200], loss: 198.931274, mae: 27.529501, mean_q: -26.910486\n",
            " 1151127/10000000: episode: 5727, duration: 1.401s, episode steps: 201, steps per second: 144, episode reward: -224.400, mean reward: -1.116 [-112.200, 93.000], mean action: 1.861 [0.000, 10.000], mean observation: 29.385 [0.003, 461.100], loss: 319.499634, mae: 28.155275, mean_q: -27.132812\n",
            " 1151328/10000000: episode: 5728, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -410.400, mean reward: -2.042 [-205.200, 23.600], mean action: 1.627 [0.000, 10.000], mean observation: 38.354 [0.000, 497.600], loss: 245.292374, mae: 27.692398, mean_q: -26.672951\n",
            " 1151529/10000000: episode: 5729, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -815.000, mean reward: -4.055 [-407.500, 12.600], mean action: 2.368 [0.000, 10.000], mean observation: 33.564 [0.001, 675.300], loss: 317.551605, mae: 27.577972, mean_q: -27.098444\n",
            " 1151730/10000000: episode: 5730, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -406.600, mean reward: -2.023 [-203.300, 99.300], mean action: 2.891 [0.000, 10.000], mean observation: 32.426 [0.001, 605.800], loss: 209.489639, mae: 27.751627, mean_q: -27.777824\n",
            " 1151931/10000000: episode: 5731, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: 87.600, mean reward: 0.436 [-10.000, 161.700], mean action: 1.975 [0.000, 10.000], mean observation: 28.636 [0.000, 527.500], loss: 146.727142, mae: 27.860464, mean_q: -27.463537\n",
            " 1152132/10000000: episode: 5732, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: 107.800, mean reward: 0.536 [-10.000, 136.000], mean action: 2.149 [0.000, 10.000], mean observation: 31.948 [0.001, 466.900], loss: 235.631378, mae: 28.130636, mean_q: -27.708853\n",
            " 1152333/10000000: episode: 5733, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -51.200, mean reward: -0.255 [-25.600, 140.500], mean action: 1.925 [0.000, 10.000], mean observation: 37.029 [0.000, 544.700], loss: 212.637711, mae: 27.897884, mean_q: -27.511824\n",
            " 1152534/10000000: episode: 5734, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -278.400, mean reward: -1.385 [-139.200, 90.300], mean action: 1.925 [0.000, 10.000], mean observation: 36.996 [0.002, 532.900], loss: 173.084259, mae: 28.018446, mean_q: -27.517038\n",
            " 1152735/10000000: episode: 5735, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -447.000, mean reward: -2.224 [-223.500, 37.600], mean action: 1.507 [0.000, 10.000], mean observation: 33.128 [0.001, 647.400], loss: 200.489929, mae: 28.489683, mean_q: -27.164053\n",
            " 1152936/10000000: episode: 5736, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -217.600, mean reward: -1.083 [-108.800, 76.800], mean action: 1.642 [0.000, 10.000], mean observation: 30.777 [0.000, 602.400], loss: 303.730499, mae: 28.372227, mean_q: -27.201441\n",
            " 1153137/10000000: episode: 5737, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -601.600, mean reward: -2.993 [-300.800, 25.500], mean action: 1.776 [0.000, 8.000], mean observation: 34.444 [0.002, 453.500], loss: 194.812881, mae: 28.638847, mean_q: -27.929167\n",
            " 1153338/10000000: episode: 5738, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -282.200, mean reward: -1.404 [-141.100, 74.400], mean action: 1.990 [0.000, 10.000], mean observation: 29.829 [0.001, 458.100], loss: 276.985260, mae: 28.434420, mean_q: -27.908539\n",
            " 1153539/10000000: episode: 5739, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -204.200, mean reward: -1.016 [-102.100, 89.200], mean action: 1.915 [0.000, 10.000], mean observation: 31.256 [0.000, 695.400], loss: 235.069504, mae: 29.177692, mean_q: -28.644779\n",
            " 1153740/10000000: episode: 5740, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: 125.600, mean reward: 0.625 [-10.000, 105.900], mean action: 2.109 [0.000, 10.000], mean observation: 34.328 [0.000, 660.900], loss: 163.773132, mae: 28.987099, mean_q: -28.545473\n",
            " 1153941/10000000: episode: 5741, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -235.200, mean reward: -1.170 [-117.600, 106.000], mean action: 2.080 [0.000, 10.000], mean observation: 35.012 [0.001, 472.200], loss: 309.255554, mae: 28.665615, mean_q: -28.268965\n",
            " 1154142/10000000: episode: 5742, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: 321.000, mean reward: 1.597 [-10.000, 400.000], mean action: 2.289 [0.000, 10.000], mean observation: 33.573 [0.000, 446.900], loss: 274.431976, mae: 28.690926, mean_q: -28.564180\n",
            " 1154343/10000000: episode: 5743, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 374.000, mean reward: 1.861 [-10.000, 401.400], mean action: 1.970 [0.000, 10.000], mean observation: 34.542 [0.001, 654.600], loss: 201.263870, mae: 29.394085, mean_q: -29.094862\n",
            " 1154544/10000000: episode: 5744, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -400.000, mean reward: -1.990 [-200.000, 48.700], mean action: 2.677 [0.000, 10.000], mean observation: 28.854 [0.000, 466.800], loss: 254.781631, mae: 29.461826, mean_q: -29.574574\n",
            " 1154745/10000000: episode: 5745, duration: 1.382s, episode steps: 201, steps per second: 145, episode reward: -333.000, mean reward: -1.657 [-166.500, 72.200], mean action: 2.517 [0.000, 10.000], mean observation: 35.487 [0.000, 467.400], loss: 173.287277, mae: 29.799538, mean_q: -29.801056\n",
            " 1154946/10000000: episode: 5746, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -398.400, mean reward: -1.982 [-199.200, 28.000], mean action: 1.821 [0.000, 10.000], mean observation: 35.139 [0.000, 697.600], loss: 225.746078, mae: 30.202164, mean_q: -29.660162\n",
            " 1155147/10000000: episode: 5747, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -276.200, mean reward: -1.374 [-138.100, 160.300], mean action: 1.950 [0.000, 10.000], mean observation: 33.172 [0.000, 443.500], loss: 189.214111, mae: 30.055756, mean_q: -29.628557\n",
            " 1155348/10000000: episode: 5748, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -310.600, mean reward: -1.545 [-155.300, 64.500], mean action: 1.905 [0.000, 10.000], mean observation: 31.054 [0.001, 470.800], loss: 187.248093, mae: 30.229069, mean_q: -29.139965\n",
            " 1155549/10000000: episode: 5749, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -19.400, mean reward: -0.097 [-10.000, 116.000], mean action: 1.955 [0.000, 10.000], mean observation: 31.300 [0.002, 400.000], loss: 226.282257, mae: 29.413687, mean_q: -28.643112\n",
            " 1155750/10000000: episode: 5750, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: 12.400, mean reward: 0.062 [-10.000, 216.000], mean action: 2.264 [0.000, 10.000], mean observation: 31.364 [0.002, 515.900], loss: 192.547073, mae: 29.036594, mean_q: -28.542864\n",
            " 1155951/10000000: episode: 5751, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -172.200, mean reward: -0.857 [-86.100, 156.000], mean action: 2.204 [0.000, 10.000], mean observation: 34.584 [0.002, 454.800], loss: 200.504944, mae: 29.153883, mean_q: -28.434303\n",
            " 1156152/10000000: episode: 5752, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -199.400, mean reward: -0.992 [-99.700, 180.600], mean action: 2.423 [0.000, 10.000], mean observation: 32.508 [0.005, 467.700], loss: 158.882217, mae: 29.387642, mean_q: -28.986023\n",
            " 1156353/10000000: episode: 5753, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -344.600, mean reward: -1.714 [-172.300, 53.000], mean action: 2.567 [0.000, 10.000], mean observation: 25.448 [0.000, 638.600], loss: 316.337494, mae: 29.163111, mean_q: -29.078159\n",
            " 1156554/10000000: episode: 5754, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -512.000, mean reward: -2.547 [-256.000, 77.400], mean action: 2.582 [0.000, 10.000], mean observation: 38.861 [0.001, 584.000], loss: 217.500809, mae: 28.981146, mean_q: -28.474754\n",
            " 1156755/10000000: episode: 5755, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -438.800, mean reward: -2.183 [-219.400, 66.600], mean action: 2.090 [0.000, 10.000], mean observation: 30.868 [0.000, 530.500], loss: 328.550903, mae: 29.561666, mean_q: -28.938910\n",
            " 1156956/10000000: episode: 5756, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -335.200, mean reward: -1.668 [-167.600, 93.600], mean action: 2.045 [0.000, 10.000], mean observation: 33.291 [0.000, 796.200], loss: 241.429062, mae: 29.502583, mean_q: -29.189026\n",
            " 1157157/10000000: episode: 5757, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -372.600, mean reward: -1.854 [-186.300, 39.600], mean action: 1.791 [0.000, 10.000], mean observation: 38.691 [0.000, 588.400], loss: 157.887848, mae: 29.455208, mean_q: -29.056631\n",
            " 1157358/10000000: episode: 5758, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: 304.200, mean reward: 1.513 [-10.000, 420.500], mean action: 1.826 [0.000, 10.000], mean observation: 31.266 [0.000, 507.900], loss: 282.998993, mae: 29.896946, mean_q: -29.280390\n",
            " 1157559/10000000: episode: 5759, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 240.800, mean reward: 1.198 [-10.000, 294.800], mean action: 1.622 [0.000, 10.000], mean observation: 37.160 [0.001, 466.300], loss: 221.599426, mae: 30.190104, mean_q: -29.363909\n",
            " 1157760/10000000: episode: 5760, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -383.000, mean reward: -1.905 [-191.500, 44.200], mean action: 2.055 [0.000, 7.000], mean observation: 29.786 [0.003, 438.400], loss: 193.726089, mae: 29.719645, mean_q: -29.159466\n",
            " 1157961/10000000: episode: 5761, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: 519.400, mean reward: 2.584 [-10.000, 388.500], mean action: 1.537 [0.000, 10.000], mean observation: 26.103 [0.002, 545.200], loss: 387.338928, mae: 29.707867, mean_q: -28.126448\n",
            " 1158162/10000000: episode: 5762, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -133.600, mean reward: -0.665 [-66.800, 100.000], mean action: 1.682 [0.000, 10.000], mean observation: 37.602 [0.000, 491.000], loss: 158.749344, mae: 29.157812, mean_q: -27.714340\n",
            " 1158363/10000000: episode: 5763, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -46.000, mean reward: -0.229 [-23.000, 134.600], mean action: 1.607 [0.000, 7.000], mean observation: 31.784 [0.001, 481.400], loss: 270.570618, mae: 28.652124, mean_q: -27.376366\n",
            " 1158564/10000000: episode: 5764, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -356.400, mean reward: -1.773 [-178.200, 88.400], mean action: 2.234 [0.000, 8.000], mean observation: 30.319 [0.002, 455.400], loss: 179.835587, mae: 28.372293, mean_q: -27.658213\n",
            " 1158765/10000000: episode: 5765, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -525.000, mean reward: -2.612 [-262.500, 28.200], mean action: 2.020 [0.000, 10.000], mean observation: 30.606 [0.000, 746.900], loss: 143.841766, mae: 28.362305, mean_q: -27.572428\n",
            " 1158966/10000000: episode: 5766, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -452.000, mean reward: -2.249 [-226.000, 27.000], mean action: 2.010 [0.000, 10.000], mean observation: 33.905 [0.001, 583.400], loss: 182.445694, mae: 28.650139, mean_q: -27.829428\n",
            " 1159167/10000000: episode: 5767, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -397.800, mean reward: -1.979 [-198.900, 77.700], mean action: 1.756 [0.000, 10.000], mean observation: 31.741 [0.000, 702.100], loss: 229.710892, mae: 28.497318, mean_q: -27.866013\n",
            " 1159368/10000000: episode: 5768, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: 133.800, mean reward: 0.666 [-10.000, 213.000], mean action: 1.975 [0.000, 10.000], mean observation: 38.198 [0.000, 653.600], loss: 239.595810, mae: 28.854454, mean_q: -28.158224\n",
            " 1159569/10000000: episode: 5769, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -338.600, mean reward: -1.685 [-169.300, 40.600], mean action: 1.896 [0.000, 10.000], mean observation: 29.254 [0.002, 509.800], loss: 170.838196, mae: 29.188429, mean_q: -28.179924\n",
            " 1159770/10000000: episode: 5770, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -207.000, mean reward: -1.030 [-103.500, 57.000], mean action: 2.124 [0.000, 10.000], mean observation: 36.493 [0.000, 630.900], loss: 364.042786, mae: 28.693420, mean_q: -28.023127\n",
            " 1159971/10000000: episode: 5771, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: 171.600, mean reward: 0.854 [-10.000, 223.300], mean action: 2.164 [0.000, 10.000], mean observation: 38.393 [0.001, 571.000], loss: 236.482681, mae: 28.781143, mean_q: -28.235035\n",
            " 1160172/10000000: episode: 5772, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -390.800, mean reward: -1.944 [-195.400, 24.000], mean action: 1.527 [0.000, 10.000], mean observation: 37.053 [0.001, 591.000], loss: 144.643127, mae: 29.108982, mean_q: -27.897320\n",
            " 1160373/10000000: episode: 5773, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -165.400, mean reward: -0.823 [-82.700, 82.800], mean action: 1.677 [0.000, 7.000], mean observation: 29.876 [0.001, 532.400], loss: 186.111572, mae: 28.685030, mean_q: -27.490583\n",
            " 1160574/10000000: episode: 5774, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: 369.800, mean reward: 1.840 [-10.000, 252.000], mean action: 2.214 [0.000, 10.000], mean observation: 32.553 [0.001, 595.600], loss: 337.733368, mae: 28.259464, mean_q: -27.620714\n",
            " 1160775/10000000: episode: 5775, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 89.200, mean reward: 0.444 [-10.000, 145.200], mean action: 2.080 [0.000, 10.000], mean observation: 29.397 [0.000, 530.000], loss: 270.534424, mae: 27.930264, mean_q: -26.886000\n",
            " 1160976/10000000: episode: 5776, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: 351.800, mean reward: 1.750 [-10.000, 371.400], mean action: 2.557 [0.000, 10.000], mean observation: 30.714 [0.002, 453.300], loss: 293.652374, mae: 27.321999, mean_q: -26.438351\n",
            " 1161177/10000000: episode: 5777, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -161.800, mean reward: -0.805 [-80.900, 88.900], mean action: 2.801 [0.000, 7.000], mean observation: 36.029 [0.001, 591.100], loss: 472.226471, mae: 27.866320, mean_q: -27.347132\n",
            " 1161378/10000000: episode: 5778, duration: 1.612s, episode steps: 201, steps per second: 125, episode reward: -613.800, mean reward: -3.054 [-306.900, 60.200], mean action: 3.000 [0.000, 10.000], mean observation: 27.394 [0.003, 399.800], loss: 391.821838, mae: 28.029671, mean_q: -27.337147\n",
            " 1161579/10000000: episode: 5779, duration: 1.674s, episode steps: 201, steps per second: 120, episode reward: 658.200, mean reward: 3.275 [-10.000, 658.000], mean action: 2.149 [0.000, 10.000], mean observation: 39.752 [0.001, 598.700], loss: 184.696869, mae: 27.958143, mean_q: -27.243620\n",
            " 1161780/10000000: episode: 5780, duration: 1.681s, episode steps: 201, steps per second: 120, episode reward: -443.800, mean reward: -2.208 [-221.900, 42.000], mean action: 2.104 [0.000, 10.000], mean observation: 27.741 [0.002, 369.100], loss: 241.142776, mae: 28.230503, mean_q: -27.667337\n",
            " 1161981/10000000: episode: 5781, duration: 1.649s, episode steps: 201, steps per second: 122, episode reward: 93.400, mean reward: 0.465 [-7.000, 172.500], mean action: 2.090 [0.000, 7.000], mean observation: 32.659 [0.000, 461.800], loss: 175.374283, mae: 28.259649, mean_q: -27.500139\n",
            " 1162182/10000000: episode: 5782, duration: 1.693s, episode steps: 201, steps per second: 119, episode reward: -27.600, mean reward: -0.137 [-13.800, 122.500], mean action: 2.706 [0.000, 10.000], mean observation: 33.204 [0.000, 540.000], loss: 257.045349, mae: 28.155493, mean_q: -27.465296\n",
            " 1162383/10000000: episode: 5783, duration: 1.715s, episode steps: 201, steps per second: 117, episode reward: -269.600, mean reward: -1.341 [-134.800, 95.900], mean action: 3.353 [0.000, 10.000], mean observation: 28.032 [0.000, 663.800], loss: 136.038589, mae: 27.939026, mean_q: -27.268707\n",
            " 1162584/10000000: episode: 5784, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -550.600, mean reward: -2.739 [-275.300, 59.500], mean action: 2.692 [0.000, 10.000], mean observation: 35.084 [0.000, 463.200], loss: 437.104065, mae: 27.452532, mean_q: -26.591564\n",
            " 1162785/10000000: episode: 5785, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -708.600, mean reward: -3.525 [-354.300, 60.600], mean action: 2.856 [0.000, 10.000], mean observation: 32.436 [0.001, 639.500], loss: 151.762589, mae: 27.181740, mean_q: -26.696911\n",
            " 1162986/10000000: episode: 5786, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -391.200, mean reward: -1.946 [-195.600, 141.400], mean action: 3.025 [0.000, 10.000], mean observation: 31.861 [0.001, 619.200], loss: 178.206024, mae: 27.213396, mean_q: -26.641266\n",
            " 1163187/10000000: episode: 5787, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 172.400, mean reward: 0.858 [-10.000, 270.900], mean action: 2.537 [0.000, 10.000], mean observation: 35.324 [0.001, 651.100], loss: 316.157501, mae: 27.301262, mean_q: -26.498064\n",
            " 1163388/10000000: episode: 5788, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -709.800, mean reward: -3.531 [-354.900, 33.500], mean action: 2.393 [0.000, 10.000], mean observation: 39.270 [0.000, 818.100], loss: 185.061295, mae: 27.357029, mean_q: -26.707949\n",
            " 1163589/10000000: episode: 5789, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -277.000, mean reward: -1.378 [-138.500, 143.500], mean action: 2.990 [0.000, 10.000], mean observation: 28.362 [0.003, 540.500], loss: 269.003418, mae: 27.447071, mean_q: -26.767126\n",
            " 1163790/10000000: episode: 5790, duration: 1.543s, episode steps: 201, steps per second: 130, episode reward: -874.400, mean reward: -4.350 [-437.200, 30.200], mean action: 2.791 [0.000, 10.000], mean observation: 36.615 [0.001, 565.900], loss: 271.750305, mae: 27.174280, mean_q: -26.568298\n",
            " 1163991/10000000: episode: 5791, duration: 1.530s, episode steps: 201, steps per second: 131, episode reward: -466.600, mean reward: -2.321 [-233.300, 61.500], mean action: 2.527 [0.000, 10.000], mean observation: 32.560 [0.002, 488.800], loss: 157.150421, mae: 27.219229, mean_q: -26.421268\n",
            " 1164192/10000000: episode: 5792, duration: 1.528s, episode steps: 201, steps per second: 132, episode reward: -434.400, mean reward: -2.161 [-217.200, 51.400], mean action: 2.015 [0.000, 10.000], mean observation: 30.169 [0.000, 720.900], loss: 216.861023, mae: 27.802050, mean_q: -26.720573\n",
            " 1164393/10000000: episode: 5793, duration: 1.565s, episode steps: 201, steps per second: 128, episode reward: -71.000, mean reward: -0.353 [-35.500, 107.100], mean action: 2.095 [0.000, 7.000], mean observation: 35.782 [0.002, 624.500], loss: 299.500885, mae: 27.725080, mean_q: -26.524759\n",
            " 1164594/10000000: episode: 5794, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -504.800, mean reward: -2.511 [-252.400, 61.600], mean action: 2.274 [0.000, 7.000], mean observation: 28.542 [0.001, 488.700], loss: 185.505707, mae: 27.471155, mean_q: -26.543716\n",
            " 1164795/10000000: episode: 5795, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -7.600, mean reward: -0.038 [-10.000, 130.900], mean action: 2.269 [0.000, 10.000], mean observation: 32.632 [0.002, 476.600], loss: 217.402283, mae: 27.900133, mean_q: -26.871223\n",
            " 1164996/10000000: episode: 5796, duration: 1.534s, episode steps: 201, steps per second: 131, episode reward: -83.400, mean reward: -0.415 [-41.700, 230.600], mean action: 2.249 [0.000, 10.000], mean observation: 37.802 [0.000, 615.100], loss: 229.586945, mae: 27.779827, mean_q: -27.130209\n",
            " 1165197/10000000: episode: 5797, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -598.000, mean reward: -2.975 [-299.000, 83.300], mean action: 2.866 [0.000, 10.000], mean observation: 35.981 [0.000, 907.100], loss: 156.648560, mae: 27.699989, mean_q: -27.106852\n",
            " 1165398/10000000: episode: 5798, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 1.000, mean reward: 0.005 [-10.000, 169.200], mean action: 2.428 [0.000, 10.000], mean observation: 32.510 [0.001, 508.500], loss: 220.100067, mae: 27.673191, mean_q: -27.013634\n",
            " 1165599/10000000: episode: 5799, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: 5.800, mean reward: 0.029 [-10.000, 295.500], mean action: 2.542 [0.000, 10.000], mean observation: 35.998 [0.001, 507.200], loss: 146.745819, mae: 27.368494, mean_q: -27.131680\n",
            " 1165800/10000000: episode: 5800, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -1.600, mean reward: -0.008 [-10.000, 288.400], mean action: 2.443 [0.000, 10.000], mean observation: 31.563 [0.002, 522.800], loss: 253.146164, mae: 27.931517, mean_q: -27.478151\n",
            " 1166001/10000000: episode: 5801, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -271.400, mean reward: -1.350 [-135.700, 99.500], mean action: 2.095 [0.000, 10.000], mean observation: 28.594 [0.000, 623.900], loss: 283.189087, mae: 28.167978, mean_q: -27.207653\n",
            " 1166202/10000000: episode: 5802, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: 445.400, mean reward: 2.216 [-7.000, 524.700], mean action: 2.353 [0.000, 7.000], mean observation: 31.211 [0.001, 402.000], loss: 221.686920, mae: 28.103846, mean_q: -27.045582\n",
            " 1166403/10000000: episode: 5803, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -42.400, mean reward: -0.211 [-21.200, 140.800], mean action: 2.368 [0.000, 10.000], mean observation: 34.999 [0.000, 542.000], loss: 336.591888, mae: 28.055485, mean_q: -27.167677\n",
            " 1166604/10000000: episode: 5804, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -189.600, mean reward: -0.943 [-94.800, 84.800], mean action: 2.134 [0.000, 10.000], mean observation: 34.423 [0.000, 781.200], loss: 244.454254, mae: 27.877270, mean_q: -26.698465\n",
            " 1166805/10000000: episode: 5805, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: 13.200, mean reward: 0.066 [-10.000, 87.000], mean action: 2.254 [0.000, 10.000], mean observation: 27.135 [0.001, 492.700], loss: 321.733398, mae: 27.335247, mean_q: -25.915056\n",
            " 1167006/10000000: episode: 5806, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -52.800, mean reward: -0.263 [-26.400, 168.000], mean action: 2.483 [0.000, 10.000], mean observation: 34.915 [0.001, 488.900], loss: 307.641235, mae: 26.576853, mean_q: -25.349503\n",
            " 1167207/10000000: episode: 5807, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 336.600, mean reward: 1.675 [-7.000, 337.500], mean action: 1.836 [0.000, 7.000], mean observation: 34.480 [0.001, 511.400], loss: 182.844040, mae: 26.685478, mean_q: -25.097631\n",
            " 1167408/10000000: episode: 5808, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: 67.600, mean reward: 0.336 [-10.000, 203.000], mean action: 2.488 [0.000, 10.000], mean observation: 34.914 [0.000, 571.200], loss: 361.391388, mae: 26.468597, mean_q: -25.291653\n",
            " 1167609/10000000: episode: 5809, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -457.200, mean reward: -2.275 [-228.600, 35.000], mean action: 2.373 [0.000, 10.000], mean observation: 34.288 [0.000, 764.600], loss: 221.154144, mae: 26.566868, mean_q: -25.629169\n",
            " 1167810/10000000: episode: 5810, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: 19.800, mean reward: 0.099 [-10.000, 157.000], mean action: 2.632 [0.000, 10.000], mean observation: 25.061 [0.001, 516.900], loss: 154.779648, mae: 26.374763, mean_q: -25.872799\n",
            " 1168011/10000000: episode: 5811, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -758.000, mean reward: -3.771 [-379.000, 88.900], mean action: 2.806 [0.000, 10.000], mean observation: 33.677 [0.000, 598.900], loss: 277.669983, mae: 26.070074, mean_q: -25.815599\n",
            " 1168212/10000000: episode: 5812, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -890.400, mean reward: -4.430 [-445.200, 38.500], mean action: 2.910 [0.000, 10.000], mean observation: 38.490 [0.002, 508.700], loss: 210.247818, mae: 25.866400, mean_q: -25.421219\n",
            " 1168413/10000000: episode: 5813, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -305.600, mean reward: -1.520 [-152.800, 208.600], mean action: 2.881 [0.000, 10.000], mean observation: 34.283 [0.000, 792.800], loss: 377.417999, mae: 25.637142, mean_q: -25.126862\n",
            " 1168614/10000000: episode: 5814, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -260.400, mean reward: -1.296 [-130.200, 63.600], mean action: 2.756 [0.000, 10.000], mean observation: 38.066 [0.000, 746.700], loss: 311.433838, mae: 25.312830, mean_q: -24.542974\n",
            " 1168815/10000000: episode: 5815, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -371.800, mean reward: -1.850 [-185.900, 41.100], mean action: 2.239 [0.000, 10.000], mean observation: 30.464 [0.001, 375.100], loss: 195.547943, mae: 25.656940, mean_q: -24.692749\n",
            " 1169016/10000000: episode: 5816, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -637.400, mean reward: -3.171 [-318.700, 77.000], mean action: 2.493 [0.000, 10.000], mean observation: 35.434 [0.000, 541.500], loss: 178.993774, mae: 25.709976, mean_q: -24.633566\n",
            " 1169217/10000000: episode: 5817, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -459.400, mean reward: -2.286 [-229.700, 39.600], mean action: 2.244 [0.000, 10.000], mean observation: 31.035 [0.004, 519.300], loss: 203.393738, mae: 25.637964, mean_q: -24.596704\n",
            " 1169418/10000000: episode: 5818, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -440.400, mean reward: -2.191 [-220.200, 35.500], mean action: 2.214 [0.000, 10.000], mean observation: 34.288 [0.001, 460.100], loss: 177.199997, mae: 25.683996, mean_q: -24.456549\n",
            " 1169619/10000000: episode: 5819, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: 2985.400, mean reward: 14.853 [-10.000, 1748.000], mean action: 2.338 [0.000, 10.000], mean observation: 30.376 [0.003, 566.800], loss: 169.479553, mae: 25.485584, mean_q: -24.240593\n",
            " 1169820/10000000: episode: 5820, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -798.200, mean reward: -3.971 [-399.100, 41.000], mean action: 2.627 [0.000, 10.000], mean observation: 35.828 [0.000, 579.300], loss: 232.927048, mae: 25.217812, mean_q: -24.357374\n",
            " 1170021/10000000: episode: 5821, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -182.000, mean reward: -0.905 [-91.000, 112.700], mean action: 2.841 [0.000, 10.000], mean observation: 27.911 [0.001, 550.200], loss: 329.376282, mae: 24.579397, mean_q: -23.645351\n",
            " 1170222/10000000: episode: 5822, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -724.800, mean reward: -3.606 [-362.400, 53.700], mean action: 2.756 [0.000, 10.000], mean observation: 31.441 [0.001, 548.600], loss: 153.550919, mae: 25.072124, mean_q: -24.063696\n",
            " 1170423/10000000: episode: 5823, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -404.200, mean reward: -2.011 [-202.100, 35.400], mean action: 2.194 [0.000, 10.000], mean observation: 35.707 [0.002, 494.100], loss: 201.760025, mae: 24.980871, mean_q: -23.733984\n",
            " 1170624/10000000: episode: 5824, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -613.600, mean reward: -3.053 [-306.800, 61.000], mean action: 2.353 [0.000, 7.000], mean observation: 33.923 [0.000, 525.700], loss: 186.565277, mae: 25.000767, mean_q: -24.169676\n",
            " 1170825/10000000: episode: 5825, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -290.600, mean reward: -1.446 [-145.300, 74.000], mean action: 2.139 [0.000, 10.000], mean observation: 38.359 [0.000, 603.200], loss: 214.338364, mae: 25.626595, mean_q: -24.848545\n",
            " 1171026/10000000: episode: 5826, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 374.400, mean reward: 1.863 [-10.000, 326.200], mean action: 3.090 [0.000, 10.000], mean observation: 33.918 [0.000, 803.400], loss: 254.091339, mae: 25.378225, mean_q: -24.777166\n",
            " 1171227/10000000: episode: 5827, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -688.000, mean reward: -3.423 [-344.000, 38.400], mean action: 2.771 [0.000, 10.000], mean observation: 30.345 [0.001, 455.800], loss: 235.153534, mae: 25.534811, mean_q: -24.857174\n",
            " 1171428/10000000: episode: 5828, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -383.400, mean reward: -1.907 [-191.700, 30.900], mean action: 2.413 [0.000, 10.000], mean observation: 35.304 [0.002, 525.700], loss: 200.759872, mae: 25.435867, mean_q: -24.743584\n",
            " 1171629/10000000: episode: 5829, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -772.000, mean reward: -3.841 [-386.000, 18.600], mean action: 2.318 [0.000, 10.000], mean observation: 34.482 [0.000, 545.700], loss: 185.808762, mae: 25.357491, mean_q: -24.615788\n",
            " 1171830/10000000: episode: 5830, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -301.200, mean reward: -1.499 [-150.600, 115.500], mean action: 2.378 [0.000, 10.000], mean observation: 35.040 [0.001, 622.400], loss: 137.866531, mae: 25.344046, mean_q: -24.527664\n",
            " 1172031/10000000: episode: 5831, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -390.000, mean reward: -1.940 [-195.000, 106.200], mean action: 2.403 [0.000, 10.000], mean observation: 36.412 [0.000, 430.400], loss: 201.851212, mae: 24.833645, mean_q: -24.296909\n",
            " 1172232/10000000: episode: 5832, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: 653.800, mean reward: 3.253 [-10.000, 326.900], mean action: 2.851 [0.000, 10.000], mean observation: 36.511 [0.000, 669.400], loss: 350.203461, mae: 25.120127, mean_q: -24.766333\n",
            " 1172433/10000000: episode: 5833, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -720.400, mean reward: -3.584 [-360.200, 124.600], mean action: 2.995 [0.000, 10.000], mean observation: 33.758 [0.002, 511.500], loss: 161.202957, mae: 25.215601, mean_q: -24.578558\n",
            " 1172634/10000000: episode: 5834, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -312.600, mean reward: -1.555 [-156.300, 89.000], mean action: 2.582 [0.000, 10.000], mean observation: 36.608 [0.000, 663.000], loss: 379.657440, mae: 25.453037, mean_q: -24.684671\n",
            " 1172835/10000000: episode: 5835, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -169.000, mean reward: -0.841 [-84.500, 60.900], mean action: 1.602 [0.000, 10.000], mean observation: 38.133 [0.000, 515.300], loss: 175.235168, mae: 25.595394, mean_q: -24.476311\n",
            " 1173036/10000000: episode: 5836, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -533.800, mean reward: -2.656 [-266.900, 60.200], mean action: 2.234 [0.000, 10.000], mean observation: 30.738 [0.001, 637.900], loss: 484.030792, mae: 25.484423, mean_q: -24.522552\n",
            " 1173237/10000000: episode: 5837, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -531.000, mean reward: -2.642 [-265.500, 84.700], mean action: 2.657 [0.000, 10.000], mean observation: 31.391 [0.001, 539.300], loss: 471.600708, mae: 24.926685, mean_q: -24.104439\n",
            " 1173438/10000000: episode: 5838, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -550.400, mean reward: -2.738 [-275.200, 107.400], mean action: 2.716 [0.000, 10.000], mean observation: 38.248 [0.001, 581.100], loss: 186.612244, mae: 24.770470, mean_q: -24.196592\n",
            " 1173639/10000000: episode: 5839, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -728.400, mean reward: -3.624 [-364.200, 45.400], mean action: 2.796 [0.000, 10.000], mean observation: 33.294 [0.000, 622.800], loss: 326.384888, mae: 25.156836, mean_q: -24.443277\n",
            " 1173840/10000000: episode: 5840, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -889.000, mean reward: -4.423 [-444.500, 27.600], mean action: 3.015 [0.000, 10.000], mean observation: 34.226 [0.001, 570.400], loss: 160.370361, mae: 24.893715, mean_q: -24.478859\n",
            " 1174041/10000000: episode: 5841, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -800.200, mean reward: -3.981 [-400.100, 64.000], mean action: 2.597 [0.000, 10.000], mean observation: 34.226 [0.001, 424.400], loss: 200.789825, mae: 25.298580, mean_q: -24.984215\n",
            " 1174242/10000000: episode: 5842, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -508.200, mean reward: -2.528 [-254.100, 80.500], mean action: 2.294 [0.000, 10.000], mean observation: 32.950 [0.000, 749.600], loss: 175.483185, mae: 25.565260, mean_q: -25.009502\n",
            " 1174443/10000000: episode: 5843, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: 208.200, mean reward: 1.036 [-10.000, 154.000], mean action: 2.090 [0.000, 10.000], mean observation: 28.835 [0.001, 587.600], loss: 213.702713, mae: 25.497774, mean_q: -24.693190\n",
            " 1174644/10000000: episode: 5844, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: 559.600, mean reward: 2.784 [-10.000, 404.200], mean action: 1.955 [0.000, 10.000], mean observation: 32.923 [0.002, 516.600], loss: 165.880539, mae: 25.806217, mean_q: -24.721127\n",
            " 1174845/10000000: episode: 5845, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: -769.200, mean reward: -3.827 [-384.600, 11.400], mean action: 2.154 [0.000, 10.000], mean observation: 29.216 [0.001, 512.300], loss: 180.113220, mae: 25.930319, mean_q: -25.081554\n",
            " 1175046/10000000: episode: 5846, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -264.000, mean reward: -1.313 [-132.000, 136.600], mean action: 2.587 [0.000, 10.000], mean observation: 33.948 [0.005, 377.000], loss: 158.468491, mae: 26.063997, mean_q: -25.491581\n",
            " 1175247/10000000: episode: 5847, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -691.200, mean reward: -3.439 [-345.600, 40.800], mean action: 2.891 [0.000, 10.000], mean observation: 32.768 [0.000, 590.900], loss: 267.526978, mae: 26.212515, mean_q: -25.737217\n",
            " 1175448/10000000: episode: 5848, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -389.800, mean reward: -1.939 [-194.900, 97.000], mean action: 3.114 [0.000, 10.000], mean observation: 33.657 [0.003, 566.300], loss: 563.401001, mae: 25.532290, mean_q: -24.998928\n",
            " 1175649/10000000: episode: 5849, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -462.800, mean reward: -2.302 [-231.400, 109.000], mean action: 3.791 [0.000, 10.000], mean observation: 24.817 [0.000, 446.200], loss: 166.932236, mae: 24.607840, mean_q: -23.840437\n",
            " 1175850/10000000: episode: 5850, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -441.200, mean reward: -2.195 [-220.600, 115.200], mean action: 3.692 [0.000, 10.000], mean observation: 36.071 [0.001, 504.300], loss: 330.463470, mae: 24.591049, mean_q: -23.815483\n",
            " 1176051/10000000: episode: 5851, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -123.000, mean reward: -0.612 [-61.500, 176.400], mean action: 3.841 [0.000, 10.000], mean observation: 33.524 [0.001, 510.800], loss: 185.664688, mae: 23.915657, mean_q: -23.419020\n",
            " 1176252/10000000: episode: 5852, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -574.400, mean reward: -2.858 [-287.200, 114.900], mean action: 3.398 [0.000, 10.000], mean observation: 28.522 [0.002, 487.500], loss: 292.570587, mae: 24.303917, mean_q: -23.897356\n",
            " 1176453/10000000: episode: 5853, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -285.800, mean reward: -1.422 [-142.900, 108.600], mean action: 2.930 [0.000, 10.000], mean observation: 36.668 [0.000, 637.000], loss: 437.109650, mae: 24.746618, mean_q: -24.205200\n",
            " 1176654/10000000: episode: 5854, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -160.000, mean reward: -0.796 [-80.000, 174.300], mean action: 3.423 [0.000, 10.000], mean observation: 29.408 [0.000, 640.400], loss: 339.017426, mae: 24.665409, mean_q: -24.270491\n",
            " 1176855/10000000: episode: 5855, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: 237.000, mean reward: 1.179 [-10.000, 317.000], mean action: 2.990 [0.000, 10.000], mean observation: 28.064 [0.001, 418.000], loss: 123.514656, mae: 24.623648, mean_q: -24.088005\n",
            " 1177056/10000000: episode: 5856, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -70.800, mean reward: -0.352 [-35.400, 115.200], mean action: 2.751 [0.000, 10.000], mean observation: 37.917 [0.002, 512.800], loss: 135.270050, mae: 25.105871, mean_q: -24.497171\n",
            " 1177257/10000000: episode: 5857, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -927.400, mean reward: -4.614 [-463.700, 32.000], mean action: 3.323 [0.000, 10.000], mean observation: 31.432 [0.000, 558.800], loss: 655.770081, mae: 24.653421, mean_q: -24.240242\n",
            " 1177458/10000000: episode: 5858, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -655.800, mean reward: -3.263 [-327.900, 122.000], mean action: 3.483 [0.000, 10.000], mean observation: 39.904 [0.001, 584.600], loss: 230.552353, mae: 24.068903, mean_q: -23.533121\n",
            " 1177659/10000000: episode: 5859, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -374.000, mean reward: -1.861 [-187.000, 433.000], mean action: 3.602 [0.000, 10.000], mean observation: 32.633 [0.000, 455.400], loss: 281.231293, mae: 23.797796, mean_q: -23.508593\n",
            " 1177860/10000000: episode: 5860, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -1158.000, mean reward: -5.761 [-579.000, 48.600], mean action: 3.657 [0.000, 10.000], mean observation: 34.145 [0.001, 510.200], loss: 215.170319, mae: 24.193014, mean_q: -23.965557\n",
            " 1178061/10000000: episode: 5861, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -1221.000, mean reward: -6.075 [-610.500, 60.200], mean action: 4.104 [0.000, 10.000], mean observation: 32.358 [0.000, 708.200], loss: 316.161835, mae: 24.737484, mean_q: -24.582144\n",
            " 1178262/10000000: episode: 5862, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -745.600, mean reward: -3.709 [-372.800, 70.000], mean action: 3.383 [0.000, 10.000], mean observation: 32.705 [0.002, 436.000], loss: 200.681381, mae: 24.701456, mean_q: -24.384073\n",
            " 1178463/10000000: episode: 5863, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -102.000, mean reward: -0.507 [-51.000, 130.900], mean action: 2.811 [0.000, 10.000], mean observation: 38.235 [0.000, 500.200], loss: 214.676987, mae: 25.050476, mean_q: -24.584669\n",
            " 1178664/10000000: episode: 5864, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: 3655.000, mean reward: 18.184 [-10.000, 1947.000], mean action: 2.532 [0.000, 10.000], mean observation: 34.868 [0.000, 425.200], loss: 209.347824, mae: 25.143263, mean_q: -24.506563\n",
            " 1178865/10000000: episode: 5865, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -300.000, mean reward: -1.493 [-150.000, 45.600], mean action: 1.796 [0.000, 10.000], mean observation: 34.532 [0.000, 571.900], loss: 476.008789, mae: 25.529249, mean_q: -24.373409\n",
            " 1179066/10000000: episode: 5866, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -855.800, mean reward: -4.258 [-427.900, 10.200], mean action: 2.209 [0.000, 10.000], mean observation: 31.752 [0.002, 494.100], loss: 405.863678, mae: 25.185095, mean_q: -24.304703\n",
            " 1179267/10000000: episode: 5867, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: -21.400, mean reward: -0.106 [-10.700, 161.400], mean action: 2.403 [0.000, 10.000], mean observation: 28.025 [0.000, 490.900], loss: 660.825256, mae: 24.878965, mean_q: -23.818153\n",
            " 1179468/10000000: episode: 5868, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: 17.000, mean reward: 0.085 [-10.000, 137.000], mean action: 2.244 [0.000, 10.000], mean observation: 29.774 [0.001, 457.300], loss: 390.379303, mae: 24.682617, mean_q: -23.404533\n",
            " 1179669/10000000: episode: 5869, duration: 1.515s, episode steps: 201, steps per second: 133, episode reward: 269.400, mean reward: 1.340 [-10.000, 323.400], mean action: 2.080 [0.000, 10.000], mean observation: 33.157 [0.000, 798.300], loss: 219.079346, mae: 24.262714, mean_q: -23.042578\n",
            " 1179870/10000000: episode: 5870, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -457.200, mean reward: -2.275 [-228.600, 57.600], mean action: 2.378 [0.000, 10.000], mean observation: 33.237 [0.001, 556.200], loss: 505.419037, mae: 23.968969, mean_q: -22.879116\n",
            " 1180071/10000000: episode: 5871, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -598.400, mean reward: -2.977 [-299.200, 44.500], mean action: 2.224 [0.000, 10.000], mean observation: 30.515 [0.000, 481.500], loss: 601.223389, mae: 24.187420, mean_q: -23.264730\n",
            " 1180272/10000000: episode: 5872, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -65.200, mean reward: -0.324 [-32.600, 56.600], mean action: 1.786 [0.000, 10.000], mean observation: 34.925 [0.000, 668.900], loss: 191.144592, mae: 24.563963, mean_q: -23.374914\n",
            " 1180473/10000000: episode: 5873, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -365.800, mean reward: -1.820 [-182.900, 93.600], mean action: 2.144 [0.000, 10.000], mean observation: 29.859 [0.000, 383.900], loss: 199.791245, mae: 24.726826, mean_q: -23.520687\n",
            " 1180674/10000000: episode: 5874, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -403.600, mean reward: -2.008 [-201.800, 120.000], mean action: 2.184 [0.000, 10.000], mean observation: 35.123 [0.000, 605.900], loss: 180.013565, mae: 24.758337, mean_q: -23.921545\n",
            " 1180875/10000000: episode: 5875, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -495.600, mean reward: -2.466 [-247.800, 44.000], mean action: 1.811 [0.000, 10.000], mean observation: 35.134 [0.000, 650.000], loss: 234.811951, mae: 24.891853, mean_q: -24.022844\n",
            " 1181076/10000000: episode: 5876, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -206.800, mean reward: -1.029 [-103.400, 176.500], mean action: 2.045 [0.000, 10.000], mean observation: 32.927 [0.001, 574.000], loss: 361.228027, mae: 24.955194, mean_q: -23.987030\n",
            " 1181277/10000000: episode: 5877, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -79.000, mean reward: -0.393 [-39.500, 161.500], mean action: 1.940 [0.000, 10.000], mean observation: 34.100 [0.002, 420.100], loss: 212.534943, mae: 25.107025, mean_q: -23.950277\n",
            " 1181478/10000000: episode: 5878, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: 711.800, mean reward: 3.541 [-7.000, 355.900], mean action: 1.736 [0.000, 7.000], mean observation: 41.046 [0.001, 510.400], loss: 210.544296, mae: 24.881021, mean_q: -23.979065\n",
            " 1181679/10000000: episode: 5879, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -173.600, mean reward: -0.864 [-86.800, 64.500], mean action: 2.060 [0.000, 10.000], mean observation: 32.100 [0.002, 508.900], loss: 498.319733, mae: 24.728214, mean_q: -23.929602\n",
            " 1181880/10000000: episode: 5880, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 769.600, mean reward: 3.829 [-10.000, 675.000], mean action: 2.493 [0.000, 10.000], mean observation: 34.166 [0.000, 447.500], loss: 289.922455, mae: 24.859745, mean_q: -24.228632\n",
            " 1182081/10000000: episode: 5881, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -274.800, mean reward: -1.367 [-137.400, 191.000], mean action: 2.761 [0.000, 10.000], mean observation: 28.499 [0.001, 618.400], loss: 229.230881, mae: 24.793135, mean_q: -24.282017\n",
            " 1182282/10000000: episode: 5882, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 1116.200, mean reward: 5.553 [-10.000, 558.100], mean action: 3.701 [0.000, 10.000], mean observation: 32.567 [0.001, 519.700], loss: 271.552856, mae: 25.129232, mean_q: -24.915121\n",
            " 1182483/10000000: episode: 5883, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -567.200, mean reward: -2.822 [-283.600, 138.000], mean action: 3.284 [0.000, 10.000], mean observation: 32.163 [0.000, 586.000], loss: 439.263763, mae: 25.601660, mean_q: -25.435688\n",
            " 1182684/10000000: episode: 5884, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -502.800, mean reward: -2.501 [-251.400, 274.000], mean action: 3.617 [0.000, 10.000], mean observation: 32.956 [0.001, 471.000], loss: 273.455200, mae: 25.855824, mean_q: -25.623728\n",
            " 1182885/10000000: episode: 5885, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: 95.600, mean reward: 0.476 [-10.000, 183.000], mean action: 2.597 [0.000, 10.000], mean observation: 30.887 [0.001, 431.000], loss: 339.640167, mae: 26.038523, mean_q: -25.607180\n",
            " 1183086/10000000: episode: 5886, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -424.400, mean reward: -2.111 [-212.200, 86.400], mean action: 2.706 [0.000, 10.000], mean observation: 31.369 [0.001, 459.200], loss: 182.278702, mae: 26.247372, mean_q: -26.029343\n",
            " 1183287/10000000: episode: 5887, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -801.400, mean reward: -3.987 [-400.700, 40.000], mean action: 2.413 [0.000, 10.000], mean observation: 33.812 [0.001, 497.500], loss: 415.542023, mae: 26.626482, mean_q: -26.265646\n",
            " 1183488/10000000: episode: 5888, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -327.400, mean reward: -1.629 [-163.700, 164.100], mean action: 2.219 [0.000, 10.000], mean observation: 31.287 [0.002, 439.900], loss: 186.480392, mae: 26.901680, mean_q: -26.395065\n",
            " 1183689/10000000: episode: 5889, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -435.000, mean reward: -2.164 [-217.500, 61.200], mean action: 2.085 [0.000, 10.000], mean observation: 28.701 [0.002, 430.700], loss: 246.333954, mae: 26.810198, mean_q: -26.231472\n",
            " 1183890/10000000: episode: 5890, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -751.000, mean reward: -3.736 [-375.500, 26.400], mean action: 2.428 [0.000, 10.000], mean observation: 36.288 [0.000, 552.400], loss: 137.415833, mae: 26.844898, mean_q: -26.591200\n",
            " 1184091/10000000: episode: 5891, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -484.400, mean reward: -2.410 [-242.200, 57.400], mean action: 2.313 [0.000, 10.000], mean observation: 32.278 [0.000, 720.900], loss: 161.585175, mae: 27.617266, mean_q: -27.399332\n",
            " 1184292/10000000: episode: 5892, duration: 1.642s, episode steps: 201, steps per second: 122, episode reward: -98.200, mean reward: -0.489 [-49.100, 175.200], mean action: 2.488 [0.000, 10.000], mean observation: 31.537 [0.001, 500.800], loss: 383.759705, mae: 27.824257, mean_q: -27.600790\n",
            " 1184493/10000000: episode: 5893, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: 49.200, mean reward: 0.245 [-10.000, 232.000], mean action: 2.199 [0.000, 10.000], mean observation: 33.197 [0.001, 441.700], loss: 497.125610, mae: 27.718891, mean_q: -26.910723\n",
            " 1184694/10000000: episode: 5894, duration: 1.634s, episode steps: 201, steps per second: 123, episode reward: 960.000, mean reward: 4.776 [-10.000, 480.000], mean action: 2.204 [0.000, 10.000], mean observation: 28.886 [0.000, 348.700], loss: 248.425095, mae: 27.550037, mean_q: -26.994753\n",
            " 1184895/10000000: episode: 5895, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -359.200, mean reward: -1.787 [-179.600, 73.500], mean action: 2.358 [0.000, 10.000], mean observation: 34.747 [0.002, 537.100], loss: 515.653687, mae: 27.504921, mean_q: -27.158144\n",
            " 1185096/10000000: episode: 5896, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -447.800, mean reward: -2.228 [-223.900, 63.000], mean action: 2.463 [0.000, 10.000], mean observation: 33.171 [0.002, 562.800], loss: 220.253098, mae: 27.258045, mean_q: -26.906994\n",
            " 1185297/10000000: episode: 5897, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -139.200, mean reward: -0.693 [-69.600, 108.800], mean action: 2.313 [0.000, 7.000], mean observation: 31.046 [0.001, 530.600], loss: 486.078674, mae: 27.332573, mean_q: -27.105610\n",
            " 1185498/10000000: episode: 5898, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -262.600, mean reward: -1.306 [-131.300, 138.000], mean action: 3.050 [0.000, 10.000], mean observation: 38.194 [0.000, 693.300], loss: 502.938446, mae: 27.292366, mean_q: -27.328882\n",
            " 1185699/10000000: episode: 5899, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -623.200, mean reward: -3.100 [-311.600, 115.500], mean action: 3.244 [0.000, 10.000], mean observation: 34.199 [0.000, 727.600], loss: 177.537003, mae: 27.539951, mean_q: -27.625957\n",
            " 1185900/10000000: episode: 5900, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -442.800, mean reward: -2.203 [-221.400, 51.000], mean action: 2.587 [0.000, 10.000], mean observation: 35.541 [0.002, 479.600], loss: 206.902252, mae: 27.962294, mean_q: -28.028368\n",
            " 1186101/10000000: episode: 5901, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -345.800, mean reward: -1.720 [-172.900, 35.500], mean action: 2.478 [0.000, 10.000], mean observation: 32.906 [0.001, 606.200], loss: 456.583344, mae: 28.019638, mean_q: -27.799334\n",
            " 1186302/10000000: episode: 5902, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: -549.400, mean reward: -2.733 [-274.700, 66.700], mean action: 2.388 [0.000, 10.000], mean observation: 33.998 [0.002, 597.900], loss: 174.123093, mae: 28.470589, mean_q: -28.325432\n",
            " 1186503/10000000: episode: 5903, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: -258.400, mean reward: -1.286 [-129.200, 70.800], mean action: 2.025 [0.000, 10.000], mean observation: 30.747 [0.003, 482.900], loss: 261.638397, mae: 28.313833, mean_q: -28.129181\n",
            " 1186704/10000000: episode: 5904, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -441.000, mean reward: -2.194 [-220.500, 72.600], mean action: 2.905 [0.000, 10.000], mean observation: 27.373 [0.002, 435.000], loss: 273.077515, mae: 28.249874, mean_q: -28.293512\n",
            " 1186905/10000000: episode: 5905, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -869.000, mean reward: -4.323 [-434.500, 63.600], mean action: 3.229 [0.000, 10.000], mean observation: 32.717 [0.001, 697.100], loss: 271.681000, mae: 27.944565, mean_q: -28.158869\n",
            " 1187106/10000000: episode: 5906, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -1076.400, mean reward: -5.355 [-538.200, 35.400], mean action: 3.597 [0.000, 10.000], mean observation: 32.481 [0.001, 562.800], loss: 225.576569, mae: 28.180424, mean_q: -28.513735\n",
            " 1187307/10000000: episode: 5907, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -1038.600, mean reward: -5.167 [-519.300, 77.400], mean action: 3.308 [0.000, 10.000], mean observation: 34.386 [0.000, 458.900], loss: 493.056763, mae: 28.275507, mean_q: -28.675762\n",
            " 1187508/10000000: episode: 5908, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -773.000, mean reward: -3.846 [-386.500, 33.000], mean action: 2.522 [0.000, 10.000], mean observation: 30.472 [0.001, 522.200], loss: 254.354950, mae: 28.520630, mean_q: -28.549345\n",
            " 1187709/10000000: episode: 5909, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: 1066.000, mean reward: 5.303 [-10.000, 533.000], mean action: 2.294 [0.000, 10.000], mean observation: 30.644 [0.000, 527.200], loss: 220.856339, mae: 28.614384, mean_q: -28.449728\n",
            " 1187910/10000000: episode: 5910, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -685.800, mean reward: -3.412 [-342.900, 82.000], mean action: 2.617 [0.000, 10.000], mean observation: 35.122 [0.000, 424.800], loss: 530.158386, mae: 28.020493, mean_q: -28.078672\n",
            " 1188111/10000000: episode: 5911, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -746.000, mean reward: -3.711 [-373.000, 63.600], mean action: 3.174 [0.000, 10.000], mean observation: 34.341 [0.000, 765.000], loss: 264.774902, mae: 28.251181, mean_q: -28.306732\n",
            " 1188312/10000000: episode: 5912, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 401.400, mean reward: 1.997 [-10.000, 510.000], mean action: 3.493 [0.000, 10.000], mean observation: 31.291 [0.000, 638.500], loss: 570.774658, mae: 28.680275, mean_q: -29.022442\n",
            " 1188513/10000000: episode: 5913, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -262.800, mean reward: -1.307 [-131.400, 123.900], mean action: 2.851 [0.000, 10.000], mean observation: 35.070 [0.001, 534.900], loss: 685.894958, mae: 29.062428, mean_q: -29.351446\n",
            " 1188714/10000000: episode: 5914, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: 54.000, mean reward: 0.269 [-10.000, 463.200], mean action: 3.174 [0.000, 10.000], mean observation: 37.741 [0.000, 793.800], loss: 257.634430, mae: 29.025877, mean_q: -29.026012\n",
            " 1188915/10000000: episode: 5915, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -353.400, mean reward: -1.758 [-176.700, 97.800], mean action: 2.652 [0.000, 10.000], mean observation: 36.038 [0.001, 681.600], loss: 510.333099, mae: 29.207468, mean_q: -29.188080\n",
            " 1189116/10000000: episode: 5916, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -554.200, mean reward: -2.757 [-277.100, 108.500], mean action: 2.478 [0.000, 10.000], mean observation: 39.466 [0.000, 701.500], loss: 253.961990, mae: 29.338240, mean_q: -29.366373\n",
            " 1189317/10000000: episode: 5917, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: 157.000, mean reward: 0.781 [-10.000, 209.400], mean action: 3.214 [0.000, 10.000], mean observation: 34.267 [0.000, 783.800], loss: 270.909698, mae: 29.033335, mean_q: -29.010275\n",
            " 1189518/10000000: episode: 5918, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 12.600, mean reward: 0.063 [-10.000, 114.000], mean action: 2.925 [0.000, 10.000], mean observation: 36.026 [0.000, 642.000], loss: 247.663284, mae: 29.271864, mean_q: -29.312393\n",
            " 1189719/10000000: episode: 5919, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -27.800, mean reward: -0.138 [-13.900, 144.900], mean action: 2.687 [0.000, 10.000], mean observation: 34.337 [0.002, 515.200], loss: 221.316147, mae: 29.361353, mean_q: -29.369488\n",
            " 1189920/10000000: episode: 5920, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -467.400, mean reward: -2.325 [-233.700, 84.000], mean action: 2.448 [0.000, 8.000], mean observation: 37.222 [0.001, 430.300], loss: 252.622375, mae: 29.570223, mean_q: -29.529556\n",
            " 1190121/10000000: episode: 5921, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -508.200, mean reward: -2.528 [-254.100, 196.700], mean action: 3.065 [0.000, 10.000], mean observation: 37.391 [0.000, 673.700], loss: 331.183960, mae: 29.768011, mean_q: -30.029602\n",
            " 1190322/10000000: episode: 5922, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -764.400, mean reward: -3.803 [-382.200, 34.000], mean action: 3.030 [0.000, 10.000], mean observation: 29.594 [0.001, 446.200], loss: 246.163483, mae: 30.252291, mean_q: -30.809271\n",
            " 1190523/10000000: episode: 5923, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -241.600, mean reward: -1.202 [-120.800, 134.500], mean action: 2.592 [0.000, 8.000], mean observation: 41.085 [0.001, 558.800], loss: 233.985107, mae: 30.845713, mean_q: -31.314030\n",
            " 1190724/10000000: episode: 5924, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -782.600, mean reward: -3.894 [-391.300, 87.100], mean action: 2.562 [0.000, 10.000], mean observation: 32.064 [0.002, 533.200], loss: 273.069794, mae: 30.738029, mean_q: -30.921015\n",
            " 1190925/10000000: episode: 5925, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -80.200, mean reward: -0.399 [-40.100, 250.400], mean action: 2.821 [0.000, 8.000], mean observation: 33.982 [0.000, 933.200], loss: 493.433105, mae: 30.564522, mean_q: -30.635279\n",
            " 1191126/10000000: episode: 5926, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -152.200, mean reward: -0.757 [-76.100, 99.500], mean action: 2.677 [0.000, 10.000], mean observation: 33.045 [0.000, 684.900], loss: 222.737000, mae: 30.524317, mean_q: -30.855474\n",
            " 1191327/10000000: episode: 5927, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -181.600, mean reward: -0.903 [-90.800, 146.000], mean action: 3.070 [0.000, 10.000], mean observation: 30.424 [0.001, 556.700], loss: 991.355591, mae: 30.451113, mean_q: -30.899691\n",
            " 1191528/10000000: episode: 5928, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -588.200, mean reward: -2.926 [-294.100, 51.100], mean action: 3.100 [0.000, 10.000], mean observation: 34.941 [0.000, 528.900], loss: 420.297729, mae: 29.972540, mean_q: -30.610716\n",
            " 1191729/10000000: episode: 5929, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -675.000, mean reward: -3.358 [-337.500, 42.000], mean action: 3.080 [0.000, 10.000], mean observation: 36.686 [0.000, 607.900], loss: 438.129730, mae: 30.351971, mean_q: -31.089231\n",
            " 1191930/10000000: episode: 5930, duration: 1.395s, episode steps: 201, steps per second: 144, episode reward: -410.400, mean reward: -2.042 [-205.200, 140.500], mean action: 3.294 [0.000, 10.000], mean observation: 36.196 [0.001, 528.400], loss: 376.193787, mae: 30.335899, mean_q: -30.595110\n",
            " 1192131/10000000: episode: 5931, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -496.200, mean reward: -2.469 [-248.100, 292.000], mean action: 3.398 [0.000, 10.000], mean observation: 30.656 [0.000, 455.800], loss: 1041.060913, mae: 30.556919, mean_q: -30.798697\n",
            " 1192332/10000000: episode: 5932, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: 325.000, mean reward: 1.617 [-10.000, 266.000], mean action: 3.085 [0.000, 10.000], mean observation: 29.334 [0.003, 464.200], loss: 321.449463, mae: 30.147203, mean_q: -30.131603\n",
            " 1192533/10000000: episode: 5933, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -512.200, mean reward: -2.548 [-256.100, 102.500], mean action: 3.095 [0.000, 10.000], mean observation: 36.449 [0.000, 818.500], loss: 401.530701, mae: 29.997398, mean_q: -29.908222\n",
            " 1192734/10000000: episode: 5934, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -1020.800, mean reward: -5.079 [-510.400, 30.000], mean action: 3.045 [0.000, 10.000], mean observation: 26.768 [0.001, 619.000], loss: 275.874390, mae: 29.693111, mean_q: -29.629503\n",
            " 1192935/10000000: episode: 5935, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: 318.800, mean reward: 1.586 [-10.000, 287.500], mean action: 2.433 [0.000, 10.000], mean observation: 34.003 [0.001, 461.700], loss: 471.951721, mae: 29.946945, mean_q: -29.998800\n",
            " 1193136/10000000: episode: 5936, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: 319.000, mean reward: 1.587 [-10.000, 377.400], mean action: 2.517 [0.000, 10.000], mean observation: 36.686 [0.002, 402.500], loss: 233.970078, mae: 30.030708, mean_q: -29.992270\n",
            " 1193337/10000000: episode: 5937, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -866.000, mean reward: -4.308 [-433.000, 34.000], mean action: 3.085 [0.000, 10.000], mean observation: 33.069 [0.000, 501.600], loss: 267.937378, mae: 29.865477, mean_q: -29.756948\n",
            " 1193538/10000000: episode: 5938, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -383.800, mean reward: -1.909 [-191.900, 135.000], mean action: 3.109 [0.000, 10.000], mean observation: 37.076 [0.001, 492.600], loss: 716.374146, mae: 29.612230, mean_q: -29.432228\n",
            " 1193739/10000000: episode: 5939, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 369.400, mean reward: 1.838 [-10.000, 492.000], mean action: 2.751 [0.000, 10.000], mean observation: 31.297 [0.000, 590.900], loss: 243.642258, mae: 29.092447, mean_q: -28.705257\n",
            " 1193940/10000000: episode: 5940, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: 688.400, mean reward: 3.425 [-10.000, 494.500], mean action: 2.801 [0.000, 10.000], mean observation: 34.733 [0.000, 747.100], loss: 272.484039, mae: 29.819569, mean_q: -29.766737\n",
            " 1194141/10000000: episode: 5941, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -249.000, mean reward: -1.239 [-124.500, 172.800], mean action: 2.234 [0.000, 10.000], mean observation: 34.637 [0.000, 462.000], loss: 473.794800, mae: 29.671404, mean_q: -29.722443\n",
            " 1194342/10000000: episode: 5942, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -349.000, mean reward: -1.736 [-174.500, 50.100], mean action: 2.448 [0.000, 10.000], mean observation: 25.623 [0.001, 534.800], loss: 260.431122, mae: 29.784447, mean_q: -30.012934\n",
            " 1194543/10000000: episode: 5943, duration: 1.521s, episode steps: 201, steps per second: 132, episode reward: -433.400, mean reward: -2.156 [-216.700, 193.000], mean action: 3.000 [0.000, 10.000], mean observation: 30.112 [0.000, 580.300], loss: 560.327026, mae: 29.748688, mean_q: -29.845514\n",
            " 1194744/10000000: episode: 5944, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -524.200, mean reward: -2.608 [-262.100, 135.500], mean action: 2.940 [0.000, 10.000], mean observation: 37.075 [0.000, 522.100], loss: 253.310181, mae: 30.146795, mean_q: -30.293060\n",
            " 1194945/10000000: episode: 5945, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -74.200, mean reward: -0.369 [-37.100, 92.000], mean action: 3.030 [0.000, 10.000], mean observation: 27.031 [0.001, 590.600], loss: 486.737183, mae: 30.146353, mean_q: -30.201872\n",
            " 1195146/10000000: episode: 5946, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -807.400, mean reward: -4.017 [-403.700, 76.200], mean action: 2.866 [0.000, 10.000], mean observation: 36.220 [0.000, 813.800], loss: 203.591568, mae: 30.075397, mean_q: -30.123964\n",
            " 1195347/10000000: episode: 5947, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -533.200, mean reward: -2.653 [-266.600, 82.000], mean action: 2.179 [0.000, 10.000], mean observation: 39.071 [0.001, 632.400], loss: 250.384995, mae: 30.717808, mean_q: -30.665430\n",
            " 1195548/10000000: episode: 5948, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -550.800, mean reward: -2.740 [-275.400, 82.200], mean action: 2.015 [0.000, 10.000], mean observation: 30.233 [0.003, 531.400], loss: 482.361481, mae: 30.723215, mean_q: -30.652504\n",
            " 1195749/10000000: episode: 5949, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -740.600, mean reward: -3.685 [-370.300, 68.000], mean action: 2.876 [0.000, 10.000], mean observation: 35.360 [0.002, 519.900], loss: 224.551331, mae: 30.591530, mean_q: -30.759203\n",
            " 1195950/10000000: episode: 5950, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -332.600, mean reward: -1.655 [-166.300, 142.000], mean action: 2.373 [0.000, 7.000], mean observation: 37.777 [0.001, 518.800], loss: 586.632690, mae: 30.939905, mean_q: -31.121452\n",
            " 1196151/10000000: episode: 5951, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: 127.200, mean reward: 0.633 [-10.000, 342.600], mean action: 2.284 [0.000, 10.000], mean observation: 37.963 [0.001, 554.400], loss: 607.845520, mae: 30.784657, mean_q: -30.914883\n",
            " 1196352/10000000: episode: 5952, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -473.200, mean reward: -2.354 [-236.600, 49.800], mean action: 2.900 [0.000, 8.000], mean observation: 38.468 [0.001, 627.200], loss: 686.785706, mae: 30.966583, mean_q: -31.215149\n",
            " 1196553/10000000: episode: 5953, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: 16.400, mean reward: 0.082 [-10.000, 171.000], mean action: 2.771 [0.000, 10.000], mean observation: 34.709 [0.000, 604.500], loss: 775.973572, mae: 30.965719, mean_q: -31.148378\n",
            " 1196754/10000000: episode: 5954, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -353.400, mean reward: -1.758 [-176.700, 84.900], mean action: 2.632 [0.000, 7.000], mean observation: 32.999 [0.001, 500.600], loss: 462.901703, mae: 30.723759, mean_q: -31.045452\n",
            " 1196955/10000000: episode: 5955, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -610.600, mean reward: -3.038 [-305.300, 65.000], mean action: 2.786 [0.000, 10.000], mean observation: 29.316 [0.001, 464.600], loss: 252.851059, mae: 30.887995, mean_q: -31.335564\n",
            " 1197156/10000000: episode: 5956, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 75.200, mean reward: 0.374 [-10.000, 177.000], mean action: 2.398 [0.000, 10.000], mean observation: 30.736 [0.004, 531.100], loss: 399.346436, mae: 31.145634, mean_q: -31.626108\n",
            " 1197357/10000000: episode: 5957, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -36.600, mean reward: -0.182 [-18.300, 200.500], mean action: 2.463 [0.000, 10.000], mean observation: 35.827 [0.001, 507.400], loss: 192.045395, mae: 30.750889, mean_q: -31.183516\n",
            " 1197558/10000000: episode: 5958, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -666.000, mean reward: -3.313 [-333.000, 39.600], mean action: 2.781 [0.000, 10.000], mean observation: 34.374 [0.000, 784.200], loss: 485.387970, mae: 30.842438, mean_q: -31.563923\n",
            " 1197759/10000000: episode: 5959, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -808.400, mean reward: -4.022 [-404.200, 54.800], mean action: 3.179 [0.000, 10.000], mean observation: 36.518 [0.000, 722.600], loss: 658.619019, mae: 31.037910, mean_q: -31.872227\n",
            " 1197960/10000000: episode: 5960, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -651.200, mean reward: -3.240 [-325.600, 137.200], mean action: 3.318 [0.000, 10.000], mean observation: 33.126 [0.001, 542.200], loss: 476.236877, mae: 30.894575, mean_q: -31.697548\n",
            " 1198161/10000000: episode: 5961, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -60.000, mean reward: -0.299 [-30.000, 350.200], mean action: 3.517 [0.000, 10.000], mean observation: 34.986 [0.001, 448.100], loss: 512.968140, mae: 31.191820, mean_q: -31.740734\n",
            " 1198362/10000000: episode: 5962, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -683.200, mean reward: -3.399 [-341.600, 102.600], mean action: 2.995 [0.000, 10.000], mean observation: 29.490 [0.000, 527.300], loss: 195.846848, mae: 30.977438, mean_q: -31.728365\n",
            " 1198563/10000000: episode: 5963, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -666.200, mean reward: -3.314 [-333.100, 60.000], mean action: 3.199 [0.000, 10.000], mean observation: 32.854 [0.003, 542.400], loss: 214.286636, mae: 31.200274, mean_q: -31.962748\n",
            " 1198764/10000000: episode: 5964, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -989.600, mean reward: -4.923 [-494.800, 77.000], mean action: 3.070 [0.000, 10.000], mean observation: 38.711 [0.000, 694.400], loss: 186.815536, mae: 31.384129, mean_q: -32.122066\n",
            " 1198965/10000000: episode: 5965, duration: 1.387s, episode steps: 201, steps per second: 145, episode reward: -186.800, mean reward: -0.929 [-93.400, 209.400], mean action: 2.647 [0.000, 10.000], mean observation: 34.617 [0.000, 598.300], loss: 373.416656, mae: 31.349346, mean_q: -31.986017\n",
            " 1199166/10000000: episode: 5966, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -692.000, mean reward: -3.443 [-346.000, 31.000], mean action: 2.463 [0.000, 10.000], mean observation: 29.969 [0.004, 555.700], loss: 479.267853, mae: 31.544870, mean_q: -31.907917\n",
            " 1199367/10000000: episode: 5967, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -367.800, mean reward: -1.830 [-183.900, 121.000], mean action: 2.821 [0.000, 10.000], mean observation: 34.699 [0.000, 669.900], loss: 198.941193, mae: 31.649677, mean_q: -32.454758\n",
            " 1199568/10000000: episode: 5968, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -364.800, mean reward: -1.815 [-182.400, 98.400], mean action: 2.866 [0.000, 10.000], mean observation: 39.933 [0.001, 662.900], loss: 213.086594, mae: 31.701021, mean_q: -32.456833\n",
            " 1199769/10000000: episode: 5969, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -148.800, mean reward: -0.740 [-74.400, 219.000], mean action: 3.418 [0.000, 10.000], mean observation: 35.372 [0.002, 482.600], loss: 1042.583252, mae: 31.624437, mean_q: -32.607525\n",
            " 1199970/10000000: episode: 5970, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: 1978.800, mean reward: 9.845 [-10.000, 989.400], mean action: 3.448 [0.000, 10.000], mean observation: 35.066 [0.001, 419.000], loss: 237.659042, mae: 31.761784, mean_q: -32.782593\n",
            " 1200171/10000000: episode: 5971, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -227.400, mean reward: -1.131 [-113.700, 296.000], mean action: 2.801 [0.000, 10.000], mean observation: 35.985 [0.000, 654.700], loss: 549.686829, mae: 31.611721, mean_q: -32.361721\n",
            " 1200372/10000000: episode: 5972, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -939.000, mean reward: -4.672 [-469.500, 81.200], mean action: 3.264 [0.000, 10.000], mean observation: 33.260 [0.000, 706.400], loss: 299.366577, mae: 31.740564, mean_q: -32.721878\n",
            " 1200573/10000000: episode: 5973, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -909.000, mean reward: -4.522 [-454.500, 51.600], mean action: 3.507 [0.000, 10.000], mean observation: 32.334 [0.001, 434.500], loss: 211.676575, mae: 31.923891, mean_q: -33.096043\n",
            " 1200774/10000000: episode: 5974, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 969.800, mean reward: 4.825 [-10.000, 818.000], mean action: 3.090 [0.000, 10.000], mean observation: 32.949 [0.001, 568.100], loss: 224.820404, mae: 32.583954, mean_q: -33.606743\n",
            " 1200975/10000000: episode: 5975, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -601.400, mean reward: -2.992 [-300.700, 113.000], mean action: 3.517 [0.000, 10.000], mean observation: 30.027 [0.001, 676.900], loss: 469.286041, mae: 32.783813, mean_q: -33.886784\n",
            " 1201176/10000000: episode: 5976, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -1106.600, mean reward: -5.505 [-553.300, 16.000], mean action: 3.318 [0.000, 10.000], mean observation: 37.433 [0.000, 714.300], loss: 246.031479, mae: 32.990170, mean_q: -34.295818\n",
            " 1201377/10000000: episode: 5977, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -916.600, mean reward: -4.560 [-458.300, 46.200], mean action: 3.199 [0.000, 10.000], mean observation: 36.512 [0.000, 601.500], loss: 208.645172, mae: 33.220779, mean_q: -34.520702\n",
            " 1201578/10000000: episode: 5978, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -323.400, mean reward: -1.609 [-161.700, 82.800], mean action: 3.826 [0.000, 10.000], mean observation: 34.469 [0.000, 582.800], loss: 273.065063, mae: 33.553993, mean_q: -34.978195\n",
            " 1201779/10000000: episode: 5979, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -560.800, mean reward: -2.790 [-280.400, 72.000], mean action: 3.289 [0.000, 10.000], mean observation: 33.672 [0.001, 498.800], loss: 420.240753, mae: 33.939667, mean_q: -34.977798\n",
            " 1201980/10000000: episode: 5980, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: 4.000, mean reward: 0.020 [-10.000, 163.200], mean action: 3.184 [0.000, 10.000], mean observation: 28.472 [0.002, 508.500], loss: 232.285706, mae: 33.979897, mean_q: -34.927658\n",
            " 1202181/10000000: episode: 5981, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: 958.600, mean reward: 4.769 [-10.000, 687.600], mean action: 2.821 [0.000, 10.000], mean observation: 31.393 [0.000, 413.800], loss: 322.722504, mae: 34.103649, mean_q: -35.113621\n",
            " 1202382/10000000: episode: 5982, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: 341.200, mean reward: 1.698 [-10.000, 465.500], mean action: 2.025 [0.000, 10.000], mean observation: 35.023 [0.000, 367.900], loss: 445.491638, mae: 34.370850, mean_q: -35.214012\n",
            " 1202583/10000000: episode: 5983, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: 501.000, mean reward: 2.493 [-10.000, 601.000], mean action: 2.204 [0.000, 10.000], mean observation: 37.268 [0.001, 630.500], loss: 356.289948, mae: 34.641937, mean_q: -35.497074\n",
            " 1202784/10000000: episode: 5984, duration: 1.385s, episode steps: 201, steps per second: 145, episode reward: 150.800, mean reward: 0.750 [-10.000, 160.000], mean action: 2.473 [0.000, 10.000], mean observation: 31.168 [0.001, 518.600], loss: 488.848572, mae: 34.160686, mean_q: -35.025131\n",
            " 1202985/10000000: episode: 5985, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -534.600, mean reward: -2.660 [-267.300, 49.500], mean action: 2.328 [0.000, 10.000], mean observation: 36.055 [0.000, 562.200], loss: 308.936707, mae: 33.963188, mean_q: -34.645042\n",
            " 1203186/10000000: episode: 5986, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: 581.800, mean reward: 2.895 [-10.000, 290.900], mean action: 2.512 [0.000, 10.000], mean observation: 32.940 [0.002, 637.200], loss: 315.535278, mae: 33.936943, mean_q: -34.692650\n",
            " 1203387/10000000: episode: 5987, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -367.400, mean reward: -1.828 [-183.700, 208.500], mean action: 2.731 [0.000, 10.000], mean observation: 27.063 [0.005, 368.500], loss: 479.339935, mae: 33.977871, mean_q: -34.597919\n",
            " 1203588/10000000: episode: 5988, duration: 1.387s, episode steps: 201, steps per second: 145, episode reward: 267.400, mean reward: 1.330 [-10.000, 133.700], mean action: 2.323 [0.000, 10.000], mean observation: 32.495 [0.002, 607.700], loss: 281.854279, mae: 33.573044, mean_q: -34.144577\n",
            " 1203789/10000000: episode: 5989, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -476.400, mean reward: -2.370 [-238.200, 46.200], mean action: 2.303 [0.000, 10.000], mean observation: 28.348 [0.001, 497.800], loss: 233.043777, mae: 33.728458, mean_q: -34.152817\n",
            " 1203990/10000000: episode: 5990, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -625.800, mean reward: -3.113 [-312.900, 13.200], mean action: 1.955 [0.000, 10.000], mean observation: 38.739 [0.001, 456.700], loss: 253.876724, mae: 33.805038, mean_q: -34.093788\n",
            " 1204191/10000000: episode: 5991, duration: 1.568s, episode steps: 201, steps per second: 128, episode reward: -488.600, mean reward: -2.431 [-244.300, 35.000], mean action: 2.114 [0.000, 10.000], mean observation: 33.497 [0.000, 772.000], loss: 207.149567, mae: 33.638920, mean_q: -34.538601\n",
            " 1204392/10000000: episode: 5992, duration: 1.620s, episode steps: 201, steps per second: 124, episode reward: -522.200, mean reward: -2.598 [-261.100, 47.700], mean action: 2.050 [0.000, 10.000], mean observation: 31.330 [0.001, 558.800], loss: 508.529785, mae: 33.888103, mean_q: -34.623920\n",
            " 1204593/10000000: episode: 5993, duration: 1.608s, episode steps: 201, steps per second: 125, episode reward: -209.800, mean reward: -1.044 [-104.900, 140.000], mean action: 2.164 [0.000, 10.000], mean observation: 30.176 [0.000, 501.100], loss: 281.371307, mae: 33.818661, mean_q: -34.699356\n",
            " 1204794/10000000: episode: 5994, duration: 1.566s, episode steps: 201, steps per second: 128, episode reward: 939.000, mean reward: 4.672 [-10.000, 592.000], mean action: 2.418 [0.000, 10.000], mean observation: 36.049 [0.001, 645.600], loss: 272.233551, mae: 33.717030, mean_q: -34.553913\n",
            " 1204995/10000000: episode: 5995, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -679.600, mean reward: -3.381 [-339.800, 17.500], mean action: 2.154 [0.000, 10.000], mean observation: 30.852 [0.001, 487.300], loss: 548.843872, mae: 33.617737, mean_q: -34.536747\n",
            " 1205196/10000000: episode: 5996, duration: 1.600s, episode steps: 201, steps per second: 126, episode reward: 348.200, mean reward: 1.732 [-10.000, 174.100], mean action: 2.055 [0.000, 10.000], mean observation: 35.213 [0.000, 555.400], loss: 570.610474, mae: 33.433304, mean_q: -34.238674\n",
            " 1205397/10000000: episode: 5997, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -662.600, mean reward: -3.297 [-331.300, 43.200], mean action: 2.478 [0.000, 10.000], mean observation: 33.692 [0.000, 553.100], loss: 383.742493, mae: 33.160404, mean_q: -33.962254\n",
            " 1205598/10000000: episode: 5998, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -346.800, mean reward: -1.725 [-173.400, 123.900], mean action: 2.687 [0.000, 10.000], mean observation: 38.215 [0.000, 668.100], loss: 677.695740, mae: 33.087914, mean_q: -33.807533\n",
            " 1205799/10000000: episode: 5999, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -958.800, mean reward: -4.770 [-479.400, 20.000], mean action: 2.960 [0.000, 10.000], mean observation: 31.218 [0.000, 483.800], loss: 334.041504, mae: 32.828491, mean_q: -33.599659\n",
            " 1206000/10000000: episode: 6000, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -145.200, mean reward: -0.722 [-72.600, 159.000], mean action: 2.697 [0.000, 10.000], mean observation: 27.767 [0.002, 513.300], loss: 468.477142, mae: 32.683174, mean_q: -33.437111\n",
            " 1206201/10000000: episode: 6001, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -678.600, mean reward: -3.376 [-339.300, 47.400], mean action: 2.856 [0.000, 10.000], mean observation: 33.482 [0.000, 606.600], loss: 241.005981, mae: 32.630302, mean_q: -33.386684\n",
            " 1206402/10000000: episode: 6002, duration: 1.545s, episode steps: 201, steps per second: 130, episode reward: 168.400, mean reward: 0.838 [-10.000, 493.600], mean action: 3.075 [0.000, 10.000], mean observation: 36.547 [0.001, 493.400], loss: 468.212646, mae: 32.836723, mean_q: -33.857586\n",
            " 1206603/10000000: episode: 6003, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -780.200, mean reward: -3.882 [-390.100, 36.500], mean action: 3.030 [0.000, 10.000], mean observation: 33.732 [0.002, 578.200], loss: 376.456146, mae: 32.667397, mean_q: -33.538494\n",
            " 1206804/10000000: episode: 6004, duration: 2.361s, episode steps: 201, steps per second: 85, episode reward: -386.400, mean reward: -1.922 [-193.200, 279.000], mean action: 2.692 [0.000, 10.000], mean observation: 32.126 [0.001, 461.100], loss: 223.151260, mae: 32.449612, mean_q: -33.127659\n",
            " 1207005/10000000: episode: 6005, duration: 2.335s, episode steps: 201, steps per second: 86, episode reward: -680.000, mean reward: -3.383 [-340.000, 35.400], mean action: 2.682 [0.000, 10.000], mean observation: 40.579 [0.000, 527.200], loss: 278.555145, mae: 32.576130, mean_q: -33.248196\n",
            " 1207206/10000000: episode: 6006, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -553.800, mean reward: -2.755 [-276.900, 81.600], mean action: 2.323 [0.000, 10.000], mean observation: 30.632 [0.001, 675.300], loss: 590.231873, mae: 32.265110, mean_q: -32.883026\n",
            " 1207407/10000000: episode: 6007, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -377.000, mean reward: -1.876 [-188.500, 48.300], mean action: 2.104 [0.000, 10.000], mean observation: 31.100 [0.001, 472.800], loss: 357.942657, mae: 32.319752, mean_q: -33.013279\n",
            " 1207608/10000000: episode: 6008, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -243.400, mean reward: -1.211 [-121.700, 161.700], mean action: 2.463 [0.000, 10.000], mean observation: 32.856 [0.000, 527.500], loss: 470.017029, mae: 32.671619, mean_q: -33.712162\n",
            " 1207809/10000000: episode: 6009, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: 195.800, mean reward: 0.974 [-10.000, 163.200], mean action: 2.945 [0.000, 10.000], mean observation: 30.530 [0.000, 544.700], loss: 271.545807, mae: 32.841785, mean_q: -34.033993\n",
            " 1208010/10000000: episode: 6010, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -553.200, mean reward: -2.752 [-276.600, 64.000], mean action: 2.925 [0.000, 10.000], mean observation: 39.124 [0.001, 532.900], loss: 215.895966, mae: 33.002399, mean_q: -34.324089\n",
            " 1208211/10000000: episode: 6011, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -36.200, mean reward: -0.180 [-18.100, 291.600], mean action: 2.502 [0.000, 10.000], mean observation: 34.035 [0.002, 510.800], loss: 317.623199, mae: 33.462959, mean_q: -34.343304\n",
            " 1208412/10000000: episode: 6012, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -668.200, mean reward: -3.324 [-334.100, 43.200], mean action: 2.164 [0.000, 10.000], mean observation: 32.030 [0.001, 647.400], loss: 395.435547, mae: 32.928055, mean_q: -33.651527\n",
            " 1208613/10000000: episode: 6013, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -589.200, mean reward: -2.931 [-294.600, 69.200], mean action: 2.627 [0.000, 10.000], mean observation: 32.079 [0.000, 602.400], loss: 257.841095, mae: 33.238724, mean_q: -34.009918\n",
            " 1208814/10000000: episode: 6014, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -507.600, mean reward: -2.525 [-253.800, 47.800], mean action: 2.169 [0.000, 10.000], mean observation: 33.346 [0.002, 364.300], loss: 771.969971, mae: 33.485203, mean_q: -34.257938\n",
            " 1209015/10000000: episode: 6015, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: -160.000, mean reward: -0.796 [-80.000, 135.000], mean action: 2.886 [0.000, 10.000], mean observation: 27.446 [0.001, 458.100], loss: 268.790405, mae: 33.234547, mean_q: -34.366116\n",
            " 1209216/10000000: episode: 6016, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -412.600, mean reward: -2.053 [-206.300, 90.300], mean action: 3.154 [0.000, 10.000], mean observation: 32.443 [0.000, 695.400], loss: 524.217712, mae: 33.331497, mean_q: -34.345993\n",
            " 1209417/10000000: episode: 6017, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -457.400, mean reward: -2.276 [-228.700, 190.800], mean action: 2.910 [0.000, 10.000], mean observation: 38.417 [0.000, 660.900], loss: 253.073288, mae: 33.590256, mean_q: -34.688900\n",
            " 1209618/10000000: episode: 6018, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: 411.600, mean reward: 2.048 [-10.000, 249.900], mean action: 2.781 [0.000, 10.000], mean observation: 31.433 [0.001, 400.300], loss: 486.197388, mae: 34.053032, mean_q: -35.135063\n",
            " 1209819/10000000: episode: 6019, duration: 1.522s, episode steps: 201, steps per second: 132, episode reward: -43.200, mean reward: -0.215 [-21.600, 335.300], mean action: 2.925 [0.000, 10.000], mean observation: 33.755 [0.000, 446.900], loss: 325.674011, mae: 33.863564, mean_q: -35.042591\n",
            " 1210020/10000000: episode: 6020, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: 153.600, mean reward: 0.764 [-10.000, 334.500], mean action: 2.403 [0.000, 10.000], mean observation: 32.905 [0.001, 654.600], loss: 271.216095, mae: 33.765381, mean_q: -34.773720\n",
            " 1210221/10000000: episode: 6021, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 492.000, mean reward: 2.448 [-10.000, 340.900], mean action: 2.199 [0.000, 10.000], mean observation: 31.968 [0.000, 467.400], loss: 761.485413, mae: 33.972054, mean_q: -34.832760\n",
            " 1210422/10000000: episode: 6022, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -469.200, mean reward: -2.334 [-234.600, 57.600], mean action: 2.498 [0.000, 10.000], mean observation: 35.702 [0.000, 420.600], loss: 428.246643, mae: 33.806530, mean_q: -34.862732\n",
            " 1210623/10000000: episode: 6023, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -189.000, mean reward: -0.940 [-94.500, 124.400], mean action: 2.303 [0.000, 10.000], mean observation: 33.590 [0.000, 697.600], loss: 228.531174, mae: 33.579723, mean_q: -34.671928\n",
            " 1210824/10000000: episode: 6024, duration: 1.938s, episode steps: 201, steps per second: 104, episode reward: -643.000, mean reward: -3.199 [-321.500, 62.300], mean action: 2.846 [0.000, 10.000], mean observation: 31.170 [0.000, 443.500], loss: 371.871826, mae: 33.358368, mean_q: -34.474445\n",
            " 1211025/10000000: episode: 6025, duration: 1.981s, episode steps: 201, steps per second: 101, episode reward: -496.800, mean reward: -2.472 [-248.400, 79.600], mean action: 2.891 [0.000, 10.000], mean observation: 33.934 [0.001, 470.800], loss: 315.301575, mae: 33.739880, mean_q: -34.747940\n",
            " 1211226/10000000: episode: 6026, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: 146.400, mean reward: 0.728 [-10.000, 252.700], mean action: 3.398 [0.000, 10.000], mean observation: 28.320 [0.002, 400.000], loss: 287.114288, mae: 33.490726, mean_q: -34.676880\n",
            " 1211427/10000000: episode: 6027, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -340.800, mean reward: -1.696 [-170.400, 117.600], mean action: 2.816 [0.000, 10.000], mean observation: 33.547 [0.002, 515.900], loss: 165.684525, mae: 34.056179, mean_q: -35.120663\n",
            " 1211628/10000000: episode: 6028, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -528.000, mean reward: -2.627 [-264.000, 62.400], mean action: 2.413 [0.000, 10.000], mean observation: 35.422 [0.002, 467.700], loss: 599.002991, mae: 34.233376, mean_q: -35.057564\n",
            " 1211829/10000000: episode: 6029, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 878.600, mean reward: 4.371 [-10.000, 451.500], mean action: 2.393 [0.000, 10.000], mean observation: 27.284 [0.005, 434.600], loss: 277.417480, mae: 34.498974, mean_q: -35.244591\n",
            " 1212030/10000000: episode: 6030, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -214.000, mean reward: -1.065 [-107.000, 90.900], mean action: 1.910 [0.000, 10.000], mean observation: 28.271 [0.000, 638.600], loss: 379.172333, mae: 34.202366, mean_q: -34.499527\n",
            " 1212231/10000000: episode: 6031, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -435.000, mean reward: -2.164 [-217.500, 46.400], mean action: 1.687 [0.000, 10.000], mean observation: 40.349 [0.000, 584.000], loss: 201.141525, mae: 33.680401, mean_q: -34.014069\n",
            " 1212432/10000000: episode: 6032, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -0.200, mean reward: -0.001 [-10.000, 199.800], mean action: 1.627 [0.000, 10.000], mean observation: 29.796 [0.000, 530.500], loss: 588.159424, mae: 33.574066, mean_q: -33.921879\n",
            " 1212633/10000000: episode: 6033, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -255.600, mean reward: -1.272 [-127.800, 79.000], mean action: 2.363 [0.000, 10.000], mean observation: 35.588 [0.000, 796.200], loss: 637.641235, mae: 32.826935, mean_q: -33.456497\n",
            " 1212834/10000000: episode: 6034, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -965.000, mean reward: -4.801 [-482.500, 51.000], mean action: 2.856 [0.000, 10.000], mean observation: 36.875 [0.000, 588.400], loss: 589.455200, mae: 32.689011, mean_q: -33.215248\n",
            " 1213035/10000000: episode: 6035, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -287.800, mean reward: -1.432 [-143.900, 252.300], mean action: 2.527 [0.000, 10.000], mean observation: 34.641 [0.000, 507.900], loss: 324.109283, mae: 32.606918, mean_q: -33.354420\n",
            " 1213236/10000000: episode: 6036, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -208.600, mean reward: -1.038 [-104.300, 442.200], mean action: 2.925 [0.000, 10.000], mean observation: 33.747 [0.003, 463.800], loss: 247.721115, mae: 32.542751, mean_q: -33.424961\n",
            " 1213437/10000000: episode: 6037, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -544.800, mean reward: -2.710 [-272.400, 82.400], mean action: 2.642 [0.000, 10.000], mean observation: 28.891 [0.002, 545.200], loss: 341.443207, mae: 32.348824, mean_q: -33.158466\n",
            " 1213638/10000000: episode: 6038, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -646.000, mean reward: -3.214 [-323.000, 62.000], mean action: 3.229 [0.000, 10.000], mean observation: 28.538 [0.004, 451.800], loss: 559.758789, mae: 32.116596, mean_q: -33.057915\n",
            " 1213839/10000000: episode: 6039, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -46.400, mean reward: -0.231 [-23.200, 336.500], mean action: 3.557 [0.000, 10.000], mean observation: 35.692 [0.000, 491.000], loss: 344.083008, mae: 32.029087, mean_q: -32.952332\n",
            " 1214040/10000000: episode: 6040, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -1117.000, mean reward: -5.557 [-558.500, 22.000], mean action: 3.090 [0.000, 10.000], mean observation: 33.708 [0.002, 481.400], loss: 401.706573, mae: 32.239597, mean_q: -33.005981\n",
            " 1214241/10000000: episode: 6041, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -407.400, mean reward: -2.027 [-203.700, 136.000], mean action: 2.552 [0.000, 10.000], mean observation: 30.354 [0.000, 746.900], loss: 528.069458, mae: 31.890360, mean_q: -32.662025\n",
            " 1214442/10000000: episode: 6042, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -924.800, mean reward: -4.601 [-462.400, 50.400], mean action: 3.378 [0.000, 10.000], mean observation: 31.587 [0.000, 609.400], loss: 410.391479, mae: 31.690086, mean_q: -32.754723\n",
            " 1214643/10000000: episode: 6043, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: 133.000, mean reward: 0.662 [-10.000, 460.800], mean action: 3.269 [0.000, 10.000], mean observation: 30.144 [0.001, 583.400], loss: 601.470093, mae: 32.136551, mean_q: -33.104450\n",
            " 1214844/10000000: episode: 6044, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -986.400, mean reward: -4.907 [-493.200, 57.200], mean action: 3.950 [0.000, 10.000], mean observation: 31.954 [0.000, 702.100], loss: 295.709198, mae: 32.262218, mean_q: -33.495773\n",
            " 1215045/10000000: episode: 6045, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -801.200, mean reward: -3.986 [-400.600, 213.000], mean action: 4.119 [0.000, 10.000], mean observation: 43.020 [0.000, 653.600], loss: 430.522308, mae: 32.480450, mean_q: -33.651276\n",
            " 1215246/10000000: episode: 6046, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -739.200, mean reward: -3.678 [-369.600, 98.000], mean action: 3.214 [0.000, 10.000], mean observation: 25.679 [0.000, 509.800], loss: 751.238892, mae: 32.662628, mean_q: -33.685032\n",
            " 1215447/10000000: episode: 6047, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -1219.800, mean reward: -6.069 [-609.900, 24.000], mean action: 3.846 [0.000, 10.000], mean observation: 37.466 [0.002, 630.900], loss: 319.158966, mae: 33.471691, mean_q: -34.713203\n",
            " 1215648/10000000: episode: 6048, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: 837.400, mean reward: 4.166 [-10.000, 421.500], mean action: 3.294 [0.000, 10.000], mean observation: 37.136 [0.001, 571.000], loss: 484.353821, mae: 33.637318, mean_q: -34.647129\n",
            " 1215849/10000000: episode: 6049, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -936.400, mean reward: -4.659 [-468.200, 39.500], mean action: 3.184 [0.000, 10.000], mean observation: 36.474 [0.001, 591.000], loss: 586.277710, mae: 33.431221, mean_q: -34.335594\n",
            " 1216050/10000000: episode: 6050, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -734.000, mean reward: -3.652 [-367.000, 54.600], mean action: 3.378 [0.000, 10.000], mean observation: 29.242 [0.001, 532.400], loss: 254.557175, mae: 33.617195, mean_q: -34.635830\n",
            " 1216251/10000000: episode: 6051, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -200.600, mean reward: -0.998 [-100.300, 92.000], mean action: 2.821 [0.000, 10.000], mean observation: 33.852 [0.002, 595.600], loss: 842.705139, mae: 33.228306, mean_q: -34.228710\n",
            " 1216452/10000000: episode: 6052, duration: 1.473s, episode steps: 201, steps per second: 137, episode reward: -480.400, mean reward: -2.390 [-240.200, 48.400], mean action: 2.766 [0.000, 10.000], mean observation: 33.025 [0.000, 530.000], loss: 408.429718, mae: 33.067616, mean_q: -33.997757\n",
            " 1216653/10000000: episode: 6053, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: 207.000, mean reward: 1.030 [-10.000, 203.000], mean action: 2.697 [0.000, 10.000], mean observation: 27.401 [0.002, 504.200], loss: 363.310638, mae: 32.969818, mean_q: -33.911449\n",
            " 1216854/10000000: episode: 6054, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -760.800, mean reward: -3.785 [-380.400, 27.600], mean action: 2.597 [0.000, 10.000], mean observation: 34.052 [0.001, 591.100], loss: 310.794373, mae: 33.272175, mean_q: -34.175491\n",
            " 1217055/10000000: episode: 6055, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -7.800, mean reward: -0.039 [-10.000, 293.500], mean action: 2.627 [0.000, 10.000], mean observation: 30.264 [0.003, 545.800], loss: 367.869934, mae: 33.335938, mean_q: -33.972263\n",
            " 1217256/10000000: episode: 6056, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -799.200, mean reward: -3.976 [-399.600, 32.000], mean action: 2.657 [0.000, 10.000], mean observation: 39.003 [0.001, 598.700], loss: 316.977722, mae: 33.501053, mean_q: -34.367668\n",
            " 1217457/10000000: episode: 6057, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -594.000, mean reward: -2.955 [-297.000, 47.400], mean action: 2.697 [0.000, 10.000], mean observation: 29.192 [0.002, 308.400], loss: 292.231659, mae: 33.740395, mean_q: -34.501450\n",
            " 1217658/10000000: episode: 6058, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -160.000, mean reward: -0.796 [-80.000, 184.800], mean action: 2.866 [0.000, 10.000], mean observation: 29.357 [0.000, 479.300], loss: 669.301697, mae: 34.092373, mean_q: -35.384605\n",
            " 1217859/10000000: episode: 6059, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -752.600, mean reward: -3.744 [-376.300, 105.000], mean action: 3.622 [0.000, 10.000], mean observation: 34.346 [0.000, 663.800], loss: 577.426697, mae: 33.920589, mean_q: -35.413887\n",
            " 1218060/10000000: episode: 6060, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -84.600, mean reward: -0.421 [-42.300, 123.300], mean action: 2.801 [0.000, 10.000], mean observation: 29.926 [0.001, 477.000], loss: 388.549561, mae: 34.303642, mean_q: -35.261841\n",
            " 1218261/10000000: episode: 6061, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -666.200, mean reward: -3.314 [-333.100, 28.600], mean action: 2.463 [0.000, 10.000], mean observation: 34.583 [0.000, 639.500], loss: 294.251862, mae: 34.755684, mean_q: -35.738411\n",
            " 1218462/10000000: episode: 6062, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -629.200, mean reward: -3.130 [-314.600, 60.600], mean action: 2.259 [0.000, 10.000], mean observation: 33.014 [0.001, 619.200], loss: 231.529358, mae: 34.900845, mean_q: -35.661785\n",
            " 1218663/10000000: episode: 6063, duration: 1.552s, episode steps: 201, steps per second: 129, episode reward: -320.200, mean reward: -1.593 [-160.100, 78.300], mean action: 2.308 [0.000, 10.000], mean observation: 29.311 [0.001, 523.600], loss: 206.933960, mae: 34.288597, mean_q: -35.109375\n",
            " 1218864/10000000: episode: 6064, duration: 1.564s, episode steps: 201, steps per second: 128, episode reward: -416.200, mean reward: -2.071 [-208.100, 77.400], mean action: 1.930 [0.000, 10.000], mean observation: 37.437 [0.000, 651.100], loss: 683.895447, mae: 34.584732, mean_q: -34.990326\n",
            " 1219065/10000000: episode: 6065, duration: 1.540s, episode steps: 201, steps per second: 131, episode reward: -645.000, mean reward: -3.209 [-322.500, 19.600], mean action: 2.219 [0.000, 10.000], mean observation: 37.463 [0.000, 818.100], loss: 281.311218, mae: 34.566978, mean_q: -35.179340\n",
            " 1219266/10000000: episode: 6066, duration: 1.546s, episode steps: 201, steps per second: 130, episode reward: -127.800, mean reward: -0.636 [-63.900, 100.800], mean action: 2.687 [0.000, 10.000], mean observation: 31.123 [0.003, 540.500], loss: 591.945557, mae: 33.923256, mean_q: -34.617371\n",
            " 1219467/10000000: episode: 6067, duration: 1.598s, episode steps: 201, steps per second: 126, episode reward: -683.800, mean reward: -3.402 [-341.900, 36.500], mean action: 2.637 [0.000, 10.000], mean observation: 33.787 [0.001, 565.900], loss: 723.085999, mae: 33.364624, mean_q: -34.043056\n",
            " 1219668/10000000: episode: 6068, duration: 1.577s, episode steps: 201, steps per second: 127, episode reward: -614.600, mean reward: -3.058 [-307.300, 102.500], mean action: 2.403 [0.000, 10.000], mean observation: 33.666 [0.000, 720.900], loss: 280.099579, mae: 32.746407, mean_q: -33.123417\n",
            " 1219869/10000000: episode: 6069, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: 541.000, mean reward: 2.692 [-10.000, 535.500], mean action: 2.005 [0.000, 10.000], mean observation: 30.970 [0.002, 624.500], loss: 572.735229, mae: 32.601238, mean_q: -32.969883\n",
            " 1220070/10000000: episode: 6070, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -387.800, mean reward: -1.929 [-193.900, 38.000], mean action: 2.303 [0.000, 10.000], mean observation: 35.337 [0.002, 503.100], loss: 441.678711, mae: 32.788544, mean_q: -33.302208\n",
            " 1220271/10000000: episode: 6071, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -286.400, mean reward: -1.425 [-143.200, 143.000], mean action: 2.303 [0.000, 10.000], mean observation: 26.719 [0.001, 462.800], loss: 279.582825, mae: 32.768887, mean_q: -33.020576\n",
            " 1220472/10000000: episode: 6072, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 32.600, mean reward: 0.162 [-10.000, 130.900], mean action: 1.711 [0.000, 10.000], mean observation: 33.987 [0.000, 476.600], loss: 218.707687, mae: 32.840233, mean_q: -33.161572\n",
            " 1220673/10000000: episode: 6073, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -441.800, mean reward: -2.198 [-220.900, 27.800], mean action: 1.498 [0.000, 9.000], mean observation: 40.119 [0.001, 642.700], loss: 316.122681, mae: 32.978199, mean_q: -33.195644\n",
            " 1220874/10000000: episode: 6074, duration: 1.378s, episode steps: 201, steps per second: 146, episode reward: 94.000, mean reward: 0.468 [-10.000, 84.600], mean action: 1.522 [0.000, 10.000], mean observation: 35.012 [0.000, 907.100], loss: 387.380249, mae: 32.958164, mean_q: -32.818504\n",
            " 1221075/10000000: episode: 6075, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -272.600, mean reward: -1.356 [-136.300, 25.600], mean action: 1.493 [0.000, 10.000], mean observation: 30.827 [0.001, 399.700], loss: 322.280701, mae: 32.431393, mean_q: -32.341938\n",
            " 1221276/10000000: episode: 6076, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -417.200, mean reward: -2.076 [-208.600, 16.600], mean action: 1.249 [0.000, 10.000], mean observation: 33.828 [0.001, 507.200], loss: 378.087891, mae: 32.104858, mean_q: -31.668591\n",
            " 1221477/10000000: episode: 6077, duration: 1.364s, episode steps: 201, steps per second: 147, episode reward: -289.600, mean reward: -1.441 [-144.800, 26.100], mean action: 1.338 [0.000, 10.000], mean observation: 30.942 [0.002, 522.800], loss: 219.006027, mae: 31.482145, mean_q: -30.996941\n",
            " 1221678/10000000: episode: 6078, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: 41.600, mean reward: 0.207 [-10.000, 78.600], mean action: 1.915 [0.000, 10.000], mean observation: 29.762 [0.000, 623.900], loss: 209.266403, mae: 30.840454, mean_q: -30.557037\n",
            " 1221879/10000000: episode: 6079, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -351.200, mean reward: -1.747 [-175.600, 42.600], mean action: 1.687 [0.000, 7.000], mean observation: 34.361 [0.002, 542.000], loss: 270.278290, mae: 31.248791, mean_q: -30.978535\n",
            " 1222080/10000000: episode: 6080, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: 487.600, mean reward: 2.426 [-10.000, 281.600], mean action: 1.761 [0.000, 10.000], mean observation: 32.918 [0.000, 493.100], loss: 267.455597, mae: 31.071207, mean_q: -30.625490\n",
            " 1222281/10000000: episode: 6081, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -6.600, mean reward: -0.033 [-7.000, 108.000], mean action: 1.433 [0.000, 7.000], mean observation: 32.799 [0.000, 781.200], loss: 560.285889, mae: 31.090431, mean_q: -30.388599\n",
            " 1222482/10000000: episode: 6082, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: 58.800, mean reward: 0.293 [-10.000, 127.400], mean action: 1.289 [0.000, 10.000], mean observation: 30.855 [0.001, 492.700], loss: 411.730835, mae: 30.207457, mean_q: -29.008850\n",
            " 1222683/10000000: episode: 6083, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: 336.200, mean reward: 1.673 [-10.000, 352.500], mean action: 1.294 [0.000, 10.000], mean observation: 36.049 [0.001, 467.200], loss: 207.577850, mae: 29.236376, mean_q: -28.139687\n",
            " 1222884/10000000: episode: 6084, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: -223.400, mean reward: -1.111 [-111.700, 67.500], mean action: 1.517 [0.000, 7.000], mean observation: 33.976 [0.001, 511.400], loss: 664.584290, mae: 29.147160, mean_q: -28.405148\n",
            " 1223085/10000000: episode: 6085, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: 289.800, mean reward: 1.442 [-10.000, 203.000], mean action: 2.129 [0.000, 10.000], mean observation: 33.905 [0.000, 764.600], loss: 340.673950, mae: 29.343536, mean_q: -29.037395\n",
            " 1223286/10000000: episode: 6086, duration: 1.529s, episode steps: 201, steps per second: 131, episode reward: -468.600, mean reward: -2.331 [-234.300, 62.800], mean action: 2.139 [0.000, 8.000], mean observation: 30.861 [0.000, 498.200], loss: 328.214172, mae: 29.463892, mean_q: -29.282537\n",
            " 1223487/10000000: episode: 6087, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -634.600, mean reward: -3.157 [-317.300, 30.600], mean action: 2.244 [0.000, 10.000], mean observation: 27.728 [0.001, 516.900], loss: 284.971832, mae: 29.856018, mean_q: -29.753916\n",
            " 1223688/10000000: episode: 6088, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -214.000, mean reward: -1.065 [-107.000, 111.300], mean action: 1.955 [0.000, 9.000], mean observation: 33.642 [0.000, 598.900], loss: 270.059845, mae: 30.304314, mean_q: -29.984125\n",
            " 1223889/10000000: episode: 6089, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -524.200, mean reward: -2.608 [-262.100, 62.600], mean action: 1.866 [0.000, 9.000], mean observation: 41.265 [0.000, 792.800], loss: 386.017700, mae: 30.051474, mean_q: -29.770748\n",
            " 1224090/10000000: episode: 6090, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -653.800, mean reward: -3.253 [-326.900, 47.700], mean action: 2.104 [0.000, 7.000], mean observation: 28.580 [0.001, 420.000], loss: 300.984589, mae: 29.823826, mean_q: -29.617691\n",
            " 1224291/10000000: episode: 6091, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -346.000, mean reward: -1.721 [-173.000, 42.400], mean action: 2.040 [0.000, 10.000], mean observation: 40.699 [0.000, 746.700], loss: 590.987549, mae: 30.830946, mean_q: -30.607237\n",
            " 1224492/10000000: episode: 6092, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -167.600, mean reward: -0.834 [-83.800, 68.500], mean action: 2.194 [0.000, 8.000], mean observation: 31.295 [0.001, 375.100], loss: 173.952148, mae: 30.769773, mean_q: -30.585686\n",
            " 1224693/10000000: episode: 6093, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -335.600, mean reward: -1.670 [-167.800, 61.600], mean action: 1.910 [0.000, 10.000], mean observation: 35.100 [0.000, 541.500], loss: 605.611267, mae: 31.079981, mean_q: -30.770094\n",
            " 1224894/10000000: episode: 6094, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -180.000, mean reward: -0.896 [-90.000, 78.200], mean action: 2.184 [0.000, 10.000], mean observation: 27.506 [0.001, 519.300], loss: 436.029633, mae: 31.265770, mean_q: -31.108116\n",
            " 1225095/10000000: episode: 6095, duration: 1.501s, episode steps: 201, steps per second: 134, episode reward: -568.600, mean reward: -2.829 [-284.300, 34.200], mean action: 1.925 [0.000, 10.000], mean observation: 36.288 [0.002, 460.100], loss: 846.074158, mae: 30.859346, mean_q: -30.626459\n",
            " 1225296/10000000: episode: 6096, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: 895.600, mean reward: 4.456 [-10.000, 447.800], mean action: 2.179 [0.000, 10.000], mean observation: 34.340 [0.000, 579.300], loss: 207.037079, mae: 30.830307, mean_q: -30.767620\n",
            " 1225497/10000000: episode: 6097, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -459.200, mean reward: -2.285 [-229.600, 115.200], mean action: 2.388 [0.000, 10.000], mean observation: 31.995 [0.001, 576.200], loss: 212.716537, mae: 30.710943, mean_q: -30.595728\n",
            " 1225698/10000000: episode: 6098, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -128.200, mean reward: -0.638 [-64.100, 70.500], mean action: 1.955 [0.000, 10.000], mean observation: 28.865 [0.001, 550.200], loss: 292.569824, mae: 30.487543, mean_q: -30.125029\n",
            " 1225899/10000000: episode: 6099, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -607.000, mean reward: -3.020 [-303.500, 42.000], mean action: 2.199 [0.000, 8.000], mean observation: 32.999 [0.001, 548.600], loss: 838.003723, mae: 30.049751, mean_q: -29.758934\n",
            " 1226100/10000000: episode: 6100, duration: 1.589s, episode steps: 201, steps per second: 126, episode reward: -264.000, mean reward: -1.313 [-132.000, 52.800], mean action: 1.935 [0.000, 10.000], mean observation: 34.748 [0.001, 494.100], loss: 302.333344, mae: 29.794310, mean_q: -29.406017\n",
            " 1226301/10000000: episode: 6101, duration: 1.621s, episode steps: 201, steps per second: 124, episode reward: -403.400, mean reward: -2.007 [-201.700, 45.600], mean action: 1.677 [0.000, 7.000], mean observation: 34.059 [0.000, 525.700], loss: 206.875443, mae: 30.226002, mean_q: -29.872765\n",
            " 1226502/10000000: episode: 6102, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -469.000, mean reward: -2.333 [-234.500, 29.600], mean action: 1.463 [0.000, 10.000], mean observation: 42.108 [0.000, 803.400], loss: 316.662628, mae: 30.619825, mean_q: -30.160442\n",
            " 1226703/10000000: episode: 6103, duration: 1.576s, episode steps: 201, steps per second: 128, episode reward: 74.400, mean reward: 0.370 [-8.000, 139.800], mean action: 1.786 [0.000, 8.000], mean observation: 28.409 [0.000, 597.200], loss: 408.011749, mae: 30.315485, mean_q: -29.780312\n",
            " 1226904/10000000: episode: 6104, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -119.200, mean reward: -0.593 [-59.600, 84.800], mean action: 1.896 [0.000, 10.000], mean observation: 30.912 [0.002, 455.800], loss: 638.434204, mae: 29.992235, mean_q: -29.566969\n",
            " 1227105/10000000: episode: 6105, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -626.200, mean reward: -3.115 [-313.100, 19.000], mean action: 2.139 [0.000, 10.000], mean observation: 40.598 [0.001, 545.700], loss: 688.199951, mae: 29.293365, mean_q: -29.207260\n",
            " 1227306/10000000: episode: 6106, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -563.600, mean reward: -2.804 [-281.800, 57.000], mean action: 2.244 [0.000, 10.000], mean observation: 30.251 [0.000, 581.400], loss: 391.864807, mae: 29.303957, mean_q: -29.217730\n",
            " 1227507/10000000: episode: 6107, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -527.400, mean reward: -2.624 [-263.700, 77.000], mean action: 2.418 [0.000, 10.000], mean observation: 35.178 [0.000, 622.400], loss: 287.656342, mae: 29.266106, mean_q: -29.321119\n",
            " 1227708/10000000: episode: 6108, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -496.400, mean reward: -2.470 [-248.200, 27.600], mean action: 1.980 [0.000, 10.000], mean observation: 35.619 [0.001, 430.400], loss: 334.044037, mae: 29.845232, mean_q: -29.764978\n",
            " 1227909/10000000: episode: 6109, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 607.000, mean reward: 3.020 [-10.000, 303.500], mean action: 2.587 [0.000, 10.000], mean observation: 37.010 [0.000, 669.400], loss: 407.095764, mae: 29.876642, mean_q: -29.850328\n",
            " 1228110/10000000: episode: 6110, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -495.400, mean reward: -2.465 [-247.700, 89.000], mean action: 2.493 [0.000, 10.000], mean observation: 33.819 [0.002, 511.500], loss: 423.028748, mae: 29.919697, mean_q: -30.050110\n",
            " 1228311/10000000: episode: 6111, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -3.000, mean reward: -0.015 [-10.000, 89.000], mean action: 2.114 [0.000, 10.000], mean observation: 36.800 [0.000, 663.000], loss: 310.550964, mae: 29.930601, mean_q: -29.895298\n",
            " 1228512/10000000: episode: 6112, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -555.600, mean reward: -2.764 [-277.800, 47.500], mean action: 2.393 [0.000, 10.000], mean observation: 37.081 [0.000, 637.900], loss: 203.838211, mae: 30.199482, mean_q: -30.114346\n",
            " 1228713/10000000: episode: 6113, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -373.000, mean reward: -1.856 [-186.500, 95.500], mean action: 2.303 [0.000, 10.000], mean observation: 34.635 [0.002, 542.900], loss: 338.127167, mae: 30.321079, mean_q: -30.076990\n",
            " 1228914/10000000: episode: 6114, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -486.800, mean reward: -2.422 [-243.400, 60.500], mean action: 2.498 [0.000, 10.000], mean observation: 31.870 [0.001, 581.100], loss: 241.256882, mae: 30.294086, mean_q: -30.359186\n",
            " 1229115/10000000: episode: 6115, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -749.600, mean reward: -3.729 [-374.800, 35.800], mean action: 2.234 [0.000, 7.000], mean observation: 32.484 [0.001, 469.000], loss: 237.460037, mae: 30.250044, mean_q: -30.192614\n",
            " 1229316/10000000: episode: 6116, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -301.000, mean reward: -1.498 [-150.500, 112.700], mean action: 2.249 [0.000, 10.000], mean observation: 35.902 [0.000, 622.800], loss: 221.508545, mae: 30.273893, mean_q: -30.378939\n",
            " 1229517/10000000: episode: 6117, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -573.200, mean reward: -2.852 [-286.600, 34.800], mean action: 2.134 [0.000, 10.000], mean observation: 34.056 [0.001, 446.000], loss: 355.808807, mae: 30.508095, mean_q: -30.829189\n",
            " 1229718/10000000: episode: 6118, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -569.400, mean reward: -2.833 [-284.700, 34.200], mean action: 2.144 [0.000, 10.000], mean observation: 32.734 [0.001, 424.400], loss: 317.895172, mae: 30.779243, mean_q: -31.020517\n",
            " 1229919/10000000: episode: 6119, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: 16.800, mean reward: 0.084 [-10.000, 231.000], mean action: 2.214 [0.000, 10.000], mean observation: 32.213 [0.000, 749.600], loss: 335.978027, mae: 30.476107, mean_q: -30.851067\n",
            " 1230120/10000000: episode: 6120, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -527.400, mean reward: -2.624 [-263.700, 41.300], mean action: 2.672 [0.000, 10.000], mean observation: 30.295 [0.001, 587.600], loss: 329.652679, mae: 30.676048, mean_q: -31.103712\n",
            " 1230321/10000000: episode: 6121, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: 2084.600, mean reward: 10.371 [-10.000, 1042.300], mean action: 2.896 [0.000, 10.000], mean observation: 36.189 [0.002, 516.600], loss: 361.576172, mae: 31.152061, mean_q: -31.701693\n",
            " 1230522/10000000: episode: 6122, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -359.200, mean reward: -1.787 [-179.600, 100.000], mean action: 2.562 [0.000, 10.000], mean observation: 27.825 [0.001, 415.200], loss: 275.949646, mae: 31.204412, mean_q: -31.727903\n",
            " 1230723/10000000: episode: 6123, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -308.200, mean reward: -1.533 [-154.100, 147.700], mean action: 2.716 [0.000, 10.000], mean observation: 35.670 [0.004, 509.100], loss: 294.671265, mae: 30.982092, mean_q: -31.595785\n",
            " 1230924/10000000: episode: 6124, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -284.600, mean reward: -1.416 [-142.300, 96.800], mean action: 2.448 [0.000, 10.000], mean observation: 27.996 [0.000, 590.900], loss: 405.929443, mae: 31.146296, mean_q: -31.313282\n",
            " 1231125/10000000: episode: 6125, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -335.400, mean reward: -1.669 [-167.700, 58.200], mean action: 2.030 [0.000, 10.000], mean observation: 33.162 [0.003, 566.300], loss: 234.146790, mae: 31.246769, mean_q: -31.563360\n",
            " 1231326/10000000: episode: 6126, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -389.200, mean reward: -1.936 [-194.600, 38.000], mean action: 1.866 [0.000, 10.000], mean observation: 28.521 [0.000, 486.700], loss: 262.252258, mae: 31.659281, mean_q: -31.867899\n",
            " 1231527/10000000: episode: 6127, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -320.400, mean reward: -1.594 [-160.200, 67.500], mean action: 2.075 [0.000, 10.000], mean observation: 34.486 [0.001, 510.800], loss: 262.208496, mae: 31.408300, mean_q: -31.596523\n",
            " 1231728/10000000: episode: 6128, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: 416.400, mean reward: 2.072 [-8.000, 208.200], mean action: 2.308 [0.000, 8.000], mean observation: 31.946 [0.001, 423.300], loss: 386.195709, mae: 31.571367, mean_q: -31.884781\n",
            " 1231929/10000000: episode: 6129, duration: 1.507s, episode steps: 201, steps per second: 133, episode reward: -37.200, mean reward: -0.185 [-18.600, 114.900], mean action: 2.358 [0.000, 10.000], mean observation: 34.023 [0.000, 637.000], loss: 408.475983, mae: 32.002670, mean_q: -32.777115\n",
            " 1232130/10000000: episode: 6130, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -174.400, mean reward: -0.868 [-87.200, 271.500], mean action: 2.806 [0.000, 10.000], mean observation: 31.047 [0.002, 439.600], loss: 394.330139, mae: 32.281742, mean_q: -33.151516\n",
            " 1232331/10000000: episode: 6131, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: 818.600, mean reward: 4.073 [-10.000, 519.000], mean action: 2.925 [0.000, 10.000], mean observation: 30.747 [0.000, 640.400], loss: 230.140671, mae: 32.118626, mean_q: -33.237823\n",
            " 1232532/10000000: episode: 6132, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -694.800, mean reward: -3.457 [-347.400, 70.100], mean action: 2.945 [0.000, 10.000], mean observation: 33.626 [0.001, 512.800], loss: 243.058853, mae: 32.223061, mean_q: -33.443211\n",
            " 1232733/10000000: episode: 6133, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -434.000, mean reward: -2.159 [-217.000, 107.100], mean action: 3.005 [0.000, 10.000], mean observation: 32.256 [0.000, 558.800], loss: 198.569153, mae: 32.830936, mean_q: -33.919685\n",
            " 1232934/10000000: episode: 6134, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -72.000, mean reward: -0.358 [-36.000, 90.300], mean action: 2.592 [0.000, 10.000], mean observation: 34.656 [0.003, 539.800], loss: 176.129898, mae: 33.102795, mean_q: -34.057713\n",
            " 1233135/10000000: episode: 6135, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -545.600, mean reward: -2.714 [-272.800, 54.000], mean action: 2.498 [0.000, 10.000], mean observation: 35.462 [0.001, 584.600], loss: 401.093292, mae: 32.883572, mean_q: -33.366692\n",
            " 1233336/10000000: episode: 6136, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -529.400, mean reward: -2.634 [-264.700, 43.300], mean action: 2.139 [0.000, 10.000], mean observation: 36.975 [0.000, 455.400], loss: 221.572647, mae: 33.026051, mean_q: -33.332027\n",
            " 1233537/10000000: episode: 6137, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -49.400, mean reward: -0.246 [-24.700, 76.500], mean action: 2.035 [0.000, 10.000], mean observation: 31.429 [0.001, 510.200], loss: 361.290436, mae: 33.177753, mean_q: -33.421909\n",
            " 1233738/10000000: episode: 6138, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -588.000, mean reward: -2.925 [-294.000, 34.400], mean action: 2.726 [0.000, 10.000], mean observation: 31.714 [0.000, 708.200], loss: 293.528015, mae: 32.761154, mean_q: -33.171398\n",
            " 1233939/10000000: episode: 6139, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -40.200, mean reward: -0.200 [-20.100, 212.600], mean action: 2.119 [0.000, 10.000], mean observation: 37.950 [0.002, 500.200], loss: 235.082794, mae: 32.846100, mean_q: -33.132328\n",
            " 1234140/10000000: episode: 6140, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: 219.600, mean reward: 1.093 [-10.000, 109.800], mean action: 2.114 [0.000, 10.000], mean observation: 35.909 [0.000, 440.500], loss: 378.027283, mae: 32.686684, mean_q: -33.123390\n",
            " 1234341/10000000: episode: 6141, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -4.600, mean reward: -0.023 [-10.000, 74.000], mean action: 2.557 [0.000, 10.000], mean observation: 32.763 [0.000, 566.800], loss: 274.425323, mae: 32.753490, mean_q: -33.296288\n",
            " 1234542/10000000: episode: 6142, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -417.000, mean reward: -2.075 [-208.500, 37.100], mean action: 2.010 [0.000, 10.000], mean observation: 31.032 [0.001, 571.900], loss: 201.402695, mae: 32.829361, mean_q: -33.423523\n",
            " 1234743/10000000: episode: 6143, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: 86.800, mean reward: 0.432 [-10.000, 282.000], mean action: 2.090 [0.000, 10.000], mean observation: 34.290 [0.000, 494.100], loss: 292.711456, mae: 33.140942, mean_q: -33.208401\n",
            " 1234944/10000000: episode: 6144, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -101.600, mean reward: -0.505 [-50.800, 161.400], mean action: 2.294 [0.000, 10.000], mean observation: 28.469 [0.002, 465.200], loss: 208.438553, mae: 32.114716, mean_q: -32.338898\n",
            " 1235145/10000000: episode: 6145, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -208.600, mean reward: -1.038 [-104.300, 158.900], mean action: 2.239 [0.000, 10.000], mean observation: 31.498 [0.001, 533.900], loss: 250.649475, mae: 32.338783, mean_q: -32.759537\n",
            " 1235346/10000000: episode: 6146, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: 180.000, mean reward: 0.896 [-10.000, 323.400], mean action: 2.736 [0.000, 10.000], mean observation: 32.999 [0.000, 798.300], loss: 419.920471, mae: 32.303509, mean_q: -32.949417\n",
            " 1235547/10000000: episode: 6147, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -309.400, mean reward: -1.539 [-154.700, 201.600], mean action: 2.846 [0.000, 10.000], mean observation: 30.445 [0.001, 434.300], loss: 311.038391, mae: 32.468006, mean_q: -32.978683\n",
            " 1235748/10000000: episode: 6148, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -611.200, mean reward: -3.041 [-305.600, 82.800], mean action: 3.045 [0.000, 10.000], mean observation: 32.534 [0.000, 668.900], loss: 222.425797, mae: 32.464668, mean_q: -33.235008\n",
            " 1235949/10000000: episode: 6149, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: 412.000, mean reward: 2.050 [-10.000, 375.900], mean action: 2.443 [0.000, 10.000], mean observation: 34.332 [0.000, 506.300], loss: 320.400696, mae: 32.334637, mean_q: -32.812569\n",
            " 1236150/10000000: episode: 6150, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -434.800, mean reward: -2.163 [-217.400, 126.500], mean action: 2.965 [0.000, 10.000], mean observation: 29.538 [0.003, 383.900], loss: 304.376892, mae: 32.878143, mean_q: -33.350193\n",
            " 1236351/10000000: episode: 6151, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -522.800, mean reward: -2.601 [-261.400, 120.000], mean action: 2.900 [0.000, 10.000], mean observation: 37.352 [0.000, 605.900], loss: 291.291931, mae: 32.957291, mean_q: -33.414299\n",
            " 1236552/10000000: episode: 6152, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -330.200, mean reward: -1.643 [-165.100, 106.400], mean action: 2.980 [0.000, 10.000], mean observation: 35.094 [0.000, 650.000], loss: 301.212585, mae: 32.847317, mean_q: -33.378815\n",
            " 1236753/10000000: episode: 6153, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: 100.800, mean reward: 0.501 [-10.000, 450.000], mean action: 2.950 [0.000, 10.000], mean observation: 30.141 [0.002, 432.300], loss: 502.976105, mae: 32.624981, mean_q: -33.048138\n",
            " 1236954/10000000: episode: 6154, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: -249.800, mean reward: -1.243 [-124.900, 161.500], mean action: 2.891 [0.000, 10.000], mean observation: 36.096 [0.003, 420.100], loss: 267.805023, mae: 31.825691, mean_q: -32.169647\n",
            " 1237155/10000000: episode: 6155, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -378.000, mean reward: -1.881 [-189.000, 181.500], mean action: 2.627 [0.000, 10.000], mean observation: 39.292 [0.001, 510.400], loss: 294.855774, mae: 31.894726, mean_q: -32.483109\n",
            " 1237356/10000000: episode: 6156, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: 505.200, mean reward: 2.513 [-10.000, 375.400], mean action: 2.881 [0.000, 10.000], mean observation: 33.469 [0.000, 508.900], loss: 305.128021, mae: 32.192352, mean_q: -33.041553\n",
            " 1237557/10000000: episode: 6157, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: 2665.800, mean reward: 13.263 [-10.000, 1687.500], mean action: 2.776 [0.000, 10.000], mean observation: 31.640 [0.002, 447.500], loss: 355.950134, mae: 32.350254, mean_q: -32.914795\n",
            " 1237758/10000000: episode: 6158, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 174.400, mean reward: 0.868 [-10.000, 291.500], mean action: 2.124 [0.000, 10.000], mean observation: 31.322 [0.001, 618.400], loss: 276.696838, mae: 32.599991, mean_q: -32.834679\n",
            " 1237959/10000000: episode: 6159, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: 32.400, mean reward: 0.161 [-10.000, 240.600], mean action: 2.597 [0.000, 10.000], mean observation: 33.041 [0.001, 512.000], loss: 370.028229, mae: 32.579754, mean_q: -33.127087\n",
            " 1238160/10000000: episode: 6160, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -571.800, mean reward: -2.845 [-285.900, 40.800], mean action: 2.562 [0.000, 10.000], mean observation: 32.905 [0.000, 586.000], loss: 251.397461, mae: 32.288818, mean_q: -32.849926\n",
            " 1238361/10000000: episode: 6161, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -510.800, mean reward: -2.541 [-255.400, 16.000], mean action: 1.791 [0.000, 10.000], mean observation: 31.144 [0.001, 423.700], loss: 329.418884, mae: 32.665684, mean_q: -32.489845\n",
            " 1238562/10000000: episode: 6162, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -224.600, mean reward: -1.117 [-112.300, 86.400], mean action: 2.154 [0.000, 10.000], mean observation: 31.487 [0.001, 459.200], loss: 366.664642, mae: 32.293346, mean_q: -32.227627\n",
            " 1238763/10000000: episode: 6163, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -590.600, mean reward: -2.938 [-295.300, 30.800], mean action: 2.373 [0.000, 10.000], mean observation: 31.400 [0.001, 422.300], loss: 493.974457, mae: 31.726868, mean_q: -31.699923\n",
            " 1238964/10000000: episode: 6164, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -428.800, mean reward: -2.133 [-214.400, 164.100], mean action: 2.303 [0.000, 10.000], mean observation: 32.203 [0.001, 497.500], loss: 299.771912, mae: 31.505102, mean_q: -31.186338\n",
            " 1239165/10000000: episode: 6165, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: 47.800, mean reward: 0.238 [-8.000, 73.200], mean action: 2.647 [0.000, 10.000], mean observation: 31.469 [0.002, 439.900], loss: 221.495239, mae: 31.186779, mean_q: -31.278854\n",
            " 1239366/10000000: episode: 6166, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -485.800, mean reward: -2.417 [-242.900, 52.800], mean action: 2.532 [0.000, 10.000], mean observation: 30.521 [0.002, 527.400], loss: 277.280212, mae: 31.153326, mean_q: -31.312250\n",
            " 1239567/10000000: episode: 6167, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -389.000, mean reward: -1.935 [-194.500, 143.800], mean action: 2.512 [0.000, 7.000], mean observation: 36.940 [0.000, 720.900], loss: 539.430237, mae: 30.842499, mean_q: -30.922142\n",
            " 1239768/10000000: episode: 6168, duration: 1.506s, episode steps: 201, steps per second: 134, episode reward: -432.600, mean reward: -2.152 [-216.300, 38.100], mean action: 2.438 [0.000, 7.000], mean observation: 30.981 [0.001, 500.800], loss: 339.472565, mae: 30.968325, mean_q: -31.134546\n",
            " 1239969/10000000: episode: 6169, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: 191.800, mean reward: 0.954 [-10.000, 204.500], mean action: 2.731 [0.000, 10.000], mean observation: 32.602 [0.001, 424.400], loss: 300.264496, mae: 30.958738, mean_q: -31.225153\n",
            " 1240170/10000000: episode: 6170, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -70.800, mean reward: -0.352 [-35.400, 146.200], mean action: 2.488 [0.000, 10.000], mean observation: 32.063 [0.000, 441.700], loss: 378.720154, mae: 30.798820, mean_q: -31.037848\n",
            " 1240371/10000000: episode: 6171, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: 625.400, mean reward: 3.111 [-10.000, 312.700], mean action: 2.388 [0.000, 10.000], mean observation: 27.067 [0.003, 380.000], loss: 284.860229, mae: 31.006025, mean_q: -31.065491\n",
            " 1240572/10000000: episode: 6172, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -368.600, mean reward: -1.834 [-184.300, 52.500], mean action: 2.025 [0.000, 10.000], mean observation: 35.943 [0.002, 537.100], loss: 200.825012, mae: 30.865046, mean_q: -30.493101\n",
            " 1240773/10000000: episode: 6173, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -538.600, mean reward: -2.680 [-269.300, 27.000], mean action: 1.965 [0.000, 8.000], mean observation: 33.536 [0.001, 562.800], loss: 268.339905, mae: 30.809704, mean_q: -30.422337\n",
            " 1240974/10000000: episode: 6174, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: 153.400, mean reward: 0.763 [-10.000, 326.400], mean action: 2.189 [0.000, 10.000], mean observation: 30.781 [0.001, 530.600], loss: 638.071472, mae: 30.494938, mean_q: -30.232096\n",
            " 1241175/10000000: episode: 6175, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -551.600, mean reward: -2.744 [-275.800, 38.000], mean action: 2.114 [0.000, 10.000], mean observation: 40.960 [0.000, 693.300], loss: 502.385193, mae: 29.948250, mean_q: -29.543348\n",
            " 1241376/10000000: episode: 6176, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -384.800, mean reward: -1.914 [-192.400, 42.000], mean action: 2.000 [0.000, 10.000], mean observation: 33.207 [0.000, 727.600], loss: 320.011169, mae: 29.565912, mean_q: -28.915297\n",
            " 1241577/10000000: episode: 6177, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: 128.000, mean reward: 0.637 [-10.000, 129.000], mean action: 2.085 [0.000, 10.000], mean observation: 33.959 [0.002, 453.000], loss: 488.338348, mae: 29.442207, mean_q: -28.869480\n",
            " 1241778/10000000: episode: 6178, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -215.600, mean reward: -1.073 [-107.800, 51.600], mean action: 2.348 [0.000, 7.000], mean observation: 33.143 [0.001, 606.200], loss: 290.877106, mae: 29.466869, mean_q: -28.680450\n",
            " 1241979/10000000: episode: 6179, duration: 1.385s, episode steps: 201, steps per second: 145, episode reward: -215.200, mean reward: -1.071 [-107.600, 133.400], mean action: 2.234 [0.000, 7.000], mean observation: 35.569 [0.003, 593.100], loss: 406.355408, mae: 29.254459, mean_q: -28.800686\n",
            " 1242180/10000000: episode: 6180, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -295.800, mean reward: -1.472 [-147.900, 38.600], mean action: 1.925 [0.000, 10.000], mean observation: 30.178 [0.002, 482.900], loss: 290.616882, mae: 29.279367, mean_q: -28.906887\n",
            " 1242381/10000000: episode: 6181, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -253.800, mean reward: -1.263 [-126.900, 64.000], mean action: 2.552 [0.000, 10.000], mean observation: 30.306 [0.001, 697.100], loss: 452.315521, mae: 29.297792, mean_q: -28.923056\n",
            " 1242582/10000000: episode: 6182, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -632.800, mean reward: -3.148 [-316.400, 86.000], mean action: 2.711 [0.000, 10.000], mean observation: 29.914 [0.001, 562.800], loss: 612.809631, mae: 28.758238, mean_q: -28.360992\n",
            " 1242783/10000000: episode: 6183, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -709.400, mean reward: -3.529 [-354.700, 26.500], mean action: 2.721 [0.000, 10.000], mean observation: 29.201 [0.001, 471.400], loss: 216.346863, mae: 28.634085, mean_q: -28.420597\n",
            " 1242984/10000000: episode: 6184, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -851.200, mean reward: -4.235 [-425.600, 51.600], mean action: 2.547 [0.000, 10.000], mean observation: 35.706 [0.000, 522.200], loss: 248.547165, mae: 28.255972, mean_q: -28.085903\n",
            " 1243185/10000000: episode: 6185, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -143.600, mean reward: -0.714 [-71.800, 145.000], mean action: 2.542 [0.000, 8.000], mean observation: 30.667 [0.001, 521.600], loss: 188.738068, mae: 28.404259, mean_q: -28.312757\n",
            " 1243386/10000000: episode: 6186, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -126.800, mean reward: -0.631 [-63.400, 123.600], mean action: 2.612 [0.000, 8.000], mean observation: 31.622 [0.000, 527.200], loss: 260.967590, mae: 28.713095, mean_q: -28.645298\n",
            " 1243587/10000000: episode: 6187, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -834.800, mean reward: -4.153 [-417.400, 39.000], mean action: 2.881 [0.000, 10.000], mean observation: 35.978 [0.000, 451.200], loss: 575.073975, mae: 29.314749, mean_q: -29.744272\n",
            " 1243788/10000000: episode: 6188, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -877.200, mean reward: -4.364 [-438.600, 39.900], mean action: 2.871 [0.000, 10.000], mean observation: 30.060 [0.000, 765.000], loss: 260.269714, mae: 29.539524, mean_q: -29.778336\n",
            " 1243989/10000000: episode: 6189, duration: 1.374s, episode steps: 201, steps per second: 146, episode reward: 288.000, mean reward: 1.433 [-10.000, 330.000], mean action: 2.657 [0.000, 10.000], mean observation: 35.805 [0.000, 638.500], loss: 569.233765, mae: 29.697853, mean_q: -29.880928\n",
            " 1244190/10000000: episode: 6190, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: 107.400, mean reward: 0.534 [-8.000, 305.200], mean action: 2.910 [0.000, 9.000], mean observation: 32.374 [0.000, 534.900], loss: 460.974884, mae: 29.257502, mean_q: -29.288790\n",
            " 1244391/10000000: episode: 6191, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -673.800, mean reward: -3.352 [-336.900, 55.300], mean action: 2.826 [0.000, 8.000], mean observation: 38.991 [0.000, 793.800], loss: 268.011414, mae: 29.231125, mean_q: -29.288248\n",
            " 1244592/10000000: episode: 6192, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -719.600, mean reward: -3.580 [-359.800, 55.300], mean action: 2.363 [0.000, 8.000], mean observation: 36.744 [0.000, 701.500], loss: 196.383789, mae: 29.431931, mean_q: -29.489962\n",
            " 1244793/10000000: episode: 6193, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -765.000, mean reward: -3.806 [-382.500, 18.900], mean action: 2.249 [0.000, 9.000], mean observation: 40.197 [0.000, 628.000], loss: 467.935364, mae: 29.758276, mean_q: -29.976324\n",
            " 1244994/10000000: episode: 6194, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: 758.000, mean reward: 3.771 [-8.000, 379.000], mean action: 2.408 [0.000, 8.000], mean observation: 33.351 [0.000, 783.800], loss: 267.353363, mae: 30.294611, mean_q: -30.319817\n",
            " 1245195/10000000: episode: 6195, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: 17.800, mean reward: 0.089 [-8.000, 103.500], mean action: 2.055 [0.000, 8.000], mean observation: 36.335 [0.000, 642.000], loss: 310.180695, mae: 29.817345, mean_q: -29.485600\n",
            " 1245396/10000000: episode: 6196, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 104.400, mean reward: 0.519 [-8.000, 144.900], mean action: 1.955 [0.000, 8.000], mean observation: 34.571 [0.002, 515.200], loss: 270.987579, mae: 29.460815, mean_q: -29.128345\n",
            " 1245597/10000000: episode: 6197, duration: 1.387s, episode steps: 201, steps per second: 145, episode reward: -56.000, mean reward: -0.279 [-28.000, 196.700], mean action: 1.726 [0.000, 7.000], mean observation: 35.525 [0.001, 430.300], loss: 393.594696, mae: 29.116386, mean_q: -28.429741\n",
            " 1245798/10000000: episode: 6198, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -369.000, mean reward: -1.836 [-184.500, 42.000], mean action: 1.811 [0.000, 8.000], mean observation: 39.036 [0.000, 673.700], loss: 453.254730, mae: 29.045048, mean_q: -28.421190\n",
            " 1245999/10000000: episode: 6199, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -182.200, mean reward: -0.906 [-91.100, 170.000], mean action: 2.104 [0.000, 10.000], mean observation: 32.316 [0.001, 459.000], loss: 202.914444, mae: 29.051680, mean_q: -28.669466\n",
            " 1246200/10000000: episode: 6200, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -452.800, mean reward: -2.253 [-226.400, 80.700], mean action: 2.303 [0.000, 8.000], mean observation: 36.202 [0.001, 558.800], loss: 321.873749, mae: 28.994286, mean_q: -28.886702\n",
            " 1246401/10000000: episode: 6201, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -29.000, mean reward: -0.144 [-14.500, 187.800], mean action: 2.448 [0.000, 10.000], mean observation: 35.337 [0.000, 933.200], loss: 304.256805, mae: 28.661953, mean_q: -28.659538\n",
            " 1246602/10000000: episode: 6202, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -320.000, mean reward: -1.592 [-160.000, 56.000], mean action: 2.274 [0.000, 10.000], mean observation: 31.555 [0.000, 684.900], loss: 356.146423, mae: 28.824104, mean_q: -28.759222\n",
            " 1246803/10000000: episode: 6203, duration: 1.639s, episode steps: 201, steps per second: 123, episode reward: -23.400, mean reward: -0.116 [-11.700, 172.800], mean action: 2.418 [0.000, 8.000], mean observation: 32.157 [0.000, 567.300], loss: 318.182861, mae: 28.876356, mean_q: -28.518589\n",
            " 1247004/10000000: episode: 6204, duration: 1.643s, episode steps: 201, steps per second: 122, episode reward: -471.600, mean reward: -2.346 [-235.800, 55.800], mean action: 2.279 [0.000, 8.000], mean observation: 30.396 [0.001, 556.700], loss: 302.746155, mae: 28.957508, mean_q: -28.554625\n",
            " 1247205/10000000: episode: 6205, duration: 1.630s, episode steps: 201, steps per second: 123, episode reward: -705.600, mean reward: -3.510 [-352.800, 15.600], mean action: 2.070 [0.000, 8.000], mean observation: 36.560 [0.000, 444.500], loss: 358.653656, mae: 29.018841, mean_q: -28.866531\n",
            " 1247406/10000000: episode: 6206, duration: 1.695s, episode steps: 201, steps per second: 119, episode reward: -716.000, mean reward: -3.562 [-358.000, 50.000], mean action: 2.512 [0.000, 10.000], mean observation: 35.677 [0.000, 607.900], loss: 357.495636, mae: 28.513838, mean_q: -28.432854\n",
            " 1247607/10000000: episode: 6207, duration: 1.651s, episode steps: 201, steps per second: 122, episode reward: -200.000, mean reward: -0.995 [-100.000, 115.000], mean action: 2.204 [0.000, 10.000], mean observation: 36.464 [0.000, 528.400], loss: 276.523193, mae: 28.557987, mean_q: -28.156693\n",
            " 1247808/10000000: episode: 6208, duration: 1.610s, episode steps: 201, steps per second: 125, episode reward: 117.000, mean reward: 0.582 [-7.000, 350.400], mean action: 2.219 [0.000, 7.000], mean observation: 30.851 [0.001, 455.800], loss: 363.383972, mae: 28.689144, mean_q: -28.151436\n",
            " 1248009/10000000: episode: 6209, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -95.600, mean reward: -0.476 [-47.800, 128.000], mean action: 2.244 [0.000, 8.000], mean observation: 29.626 [0.000, 818.500], loss: 228.081314, mae: 28.835228, mean_q: -28.646305\n",
            " 1248210/10000000: episode: 6210, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -429.000, mean reward: -2.134 [-214.500, 37.200], mean action: 2.085 [0.000, 10.000], mean observation: 33.380 [0.002, 441.900], loss: 238.627960, mae: 28.744841, mean_q: -28.709358\n",
            " 1248411/10000000: episode: 6211, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -908.600, mean reward: -4.520 [-454.300, 15.000], mean action: 2.577 [0.000, 10.000], mean observation: 29.171 [0.001, 619.000], loss: 238.230881, mae: 28.593996, mean_q: -28.606535\n",
            " 1248612/10000000: episode: 6212, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -368.000, mean reward: -1.831 [-184.000, 112.500], mean action: 2.403 [0.000, 10.000], mean observation: 34.177 [0.001, 412.100], loss: 272.883270, mae: 28.616158, mean_q: -28.566853\n",
            " 1248813/10000000: episode: 6213, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -97.800, mean reward: -0.487 [-48.900, 188.700], mean action: 1.786 [0.000, 10.000], mean observation: 34.594 [0.002, 402.500], loss: 169.573029, mae: 28.943609, mean_q: -28.760153\n",
            " 1249014/10000000: episode: 6214, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -182.600, mean reward: -0.908 [-91.300, 51.000], mean action: 1.801 [0.000, 10.000], mean observation: 35.486 [0.000, 501.600], loss: 265.428741, mae: 29.130024, mean_q: -28.963905\n",
            " 1249215/10000000: episode: 6215, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: 90.400, mean reward: 0.450 [-8.000, 162.000], mean action: 1.766 [0.000, 8.000], mean observation: 34.770 [0.001, 492.600], loss: 206.673355, mae: 29.085112, mean_q: -29.065401\n",
            " 1249416/10000000: episode: 6216, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -33.400, mean reward: -0.166 [-16.700, 196.800], mean action: 1.900 [0.000, 8.000], mean observation: 31.875 [0.000, 590.900], loss: 453.480225, mae: 29.238791, mean_q: -28.834106\n",
            " 1249617/10000000: episode: 6217, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: 1258.800, mean reward: 6.263 [-10.000, 629.400], mean action: 2.174 [0.000, 10.000], mean observation: 37.394 [0.000, 747.100], loss: 493.177887, mae: 28.796093, mean_q: -28.504023\n",
            " 1249818/10000000: episode: 6218, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -345.000, mean reward: -1.716 [-172.500, 39.600], mean action: 1.955 [0.000, 8.000], mean observation: 29.964 [0.000, 462.000], loss: 273.726166, mae: 28.170864, mean_q: -27.634819\n",
            " 1250019/10000000: episode: 6219, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: 98.000, mean reward: 0.488 [-8.000, 110.400], mean action: 2.139 [0.000, 8.000], mean observation: 30.596 [0.000, 534.800], loss: 295.954742, mae: 28.038870, mean_q: -27.569530\n",
            " 1250220/10000000: episode: 6220, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -378.400, mean reward: -1.883 [-189.200, 96.500], mean action: 2.005 [0.000, 7.000], mean observation: 28.991 [0.000, 580.300], loss: 378.249878, mae: 28.282372, mean_q: -27.845650\n",
            " 1250421/10000000: episode: 6221, duration: 1.391s, episode steps: 201, steps per second: 144, episode reward: -605.600, mean reward: -3.013 [-302.800, 55.300], mean action: 2.393 [0.000, 8.000], mean observation: 38.889 [0.000, 522.100], loss: 346.360565, mae: 27.832649, mean_q: -27.588474\n",
            " 1250622/10000000: episode: 6222, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -463.200, mean reward: -2.304 [-231.600, 55.800], mean action: 2.483 [0.000, 10.000], mean observation: 28.781 [0.000, 813.800], loss: 205.964050, mae: 27.486698, mean_q: -27.350494\n",
            " 1250823/10000000: episode: 6223, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -546.200, mean reward: -2.717 [-273.100, 38.100], mean action: 2.104 [0.000, 7.000], mean observation: 34.812 [0.000, 530.600], loss: 272.247589, mae: 27.365891, mean_q: -27.076982\n",
            " 1251024/10000000: episode: 6224, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -511.800, mean reward: -2.546 [-255.900, 82.000], mean action: 2.284 [0.000, 9.000], mean observation: 34.113 [0.001, 632.400], loss: 193.857025, mae: 27.391487, mean_q: -27.240309\n",
            " 1251225/10000000: episode: 6225, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -588.000, mean reward: -2.925 [-294.000, 66.400], mean action: 2.552 [0.000, 10.000], mean observation: 32.819 [0.003, 531.400], loss: 166.946091, mae: 27.621647, mean_q: -27.655998\n",
            " 1251426/10000000: episode: 6226, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -470.800, mean reward: -2.342 [-235.400, 95.200], mean action: 2.303 [0.000, 10.000], mean observation: 34.177 [0.001, 519.900], loss: 185.016525, mae: 27.998980, mean_q: -28.000401\n",
            " 1251627/10000000: episode: 6227, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -328.400, mean reward: -1.634 [-164.200, 39.200], mean action: 1.990 [0.000, 8.000], mean observation: 39.547 [0.001, 518.800], loss: 347.005493, mae: 28.388760, mean_q: -27.940678\n",
            " 1251828/10000000: episode: 6228, duration: 1.391s, episode steps: 201, steps per second: 144, episode reward: 200.200, mean reward: 0.996 [-8.000, 106.500], mean action: 1.891 [0.000, 8.000], mean observation: 37.896 [0.001, 627.200], loss: 277.950043, mae: 28.275694, mean_q: -27.854944\n",
            " 1252029/10000000: episode: 6229, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: 524.400, mean reward: 2.609 [-9.000, 371.400], mean action: 2.204 [0.000, 9.000], mean observation: 38.251 [0.001, 501.700], loss: 346.469360, mae: 28.351582, mean_q: -28.096146\n",
            " 1252230/10000000: episode: 6230, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: 205.800, mean reward: 1.024 [-10.000, 205.200], mean action: 2.020 [0.000, 10.000], mean observation: 32.696 [0.000, 604.500], loss: 311.274414, mae: 28.232262, mean_q: -27.801729\n",
            " 1252431/10000000: episode: 6231, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -313.000, mean reward: -1.557 [-156.500, 60.600], mean action: 1.935 [0.000, 10.000], mean observation: 32.782 [0.001, 437.600], loss: 278.194611, mae: 27.936436, mean_q: -27.294674\n",
            " 1252632/10000000: episode: 6232, duration: 1.395s, episode steps: 201, steps per second: 144, episode reward: -487.200, mean reward: -2.424 [-243.600, 53.400], mean action: 2.050 [0.000, 10.000], mean observation: 30.562 [0.001, 464.600], loss: 270.751160, mae: 27.785103, mean_q: -27.393614\n",
            " 1252833/10000000: episode: 6233, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -186.000, mean reward: -0.925 [-93.000, 100.800], mean action: 2.711 [0.000, 8.000], mean observation: 31.479 [0.001, 531.100], loss: 260.708710, mae: 27.977024, mean_q: -28.075462\n",
            " 1253034/10000000: episode: 6234, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: -388.400, mean reward: -1.932 [-194.200, 189.600], mean action: 3.239 [0.000, 9.000], mean observation: 36.344 [0.002, 439.400], loss: 205.046066, mae: 28.218611, mean_q: -28.745739\n",
            " 1253235/10000000: episode: 6235, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -429.000, mean reward: -2.134 [-214.500, 79.200], mean action: 2.975 [0.000, 9.000], mean observation: 33.814 [0.000, 784.200], loss: 164.986877, mae: 28.941771, mean_q: -29.436335\n",
            " 1253436/10000000: episode: 6236, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -171.400, mean reward: -0.853 [-85.700, 274.000], mean action: 2.881 [0.000, 7.000], mean observation: 38.198 [0.000, 722.600], loss: 402.069794, mae: 29.354393, mean_q: -29.612953\n",
            " 1253637/10000000: episode: 6237, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: -250.000, mean reward: -1.244 [-125.000, 201.900], mean action: 2.085 [0.000, 9.000], mean observation: 31.515 [0.001, 542.200], loss: 260.234589, mae: 29.716728, mean_q: -29.257591\n",
            " 1253838/10000000: episode: 6238, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -85.400, mean reward: -0.425 [-42.700, 175.100], mean action: 2.423 [0.000, 10.000], mean observation: 36.883 [0.001, 527.300], loss: 240.965988, mae: 29.221340, mean_q: -29.245880\n",
            " 1254039/10000000: episode: 6239, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -333.400, mean reward: -1.659 [-166.700, 102.600], mean action: 2.189 [0.000, 8.000], mean observation: 28.256 [0.000, 529.100], loss: 289.187195, mae: 28.961588, mean_q: -28.818501\n",
            " 1254240/10000000: episode: 6240, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -440.800, mean reward: -2.193 [-220.400, 43.200], mean action: 2.015 [0.000, 7.000], mean observation: 32.289 [0.000, 694.400], loss: 489.886902, mae: 28.750355, mean_q: -28.071362\n",
            " 1254441/10000000: episode: 6241, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -643.200, mean reward: -3.200 [-321.600, 11.400], mean action: 1.771 [0.000, 10.000], mean observation: 40.279 [0.001, 598.300], loss: 91.844215, mae: 28.001675, mean_q: -27.550575\n",
            " 1254642/10000000: episode: 6242, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -568.200, mean reward: -2.827 [-284.100, 25.500], mean action: 1.786 [0.000, 8.000], mean observation: 33.975 [0.000, 555.700], loss: 227.914429, mae: 28.106171, mean_q: -27.568899\n",
            " 1254843/10000000: episode: 6243, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -847.600, mean reward: -4.217 [-423.800, 34.000], mean action: 2.776 [0.000, 7.000], mean observation: 28.029 [0.003, 492.800], loss: 315.603363, mae: 27.980875, mean_q: -28.105453\n",
            " 1255044/10000000: episode: 6244, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -557.000, mean reward: -2.771 [-278.500, 76.500], mean action: 2.791 [0.000, 7.000], mean observation: 37.267 [0.000, 669.900], loss: 236.184326, mae: 28.358929, mean_q: -28.649681\n",
            " 1255245/10000000: episode: 6245, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: 415.400, mean reward: 2.067 [-9.000, 316.400], mean action: 2.647 [0.000, 9.000], mean observation: 37.205 [0.001, 662.900], loss: 202.775543, mae: 28.981451, mean_q: -29.425308\n",
            " 1255446/10000000: episode: 6246, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -306.400, mean reward: -1.524 [-153.200, 88.400], mean action: 2.522 [0.000, 10.000], mean observation: 35.605 [0.002, 482.600], loss: 493.217621, mae: 29.264536, mean_q: -29.505831\n",
            " 1255647/10000000: episode: 6247, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 88.600, mean reward: 0.441 [-10.000, 111.000], mean action: 2.572 [0.000, 10.000], mean observation: 38.715 [0.000, 654.700], loss: 139.572754, mae: 29.326361, mean_q: -29.372494\n",
            " 1255848/10000000: episode: 6248, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -464.200, mean reward: -2.309 [-232.100, 53.900], mean action: 2.473 [0.000, 8.000], mean observation: 34.355 [0.000, 706.400], loss: 220.066345, mae: 29.586098, mean_q: -29.498825\n",
            " 1256049/10000000: episode: 6249, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -718.800, mean reward: -3.576 [-359.400, 22.800], mean action: 2.254 [0.000, 10.000], mean observation: 30.920 [0.001, 480.700], loss: 260.310699, mae: 29.628088, mean_q: -29.551817\n",
            " 1256250/10000000: episode: 6250, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -76.400, mean reward: -0.380 [-38.200, 247.500], mean action: 2.358 [0.000, 8.000], mean observation: 35.188 [0.001, 422.500], loss: 221.254318, mae: 29.515232, mean_q: -29.610235\n",
            " 1256451/10000000: episode: 6251, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: 324.400, mean reward: 1.614 [-10.000, 490.800], mean action: 2.473 [0.000, 10.000], mean observation: 31.592 [0.001, 676.900], loss: 216.410538, mae: 29.483833, mean_q: -29.860743\n",
            " 1256652/10000000: episode: 6252, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -176.400, mean reward: -0.878 [-88.200, 79.200], mean action: 2.527 [0.000, 10.000], mean observation: 26.524 [0.007, 497.200], loss: 283.952576, mae: 29.644676, mean_q: -30.152071\n",
            " 1256853/10000000: episode: 6253, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -387.600, mean reward: -1.928 [-193.800, 119.400], mean action: 2.448 [0.000, 8.000], mean observation: 44.590 [0.000, 714.300], loss: 369.997131, mae: 29.558191, mean_q: -29.695017\n",
            " 1257054/10000000: episode: 6254, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -756.800, mean reward: -3.765 [-378.400, 44.800], mean action: 2.612 [0.000, 8.000], mean observation: 33.023 [0.000, 582.800], loss: 282.327576, mae: 29.944798, mean_q: -29.953850\n",
            " 1257255/10000000: episode: 6255, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 206.800, mean reward: 1.029 [-8.000, 198.600], mean action: 2.806 [0.000, 8.000], mean observation: 33.554 [0.000, 566.700], loss: 280.574615, mae: 29.883186, mean_q: -29.985085\n",
            " 1257456/10000000: episode: 6256, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -285.600, mean reward: -1.421 [-142.800, 107.100], mean action: 2.642 [0.000, 10.000], mean observation: 30.223 [0.001, 498.800], loss: 169.682800, mae: 30.011345, mean_q: -30.177637\n",
            " 1257657/10000000: episode: 6257, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -484.600, mean reward: -2.411 [-242.300, 54.400], mean action: 2.741 [0.000, 9.000], mean observation: 32.408 [0.002, 508.500], loss: 310.339050, mae: 30.033825, mean_q: -30.419397\n",
            " 1257858/10000000: episode: 6258, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -329.200, mean reward: -1.638 [-164.600, 229.200], mean action: 2.781 [0.000, 9.000], mean observation: 31.343 [0.000, 413.800], loss: 577.466553, mae: 30.394543, mean_q: -30.360142\n",
            " 1258059/10000000: episode: 6259, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -815.200, mean reward: -4.056 [-407.600, 53.600], mean action: 3.154 [0.000, 8.000], mean observation: 37.646 [0.000, 451.700], loss: 494.347290, mae: 29.848988, mean_q: -29.583799\n",
            " 1258260/10000000: episode: 6260, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -746.000, mean reward: -3.711 [-373.000, 52.400], mean action: 2.701 [0.000, 7.000], mean observation: 34.994 [0.001, 630.500], loss: 258.606201, mae: 29.687729, mean_q: -29.304174\n",
            " 1258461/10000000: episode: 6261, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -208.200, mean reward: -1.036 [-104.100, 132.100], mean action: 2.900 [0.000, 8.000], mean observation: 31.569 [0.001, 527.300], loss: 273.568726, mae: 29.558249, mean_q: -29.089142\n",
            " 1258662/10000000: episode: 6262, duration: 1.483s, episode steps: 201, steps per second: 136, episode reward: -449.000, mean reward: -2.234 [-224.500, 114.600], mean action: 2.547 [0.000, 7.000], mean observation: 34.342 [0.000, 562.200], loss: 221.508392, mae: 29.412298, mean_q: -28.935909\n",
            " 1258863/10000000: episode: 6263, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -167.600, mean reward: -0.834 [-83.800, 201.900], mean action: 2.294 [0.000, 8.000], mean observation: 33.522 [0.002, 637.200], loss: 210.707504, mae: 29.772532, mean_q: -29.259022\n",
            " 1259064/10000000: episode: 6264, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -438.800, mean reward: -2.183 [-219.400, 60.000], mean action: 2.383 [0.000, 9.000], mean observation: 25.951 [0.003, 436.400], loss: 304.097107, mae: 30.158617, mean_q: -29.800446\n",
            " 1259265/10000000: episode: 6265, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: 138.800, mean reward: 0.691 [-10.000, 79.200], mean action: 2.154 [0.000, 10.000], mean observation: 31.415 [0.001, 607.700], loss: 252.184540, mae: 30.150833, mean_q: -29.585468\n",
            " 1259466/10000000: episode: 6266, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -624.000, mean reward: -3.104 [-312.000, 33.400], mean action: 2.388 [0.000, 8.000], mean observation: 33.084 [0.001, 462.800], loss: 225.970078, mae: 30.098145, mean_q: -29.690163\n",
            " 1259667/10000000: episode: 6267, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -622.400, mean reward: -3.097 [-311.200, 28.000], mean action: 2.239 [0.000, 8.000], mean observation: 37.253 [0.003, 524.500], loss: 197.732697, mae: 29.871866, mean_q: -29.571814\n",
            " 1259868/10000000: episode: 6268, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -273.800, mean reward: -1.362 [-136.900, 52.200], mean action: 2.975 [0.000, 8.000], mean observation: 28.676 [0.000, 772.000], loss: 312.399139, mae: 29.804739, mean_q: -29.571688\n",
            " 1260069/10000000: episode: 6269, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -605.800, mean reward: -3.014 [-302.900, 94.500], mean action: 2.876 [0.000, 7.000], mean observation: 34.629 [0.000, 558.800], loss: 228.716614, mae: 29.739807, mean_q: -29.509825\n",
            " 1260270/10000000: episode: 6270, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -82.000, mean reward: -0.408 [-41.000, 99.000], mean action: 2.910 [0.000, 8.000], mean observation: 31.680 [0.001, 501.100], loss: 227.023666, mae: 29.475658, mean_q: -29.271833\n",
            " 1260471/10000000: episode: 6271, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: 59.000, mean reward: 0.294 [-8.000, 156.800], mean action: 2.721 [0.000, 8.000], mean observation: 34.070 [0.001, 645.600], loss: 408.295288, mae: 29.320822, mean_q: -28.949837\n",
            " 1260672/10000000: episode: 6272, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -422.200, mean reward: -2.100 [-211.100, 70.800], mean action: 2.761 [0.000, 8.000], mean observation: 31.627 [0.001, 516.000], loss: 258.436829, mae: 29.014313, mean_q: -28.760925\n",
            " 1260873/10000000: episode: 6273, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 63.200, mean reward: 0.314 [-9.000, 123.000], mean action: 2.881 [0.000, 9.000], mean observation: 36.291 [0.000, 555.400], loss: 237.256302, mae: 28.796715, mean_q: -28.990969\n",
            " 1261074/10000000: episode: 6274, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -647.000, mean reward: -3.219 [-323.500, 137.600], mean action: 3.279 [0.000, 8.000], mean observation: 32.754 [0.000, 553.100], loss: 261.839172, mae: 28.551422, mean_q: -28.910257\n",
            " 1261275/10000000: episode: 6275, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -144.800, mean reward: -0.720 [-72.400, 114.100], mean action: 2.975 [0.000, 8.000], mean observation: 37.097 [0.000, 668.100], loss: 342.584106, mae: 28.588156, mean_q: -28.991711\n",
            " 1261476/10000000: episode: 6276, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -586.600, mean reward: -2.918 [-293.300, 63.000], mean action: 2.597 [0.000, 9.000], mean observation: 29.601 [0.000, 483.800], loss: 409.630249, mae: 28.829699, mean_q: -28.812431\n",
            " 1261677/10000000: episode: 6277, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -62.600, mean reward: -0.311 [-31.300, 130.800], mean action: 2.274 [0.000, 8.000], mean observation: 28.838 [0.002, 513.300], loss: 232.931747, mae: 29.208515, mean_q: -29.134975\n",
            " 1261878/10000000: episode: 6278, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -738.200, mean reward: -3.673 [-369.100, 68.600], mean action: 2.697 [0.000, 9.000], mean observation: 36.765 [0.000, 606.600], loss: 184.604462, mae: 29.444407, mean_q: -29.822319\n",
            " 1262079/10000000: episode: 6279, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -834.600, mean reward: -4.152 [-417.300, 16.000], mean action: 2.542 [0.000, 8.000], mean observation: 37.584 [0.001, 440.800], loss: 521.587585, mae: 29.413231, mean_q: -29.347507\n",
            " 1262280/10000000: episode: 6280, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -591.200, mean reward: -2.941 [-295.600, 36.500], mean action: 2.706 [0.000, 8.000], mean observation: 32.570 [0.002, 578.200], loss: 360.032715, mae: 29.666567, mean_q: -29.550053\n",
            " 1262481/10000000: episode: 6281, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -885.400, mean reward: -4.405 [-442.700, 39.200], mean action: 2.791 [0.000, 9.000], mean observation: 32.077 [0.001, 478.600], loss: 259.595459, mae: 29.863771, mean_q: -30.073484\n",
            " 1262682/10000000: episode: 6282, duration: 1.511s, episode steps: 201, steps per second: 133, episode reward: -655.800, mean reward: -3.263 [-327.900, 68.400], mean action: 2.846 [0.000, 8.000], mean observation: 37.123 [0.000, 527.200], loss: 217.527344, mae: 30.154200, mean_q: -30.251165\n",
            " 1262883/10000000: episode: 6283, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: 14.800, mean reward: 0.074 [-8.000, 142.200], mean action: 2.766 [0.000, 8.000], mean observation: 32.447 [0.001, 675.300], loss: 306.845032, mae: 30.131161, mean_q: -30.300552\n",
            " 1263084/10000000: episode: 6284, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -750.000, mean reward: -3.731 [-375.000, 30.400], mean action: 2.587 [0.000, 8.000], mean observation: 30.083 [0.004, 432.400], loss: 191.363998, mae: 30.376820, mean_q: -30.421883\n",
            " 1263285/10000000: episode: 6285, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -292.800, mean reward: -1.457 [-146.400, 37.500], mean action: 2.542 [0.000, 8.000], mean observation: 31.331 [0.000, 527.500], loss: 181.050919, mae: 30.738947, mean_q: -30.831507\n",
            " 1263486/10000000: episode: 6286, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: 339.200, mean reward: 1.688 [-8.000, 289.800], mean action: 2.642 [0.000, 9.000], mean observation: 36.294 [0.000, 544.700], loss: 211.059784, mae: 30.546970, mean_q: -30.792110\n",
            " 1263687/10000000: episode: 6287, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -88.000, mean reward: -0.438 [-44.000, 74.900], mean action: 2.517 [0.000, 10.000], mean observation: 36.490 [0.002, 532.900], loss: 258.004364, mae: 31.021597, mean_q: -31.177235\n",
            " 1263888/10000000: episode: 6288, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -116.200, mean reward: -0.578 [-58.100, 157.800], mean action: 2.687 [0.000, 8.000], mean observation: 33.339 [0.001, 647.400], loss: 271.350708, mae: 31.068228, mean_q: -31.005760\n",
            " 1264089/10000000: episode: 6289, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -426.400, mean reward: -2.121 [-213.200, 81.600], mean action: 2.090 [0.000, 8.000], mean observation: 33.110 [0.001, 602.400], loss: 287.519043, mae: 31.332865, mean_q: -31.110519\n",
            " 1264290/10000000: episode: 6290, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -547.400, mean reward: -2.723 [-273.700, 39.200], mean action: 2.080 [0.000, 8.000], mean observation: 33.807 [0.000, 453.500], loss: 202.440704, mae: 31.482822, mean_q: -31.147470\n",
            " 1264491/10000000: episode: 6291, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: -262.800, mean reward: -1.307 [-131.400, 53.900], mean action: 2.642 [0.000, 9.000], mean observation: 28.447 [0.002, 364.300], loss: 214.876587, mae: 31.473982, mean_q: -31.072275\n",
            " 1264692/10000000: episode: 6292, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: 474.400, mean reward: 2.360 [-8.000, 237.200], mean action: 1.925 [0.000, 8.000], mean observation: 29.707 [0.000, 695.400], loss: 301.970428, mae: 31.432741, mean_q: -31.005106\n",
            " 1264893/10000000: episode: 6293, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -62.000, mean reward: -0.308 [-31.000, 117.500], mean action: 1.915 [0.000, 8.000], mean observation: 34.176 [0.000, 660.900], loss: 272.156708, mae: 31.543598, mean_q: -31.063318\n",
            " 1265094/10000000: episode: 6294, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: 327.200, mean reward: 1.628 [-8.000, 212.000], mean action: 2.104 [0.000, 8.000], mean observation: 34.908 [0.000, 525.900], loss: 196.943176, mae: 31.197363, mean_q: -30.573305\n",
            " 1265295/10000000: episode: 6295, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -288.800, mean reward: -1.437 [-144.400, 38.000], mean action: 1.905 [0.000, 7.000], mean observation: 34.411 [0.001, 446.900], loss: 159.068985, mae: 31.354073, mean_q: -30.874754\n",
            " 1265496/10000000: episode: 6296, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: 22.600, mean reward: 0.112 [-8.000, 199.500], mean action: 2.214 [0.000, 8.000], mean observation: 33.134 [0.000, 386.400], loss: 235.458145, mae: 31.421398, mean_q: -30.775068\n",
            " 1265697/10000000: episode: 6297, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -377.200, mean reward: -1.877 [-188.600, 94.400], mean action: 2.080 [0.000, 8.000], mean observation: 30.563 [0.001, 654.600], loss: 183.598938, mae: 31.624237, mean_q: -31.157459\n",
            " 1265898/10000000: episode: 6298, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -568.600, mean reward: -2.829 [-284.300, 20.000], mean action: 2.090 [0.000, 8.000], mean observation: 35.976 [0.000, 467.400], loss: 260.513519, mae: 31.540518, mean_q: -30.782785\n",
            " 1266099/10000000: episode: 6299, duration: 1.366s, episode steps: 201, steps per second: 147, episode reward: -226.400, mean reward: -1.126 [-113.200, 58.200], mean action: 1.741 [0.000, 8.000], mean observation: 35.070 [0.000, 697.600], loss: 664.361145, mae: 30.798098, mean_q: -29.692942\n",
            " 1266300/10000000: episode: 6300, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -150.400, mean reward: -0.748 [-75.200, 160.300], mean action: 1.970 [0.000, 10.000], mean observation: 31.452 [0.000, 499.800], loss: 194.491379, mae: 30.212257, mean_q: -29.338362\n",
            " 1266501/10000000: episode: 6301, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -381.000, mean reward: -1.896 [-190.500, 44.500], mean action: 1.985 [0.000, 8.000], mean observation: 31.515 [0.001, 443.500], loss: 241.431625, mae: 30.556810, mean_q: -29.867208\n",
            " 1266702/10000000: episode: 6302, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -36.600, mean reward: -0.182 [-18.300, 149.600], mean action: 2.313 [0.000, 8.000], mean observation: 33.756 [0.001, 470.800], loss: 156.062088, mae: 30.504601, mean_q: -30.132900\n",
            " 1266903/10000000: episode: 6303, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 202.400, mean reward: 1.007 [-8.000, 145.000], mean action: 2.219 [0.000, 8.000], mean observation: 30.619 [0.002, 447.400], loss: 173.622513, mae: 30.042255, mean_q: -29.826357\n",
            " 1267104/10000000: episode: 6304, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: 47.200, mean reward: 0.235 [-8.000, 218.400], mean action: 2.512 [0.000, 8.000], mean observation: 32.071 [0.002, 515.900], loss: 175.625519, mae: 30.240501, mean_q: -30.103296\n",
            " 1267305/10000000: episode: 6305, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -105.000, mean reward: -0.522 [-52.500, 67.200], mean action: 2.323 [0.000, 8.000], mean observation: 34.731 [0.002, 467.700], loss: 179.512161, mae: 30.663353, mean_q: -30.381380\n",
            " 1267506/10000000: episode: 6306, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -237.000, mean reward: -1.179 [-118.500, 50.500], mean action: 2.667 [0.000, 10.000], mean observation: 28.001 [0.002, 434.600], loss: 168.513840, mae: 30.644262, mean_q: -30.507774\n",
            " 1267707/10000000: episode: 6307, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -194.800, mean reward: -0.969 [-97.400, 153.000], mean action: 2.547 [0.000, 8.000], mean observation: 32.401 [0.000, 638.600], loss: 164.876434, mae: 30.892223, mean_q: -30.826464\n",
            " 1267908/10000000: episode: 6308, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -642.000, mean reward: -3.194 [-321.000, 51.600], mean action: 2.642 [0.000, 8.000], mean observation: 33.934 [0.000, 407.100], loss: 185.164932, mae: 31.159952, mean_q: -31.273651\n",
            " 1268109/10000000: episode: 6309, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -11.000, mean reward: -0.055 [-9.000, 72.500], mean action: 2.597 [0.000, 9.000], mean observation: 36.049 [0.000, 530.500], loss: 158.908707, mae: 31.359287, mean_q: -31.603161\n",
            " 1268310/10000000: episode: 6310, duration: 1.553s, episode steps: 201, steps per second: 129, episode reward: -391.600, mean reward: -1.948 [-195.800, 51.000], mean action: 2.562 [0.000, 8.000], mean observation: 33.887 [0.000, 796.200], loss: 223.795715, mae: 31.532671, mean_q: -31.492790\n",
            " 1268511/10000000: episode: 6311, duration: 1.624s, episode steps: 201, steps per second: 124, episode reward: -757.000, mean reward: -3.766 [-378.500, 16.800], mean action: 2.313 [0.000, 8.000], mean observation: 37.359 [0.000, 588.400], loss: 277.873657, mae: 31.460407, mean_q: -31.289909\n",
            " 1268712/10000000: episode: 6312, duration: 1.601s, episode steps: 201, steps per second: 126, episode reward: -613.600, mean reward: -3.053 [-306.800, 84.100], mean action: 2.572 [0.000, 10.000], mean observation: 30.902 [0.001, 466.300], loss: 158.840454, mae: 31.513235, mean_q: -31.740545\n",
            " 1268913/10000000: episode: 6313, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -604.800, mean reward: -3.009 [-302.400, 58.400], mean action: 2.851 [0.000, 10.000], mean observation: 32.961 [0.003, 438.400], loss: 268.585571, mae: 31.977705, mean_q: -31.964323\n",
            " 1269114/10000000: episode: 6314, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -191.800, mean reward: -0.954 [-95.900, 55.500], mean action: 2.313 [0.000, 8.000], mean observation: 28.019 [0.002, 545.200], loss: 214.722595, mae: 31.728933, mean_q: -31.537544\n",
            " 1269315/10000000: episode: 6315, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -524.600, mean reward: -2.610 [-262.300, 42.700], mean action: 2.393 [0.000, 8.000], mean observation: 35.068 [0.001, 491.000], loss: 170.086136, mae: 31.801666, mean_q: -31.841856\n",
            " 1269516/10000000: episode: 6316, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: 323.600, mean reward: 1.610 [-8.000, 201.900], mean action: 2.701 [0.000, 10.000], mean observation: 30.093 [0.000, 452.700], loss: 397.897461, mae: 31.423897, mean_q: -31.231762\n",
            " 1269717/10000000: episode: 6317, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -801.800, mean reward: -3.989 [-400.900, 36.800], mean action: 2.522 [0.000, 10.000], mean observation: 35.306 [0.002, 481.400], loss: 132.124908, mae: 31.157572, mean_q: -31.047800\n",
            " 1269918/10000000: episode: 6318, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -566.000, mean reward: -2.816 [-283.000, 47.200], mean action: 2.109 [0.000, 8.000], mean observation: 28.656 [0.000, 746.900], loss: 500.870575, mae: 30.891832, mean_q: -30.554708\n",
            " 1270119/10000000: episode: 6319, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -560.200, mean reward: -2.787 [-280.100, 60.000], mean action: 2.383 [0.000, 10.000], mean observation: 33.473 [0.000, 609.400], loss: 244.842560, mae: 30.715103, mean_q: -30.167501\n",
            " 1270320/10000000: episode: 6320, duration: 1.557s, episode steps: 201, steps per second: 129, episode reward: -360.600, mean reward: -1.794 [-180.300, 57.200], mean action: 1.821 [0.000, 8.000], mean observation: 30.660 [0.000, 702.100], loss: 160.577744, mae: 30.444298, mean_q: -29.819342\n",
            " 1270521/10000000: episode: 6321, duration: 1.518s, episode steps: 201, steps per second: 132, episode reward: -24.000, mean reward: -0.119 [-12.000, 61.200], mean action: 1.771 [0.000, 10.000], mean observation: 34.492 [0.000, 472.400], loss: 214.478714, mae: 30.219183, mean_q: -29.842033\n",
            " 1270722/10000000: episode: 6322, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -248.200, mean reward: -1.235 [-124.100, 127.800], mean action: 2.040 [0.000, 8.000], mean observation: 38.902 [0.000, 653.600], loss: 236.232910, mae: 30.201424, mean_q: -29.696993\n",
            " 1270923/10000000: episode: 6323, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -173.600, mean reward: -0.864 [-86.800, 75.600], mean action: 2.124 [0.000, 7.000], mean observation: 27.196 [0.000, 509.800], loss: 199.072723, mae: 29.610775, mean_q: -29.175953\n",
            " 1271124/10000000: episode: 6324, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -364.000, mean reward: -1.811 [-182.000, 54.000], mean action: 2.318 [0.000, 8.000], mean observation: 40.225 [0.002, 630.900], loss: 182.178604, mae: 29.614660, mean_q: -29.564819\n",
            " 1271325/10000000: episode: 6325, duration: 1.474s, episode steps: 201, steps per second: 136, episode reward: -320.600, mean reward: -1.595 [-160.300, 63.800], mean action: 2.642 [0.000, 8.000], mean observation: 32.832 [0.001, 571.000], loss: 169.379532, mae: 29.586685, mean_q: -29.386112\n",
            " 1271526/10000000: episode: 6326, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -650.000, mean reward: -3.234 [-325.000, 38.000], mean action: 2.398 [0.000, 8.000], mean observation: 36.033 [0.001, 591.000], loss: 187.273743, mae: 29.530565, mean_q: -29.274050\n",
            " 1271727/10000000: episode: 6327, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -161.200, mean reward: -0.802 [-80.600, 75.200], mean action: 2.527 [0.000, 10.000], mean observation: 30.984 [0.001, 595.600], loss: 476.777588, mae: 29.698471, mean_q: -29.659153\n",
            " 1271928/10000000: episode: 6328, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -413.600, mean reward: -2.058 [-206.800, 108.000], mean action: 2.672 [0.000, 10.000], mean observation: 34.192 [0.000, 530.000], loss: 258.864807, mae: 29.679556, mean_q: -29.701351\n",
            " 1272129/10000000: episode: 6329, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: 987.800, mean reward: 4.914 [-8.000, 495.200], mean action: 2.507 [0.000, 8.000], mean observation: 30.932 [0.001, 453.300], loss: 280.682129, mae: 29.484501, mean_q: -29.095055\n",
            " 1272330/10000000: episode: 6330, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: 373.000, mean reward: 1.856 [-8.000, 186.500], mean action: 1.861 [0.000, 8.000], mean observation: 31.648 [0.006, 504.200], loss: 530.408508, mae: 29.148272, mean_q: -28.476076\n",
            " 1272531/10000000: episode: 6331, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -123.400, mean reward: -0.614 [-61.700, 157.600], mean action: 2.164 [0.000, 8.000], mean observation: 30.960 [0.001, 591.100], loss: 184.949005, mae: 28.476107, mean_q: -27.890787\n",
            " 1272732/10000000: episode: 6332, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -149.000, mean reward: -0.741 [-74.500, 117.400], mean action: 1.866 [0.000, 8.000], mean observation: 34.207 [0.001, 578.100], loss: 184.597137, mae: 28.050175, mean_q: -27.430250\n",
            " 1272933/10000000: episode: 6333, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -487.400, mean reward: -2.425 [-243.700, 131.600], mean action: 2.418 [0.000, 10.000], mean observation: 34.911 [0.001, 598.700], loss: 164.581589, mae: 27.995052, mean_q: -27.687447\n",
            " 1273134/10000000: episode: 6334, duration: 1.380s, episode steps: 201, steps per second: 146, episode reward: 313.600, mean reward: 1.560 [-8.000, 172.500], mean action: 2.274 [0.000, 8.000], mean observation: 27.573 [0.003, 305.100], loss: 138.017395, mae: 28.107908, mean_q: -27.885963\n",
            " 1273335/10000000: episode: 6335, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -310.200, mean reward: -1.543 [-155.100, 70.700], mean action: 2.637 [0.000, 10.000], mean observation: 33.028 [0.000, 540.000], loss: 176.553055, mae: 28.435581, mean_q: -28.446564\n",
            " 1273536/10000000: episode: 6336, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 37.400, mean reward: 0.186 [-8.000, 105.000], mean action: 3.100 [0.000, 8.000], mean observation: 29.766 [0.000, 663.800], loss: 186.600052, mae: 28.917252, mean_q: -29.077896\n",
            " 1273737/10000000: episode: 6337, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -779.600, mean reward: -3.879 [-389.800, 30.400], mean action: 2.771 [0.000, 10.000], mean observation: 32.862 [0.000, 477.000], loss: 162.696609, mae: 29.299122, mean_q: -29.626633\n",
            " 1273938/10000000: episode: 6338, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: -403.000, mean reward: -2.005 [-201.500, 58.000], mean action: 2.592 [0.000, 8.000], mean observation: 33.288 [0.000, 639.500], loss: 610.201172, mae: 29.470791, mean_q: -29.706045\n",
            " 1274139/10000000: episode: 6339, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -215.000, mean reward: -1.070 [-107.500, 80.000], mean action: 2.577 [0.000, 10.000], mean observation: 36.190 [0.001, 619.200], loss: 396.151581, mae: 29.963654, mean_q: -30.146358\n",
            " 1274340/10000000: episode: 6340, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -838.000, mean reward: -4.169 [-419.000, 43.000], mean action: 3.244 [0.000, 10.000], mean observation: 30.178 [0.001, 651.100], loss: 249.966568, mae: 29.883688, mean_q: -30.000248\n",
            " 1274541/10000000: episode: 6341, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -2.000, mean reward: -0.010 [-10.000, 193.500], mean action: 2.960 [0.000, 10.000], mean observation: 38.004 [0.000, 818.100], loss: 632.824219, mae: 29.803682, mean_q: -29.814856\n",
            " 1274742/10000000: episode: 6342, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -152.200, mean reward: -0.757 [-76.100, 100.800], mean action: 2.672 [0.000, 10.000], mean observation: 32.815 [0.002, 429.000], loss: 349.571442, mae: 29.113789, mean_q: -28.998734\n",
            " 1274943/10000000: episode: 6343, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -785.800, mean reward: -3.909 [-392.900, 45.300], mean action: 2.552 [0.000, 8.000], mean observation: 33.221 [0.001, 565.900], loss: 346.346436, mae: 28.906401, mean_q: -28.646864\n",
            " 1275144/10000000: episode: 6344, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -570.200, mean reward: -2.837 [-285.100, 36.000], mean action: 2.229 [0.000, 10.000], mean observation: 34.939 [0.002, 446.700], loss: 262.022766, mae: 28.494503, mean_q: -28.281359\n",
            " 1275345/10000000: episode: 6345, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -546.800, mean reward: -2.720 [-273.400, 102.500], mean action: 2.801 [0.000, 8.000], mean observation: 32.325 [0.000, 720.900], loss: 139.601913, mae: 28.497591, mean_q: -28.378225\n",
            " 1275546/10000000: episode: 6346, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: 508.800, mean reward: 2.531 [-10.000, 257.000], mean action: 2.682 [0.000, 10.000], mean observation: 30.603 [0.002, 624.500], loss: 329.328827, mae: 28.535982, mean_q: -28.366289\n",
            " 1275747/10000000: episode: 6347, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 63.800, mean reward: 0.317 [-8.000, 126.000], mean action: 2.144 [0.000, 8.000], mean observation: 34.488 [0.003, 497.700], loss: 220.384140, mae: 28.503948, mean_q: -28.249378\n",
            " 1275948/10000000: episode: 6348, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -459.000, mean reward: -2.284 [-229.500, 94.400], mean action: 2.582 [0.000, 9.000], mean observation: 25.315 [0.001, 462.800], loss: 166.645386, mae: 28.509357, mean_q: -28.172691\n",
            " 1276149/10000000: episode: 6349, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: 157.400, mean reward: 0.783 [-8.000, 115.300], mean action: 2.542 [0.000, 8.000], mean observation: 37.980 [0.000, 476.600], loss: 198.983017, mae: 28.361115, mean_q: -27.954752\n",
            " 1276350/10000000: episode: 6350, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -385.800, mean reward: -1.919 [-192.900, 83.300], mean action: 2.632 [0.000, 8.000], mean observation: 37.848 [0.000, 642.700], loss: 141.861801, mae: 28.096193, mean_q: -27.727598\n",
            " 1276551/10000000: episode: 6351, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -429.000, mean reward: -2.134 [-214.500, 64.000], mean action: 2.413 [0.000, 8.000], mean observation: 34.926 [0.000, 907.100], loss: 251.385574, mae: 27.751083, mean_q: -27.293173\n",
            " 1276752/10000000: episode: 6352, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -802.200, mean reward: -3.991 [-401.100, 38.400], mean action: 2.925 [0.000, 9.000], mean observation: 32.185 [0.004, 399.700], loss: 267.161530, mae: 27.818531, mean_q: -27.845572\n",
            " 1276953/10000000: episode: 6353, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -638.800, mean reward: -3.178 [-319.400, 49.600], mean action: 2.811 [0.000, 9.000], mean observation: 33.583 [0.001, 522.800], loss: 146.788071, mae: 28.080976, mean_q: -28.148474\n",
            " 1277154/10000000: episode: 6354, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -63.600, mean reward: -0.316 [-31.800, 360.500], mean action: 3.070 [0.000, 8.000], mean observation: 31.714 [0.000, 476.000], loss: 249.719742, mae: 28.494131, mean_q: -28.634552\n",
            " 1277355/10000000: episode: 6355, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -165.400, mean reward: -0.823 [-82.700, 174.900], mean action: 2.841 [0.000, 10.000], mean observation: 29.349 [0.001, 623.900], loss: 247.202316, mae: 28.473730, mean_q: -28.294859\n",
            " 1277556/10000000: episode: 6356, duration: 1.378s, episode steps: 201, steps per second: 146, episode reward: -737.000, mean reward: -3.667 [-368.500, 39.900], mean action: 2.761 [0.000, 8.000], mean observation: 31.342 [0.002, 542.000], loss: 177.732620, mae: 28.217506, mean_q: -27.982544\n",
            " 1277757/10000000: episode: 6357, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: 904.800, mean reward: 4.501 [-9.000, 452.400], mean action: 2.896 [0.000, 9.000], mean observation: 38.913 [0.000, 515.500], loss: 206.067139, mae: 28.164625, mean_q: -27.985657\n",
            " 1277958/10000000: episode: 6358, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -369.800, mean reward: -1.840 [-184.900, 87.600], mean action: 3.169 [0.000, 8.000], mean observation: 27.571 [0.000, 781.200], loss: 118.202232, mae: 27.769604, mean_q: -27.553923\n",
            " 1278159/10000000: episode: 6359, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -432.600, mean reward: -2.152 [-216.300, 63.700], mean action: 2.816 [0.000, 10.000], mean observation: 32.500 [0.001, 492.700], loss: 365.884888, mae: 27.789572, mean_q: -27.379118\n",
            " 1278360/10000000: episode: 6360, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -43.800, mean reward: -0.218 [-21.900, 160.500], mean action: 2.279 [0.000, 8.000], mean observation: 35.135 [0.002, 467.200], loss: 208.829834, mae: 27.617682, mean_q: -27.245962\n",
            " 1278561/10000000: episode: 6361, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: 353.200, mean reward: 1.757 [-8.000, 337.500], mean action: 3.035 [0.000, 8.000], mean observation: 33.239 [0.000, 511.400], loss: 179.689072, mae: 27.749111, mean_q: -27.590811\n",
            " 1278762/10000000: episode: 6362, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -371.400, mean reward: -1.848 [-185.700, 104.300], mean action: 2.826 [0.000, 8.000], mean observation: 34.443 [0.000, 764.600], loss: 216.856537, mae: 27.706705, mean_q: -27.649555\n",
            " 1278963/10000000: episode: 6363, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -311.400, mean reward: -1.549 [-155.700, 77.600], mean action: 2.786 [0.000, 9.000], mean observation: 28.639 [0.004, 498.200], loss: 182.522583, mae: 27.618687, mean_q: -27.405321\n",
            " 1279164/10000000: episode: 6364, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -279.400, mean reward: -1.390 [-139.700, 111.300], mean action: 2.920 [0.000, 10.000], mean observation: 32.086 [0.000, 598.900], loss: 168.831741, mae: 27.696970, mean_q: -27.670427\n",
            " 1279365/10000000: episode: 6365, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -643.000, mean reward: -3.199 [-321.500, 63.500], mean action: 2.532 [0.000, 8.000], mean observation: 34.729 [0.001, 508.700], loss: 152.256454, mae: 27.717278, mean_q: -27.642221\n",
            " 1279566/10000000: episode: 6366, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -563.000, mean reward: -2.801 [-281.500, 57.200], mean action: 2.299 [0.000, 8.000], mean observation: 37.405 [0.000, 792.800], loss: 367.713898, mae: 27.715933, mean_q: -27.472816\n",
            " 1279767/10000000: episode: 6367, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -189.200, mean reward: -0.941 [-94.600, 94.500], mean action: 2.075 [0.000, 8.000], mean observation: 31.972 [0.002, 455.000], loss: 229.978531, mae: 27.976587, mean_q: -27.690800\n",
            " 1279968/10000000: episode: 6368, duration: 1.389s, episode steps: 201, steps per second: 145, episode reward: 283.800, mean reward: 1.412 [-8.000, 219.000], mean action: 2.716 [0.000, 9.000], mean observation: 38.025 [0.000, 746.700], loss: 426.598969, mae: 27.905577, mean_q: -27.713755\n",
            " 1280169/10000000: episode: 6369, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: 628.800, mean reward: 3.128 [-10.000, 597.600], mean action: 3.403 [0.000, 10.000], mean observation: 33.934 [0.001, 541.500], loss: 215.805603, mae: 27.563208, mean_q: -27.522493\n",
            " 1280370/10000000: episode: 6370, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -630.600, mean reward: -3.137 [-315.300, 78.200], mean action: 3.269 [0.000, 8.000], mean observation: 31.370 [0.000, 516.300], loss: 253.655975, mae: 27.769215, mean_q: -27.877550\n",
            " 1280571/10000000: episode: 6371, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -488.600, mean reward: -2.431 [-244.300, 64.000], mean action: 3.393 [0.000, 10.000], mean observation: 31.591 [0.001, 519.300], loss: 195.963272, mae: 27.906445, mean_q: -28.117266\n",
            " 1280772/10000000: episode: 6372, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -72.600, mean reward: -0.361 [-36.300, 225.800], mean action: 2.333 [0.000, 8.000], mean observation: 36.134 [0.003, 566.800], loss: 519.866150, mae: 28.032274, mean_q: -28.151192\n",
            " 1280973/10000000: episode: 6373, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -396.000, mean reward: -1.970 [-198.000, 41.000], mean action: 2.458 [0.000, 10.000], mean observation: 32.046 [0.000, 579.300], loss: 145.587097, mae: 28.160339, mean_q: -28.140619\n",
            " 1281174/10000000: episode: 6374, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -610.200, mean reward: -3.036 [-305.100, 73.200], mean action: 2.438 [0.000, 10.000], mean observation: 28.980 [0.001, 576.200], loss: 334.743011, mae: 27.576376, mean_q: -27.468216\n",
            " 1281375/10000000: episode: 6375, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -487.000, mean reward: -2.423 [-243.500, 84.600], mean action: 2.786 [0.000, 8.000], mean observation: 33.410 [0.001, 550.200], loss: 637.823914, mae: 27.314598, mean_q: -27.113665\n",
            " 1281576/10000000: episode: 6376, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -293.600, mean reward: -1.461 [-146.800, 158.400], mean action: 2.861 [0.000, 10.000], mean observation: 30.306 [0.001, 493.400], loss: 411.128662, mae: 27.276960, mean_q: -27.139290\n",
            " 1281777/10000000: episode: 6377, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -595.800, mean reward: -2.964 [-297.900, 67.200], mean action: 2.826 [0.000, 10.000], mean observation: 33.931 [0.001, 494.100], loss: 173.069839, mae: 27.308014, mean_q: -27.068262\n",
            " 1281978/10000000: episode: 6378, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -479.200, mean reward: -2.384 [-239.600, 61.000], mean action: 2.318 [0.000, 10.000], mean observation: 39.137 [0.000, 525.700], loss: 192.608948, mae: 27.551830, mean_q: -27.380310\n",
            " 1282179/10000000: episode: 6379, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -77.600, mean reward: -0.386 [-38.800, 233.000], mean action: 2.662 [0.000, 8.000], mean observation: 36.737 [0.000, 803.400], loss: 464.744568, mae: 27.506844, mean_q: -27.405735\n",
            " 1282380/10000000: episode: 6380, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -553.800, mean reward: -2.755 [-276.900, 94.500], mean action: 3.134 [0.000, 9.000], mean observation: 33.494 [0.000, 597.200], loss: 462.439117, mae: 27.613218, mean_q: -27.625751\n",
            " 1282581/10000000: episode: 6381, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -784.600, mean reward: -3.903 [-392.300, 38.400], mean action: 3.045 [0.000, 10.000], mean observation: 31.159 [0.002, 525.700], loss: 203.220963, mae: 27.471865, mean_q: -27.445759\n",
            " 1282782/10000000: episode: 6382, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -410.200, mean reward: -2.041 [-205.100, 82.400], mean action: 2.846 [0.000, 10.000], mean observation: 35.464 [0.001, 545.700], loss: 270.815857, mae: 27.567532, mean_q: -27.452147\n",
            " 1282983/10000000: episode: 6383, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -845.800, mean reward: -4.208 [-422.900, 30.400], mean action: 2.647 [0.000, 8.000], mean observation: 32.259 [0.000, 622.400], loss: 172.446365, mae: 27.692106, mean_q: -27.868839\n",
            " 1283184/10000000: episode: 6384, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -681.000, mean reward: -3.388 [-340.500, 35.400], mean action: 2.955 [0.000, 9.000], mean observation: 39.063 [0.000, 592.700], loss: 233.299805, mae: 27.928158, mean_q: -28.153072\n",
            " 1283385/10000000: episode: 6385, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -731.000, mean reward: -3.637 [-365.500, 32.200], mean action: 2.925 [0.000, 10.000], mean observation: 34.698 [0.000, 669.400], loss: 222.658173, mae: 28.216084, mean_q: -28.715481\n",
            " 1283586/10000000: episode: 6386, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -272.200, mean reward: -1.354 [-136.100, 102.200], mean action: 3.040 [0.000, 10.000], mean observation: 33.183 [0.000, 556.200], loss: 193.321564, mae: 28.598427, mean_q: -29.204483\n",
            " 1283787/10000000: episode: 6387, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -702.200, mean reward: -3.494 [-351.100, 66.400], mean action: 3.035 [0.000, 9.000], mean observation: 35.949 [0.002, 511.500], loss: 308.223999, mae: 28.887400, mean_q: -29.407295\n",
            " 1283988/10000000: episode: 6388, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -215.600, mean reward: -1.073 [-107.800, 156.600], mean action: 2.378 [0.000, 9.000], mean observation: 41.345 [0.000, 663.000], loss: 551.478882, mae: 28.771130, mean_q: -28.756870\n",
            " 1284189/10000000: episode: 6389, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -323.400, mean reward: -1.609 [-161.700, 81.000], mean action: 2.677 [0.000, 8.000], mean observation: 31.714 [0.000, 637.900], loss: 122.963638, mae: 28.917099, mean_q: -29.272728\n",
            " 1284390/10000000: episode: 6390, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -61.400, mean reward: -0.305 [-30.700, 152.800], mean action: 2.731 [0.000, 10.000], mean observation: 33.358 [0.001, 459.000], loss: 254.567780, mae: 29.032894, mean_q: -29.614229\n",
            " 1284591/10000000: episode: 6391, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -750.000, mean reward: -3.731 [-375.000, 80.800], mean action: 3.164 [0.000, 10.000], mean observation: 33.202 [0.001, 581.100], loss: 283.548309, mae: 29.661274, mean_q: -30.530933\n",
            " 1284792/10000000: episode: 6392, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -535.200, mean reward: -2.663 [-267.600, 71.600], mean action: 2.701 [0.000, 9.000], mean observation: 32.333 [0.001, 622.800], loss: 385.968842, mae: 29.881506, mean_q: -30.330719\n",
            " 1284993/10000000: episode: 6393, duration: 1.506s, episode steps: 201, steps per second: 133, episode reward: -725.600, mean reward: -3.610 [-362.800, 70.000], mean action: 2.930 [0.000, 10.000], mean observation: 33.174 [0.000, 570.400], loss: 390.912964, mae: 30.094910, mean_q: -30.709845\n",
            " 1285194/10000000: episode: 6394, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -658.400, mean reward: -3.276 [-329.200, 139.200], mean action: 3.154 [0.000, 9.000], mean observation: 35.707 [0.001, 446.000], loss: 429.111542, mae: 30.068619, mean_q: -31.017015\n",
            " 1285395/10000000: episode: 6395, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -379.600, mean reward: -1.889 [-189.800, 288.400], mean action: 3.517 [0.000, 9.000], mean observation: 33.182 [0.002, 498.600], loss: 233.307373, mae: 29.945232, mean_q: -30.921034\n",
            " 1285596/10000000: episode: 6396, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: 25.600, mean reward: 0.127 [-10.000, 462.000], mean action: 3.687 [0.000, 10.000], mean observation: 29.238 [0.000, 749.600], loss: 165.359894, mae: 29.641531, mean_q: -30.499670\n",
            " 1285797/10000000: episode: 6397, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -894.800, mean reward: -4.452 [-447.400, 65.000], mean action: 3.279 [0.000, 10.000], mean observation: 35.022 [0.001, 587.600], loss: 215.903809, mae: 30.170511, mean_q: -31.082163\n",
            " 1285998/10000000: episode: 6398, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 651.800, mean reward: 3.243 [-9.000, 479.500], mean action: 3.184 [0.000, 10.000], mean observation: 33.279 [0.001, 516.600], loss: 457.749756, mae: 30.513487, mean_q: -31.368225\n",
            " 1286199/10000000: episode: 6399, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -491.400, mean reward: -2.445 [-245.700, 56.000], mean action: 2.791 [0.000, 10.000], mean observation: 28.395 [0.003, 415.200], loss: 206.616104, mae: 30.461525, mean_q: -31.075541\n",
            " 1286400/10000000: episode: 6400, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -536.600, mean reward: -2.670 [-268.300, 84.400], mean action: 3.000 [0.000, 10.000], mean observation: 34.995 [0.000, 509.100], loss: 165.102859, mae: 30.346779, mean_q: -30.935549\n",
            " 1286601/10000000: episode: 6401, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -487.200, mean reward: -2.424 [-243.600, 77.400], mean action: 2.547 [0.000, 9.000], mean observation: 33.743 [0.001, 590.900], loss: 189.319901, mae: 30.505125, mean_q: -31.046938\n",
            " 1286802/10000000: episode: 6402, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -658.200, mean reward: -3.275 [-329.100, 89.100], mean action: 2.925 [0.000, 9.000], mean observation: 26.199 [0.000, 446.200], loss: 352.421783, mae: 30.761135, mean_q: -31.381598\n",
            " 1287003/10000000: episode: 6403, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -149.400, mean reward: -0.743 [-74.700, 76.000], mean action: 2.348 [0.000, 9.000], mean observation: 32.820 [0.001, 504.300], loss: 230.973572, mae: 31.047352, mean_q: -31.610638\n",
            " 1287204/10000000: episode: 6404, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -192.200, mean reward: -0.956 [-96.100, 45.000], mean action: 2.254 [0.000, 9.000], mean observation: 31.065 [0.003, 510.800], loss: 199.148773, mae: 31.055557, mean_q: -31.672274\n",
            " 1287405/10000000: episode: 6405, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -7.000, mean reward: -0.035 [-10.000, 88.200], mean action: 2.423 [0.000, 10.000], mean observation: 33.746 [0.001, 487.500], loss: 163.727158, mae: 31.364225, mean_q: -32.029663\n",
            " 1287606/10000000: episode: 6406, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 194.400, mean reward: 0.967 [-10.000, 162.900], mean action: 2.244 [0.000, 10.000], mean observation: 35.155 [0.000, 637.000], loss: 219.737488, mae: 31.423716, mean_q: -31.962177\n",
            " 1287807/10000000: episode: 6407, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -544.800, mean reward: -2.710 [-272.400, 49.600], mean action: 2.682 [0.000, 9.000], mean observation: 29.974 [0.000, 640.400], loss: 245.336731, mae: 31.608942, mean_q: -32.139851\n",
            " 1288008/10000000: episode: 6408, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -164.800, mean reward: -0.820 [-82.400, 126.800], mean action: 2.129 [0.000, 9.000], mean observation: 30.277 [0.001, 413.700], loss: 177.302155, mae: 31.684244, mean_q: -32.181946\n",
            " 1288209/10000000: episode: 6409, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -438.400, mean reward: -2.181 [-219.200, 50.400], mean action: 2.408 [0.000, 9.000], mean observation: 32.871 [0.001, 512.800], loss: 116.057388, mae: 31.702328, mean_q: -32.319740\n",
            " 1288410/10000000: episode: 6410, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: 330.600, mean reward: 1.645 [-10.000, 165.300], mean action: 2.338 [0.000, 10.000], mean observation: 30.024 [0.000, 558.800], loss: 218.350983, mae: 31.972424, mean_q: -32.529709\n",
            " 1288611/10000000: episode: 6411, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: 107.800, mean reward: 0.536 [-9.000, 169.200], mean action: 2.413 [0.000, 9.000], mean observation: 39.052 [0.001, 539.800], loss: 170.049942, mae: 32.027607, mean_q: -32.767471\n",
            " 1288812/10000000: episode: 6412, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -528.200, mean reward: -2.628 [-264.100, 123.300], mean action: 2.831 [0.000, 9.000], mean observation: 34.649 [0.000, 584.600], loss: 218.696396, mae: 32.235729, mean_q: -33.195629\n",
            " 1289013/10000000: episode: 6413, duration: 1.520s, episode steps: 201, steps per second: 132, episode reward: -406.400, mean reward: -2.022 [-203.200, 93.600], mean action: 2.811 [0.000, 10.000], mean observation: 37.945 [0.002, 510.200], loss: 213.105804, mae: 32.593697, mean_q: -33.537399\n",
            " 1289214/10000000: episode: 6414, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: -655.800, mean reward: -3.263 [-327.900, 53.200], mean action: 2.826 [0.000, 10.000], mean observation: 28.937 [0.001, 486.800], loss: 206.782455, mae: 32.748001, mean_q: -33.915096\n",
            " 1289415/10000000: episode: 6415, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -174.400, mean reward: -0.868 [-87.200, 106.300], mean action: 2.552 [0.000, 9.000], mean observation: 31.829 [0.000, 708.200], loss: 269.065643, mae: 32.852108, mean_q: -33.855675\n",
            " 1289616/10000000: episode: 6416, duration: 1.694s, episode steps: 201, steps per second: 119, episode reward: -619.600, mean reward: -3.083 [-309.800, 105.000], mean action: 2.796 [0.000, 9.000], mean observation: 39.113 [0.002, 500.200], loss: 222.149170, mae: 33.080830, mean_q: -34.151302\n",
            " 1289817/10000000: episode: 6417, duration: 1.677s, episode steps: 201, steps per second: 120, episode reward: -403.800, mean reward: -2.009 [-201.900, 91.800], mean action: 2.896 [0.000, 10.000], mean observation: 35.339 [0.000, 440.500], loss: 233.989227, mae: 33.215805, mean_q: -34.173046\n",
            " 1290018/10000000: episode: 6418, duration: 1.625s, episode steps: 201, steps per second: 124, episode reward: -72.400, mean reward: -0.360 [-36.200, 130.000], mean action: 2.816 [0.000, 10.000], mean observation: 35.031 [0.000, 566.800], loss: 203.615402, mae: 33.154068, mean_q: -34.136749\n",
            " 1290219/10000000: episode: 6419, duration: 1.623s, episode steps: 201, steps per second: 124, episode reward: -429.400, mean reward: -2.136 [-214.700, 109.800], mean action: 2.716 [0.000, 9.000], mean observation: 30.382 [0.001, 571.900], loss: 198.318375, mae: 33.723675, mean_q: -34.852219\n",
            " 1290420/10000000: episode: 6420, duration: 1.614s, episode steps: 201, steps per second: 125, episode reward: -270.000, mean reward: -1.343 [-135.000, 164.700], mean action: 2.876 [0.000, 10.000], mean observation: 30.846 [0.000, 494.100], loss: 186.642120, mae: 33.702644, mean_q: -34.769993\n",
            " 1290621/10000000: episode: 6421, duration: 1.620s, episode steps: 201, steps per second: 124, episode reward: -90.800, mean reward: -0.452 [-45.400, 269.000], mean action: 2.781 [0.000, 10.000], mean observation: 30.800 [0.001, 465.200], loss: 141.462189, mae: 34.185814, mean_q: -35.198563\n",
            " 1290822/10000000: episode: 6422, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -460.200, mean reward: -2.290 [-230.100, 68.100], mean action: 2.692 [0.000, 9.000], mean observation: 31.997 [0.000, 798.300], loss: 129.271271, mae: 34.390697, mean_q: -35.324322\n",
            " 1291023/10000000: episode: 6423, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -648.400, mean reward: -3.226 [-324.200, 63.900], mean action: 3.348 [0.000, 10.000], mean observation: 33.902 [0.001, 556.200], loss: 169.537125, mae: 35.089012, mean_q: -36.167671\n",
            " 1291224/10000000: episode: 6424, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -453.800, mean reward: -2.258 [-226.900, 181.800], mean action: 3.070 [0.000, 9.000], mean observation: 26.639 [0.002, 434.300], loss: 170.847137, mae: 35.070019, mean_q: -36.020012\n",
            " 1291425/10000000: episode: 6425, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -303.400, mean reward: -1.509 [-151.700, 124.200], mean action: 3.428 [0.000, 10.000], mean observation: 35.102 [0.000, 668.900], loss: 226.016205, mae: 35.152958, mean_q: -36.153030\n",
            " 1291626/10000000: episode: 6426, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -156.800, mean reward: -0.780 [-78.400, 157.600], mean action: 3.478 [0.000, 10.000], mean observation: 33.088 [0.000, 506.300], loss: 175.899551, mae: 34.766350, mean_q: -36.115345\n",
            " 1291827/10000000: episode: 6427, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -652.600, mean reward: -3.247 [-326.300, 101.200], mean action: 3.328 [0.000, 10.000], mean observation: 32.329 [0.000, 531.100], loss: 220.780746, mae: 34.982861, mean_q: -36.437649\n",
            " 1292028/10000000: episode: 6428, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: 687.600, mean reward: 3.421 [-9.000, 360.000], mean action: 3.119 [0.000, 9.000], mean observation: 36.842 [0.000, 650.000], loss: 172.477844, mae: 35.513588, mean_q: -36.806740\n",
            " 1292229/10000000: episode: 6429, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -379.400, mean reward: -1.888 [-189.700, 180.000], mean action: 3.383 [0.000, 10.000], mean observation: 36.328 [0.000, 574.000], loss: 264.003265, mae: 36.273952, mean_q: -37.793415\n",
            " 1292430/10000000: episode: 6430, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -494.400, mean reward: -2.460 [-247.200, 70.600], mean action: 3.204 [0.000, 10.000], mean observation: 26.091 [0.002, 420.100], loss: 227.106247, mae: 36.303989, mean_q: -37.885937\n",
            " 1292631/10000000: episode: 6431, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 218.400, mean reward: 1.087 [-10.000, 196.500], mean action: 3.358 [0.000, 10.000], mean observation: 40.491 [0.002, 470.300], loss: 163.221436, mae: 36.413010, mean_q: -38.048187\n",
            " 1292832/10000000: episode: 6432, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -442.200, mean reward: -2.200 [-221.100, 86.100], mean action: 2.736 [0.000, 10.000], mean observation: 35.806 [0.001, 510.400], loss: 186.698181, mae: 36.760654, mean_q: -38.258266\n",
            " 1293033/10000000: episode: 6433, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: 1028.400, mean reward: 5.116 [-10.000, 675.000], mean action: 3.428 [0.000, 10.000], mean observation: 33.969 [0.000, 508.900], loss: 210.266190, mae: 36.715595, mean_q: -38.106274\n",
            " 1293234/10000000: episode: 6434, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -920.800, mean reward: -4.581 [-460.400, 84.000], mean action: 3.547 [0.000, 10.000], mean observation: 31.580 [0.002, 472.800], loss: 218.758804, mae: 36.875084, mean_q: -38.515999\n",
            " 1293435/10000000: episode: 6435, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -149.000, mean reward: -0.741 [-74.500, 408.100], mean action: 3.955 [0.000, 9.000], mean observation: 29.355 [0.001, 618.400], loss: 288.907959, mae: 37.436222, mean_q: -39.387917\n",
            " 1293636/10000000: episode: 6436, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: 883.000, mean reward: 4.393 [-9.000, 641.600], mean action: 4.080 [0.000, 9.000], mean observation: 34.444 [0.001, 512.000], loss: 191.515625, mae: 38.024628, mean_q: -39.941036\n",
            " 1293837/10000000: episode: 6437, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -740.200, mean reward: -3.683 [-370.100, 144.000], mean action: 4.249 [0.000, 10.000], mean observation: 32.866 [0.000, 586.000], loss: 173.242844, mae: 38.432037, mean_q: -39.993179\n",
            " 1294038/10000000: episode: 6438, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -1409.000, mean reward: -7.010 [-704.500, 44.000], mean action: 4.134 [0.000, 9.000], mean observation: 32.874 [0.001, 431.000], loss: 252.884796, mae: 37.694767, mean_q: -39.200047\n",
            " 1294239/10000000: episode: 6439, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 256.600, mean reward: 1.277 [-10.000, 697.500], mean action: 3.876 [0.000, 10.000], mean observation: 28.489 [0.001, 459.200], loss: 251.028381, mae: 38.408810, mean_q: -40.275120\n",
            " 1294440/10000000: episode: 6440, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -1029.200, mean reward: -5.120 [-514.600, 51.200], mean action: 3.323 [0.000, 9.000], mean observation: 33.659 [0.001, 422.300], loss: 369.555206, mae: 38.065681, mean_q: -39.903393\n",
            " 1294641/10000000: episode: 6441, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: 2295.600, mean reward: 11.421 [-10.000, 1147.800], mean action: 3.896 [0.000, 10.000], mean observation: 33.252 [0.001, 497.500], loss: 182.438980, mae: 38.012081, mean_q: -40.464634\n",
            " 1294842/10000000: episode: 6442, duration: 2.027s, episode steps: 201, steps per second: 99, episode reward: -1063.600, mean reward: -5.292 [-531.800, 37.600], mean action: 4.085 [0.000, 10.000], mean observation: 29.511 [0.002, 430.700], loss: 318.877625, mae: 38.095917, mean_q: -40.023994\n",
            " 1295043/10000000: episode: 6443, duration: 1.832s, episode steps: 201, steps per second: 110, episode reward: -861.400, mean reward: -4.286 [-430.700, 81.300], mean action: 4.443 [0.000, 10.000], mean observation: 31.374 [0.002, 527.400], loss: 182.183853, mae: 37.888195, mean_q: -39.654526\n",
            " 1295244/10000000: episode: 6444, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: 433.800, mean reward: 2.158 [-10.000, 647.100], mean action: 3.299 [0.000, 10.000], mean observation: 36.149 [0.000, 720.900], loss: 156.731232, mae: 37.574448, mean_q: -38.806297\n",
            " 1295445/10000000: episode: 6445, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: 61.800, mean reward: 0.307 [-9.000, 114.600], mean action: 3.483 [0.000, 9.000], mean observation: 31.066 [0.001, 500.800], loss: 324.650055, mae: 37.643131, mean_q: -38.792336\n",
            " 1295646/10000000: episode: 6446, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -241.600, mean reward: -1.202 [-120.800, 162.400], mean action: 2.816 [0.000, 9.000], mean observation: 32.693 [0.001, 424.400], loss: 187.852692, mae: 37.286057, mean_q: -38.626919\n",
            " 1295847/10000000: episode: 6447, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 563.800, mean reward: 2.805 [-9.000, 281.900], mean action: 2.652 [0.000, 9.000], mean observation: 32.067 [0.000, 441.700], loss: 156.597565, mae: 37.447514, mean_q: -38.626740\n",
            " 1296048/10000000: episode: 6448, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -385.600, mean reward: -1.918 [-192.800, 44.100], mean action: 2.209 [0.000, 9.000], mean observation: 28.930 [0.002, 387.100], loss: 212.098236, mae: 37.787766, mean_q: -38.899864\n",
            " 1296249/10000000: episode: 6449, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -371.800, mean reward: -1.850 [-185.900, 189.000], mean action: 3.279 [0.000, 9.000], mean observation: 33.060 [0.002, 537.100], loss: 225.406174, mae: 37.561722, mean_q: -38.911831\n",
            " 1296450/10000000: episode: 6450, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: -433.800, mean reward: -2.158 [-216.900, 151.200], mean action: 2.831 [0.000, 9.000], mean observation: 35.490 [0.001, 562.800], loss: 215.922256, mae: 37.246887, mean_q: -38.376160\n",
            " 1296651/10000000: episode: 6451, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: 456.800, mean reward: 2.273 [-9.000, 228.400], mean action: 2.502 [0.000, 9.000], mean observation: 31.100 [0.000, 693.300], loss: 226.125702, mae: 36.989098, mean_q: -37.903179\n",
            " 1296852/10000000: episode: 6452, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -761.600, mean reward: -3.789 [-380.800, 48.600], mean action: 2.468 [0.000, 9.000], mean observation: 39.743 [0.001, 600.600], loss: 346.597473, mae: 36.855217, mean_q: -37.662518\n",
            " 1297053/10000000: episode: 6453, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -414.800, mean reward: -2.064 [-207.400, 138.600], mean action: 2.871 [0.000, 9.000], mean observation: 35.236 [0.000, 727.600], loss: 207.390930, mae: 36.594666, mean_q: -37.507866\n",
            " 1297254/10000000: episode: 6454, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: 538.600, mean reward: 2.680 [-9.000, 269.300], mean action: 2.463 [0.000, 9.000], mean observation: 34.345 [0.001, 606.200], loss: 286.254730, mae: 36.589005, mean_q: -37.500137\n",
            " 1297455/10000000: episode: 6455, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -689.000, mean reward: -3.428 [-344.500, 137.600], mean action: 3.169 [0.000, 9.000], mean observation: 32.014 [0.001, 597.900], loss: 240.618408, mae: 36.191456, mean_q: -37.241444\n",
            " 1297656/10000000: episode: 6456, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -439.000, mean reward: -2.184 [-219.500, 133.400], mean action: 2.682 [0.000, 9.000], mean observation: 33.755 [0.003, 593.100], loss: 219.096512, mae: 36.718739, mean_q: -37.789398\n",
            " 1297857/10000000: episode: 6457, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -628.400, mean reward: -3.126 [-314.200, 53.200], mean action: 3.100 [0.000, 10.000], mean observation: 28.449 [0.002, 435.000], loss: 244.507736, mae: 36.648647, mean_q: -37.867840\n",
            " 1298058/10000000: episode: 6458, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -377.000, mean reward: -1.876 [-188.500, 102.400], mean action: 2.836 [0.000, 9.000], mean observation: 32.397 [0.001, 697.100], loss: 326.793762, mae: 36.709362, mean_q: -37.882488\n",
            " 1298259/10000000: episode: 6459, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -401.400, mean reward: -1.997 [-200.700, 86.400], mean action: 3.279 [0.000, 9.000], mean observation: 30.357 [0.001, 562.800], loss: 288.886902, mae: 36.518070, mean_q: -37.788788\n",
            " 1298460/10000000: episode: 6460, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -1005.200, mean reward: -5.001 [-502.600, 59.000], mean action: 3.159 [0.000, 9.000], mean observation: 31.031 [0.000, 458.900], loss: 287.941040, mae: 36.455059, mean_q: -37.654507\n",
            " 1298661/10000000: episode: 6461, duration: 1.372s, episode steps: 201, steps per second: 146, episode reward: -499.200, mean reward: -2.484 [-249.600, 78.600], mean action: 2.612 [0.000, 9.000], mean observation: 34.508 [0.002, 522.200], loss: 228.610519, mae: 36.356121, mean_q: -37.263748\n",
            " 1298862/10000000: episode: 6462, duration: 1.384s, episode steps: 201, steps per second: 145, episode reward: 287.600, mean reward: 1.431 [-9.000, 258.300], mean action: 2.816 [0.000, 9.000], mean observation: 28.814 [0.001, 521.600], loss: 251.934158, mae: 36.450874, mean_q: -37.383209\n",
            " 1299063/10000000: episode: 6463, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -587.400, mean reward: -2.922 [-293.700, 31.400], mean action: 2.045 [0.000, 9.000], mean observation: 34.169 [0.000, 527.200], loss: 157.061035, mae: 36.531067, mean_q: -37.398273\n",
            " 1299264/10000000: episode: 6464, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -551.800, mean reward: -2.745 [-275.900, 81.500], mean action: 2.522 [0.000, 9.000], mean observation: 34.450 [0.003, 451.200], loss: 249.902985, mae: 36.311962, mean_q: -37.341652\n",
            " 1299465/10000000: episode: 6465, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -358.800, mean reward: -1.785 [-179.400, 136.500], mean action: 2.761 [0.000, 9.000], mean observation: 31.263 [0.000, 765.000], loss: 419.028717, mae: 36.225918, mean_q: -37.301682\n",
            " 1299666/10000000: episode: 6466, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -83.600, mean reward: -0.416 [-41.800, 220.000], mean action: 2.478 [0.000, 9.000], mean observation: 36.889 [0.002, 439.000], loss: 381.985535, mae: 36.475250, mean_q: -37.537495\n",
            " 1299867/10000000: episode: 6467, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 5.200, mean reward: 0.026 [-9.000, 308.800], mean action: 3.423 [0.000, 9.000], mean observation: 36.336 [0.000, 793.800], loss: 303.713928, mae: 35.922688, mean_q: -37.067459\n",
            " 1300068/10000000: episode: 6468, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -153.200, mean reward: -0.762 [-76.600, 110.400], mean action: 3.483 [0.000, 9.000], mean observation: 34.315 [0.000, 633.800], loss: 192.113724, mae: 36.244442, mean_q: -37.457951\n",
            " 1300269/10000000: episode: 6469, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -914.000, mean reward: -4.547 [-457.000, 18.900], mean action: 2.821 [0.000, 9.000], mean observation: 35.529 [0.000, 701.500], loss: 238.148132, mae: 36.556049, mean_q: -37.846622\n",
            " 1300470/10000000: episode: 6470, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -539.200, mean reward: -2.683 [-269.600, 174.600], mean action: 2.960 [0.000, 9.000], mean observation: 40.839 [0.002, 542.400], loss: 260.270294, mae: 36.766079, mean_q: -38.164536\n",
            " 1300671/10000000: episode: 6471, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: 992.000, mean reward: 4.935 [-9.000, 496.000], mean action: 2.806 [0.000, 9.000], mean observation: 31.104 [0.000, 783.800], loss: 265.204498, mae: 36.915211, mean_q: -38.556870\n",
            " 1300872/10000000: episode: 6472, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -27.800, mean reward: -0.138 [-13.900, 144.900], mean action: 3.239 [0.000, 10.000], mean observation: 38.490 [0.000, 620.700], loss: 315.111481, mae: 36.925983, mean_q: -38.440037\n",
            " 1301073/10000000: episode: 6473, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -327.600, mean reward: -1.630 [-163.800, 98.000], mean action: 2.970 [0.000, 9.000], mean observation: 37.358 [0.002, 515.200], loss: 221.540421, mae: 37.109024, mean_q: -38.549358\n",
            " 1301274/10000000: episode: 6474, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -508.400, mean reward: -2.529 [-254.200, 56.200], mean action: 3.104 [0.000, 9.000], mean observation: 34.232 [0.000, 673.700], loss: 539.485596, mae: 36.490822, mean_q: -37.641033\n",
            " 1301475/10000000: episode: 6475, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -730.400, mean reward: -3.634 [-365.200, 55.200], mean action: 3.328 [0.000, 9.000], mean observation: 36.908 [0.002, 446.200], loss: 305.053925, mae: 36.378536, mean_q: -37.591846\n",
            " 1301676/10000000: episode: 6476, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -259.800, mean reward: -1.293 [-129.900, 87.300], mean action: 3.438 [0.000, 10.000], mean observation: 33.337 [0.001, 459.000], loss: 171.373215, mae: 36.377029, mean_q: -37.800823\n",
            " 1301877/10000000: episode: 6477, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: 719.400, mean reward: 3.579 [-10.000, 846.000], mean action: 3.622 [0.000, 10.000], mean observation: 36.991 [0.001, 558.800], loss: 232.054565, mae: 37.021149, mean_q: -38.416138\n",
            " 1302078/10000000: episode: 6478, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: 791.600, mean reward: 3.938 [-10.000, 395.800], mean action: 3.279 [0.000, 10.000], mean observation: 32.853 [0.000, 933.200], loss: 254.897507, mae: 37.206104, mean_q: -38.777046\n",
            " 1302279/10000000: episode: 6479, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -103.400, mean reward: -0.514 [-51.700, 117.900], mean action: 3.537 [0.000, 9.000], mean observation: 32.897 [0.000, 684.900], loss: 288.108276, mae: 37.320526, mean_q: -38.877922\n",
            " 1302480/10000000: episode: 6480, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -686.400, mean reward: -3.415 [-343.200, 81.000], mean action: 3.821 [0.000, 10.000], mean observation: 30.823 [0.002, 507.800], loss: 305.278931, mae: 37.227764, mean_q: -38.744362\n",
            " 1302681/10000000: episode: 6481, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: 90.800, mean reward: 0.452 [-9.000, 143.100], mean action: 3.403 [0.000, 9.000], mean observation: 33.197 [0.000, 556.700], loss: 366.000732, mae: 37.340317, mean_q: -38.705471\n",
            " 1302882/10000000: episode: 6482, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -638.000, mean reward: -3.174 [-319.000, 100.000], mean action: 3.433 [0.000, 10.000], mean observation: 34.582 [0.001, 444.500], loss: 231.563553, mae: 37.293114, mean_q: -38.726337\n",
            " 1303083/10000000: episode: 6483, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -990.600, mean reward: -4.928 [-495.300, 42.700], mean action: 3.134 [0.000, 10.000], mean observation: 36.707 [0.000, 607.900], loss: 284.343079, mae: 37.424217, mean_q: -39.038921\n",
            " 1303284/10000000: episode: 6484, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -427.200, mean reward: -2.125 [-213.600, 123.300], mean action: 3.418 [0.000, 9.000], mean observation: 35.597 [0.000, 528.400], loss: 273.617126, mae: 37.483562, mean_q: -39.174896\n",
            " 1303485/10000000: episode: 6485, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: 428.800, mean reward: 2.133 [-10.000, 465.500], mean action: 3.169 [0.000, 10.000], mean observation: 28.715 [0.001, 455.800], loss: 281.036133, mae: 37.798477, mean_q: -39.527824\n",
            " 1303686/10000000: episode: 6486, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -355.800, mean reward: -1.770 [-177.900, 96.600], mean action: 3.488 [0.000, 10.000], mean observation: 31.597 [0.000, 818.500], loss: 412.448761, mae: 37.979065, mean_q: -39.398949\n",
            " 1303887/10000000: episode: 6487, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -836.400, mean reward: -4.161 [-418.200, 102.500], mean action: 3.537 [0.000, 10.000], mean observation: 32.490 [0.002, 441.900], loss: 241.663162, mae: 37.680992, mean_q: -39.021339\n",
            " 1304088/10000000: episode: 6488, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: -168.000, mean reward: -0.836 [-84.000, 460.000], mean action: 4.522 [0.000, 10.000], mean observation: 32.637 [0.001, 619.000], loss: 277.096924, mae: 37.643661, mean_q: -39.401619\n",
            " 1304289/10000000: episode: 6489, duration: 1.395s, episode steps: 201, steps per second: 144, episode reward: 386.200, mean reward: 1.921 [-10.000, 323.100], mean action: 4.109 [0.000, 10.000], mean observation: 33.933 [0.001, 412.100], loss: 350.262604, mae: 37.427811, mean_q: -38.794708\n",
            " 1304490/10000000: episode: 6490, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -985.800, mean reward: -4.904 [-492.900, 93.800], mean action: 4.706 [0.000, 9.000], mean observation: 33.163 [0.000, 500.900], loss: 198.965271, mae: 36.624599, mean_q: -37.842030\n",
            " 1304691/10000000: episode: 6491, duration: 1.509s, episode steps: 201, steps per second: 133, episode reward: 222.600, mean reward: 1.107 [-9.000, 243.000], mean action: 4.090 [0.000, 9.000], mean observation: 38.240 [0.001, 501.600], loss: 210.497238, mae: 36.554550, mean_q: -37.850952\n",
            " 1304892/10000000: episode: 6492, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: 53.200, mean reward: 0.265 [-10.000, 393.600], mean action: 4.104 [0.000, 10.000], mean observation: 31.862 [0.001, 590.900], loss: 313.516052, mae: 36.638702, mean_q: -37.973087\n",
            " 1305093/10000000: episode: 6493, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -375.600, mean reward: -1.869 [-187.800, 98.900], mean action: 3.179 [0.000, 9.000], mean observation: 30.389 [0.000, 567.300], loss: 303.263885, mae: 36.812927, mean_q: -37.998924\n",
            " 1305294/10000000: episode: 6494, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -769.200, mean reward: -3.827 [-384.600, 52.800], mean action: 3.353 [0.000, 9.000], mean observation: 37.615 [0.000, 747.100], loss: 255.095581, mae: 36.969307, mean_q: -38.219032\n",
            " 1305495/10000000: episode: 6495, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -136.600, mean reward: -0.680 [-68.300, 189.700], mean action: 3.582 [0.000, 9.000], mean observation: 30.116 [0.000, 534.800], loss: 373.995483, mae: 36.774521, mean_q: -38.040920\n",
            " 1305696/10000000: episode: 6496, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -271.600, mean reward: -1.351 [-135.800, 147.200], mean action: 2.886 [0.000, 9.000], mean observation: 29.313 [0.000, 478.800], loss: 253.955765, mae: 36.872490, mean_q: -38.315075\n",
            " 1305897/10000000: episode: 6497, duration: 1.383s, episode steps: 201, steps per second: 145, episode reward: -817.800, mean reward: -4.069 [-408.900, 57.900], mean action: 3.701 [0.000, 10.000], mean observation: 34.104 [0.000, 580.300], loss: 208.608322, mae: 37.095238, mean_q: -38.702328\n",
            " 1306098/10000000: episode: 6498, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: 14.600, mean reward: 0.073 [-9.000, 243.900], mean action: 4.000 [0.000, 9.000], mean observation: 33.858 [0.000, 589.600], loss: 199.969482, mae: 36.544247, mean_q: -38.044724\n",
            " 1306299/10000000: episode: 6499, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -497.200, mean reward: -2.474 [-248.600, 165.600], mean action: 3.532 [0.000, 10.000], mean observation: 30.838 [0.000, 813.800], loss: 231.506180, mae: 36.604897, mean_q: -37.897606\n",
            " 1306500/10000000: episode: 6500, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: -431.200, mean reward: -2.145 [-215.600, 230.400], mean action: 3.413 [0.000, 10.000], mean observation: 36.551 [0.000, 621.900], loss: 400.590088, mae: 36.622845, mean_q: -37.809277\n",
            " 1306701/10000000: episode: 6501, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -1250.400, mean reward: -6.221 [-625.200, 32.400], mean action: 3.403 [0.000, 10.000], mean observation: 30.730 [0.002, 632.400], loss: 269.401306, mae: 36.430206, mean_q: -37.462498\n",
            " 1306902/10000000: episode: 6502, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -481.200, mean reward: -2.394 [-240.600, 149.400], mean action: 2.761 [0.000, 9.000], mean observation: 34.492 [0.003, 531.400], loss: 239.701065, mae: 36.163425, mean_q: -37.085316\n",
            " 1307103/10000000: episode: 6503, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -906.400, mean reward: -4.509 [-453.200, 27.200], mean action: 3.010 [0.000, 10.000], mean observation: 34.925 [0.001, 519.900], loss: 399.534790, mae: 36.784458, mean_q: -38.142208\n",
            " 1307304/10000000: episode: 6504, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -595.400, mean reward: -2.962 [-297.700, 106.500], mean action: 2.970 [0.000, 10.000], mean observation: 37.913 [0.001, 447.800], loss: 255.400665, mae: 36.913116, mean_q: -38.289768\n",
            " 1307505/10000000: episode: 6505, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -59.800, mean reward: -0.298 [-29.900, 132.000], mean action: 2.910 [0.000, 9.000], mean observation: 39.794 [0.001, 627.200], loss: 144.942627, mae: 36.932297, mean_q: -38.306057\n",
            " 1307706/10000000: episode: 6506, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -496.800, mean reward: -2.472 [-248.400, 123.800], mean action: 2.662 [0.000, 10.000], mean observation: 39.497 [0.003, 501.700], loss: 246.913330, mae: 37.536797, mean_q: -38.971378\n",
            " 1307907/10000000: episode: 6507, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: 300.600, mean reward: 1.496 [-10.000, 239.400], mean action: 2.657 [0.000, 10.000], mean observation: 29.217 [0.000, 604.500], loss: 203.433960, mae: 37.632542, mean_q: -39.143414\n",
            " 1308108/10000000: episode: 6508, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: 158.200, mean reward: 0.787 [-9.000, 169.800], mean action: 2.602 [0.000, 9.000], mean observation: 31.629 [0.001, 437.600], loss: 233.520477, mae: 37.850693, mean_q: -39.330925\n",
            " 1308309/10000000: episode: 6509, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -247.000, mean reward: -1.229 [-123.500, 141.600], mean action: 3.070 [0.000, 10.000], mean observation: 29.865 [0.001, 464.600], loss: 255.849289, mae: 38.343426, mean_q: -39.874687\n",
            " 1308510/10000000: episode: 6510, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -337.800, mean reward: -1.681 [-168.900, 151.200], mean action: 3.055 [0.000, 10.000], mean observation: 35.051 [0.001, 531.100], loss: 245.365204, mae: 38.194424, mean_q: -39.727749\n",
            " 1308711/10000000: episode: 6511, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -790.600, mean reward: -3.933 [-395.300, 105.600], mean action: 3.378 [0.000, 9.000], mean observation: 37.559 [0.000, 467.700], loss: 229.579315, mae: 39.028271, mean_q: -40.548958\n",
            " 1308912/10000000: episode: 6512, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: 362.800, mean reward: 1.805 [-9.000, 493.200], mean action: 3.045 [0.000, 9.000], mean observation: 36.140 [0.000, 784.200], loss: 252.615860, mae: 39.365623, mean_q: -40.577084\n",
            " 1309113/10000000: episode: 6513, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -730.200, mean reward: -3.633 [-365.100, 67.300], mean action: 2.632 [0.000, 9.000], mean observation: 32.983 [0.000, 507.700], loss: 240.310532, mae: 38.744751, mean_q: -40.091003\n",
            " 1309314/10000000: episode: 6514, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -705.000, mean reward: -3.507 [-352.500, 60.200], mean action: 2.542 [0.000, 9.000], mean observation: 32.105 [0.001, 542.200], loss: 237.047638, mae: 39.591728, mean_q: -40.782349\n",
            " 1309515/10000000: episode: 6515, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: 1127.600, mean reward: 5.610 [-9.000, 875.500], mean action: 2.701 [0.000, 9.000], mean observation: 33.789 [0.001, 527.300], loss: 216.396072, mae: 39.848160, mean_q: -40.726414\n",
            " 1309716/10000000: episode: 6516, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -815.800, mean reward: -4.059 [-407.900, 85.500], mean action: 2.925 [0.000, 9.000], mean observation: 30.188 [0.000, 542.400], loss: 336.276886, mae: 38.963795, mean_q: -40.422783\n",
            " 1309917/10000000: episode: 6517, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -715.200, mean reward: -3.558 [-357.600, 123.200], mean action: 3.095 [0.000, 9.000], mean observation: 32.734 [0.000, 694.400], loss: 358.328278, mae: 39.402573, mean_q: -41.172390\n",
            " 1310118/10000000: episode: 6518, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -1071.600, mean reward: -5.331 [-535.800, 36.800], mean action: 2.985 [0.000, 9.000], mean observation: 43.983 [0.000, 598.300], loss: 359.678162, mae: 40.572163, mean_q: -41.418709\n",
            " 1310319/10000000: episode: 6519, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -535.800, mean reward: -2.666 [-267.900, 104.700], mean action: 2.761 [0.000, 9.000], mean observation: 30.567 [0.001, 555.700], loss: 431.787170, mae: 40.938568, mean_q: -41.717350\n",
            " 1310520/10000000: episode: 6520, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -173.600, mean reward: -0.864 [-86.800, 196.000], mean action: 3.080 [0.000, 9.000], mean observation: 28.638 [0.003, 492.800], loss: 241.025726, mae: 40.391323, mean_q: -41.676876\n",
            " 1310721/10000000: episode: 6521, duration: 1.573s, episode steps: 201, steps per second: 128, episode reward: -412.800, mean reward: -2.054 [-206.400, 131.200], mean action: 3.244 [0.000, 9.000], mean observation: 42.667 [0.000, 669.900], loss: 371.574402, mae: 40.065178, mean_q: -41.566429\n",
            " 1310922/10000000: episode: 6522, duration: 1.607s, episode steps: 201, steps per second: 125, episode reward: -477.400, mean reward: -2.375 [-238.700, 151.900], mean action: 3.010 [0.000, 10.000], mean observation: 32.554 [0.002, 441.800], loss: 367.274628, mae: 40.070286, mean_q: -41.727661\n",
            " 1311123/10000000: episode: 6523, duration: 1.657s, episode steps: 201, steps per second: 121, episode reward: 1707.000, mean reward: 8.493 [-10.000, 853.500], mean action: 3.358 [0.000, 10.000], mean observation: 36.464 [0.001, 482.600], loss: 375.601471, mae: 39.469776, mean_q: -41.331539\n",
            " 1311324/10000000: episode: 6524, duration: 1.547s, episode steps: 201, steps per second: 130, episode reward: 242.400, mean reward: 1.206 [-10.000, 259.000], mean action: 3.881 [0.000, 10.000], mean observation: 37.347 [0.000, 654.700], loss: 412.390076, mae: 38.949463, mean_q: -40.948586\n",
            " 1311525/10000000: episode: 6525, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -1083.800, mean reward: -5.392 [-541.900, 117.200], mean action: 4.015 [0.000, 9.000], mean observation: 33.300 [0.000, 706.400], loss: 384.507782, mae: 39.101467, mean_q: -41.142376\n",
            " 1311726/10000000: episode: 6526, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -1029.000, mean reward: -5.119 [-514.500, 57.400], mean action: 3.816 [0.000, 10.000], mean observation: 31.029 [0.001, 480.700], loss: 259.562622, mae: 39.134644, mean_q: -41.151974\n",
            " 1311927/10000000: episode: 6527, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -666.200, mean reward: -3.314 [-333.100, 56.700], mean action: 3.179 [0.000, 10.000], mean observation: 36.249 [0.001, 463.400], loss: 285.227661, mae: 39.671494, mean_q: -41.424232\n",
            " 1312128/10000000: episode: 6528, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -764.200, mean reward: -3.802 [-382.100, 81.800], mean action: 3.592 [0.000, 10.000], mean observation: 30.076 [0.001, 676.900], loss: 325.992859, mae: 40.529312, mean_q: -41.753021\n",
            " 1312329/10000000: episode: 6529, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -318.200, mean reward: -1.583 [-159.100, 90.400], mean action: 2.871 [0.000, 9.000], mean observation: 32.343 [0.001, 714.300], loss: 232.404266, mae: 41.000938, mean_q: -42.040016\n",
            " 1312530/10000000: episode: 6530, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 304.000, mean reward: 1.512 [-10.000, 358.200], mean action: 2.925 [0.000, 10.000], mean observation: 42.120 [0.000, 601.500], loss: 288.997559, mae: 40.296413, mean_q: -41.808739\n",
            " 1312731/10000000: episode: 6531, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -589.000, mean reward: -2.930 [-294.500, 96.600], mean action: 3.214 [0.000, 10.000], mean observation: 32.162 [0.000, 582.800], loss: 499.157562, mae: 40.594864, mean_q: -41.824657\n",
            " 1312932/10000000: episode: 6532, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -216.400, mean reward: -1.077 [-108.200, 80.000], mean action: 3.612 [0.000, 9.000], mean observation: 32.177 [0.000, 566.700], loss: 349.749176, mae: 39.457523, mean_q: -41.098595\n",
            " 1313133/10000000: episode: 6533, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -600.400, mean reward: -2.987 [-300.200, 49.600], mean action: 3.289 [0.000, 10.000], mean observation: 29.557 [0.001, 498.800], loss: 315.821960, mae: 39.994942, mean_q: -41.509777\n",
            " 1313334/10000000: episode: 6534, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 144.000, mean reward: 0.716 [-9.000, 114.600], mean action: 3.542 [0.000, 9.000], mean observation: 33.931 [0.002, 508.500], loss: 288.081451, mae: 40.190197, mean_q: -41.775574\n",
            " 1313535/10000000: episode: 6535, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -221.000, mean reward: -1.100 [-110.500, 186.200], mean action: 2.896 [0.000, 9.000], mean observation: 32.156 [0.000, 389.300], loss: 389.631531, mae: 40.754387, mean_q: -41.802395\n",
            " 1313736/10000000: episode: 6536, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -944.800, mean reward: -4.700 [-472.400, 24.800], mean action: 2.970 [0.000, 9.000], mean observation: 37.854 [0.000, 630.500], loss: 325.733368, mae: 40.553616, mean_q: -42.019833\n",
            " 1313937/10000000: episode: 6537, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: 7.200, mean reward: 0.036 [-9.000, 366.800], mean action: 3.851 [0.000, 9.000], mean observation: 30.510 [0.001, 518.600], loss: 433.937744, mae: 40.085972, mean_q: -42.140808\n",
            " 1314138/10000000: episode: 6538, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 1199.200, mean reward: 5.966 [-10.000, 924.700], mean action: 3.249 [0.000, 10.000], mean observation: 37.142 [0.000, 562.200], loss: 246.753922, mae: 40.183163, mean_q: -42.490799\n",
            " 1314339/10000000: episode: 6539, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -655.600, mean reward: -3.262 [-327.800, 63.700], mean action: 3.174 [0.000, 9.000], mean observation: 30.146 [0.002, 468.200], loss: 397.099579, mae: 40.479534, mean_q: -42.215683\n",
            " 1314540/10000000: episode: 6540, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 1381.800, mean reward: 6.875 [-9.000, 690.900], mean action: 2.940 [0.000, 9.000], mean observation: 33.422 [0.002, 637.200], loss: 202.171005, mae: 40.761429, mean_q: -42.151188\n",
            " 1314741/10000000: episode: 6541, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -395.400, mean reward: -1.967 [-197.700, 70.000], mean action: 2.796 [0.000, 10.000], mean observation: 28.374 [0.003, 515.700], loss: 410.471313, mae: 41.495041, mean_q: -42.575706\n",
            " 1314942/10000000: episode: 6542, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: 36.000, mean reward: 0.179 [-9.000, 96.000], mean action: 2.627 [0.000, 9.000], mean observation: 28.862 [0.001, 607.700], loss: 221.403381, mae: 41.611115, mean_q: -42.535603\n",
            " 1315143/10000000: episode: 6543, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -763.800, mean reward: -3.800 [-381.900, 23.100], mean action: 2.627 [0.000, 9.000], mean observation: 34.088 [0.001, 462.800], loss: 368.703857, mae: 40.519547, mean_q: -41.757179\n",
            " 1315344/10000000: episode: 6544, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -705.600, mean reward: -3.510 [-352.800, 84.000], mean action: 3.194 [0.000, 9.000], mean observation: 39.103 [0.003, 524.500], loss: 439.295624, mae: 40.111847, mean_q: -41.787098\n",
            " 1315545/10000000: episode: 6545, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -175.200, mean reward: -0.872 [-87.600, 182.700], mean action: 3.592 [0.000, 9.000], mean observation: 26.959 [0.000, 772.000], loss: 275.554077, mae: 40.115719, mean_q: -42.277248\n",
            " 1315746/10000000: episode: 6546, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -618.400, mean reward: -3.077 [-309.200, 100.000], mean action: 2.826 [0.000, 10.000], mean observation: 33.863 [0.000, 558.800], loss: 243.700806, mae: 41.495674, mean_q: -43.164635\n",
            " 1315947/10000000: episode: 6547, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: 411.400, mean reward: 2.047 [-9.000, 296.000], mean action: 2.692 [0.000, 9.000], mean observation: 33.465 [0.001, 645.600], loss: 194.630524, mae: 41.777489, mean_q: -43.220295\n",
            " 1316148/10000000: episode: 6548, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -577.400, mean reward: -2.873 [-288.700, 35.700], mean action: 2.393 [0.000, 9.000], mean observation: 34.691 [0.001, 532.400], loss: 221.019974, mae: 41.955437, mean_q: -43.351223\n",
            " 1316349/10000000: episode: 6549, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -150.800, mean reward: -0.750 [-75.400, 106.200], mean action: 2.289 [0.000, 9.000], mean observation: 30.900 [0.000, 516.000], loss: 474.382507, mae: 42.303493, mean_q: -43.340603\n",
            " 1316550/10000000: episode: 6550, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -305.400, mean reward: -1.519 [-152.700, 82.000], mean action: 2.876 [0.000, 10.000], mean observation: 37.562 [0.000, 555.400], loss: 285.517761, mae: 40.978271, mean_q: -42.664494\n",
            " 1316751/10000000: episode: 6551, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -1153.600, mean reward: -5.739 [-576.800, 54.000], mean action: 3.448 [0.000, 10.000], mean observation: 31.826 [0.002, 464.000], loss: 427.848206, mae: 40.729309, mean_q: -43.079048\n",
            " 1316952/10000000: episode: 6552, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -737.800, mean reward: -3.671 [-368.900, 94.000], mean action: 3.502 [0.000, 10.000], mean observation: 36.940 [0.000, 668.100], loss: 206.948074, mae: 41.119743, mean_q: -43.428493\n",
            " 1317153/10000000: episode: 6553, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -637.600, mean reward: -3.172 [-318.800, 27.000], mean action: 2.746 [0.000, 10.000], mean observation: 28.212 [0.000, 389.300], loss: 248.427261, mae: 41.925335, mean_q: -43.976387\n",
            " 1317354/10000000: episode: 6554, duration: 1.377s, episode steps: 201, steps per second: 146, episode reward: -202.000, mean reward: -1.005 [-101.000, 65.400], mean action: 2.398 [0.000, 9.000], mean observation: 31.471 [0.001, 587.900], loss: 366.592377, mae: 42.209213, mean_q: -43.774593\n",
            " 1317555/10000000: episode: 6555, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: 386.200, mean reward: 1.921 [-10.000, 431.200], mean action: 2.687 [0.000, 10.000], mean observation: 36.819 [0.000, 606.600], loss: 338.635651, mae: 41.585239, mean_q: -43.363266\n",
            " 1317756/10000000: episode: 6556, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -389.000, mean reward: -1.935 [-194.500, 308.500], mean action: 2.786 [0.000, 10.000], mean observation: 35.999 [0.001, 426.200], loss: 316.244629, mae: 41.338173, mean_q: -43.139332\n",
            " 1317957/10000000: episode: 6557, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -570.000, mean reward: -2.836 [-285.000, 93.000], mean action: 2.711 [0.000, 10.000], mean observation: 31.688 [0.002, 578.200], loss: 387.645203, mae: 41.342209, mean_q: -43.103973\n",
            " 1318158/10000000: episode: 6558, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -804.800, mean reward: -4.004 [-402.400, 40.500], mean action: 2.960 [0.000, 9.000], mean observation: 33.004 [0.000, 497.600], loss: 455.288513, mae: 41.544319, mean_q: -43.291706\n",
            " 1318359/10000000: episode: 6559, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -13.600, mean reward: -0.068 [-9.000, 111.600], mean action: 2.617 [0.000, 10.000], mean observation: 36.229 [0.001, 527.200], loss: 285.858246, mae: 41.919827, mean_q: -43.624825\n",
            " 1318560/10000000: episode: 6560, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -75.600, mean reward: -0.376 [-37.800, 231.700], mean action: 2.448 [0.000, 10.000], mean observation: 31.822 [0.001, 675.300], loss: 276.194305, mae: 41.935452, mean_q: -43.671623\n",
            " 1318761/10000000: episode: 6561, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -576.200, mean reward: -2.867 [-288.100, 78.400], mean action: 2.328 [0.000, 10.000], mean observation: 33.291 [0.000, 527.500], loss: 395.796265, mae: 41.523621, mean_q: -43.235512\n",
            " 1318962/10000000: episode: 6562, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -295.000, mean reward: -1.468 [-147.500, 148.800], mean action: 3.219 [0.000, 10.000], mean observation: 28.396 [0.001, 466.900], loss: 330.108246, mae: 41.940434, mean_q: -44.254646\n",
            " 1319163/10000000: episode: 6563, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: 360.000, mean reward: 1.791 [-9.000, 244.800], mean action: 2.423 [0.000, 9.000], mean observation: 33.680 [0.000, 544.700], loss: 498.035553, mae: 42.483414, mean_q: -44.112999\n",
            " 1319364/10000000: episode: 6564, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -113.200, mean reward: -0.563 [-56.600, 140.000], mean action: 2.821 [0.000, 10.000], mean observation: 37.473 [0.002, 532.900], loss: 352.827148, mae: 42.074409, mean_q: -43.700260\n",
            " 1319565/10000000: episode: 6565, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -327.200, mean reward: -1.628 [-163.600, 122.400], mean action: 2.517 [0.000, 10.000], mean observation: 33.639 [0.001, 647.400], loss: 405.755493, mae: 41.759094, mean_q: -43.331078\n",
            " 1319766/10000000: episode: 6566, duration: 1.401s, episode steps: 201, steps per second: 144, episode reward: -310.600, mean reward: -1.545 [-155.300, 54.000], mean action: 1.975 [0.000, 9.000], mean observation: 34.100 [0.000, 602.400], loss: 296.280243, mae: 42.090752, mean_q: -43.471230\n",
            " 1319967/10000000: episode: 6567, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -692.600, mean reward: -3.446 [-346.300, 22.400], mean action: 1.975 [0.000, 9.000], mean observation: 35.328 [0.002, 453.500], loss: 207.906677, mae: 41.810181, mean_q: -43.341278\n",
            " 1320168/10000000: episode: 6568, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -104.800, mean reward: -0.521 [-52.400, 212.400], mean action: 2.502 [0.000, 10.000], mean observation: 27.433 [0.002, 362.700], loss: 329.436127, mae: 41.684303, mean_q: -43.509731\n",
            " 1320369/10000000: episode: 6569, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: 243.600, mean reward: 1.212 [-9.000, 243.000], mean action: 2.418 [0.000, 9.000], mean observation: 31.630 [0.000, 695.400], loss: 263.851166, mae: 41.829224, mean_q: -43.595936\n",
            " 1320570/10000000: episode: 6570, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: 251.000, mean reward: 1.249 [-9.000, 164.500], mean action: 2.746 [0.000, 9.000], mean observation: 34.064 [0.000, 660.900], loss: 229.254913, mae: 41.643539, mean_q: -43.862057\n",
            " 1320771/10000000: episode: 6571, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: 76.000, mean reward: 0.378 [-9.000, 141.200], mean action: 2.692 [0.000, 9.000], mean observation: 33.670 [0.000, 525.900], loss: 663.979797, mae: 41.422745, mean_q: -43.246922\n",
            " 1320972/10000000: episode: 6572, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -830.800, mean reward: -4.133 [-415.400, 53.100], mean action: 3.169 [0.000, 9.000], mean observation: 33.111 [0.001, 446.900], loss: 444.550140, mae: 41.413612, mean_q: -43.271290\n",
            " 1321173/10000000: episode: 6573, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -331.400, mean reward: -1.649 [-165.700, 133.800], mean action: 3.055 [0.000, 10.000], mean observation: 35.978 [0.000, 410.900], loss: 408.859985, mae: 41.800686, mean_q: -43.721912\n",
            " 1321374/10000000: episode: 6574, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -319.000, mean reward: -1.587 [-159.500, 153.500], mean action: 2.891 [0.000, 9.000], mean observation: 27.525 [0.001, 654.600], loss: 514.562195, mae: 41.862949, mean_q: -43.767395\n",
            " 1321575/10000000: episode: 6575, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -345.400, mean reward: -1.718 [-172.700, 108.300], mean action: 2.935 [0.000, 9.000], mean observation: 35.386 [0.000, 467.400], loss: 486.162506, mae: 41.937286, mean_q: -43.620850\n",
            " 1321776/10000000: episode: 6576, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -217.200, mean reward: -1.081 [-108.600, 90.900], mean action: 2.587 [0.000, 9.000], mean observation: 37.081 [0.000, 697.600], loss: 348.519440, mae: 42.333557, mean_q: -44.197269\n",
            " 1321977/10000000: episode: 6577, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -546.400, mean reward: -2.718 [-273.200, 160.300], mean action: 3.179 [0.000, 9.000], mean observation: 31.921 [0.000, 499.800], loss: 258.240601, mae: 42.407925, mean_q: -44.393967\n",
            " 1322178/10000000: episode: 6578, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -422.200, mean reward: -2.100 [-211.100, 159.200], mean action: 3.204 [0.000, 10.000], mean observation: 32.667 [0.001, 470.800], loss: 333.104492, mae: 42.268471, mean_q: -44.259018\n",
            " 1322379/10000000: episode: 6579, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -624.000, mean reward: -3.104 [-312.000, 75.600], mean action: 2.841 [0.000, 9.000], mean observation: 30.520 [0.003, 458.200], loss: 535.037476, mae: 41.873756, mean_q: -43.319408\n",
            " 1322580/10000000: episode: 6580, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -708.400, mean reward: -3.524 [-354.200, 39.200], mean action: 2.796 [0.000, 9.000], mean observation: 33.214 [0.002, 509.500], loss: 241.703003, mae: 41.442413, mean_q: -43.069847\n",
            " 1322781/10000000: episode: 6581, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -73.400, mean reward: -0.365 [-36.700, 100.800], mean action: 2.119 [0.000, 9.000], mean observation: 30.886 [0.002, 515.900], loss: 254.310379, mae: 41.618042, mean_q: -43.087948\n",
            " 1322982/10000000: episode: 6582, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -403.600, mean reward: -2.008 [-201.800, 90.300], mean action: 2.940 [0.000, 10.000], mean observation: 34.863 [0.002, 467.700], loss: 465.699188, mae: 41.275177, mean_q: -42.814663\n",
            " 1323183/10000000: episode: 6583, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -256.200, mean reward: -1.275 [-128.100, 70.700], mean action: 2.463 [0.000, 10.000], mean observation: 25.438 [0.002, 434.600], loss: 333.946777, mae: 41.533901, mean_q: -42.922661\n",
            " 1323384/10000000: episode: 6584, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -703.600, mean reward: -3.500 [-351.800, 114.300], mean action: 3.378 [0.000, 9.000], mean observation: 36.584 [0.000, 638.600], loss: 468.801758, mae: 41.344604, mean_q: -42.897476\n",
            " 1323585/10000000: episode: 6585, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -670.400, mean reward: -3.335 [-335.200, 66.600], mean action: 3.060 [0.000, 9.000], mean observation: 31.257 [0.000, 407.100], loss: 412.588806, mae: 41.241032, mean_q: -42.911907\n",
            " 1323786/10000000: episode: 6586, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: 460.000, mean reward: 2.289 [-9.000, 230.000], mean action: 2.721 [0.000, 9.000], mean observation: 34.947 [0.002, 530.500], loss: 321.976318, mae: 41.547527, mean_q: -43.277840\n",
            " 1323987/10000000: episode: 6587, duration: 1.395s, episode steps: 201, steps per second: 144, episode reward: -625.000, mean reward: -3.109 [-312.500, 46.800], mean action: 3.388 [0.000, 10.000], mean observation: 35.383 [0.000, 796.200], loss: 274.025940, mae: 41.865444, mean_q: -43.825657\n",
            " 1324188/10000000: episode: 6588, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: 271.800, mean reward: 1.352 [-10.000, 252.300], mean action: 2.726 [0.000, 10.000], mean observation: 36.647 [0.000, 588.400], loss: 275.945984, mae: 42.096443, mean_q: -44.082352\n",
            " 1324389/10000000: episode: 6589, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -884.800, mean reward: -4.402 [-442.400, 65.800], mean action: 3.030 [0.000, 10.000], mean observation: 30.526 [0.001, 466.300], loss: 353.271698, mae: 42.092117, mean_q: -44.055874\n",
            " 1324590/10000000: episode: 6590, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: 151.200, mean reward: 0.752 [-9.000, 147.400], mean action: 3.264 [0.000, 9.000], mean observation: 34.447 [0.003, 438.400], loss: 458.419373, mae: 41.718525, mean_q: -43.934124\n",
            " 1324791/10000000: episode: 6591, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -943.600, mean reward: -4.695 [-471.800, 55.500], mean action: 3.751 [0.000, 9.000], mean observation: 25.875 [0.002, 545.200], loss: 271.817871, mae: 41.920788, mean_q: -44.216846\n",
            " 1324992/10000000: episode: 6592, duration: 1.362s, episode steps: 201, steps per second: 148, episode reward: -737.200, mean reward: -3.668 [-368.600, 54.900], mean action: 2.627 [0.000, 9.000], mean observation: 36.625 [0.001, 491.000], loss: 248.547684, mae: 42.479614, mean_q: -44.435020\n",
            " 1325193/10000000: episode: 6593, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -324.200, mean reward: -1.613 [-162.100, 201.900], mean action: 3.000 [0.000, 10.000], mean observation: 32.981 [0.000, 481.400], loss: 441.539154, mae: 42.297810, mean_q: -44.199532\n",
            " 1325394/10000000: episode: 6594, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -1012.400, mean reward: -5.037 [-506.200, 13.800], mean action: 2.746 [0.000, 10.000], mean observation: 30.989 [0.002, 446.500], loss: 563.804138, mae: 42.714397, mean_q: -44.272659\n",
            " 1325595/10000000: episode: 6595, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -627.400, mean reward: -3.121 [-313.700, 44.200], mean action: 2.209 [0.000, 9.000], mean observation: 32.252 [0.000, 746.900], loss: 435.140472, mae: 42.185944, mean_q: -43.612122\n",
            " 1325796/10000000: episode: 6596, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -40.800, mean reward: -0.203 [-20.400, 153.600], mean action: 2.647 [0.000, 9.000], mean observation: 29.121 [0.000, 583.400], loss: 478.949707, mae: 41.636292, mean_q: -43.561367\n",
            " 1325997/10000000: episode: 6597, duration: 1.391s, episode steps: 201, steps per second: 144, episode reward: -362.600, mean reward: -1.804 [-181.300, 99.900], mean action: 2.557 [0.000, 9.000], mean observation: 33.638 [0.000, 702.100], loss: 325.387878, mae: 41.672005, mean_q: -43.663654\n",
            " 1326198/10000000: episode: 6598, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -768.600, mean reward: -3.824 [-384.300, 85.200], mean action: 2.731 [0.000, 9.000], mean observation: 35.925 [0.000, 472.400], loss: 286.688477, mae: 41.955254, mean_q: -44.332256\n",
            " 1326399/10000000: episode: 6599, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -140.600, mean reward: -0.700 [-70.300, 54.000], mean action: 2.721 [0.000, 9.000], mean observation: 33.896 [0.000, 653.600], loss: 496.985840, mae: 42.177803, mean_q: -44.317951\n",
            " 1326600/10000000: episode: 6600, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -254.800, mean reward: -1.268 [-127.400, 171.500], mean action: 2.980 [0.000, 8.000], mean observation: 31.196 [0.000, 509.800], loss: 306.651733, mae: 42.683544, mean_q: -44.414707\n",
            " 1326801/10000000: episode: 6601, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: 142.400, mean reward: 0.708 [-9.000, 159.500], mean action: 2.652 [0.000, 9.000], mean observation: 40.703 [0.002, 630.900], loss: 218.874146, mae: 42.030910, mean_q: -44.099625\n",
            " 1327002/10000000: episode: 6602, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -318.200, mean reward: -1.583 [-159.100, 84.300], mean action: 2.463 [0.000, 10.000], mean observation: 34.690 [0.001, 591.000], loss: 374.590179, mae: 41.859570, mean_q: -43.594265\n",
            " 1327203/10000000: episode: 6603, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -823.400, mean reward: -4.097 [-411.700, 43.200], mean action: 3.144 [0.000, 10.000], mean observation: 32.946 [0.001, 532.400], loss: 298.124176, mae: 41.647392, mean_q: -43.692669\n",
            " 1327404/10000000: episode: 6604, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -428.000, mean reward: -2.129 [-214.000, 86.100], mean action: 2.905 [0.000, 9.000], mean observation: 33.245 [0.001, 595.600], loss: 441.712067, mae: 42.173656, mean_q: -44.111824\n",
            " 1327605/10000000: episode: 6605, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: 778.400, mean reward: 3.873 [-10.000, 599.200], mean action: 3.408 [0.000, 10.000], mean observation: 33.118 [0.000, 530.000], loss: 660.393494, mae: 42.292957, mean_q: -44.581554\n",
            " 1327806/10000000: episode: 6606, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -98.600, mean reward: -0.491 [-49.300, 371.400], mean action: 4.065 [0.000, 10.000], mean observation: 29.172 [0.002, 453.300], loss: 421.860474, mae: 41.494473, mean_q: -43.251282\n",
            " 1328007/10000000: episode: 6607, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -610.400, mean reward: -3.037 [-305.200, 162.400], mean action: 3.408 [0.000, 10.000], mean observation: 34.377 [0.001, 504.200], loss: 479.098267, mae: 41.228546, mean_q: -43.019199\n",
            " 1328208/10000000: episode: 6608, duration: 1.390s, episode steps: 201, steps per second: 145, episode reward: -702.600, mean reward: -3.496 [-351.300, 59.100], mean action: 3.229 [0.000, 9.000], mean observation: 27.464 [0.003, 591.100], loss: 320.182922, mae: 41.225155, mean_q: -43.350513\n",
            " 1328409/10000000: episode: 6609, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 59.000, mean reward: 0.294 [-10.000, 469.600], mean action: 3.468 [0.000, 10.000], mean observation: 39.357 [0.001, 598.700], loss: 419.372498, mae: 41.263916, mean_q: -43.449677\n",
            " 1328610/10000000: episode: 6610, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -307.000, mean reward: -1.527 [-153.500, 131.600], mean action: 2.856 [0.000, 9.000], mean observation: 28.162 [0.002, 369.100], loss: 484.358643, mae: 41.599892, mean_q: -43.670666\n",
            " 1328811/10000000: episode: 6611, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -865.200, mean reward: -4.304 [-432.600, 85.200], mean action: 3.219 [0.000, 10.000], mean observation: 31.611 [0.000, 305.100], loss: 374.530945, mae: 41.469460, mean_q: -43.719090\n",
            " 1329012/10000000: episode: 6612, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: 634.000, mean reward: 3.154 [-9.000, 317.000], mean action: 2.532 [0.000, 9.000], mean observation: 32.227 [0.000, 540.000], loss: 351.527130, mae: 41.563046, mean_q: -43.898010\n",
            " 1329213/10000000: episode: 6613, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -584.400, mean reward: -2.907 [-292.200, 94.400], mean action: 3.975 [0.000, 10.000], mean observation: 31.109 [0.000, 663.800], loss: 332.574524, mae: 41.803047, mean_q: -44.416794\n",
            " 1329414/10000000: episode: 6614, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -382.600, mean reward: -1.903 [-191.300, 64.000], mean action: 2.328 [0.000, 9.000], mean observation: 31.107 [0.000, 423.600], loss: 296.785065, mae: 42.441483, mean_q: -44.431141\n",
            " 1329615/10000000: episode: 6615, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -435.800, mean reward: -2.168 [-217.900, 69.600], mean action: 2.791 [0.000, 10.000], mean observation: 34.227 [0.000, 639.500], loss: 409.284668, mae: 42.694660, mean_q: -44.790665\n",
            " 1329816/10000000: episode: 6616, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -1086.800, mean reward: -5.407 [-543.400, 40.000], mean action: 3.577 [0.000, 10.000], mean observation: 33.251 [0.001, 619.200], loss: 280.128876, mae: 42.179771, mean_q: -44.224056\n",
            " 1330017/10000000: episode: 6617, duration: 1.469s, episode steps: 201, steps per second: 137, episode reward: -308.000, mean reward: -1.532 [-154.000, 101.000], mean action: 2.672 [0.000, 10.000], mean observation: 34.401 [0.001, 651.100], loss: 331.255005, mae: 42.516304, mean_q: -44.373325\n",
            " 1330218/10000000: episode: 6618, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -335.600, mean reward: -1.670 [-167.800, 42.000], mean action: 2.114 [0.000, 10.000], mean observation: 36.556 [0.000, 818.100], loss: 272.101654, mae: 42.448139, mean_q: -44.096142\n",
            " 1330419/10000000: episode: 6619, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: -149.800, mean reward: -0.745 [-74.900, 102.500], mean action: 2.463 [0.000, 10.000], mean observation: 33.285 [0.003, 540.500], loss: 284.875610, mae: 42.203892, mean_q: -44.038811\n",
            " 1330620/10000000: episode: 6620, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -317.200, mean reward: -1.578 [-158.600, 135.900], mean action: 2.209 [0.000, 10.000], mean observation: 35.444 [0.001, 565.900], loss: 281.889740, mae: 43.142544, mean_q: -44.947994\n",
            " 1330821/10000000: episode: 6621, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -509.800, mean reward: -2.536 [-254.900, 29.200], mean action: 1.930 [0.000, 10.000], mean observation: 31.360 [0.002, 412.300], loss: 291.667053, mae: 43.000652, mean_q: -44.523834\n",
            " 1331022/10000000: episode: 6622, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -320.400, mean reward: -1.594 [-160.200, 102.500], mean action: 2.164 [0.000, 10.000], mean observation: 31.420 [0.000, 720.900], loss: 357.552582, mae: 42.714661, mean_q: -44.524845\n",
            " 1331223/10000000: episode: 6623, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: 635.400, mean reward: 3.161 [-10.000, 317.700], mean action: 2.378 [0.000, 10.000], mean observation: 34.168 [0.002, 624.500], loss: 296.390503, mae: 42.571808, mean_q: -44.387806\n",
            " 1331424/10000000: episode: 6624, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -487.400, mean reward: -2.425 [-243.700, 34.200], mean action: 1.935 [0.000, 9.000], mean observation: 31.642 [0.003, 497.700], loss: 248.237778, mae: 42.562866, mean_q: -44.083981\n",
            " 1331625/10000000: episode: 6625, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -354.800, mean reward: -1.765 [-177.400, 85.800], mean action: 2.303 [0.000, 9.000], mean observation: 28.586 [0.001, 419.900], loss: 353.210114, mae: 42.059292, mean_q: -43.490391\n",
            " 1331826/10000000: episode: 6626, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: 791.600, mean reward: 3.938 [-9.000, 461.200], mean action: 2.428 [0.000, 9.000], mean observation: 38.815 [0.000, 615.100], loss: 365.568329, mae: 41.802986, mean_q: -43.060551\n",
            " 1332027/10000000: episode: 6627, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -888.400, mean reward: -4.420 [-444.200, 35.700], mean action: 2.801 [0.000, 9.000], mean observation: 33.625 [0.000, 642.700], loss: 303.335815, mae: 41.240036, mean_q: -42.735970\n",
            " 1332228/10000000: episode: 6628, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -361.400, mean reward: -1.798 [-180.700, 115.200], mean action: 2.597 [0.000, 10.000], mean observation: 35.356 [0.000, 907.100], loss: 494.607910, mae: 41.370636, mean_q: -43.146454\n",
            " 1332429/10000000: episode: 6629, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: 158.400, mean reward: 0.788 [-9.000, 295.500], mean action: 2.393 [0.000, 9.000], mean observation: 32.998 [0.001, 507.200], loss: 321.470337, mae: 41.924541, mean_q: -43.602165\n",
            " 1332630/10000000: episode: 6630, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -434.600, mean reward: -2.162 [-217.300, 127.800], mean action: 2.274 [0.000, 10.000], mean observation: 34.547 [0.002, 522.800], loss: 407.162811, mae: 42.106480, mean_q: -43.721985\n",
            " 1332831/10000000: episode: 6631, duration: 1.588s, episode steps: 201, steps per second: 127, episode reward: -378.200, mean reward: -1.882 [-189.100, 78.600], mean action: 3.100 [0.000, 9.000], mean observation: 30.070 [0.000, 623.900], loss: 339.802002, mae: 41.291756, mean_q: -43.085495\n",
            " 1333032/10000000: episode: 6632, duration: 1.670s, episode steps: 201, steps per second: 120, episode reward: -366.000, mean reward: -1.821 [-183.000, 174.900], mean action: 3.726 [0.000, 9.000], mean observation: 30.003 [0.001, 473.800], loss: 254.993317, mae: 42.255608, mean_q: -43.650089\n",
            " 1333233/10000000: episode: 6633, duration: 1.699s, episode steps: 201, steps per second: 118, episode reward: -107.400, mean reward: -0.534 [-53.700, 281.600], mean action: 2.905 [0.000, 10.000], mean observation: 34.663 [0.000, 542.000], loss: 453.524139, mae: 43.255535, mean_q: -44.420628\n",
            " 1333434/10000000: episode: 6634, duration: 1.668s, episode steps: 201, steps per second: 120, episode reward: 523.000, mean reward: 2.602 [-10.000, 311.200], mean action: 3.403 [0.000, 10.000], mean observation: 36.767 [0.000, 781.200], loss: 347.808563, mae: 42.263710, mean_q: -44.348045\n",
            " 1333635/10000000: episode: 6635, duration: 1.641s, episode steps: 201, steps per second: 122, episode reward: -133.200, mean reward: -0.663 [-66.600, 137.900], mean action: 3.348 [0.000, 9.000], mean observation: 25.424 [0.001, 492.700], loss: 382.446442, mae: 42.136768, mean_q: -44.148804\n",
            " 1333836/10000000: episode: 6636, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: 19.400, mean reward: 0.097 [-10.000, 141.000], mean action: 2.075 [0.000, 10.000], mean observation: 33.577 [0.001, 488.900], loss: 389.942505, mae: 44.020191, mean_q: -44.866085\n",
            " 1334037/10000000: episode: 6637, duration: 1.548s, episode steps: 201, steps per second: 130, episode reward: -249.000, mean reward: -1.239 [-124.500, 135.000], mean action: 3.035 [0.000, 10.000], mean observation: 33.530 [0.002, 467.200], loss: 198.336746, mae: 43.351067, mean_q: -44.192513\n",
            " 1334238/10000000: episode: 6638, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -488.800, mean reward: -2.432 [-244.400, 81.200], mean action: 2.373 [0.000, 8.000], mean observation: 34.358 [0.000, 511.400], loss: 500.806854, mae: 42.688198, mean_q: -43.836937\n",
            " 1334439/10000000: episode: 6639, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -363.600, mean reward: -1.809 [-181.800, 89.400], mean action: 3.403 [0.000, 9.000], mean observation: 36.112 [0.000, 764.600], loss: 303.660706, mae: 42.360641, mean_q: -43.930172\n",
            " 1334640/10000000: episode: 6640, duration: 1.484s, episode steps: 201, steps per second: 135, episode reward: -377.000, mean reward: -1.876 [-188.500, 125.600], mean action: 2.751 [0.000, 9.000], mean observation: 26.864 [0.004, 516.900], loss: 260.816559, mae: 42.513885, mean_q: -43.926445\n",
            " 1334841/10000000: episode: 6641, duration: 1.510s, episode steps: 201, steps per second: 133, episode reward: -671.000, mean reward: -3.338 [-335.500, 74.200], mean action: 3.214 [0.000, 9.000], mean observation: 32.120 [0.000, 598.900], loss: 378.914032, mae: 42.149536, mean_q: -43.551762\n",
            " 1335042/10000000: episode: 6642, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -566.400, mean reward: -2.818 [-283.200, 88.900], mean action: 2.582 [0.000, 9.000], mean observation: 38.441 [0.002, 508.700], loss: 354.815063, mae: 43.128380, mean_q: -44.067787\n",
            " 1335243/10000000: episode: 6643, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -106.000, mean reward: -0.527 [-53.000, 149.000], mean action: 2.647 [0.000, 9.000], mean observation: 35.292 [0.000, 792.800], loss: 407.094269, mae: 42.371647, mean_q: -43.891403\n",
            " 1335444/10000000: episode: 6644, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -110.600, mean reward: -0.550 [-55.300, 127.200], mean action: 2.990 [0.000, 10.000], mean observation: 32.226 [0.000, 746.700], loss: 350.840332, mae: 41.871658, mean_q: -43.769112\n",
            " 1335645/10000000: episode: 6645, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -344.600, mean reward: -1.714 [-172.300, 104.800], mean action: 3.179 [0.000, 9.000], mean observation: 36.692 [0.001, 593.600], loss: 445.079315, mae: 41.040092, mean_q: -42.948189\n",
            " 1335846/10000000: episode: 6646, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -271.800, mean reward: -1.352 [-135.900, 149.400], mean action: 3.224 [0.000, 10.000], mean observation: 35.694 [0.001, 541.500], loss: 356.951294, mae: 41.261856, mean_q: -43.004257\n",
            " 1336047/10000000: episode: 6647, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -461.800, mean reward: -2.298 [-230.900, 78.000], mean action: 3.274 [0.000, 9.000], mean observation: 31.446 [0.000, 519.300], loss: 333.430695, mae: 41.145443, mean_q: -42.879101\n",
            " 1336248/10000000: episode: 6648, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -494.400, mean reward: -2.460 [-247.200, 47.200], mean action: 2.846 [0.000, 9.000], mean observation: 29.430 [0.001, 460.100], loss: 321.373566, mae: 40.755333, mean_q: -42.608185\n",
            " 1336449/10000000: episode: 6649, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -808.400, mean reward: -4.022 [-404.200, 122.500], mean action: 3.045 [0.000, 9.000], mean observation: 34.078 [0.003, 566.800], loss: 286.664612, mae: 41.059265, mean_q: -42.747875\n",
            " 1336650/10000000: episode: 6650, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -704.000, mean reward: -3.502 [-352.000, 29.600], mean action: 2.831 [0.000, 10.000], mean observation: 35.235 [0.000, 579.300], loss: 328.169434, mae: 41.323193, mean_q: -42.975677\n",
            " 1336851/10000000: episode: 6651, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: 64.000, mean reward: 0.318 [-10.000, 230.400], mean action: 2.910 [0.000, 10.000], mean observation: 27.218 [0.001, 402.600], loss: 327.366730, mae: 41.363667, mean_q: -42.779316\n",
            " 1337052/10000000: episode: 6652, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -256.800, mean reward: -1.278 [-128.400, 84.000], mean action: 2.587 [0.000, 10.000], mean observation: 32.098 [0.001, 550.200], loss: 333.120911, mae: 41.730415, mean_q: -43.025944\n",
            " 1337253/10000000: episode: 6653, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -465.200, mean reward: -2.314 [-232.600, 123.900], mean action: 3.040 [0.000, 9.000], mean observation: 34.558 [0.001, 494.100], loss: 317.376312, mae: 41.468506, mean_q: -43.022564\n",
            " 1337454/10000000: episode: 6654, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -545.200, mean reward: -2.712 [-272.600, 67.200], mean action: 2.637 [0.000, 10.000], mean observation: 32.568 [0.001, 446.100], loss: 224.989426, mae: 41.831718, mean_q: -43.010029\n",
            " 1337655/10000000: episode: 6655, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -617.200, mean reward: -3.071 [-308.600, 118.400], mean action: 3.328 [0.000, 9.000], mean observation: 40.318 [0.000, 593.600], loss: 438.933777, mae: 41.117771, mean_q: -42.391010\n",
            " 1337856/10000000: episode: 6656, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: 302.000, mean reward: 1.502 [-9.000, 279.600], mean action: 2.960 [0.000, 9.000], mean observation: 34.524 [0.000, 803.400], loss: 371.694183, mae: 40.799763, mean_q: -42.018646\n",
            " 1338057/10000000: episode: 6657, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -491.400, mean reward: -2.445 [-245.700, 127.200], mean action: 2.657 [0.000, 8.000], mean observation: 33.869 [0.001, 455.800], loss: 418.043793, mae: 41.195774, mean_q: -41.756428\n",
            " 1338258/10000000: episode: 6658, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -523.800, mean reward: -2.606 [-261.900, 64.000], mean action: 2.716 [0.000, 9.000], mean observation: 32.727 [0.002, 525.700], loss: 384.671967, mae: 40.948830, mean_q: -42.306274\n",
            " 1338459/10000000: episode: 6659, duration: 1.516s, episode steps: 201, steps per second: 133, episode reward: -1220.600, mean reward: -6.073 [-610.300, 21.600], mean action: 3.622 [0.000, 9.000], mean observation: 32.345 [0.001, 545.700], loss: 402.286682, mae: 40.646477, mean_q: -42.555569\n",
            " 1338660/10000000: episode: 6660, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: -883.600, mean reward: -4.396 [-441.800, 68.400], mean action: 3.532 [0.000, 10.000], mean observation: 35.531 [0.000, 622.400], loss: 287.994354, mae: 40.304028, mean_q: -42.374416\n",
            " 1338861/10000000: episode: 6661, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -136.000, mean reward: -0.677 [-68.000, 231.000], mean action: 2.836 [0.000, 9.000], mean observation: 34.548 [0.000, 520.500], loss: 278.229034, mae: 40.531475, mean_q: -42.249603\n",
            " 1339062/10000000: episode: 6662, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: 172.000, mean reward: 0.856 [-10.000, 178.500], mean action: 2.448 [0.000, 10.000], mean observation: 37.511 [0.000, 669.400], loss: 355.439026, mae: 40.980450, mean_q: -42.522163\n",
            " 1339263/10000000: episode: 6663, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -365.200, mean reward: -1.817 [-182.600, 153.300], mean action: 2.398 [0.000, 9.000], mean observation: 33.222 [0.000, 556.200], loss: 341.189209, mae: 40.968380, mean_q: -42.543835\n",
            " 1339464/10000000: episode: 6664, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -1.600, mean reward: -0.008 [-8.000, 112.000], mean action: 2.259 [0.000, 8.000], mean observation: 39.290 [0.000, 663.000], loss: 400.285919, mae: 40.927917, mean_q: -42.528290\n",
            " 1339665/10000000: episode: 6665, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -244.600, mean reward: -1.217 [-122.300, 53.400], mean action: 2.114 [0.000, 9.000], mean observation: 37.937 [0.002, 515.300], loss: 418.677277, mae: 41.316803, mean_q: -42.677608\n",
            " 1339866/10000000: episode: 6666, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -140.600, mean reward: -0.700 [-70.300, 202.400], mean action: 2.771 [0.000, 8.000], mean observation: 29.425 [0.000, 637.900], loss: 431.701965, mae: 41.235317, mean_q: -42.947250\n",
            " 1340067/10000000: episode: 6667, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -675.000, mean reward: -3.358 [-337.500, 101.500], mean action: 3.000 [0.000, 9.000], mean observation: 33.640 [0.001, 459.000], loss: 506.700531, mae: 40.780205, mean_q: -42.398373\n",
            " 1340268/10000000: episode: 6668, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -868.600, mean reward: -4.321 [-434.300, 54.500], mean action: 3.204 [0.000, 9.000], mean observation: 35.588 [0.001, 581.100], loss: 501.502808, mae: 40.239880, mean_q: -42.059128\n",
            " 1340469/10000000: episode: 6669, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -578.800, mean reward: -2.880 [-289.400, 158.900], mean action: 3.164 [0.000, 9.000], mean observation: 34.316 [0.001, 622.800], loss: 443.779236, mae: 39.993721, mean_q: -41.710636\n",
            " 1340670/10000000: episode: 6670, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -818.200, mean reward: -4.071 [-409.100, 49.600], mean action: 2.736 [0.000, 9.000], mean observation: 30.687 [0.000, 570.400], loss: 487.418518, mae: 39.881325, mean_q: -41.284176\n",
            " 1340871/10000000: episode: 6671, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -651.400, mean reward: -3.241 [-325.700, 52.200], mean action: 2.647 [0.000, 9.000], mean observation: 36.075 [0.001, 446.000], loss: 350.994324, mae: 40.116451, mean_q: -41.584908\n",
            " 1341072/10000000: episode: 6672, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -483.000, mean reward: -2.403 [-241.500, 164.800], mean action: 2.109 [0.000, 9.000], mean observation: 33.990 [0.000, 749.600], loss: 258.403076, mae: 40.519855, mean_q: -41.778458\n",
            " 1341273/10000000: episode: 6673, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -422.800, mean reward: -2.103 [-211.400, 77.000], mean action: 2.383 [0.000, 10.000], mean observation: 26.590 [0.002, 368.100], loss: 316.867493, mae: 40.760540, mean_q: -42.219208\n",
            " 1341474/10000000: episode: 6674, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -459.000, mean reward: -2.284 [-229.500, 53.900], mean action: 2.473 [0.000, 10.000], mean observation: 34.839 [0.001, 587.600], loss: 299.926971, mae: 40.926865, mean_q: -42.192993\n",
            " 1341675/10000000: episode: 6675, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -645.800, mean reward: -3.213 [-322.900, 30.000], mean action: 2.129 [0.000, 8.000], mean observation: 31.242 [0.001, 512.300], loss: 423.647156, mae: 40.722832, mean_q: -41.994011\n",
            " 1341876/10000000: episode: 6676, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -138.200, mean reward: -0.688 [-69.100, 68.300], mean action: 1.871 [0.000, 10.000], mean observation: 34.318 [0.003, 415.200], loss: 292.928101, mae: 40.947514, mean_q: -41.952339\n",
            " 1342077/10000000: episode: 6677, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -624.400, mean reward: -3.106 [-312.200, 40.800], mean action: 2.378 [0.000, 10.000], mean observation: 31.766 [0.000, 509.100], loss: 348.460968, mae: 40.072369, mean_q: -41.065773\n",
            " 1342278/10000000: episode: 6678, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -282.800, mean reward: -1.407 [-141.400, 77.600], mean action: 2.303 [0.000, 10.000], mean observation: 33.638 [0.001, 590.900], loss: 363.704987, mae: 39.558479, mean_q: -40.269928\n",
            " 1342479/10000000: episode: 6679, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -583.200, mean reward: -2.901 [-291.600, 54.500], mean action: 2.473 [0.000, 9.000], mean observation: 23.667 [0.000, 446.200], loss: 376.496277, mae: 38.967316, mean_q: -39.694874\n",
            " 1342680/10000000: episode: 6680, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: 216.200, mean reward: 1.076 [-9.000, 202.500], mean action: 1.711 [0.000, 9.000], mean observation: 36.568 [0.001, 504.300], loss: 460.657104, mae: 38.572937, mean_q: -39.175415\n",
            " 1342881/10000000: episode: 6681, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -60.000, mean reward: -0.299 [-30.000, 50.200], mean action: 1.632 [0.000, 8.000], mean observation: 32.530 [0.001, 510.800], loss: 251.897308, mae: 38.281406, mean_q: -38.627731\n",
            " 1343082/10000000: episode: 6682, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -328.000, mean reward: -1.632 [-164.000, 38.800], mean action: 1.433 [0.000, 8.000], mean observation: 33.477 [0.002, 487.500], loss: 302.476227, mae: 38.092354, mean_q: -37.698956\n",
            " 1343283/10000000: episode: 6683, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -187.200, mean reward: -0.931 [-93.600, 54.300], mean action: 1.602 [0.000, 9.000], mean observation: 34.020 [0.000, 637.000], loss: 393.430298, mae: 37.188850, mean_q: -37.104794\n",
            " 1343484/10000000: episode: 6684, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -444.400, mean reward: -2.211 [-222.200, 51.900], mean action: 2.323 [0.000, 9.000], mean observation: 30.198 [0.000, 640.400], loss: 446.391388, mae: 35.668587, mean_q: -36.334343\n",
            " 1343685/10000000: episode: 6685, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: 571.400, mean reward: 2.843 [-10.000, 507.200], mean action: 3.040 [0.000, 10.000], mean observation: 28.206 [0.001, 413.700], loss: 330.351532, mae: 35.102989, mean_q: -36.500267\n",
            " 1343886/10000000: episode: 6686, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -542.000, mean reward: -2.697 [-271.000, 107.100], mean action: 2.692 [0.000, 9.000], mean observation: 34.912 [0.001, 512.800], loss: 334.236450, mae: 35.948780, mean_q: -37.285980\n",
            " 1344087/10000000: episode: 6687, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 36.200, mean reward: 0.180 [-10.000, 211.500], mean action: 2.975 [0.000, 10.000], mean observation: 30.863 [0.000, 558.800], loss: 209.231674, mae: 35.862324, mean_q: -37.406059\n",
            " 1344288/10000000: episode: 6688, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -522.400, mean reward: -2.599 [-261.200, 70.200], mean action: 2.925 [0.000, 9.000], mean observation: 41.182 [0.001, 584.600], loss: 282.099823, mae: 35.892334, mean_q: -37.480278\n",
            " 1344489/10000000: episode: 6689, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -889.800, mean reward: -4.427 [-444.900, 68.500], mean action: 3.194 [0.000, 10.000], mean observation: 33.051 [0.000, 455.400], loss: 321.160522, mae: 35.937256, mean_q: -37.520157\n",
            " 1344690/10000000: episode: 6690, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -243.400, mean reward: -1.211 [-121.700, 163.200], mean action: 2.313 [0.000, 10.000], mean observation: 34.322 [0.002, 510.200], loss: 249.234436, mae: 36.184910, mean_q: -37.325676\n",
            " 1344891/10000000: episode: 6691, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -200.200, mean reward: -0.996 [-100.100, 79.200], mean action: 2.100 [0.000, 10.000], mean observation: 29.387 [0.001, 486.800], loss: 339.460663, mae: 36.589153, mean_q: -37.187607\n",
            " 1345092/10000000: episode: 6692, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -318.600, mean reward: -1.585 [-159.300, 106.300], mean action: 2.473 [0.000, 9.000], mean observation: 35.376 [0.000, 708.200], loss: 447.892151, mae: 36.680927, mean_q: -37.399643\n",
            " 1345293/10000000: episode: 6693, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -38.200, mean reward: -0.190 [-19.100, 122.500], mean action: 2.199 [0.000, 9.000], mean observation: 36.818 [0.002, 500.200], loss: 238.575012, mae: 37.293293, mean_q: -37.622566\n",
            " 1345494/10000000: episode: 6694, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: 575.000, mean reward: 2.861 [-9.000, 351.000], mean action: 2.323 [0.000, 9.000], mean observation: 36.251 [0.000, 440.500], loss: 285.805023, mae: 37.044273, mean_q: -37.551258\n",
            " 1345695/10000000: episode: 6695, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -308.000, mean reward: -1.532 [-154.000, 48.000], mean action: 2.159 [0.000, 10.000], mean observation: 33.568 [0.000, 566.800], loss: 318.853394, mae: 37.140568, mean_q: -37.089931\n",
            " 1345896/10000000: episode: 6696, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -398.600, mean reward: -1.983 [-199.300, 146.400], mean action: 2.473 [0.000, 9.000], mean observation: 33.685 [0.001, 571.900], loss: 353.501709, mae: 36.365799, mean_q: -36.577866\n",
            " 1346097/10000000: episode: 6697, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -467.600, mean reward: -2.326 [-233.800, 282.000], mean action: 3.139 [0.000, 9.000], mean observation: 28.861 [0.000, 490.900], loss: 329.159332, mae: 35.786610, mean_q: -36.804329\n",
            " 1346298/10000000: episode: 6698, duration: 1.498s, episode steps: 201, steps per second: 134, episode reward: 250.600, mean reward: 1.247 [-9.000, 430.400], mean action: 3.408 [0.000, 9.000], mean observation: 28.675 [0.001, 457.300], loss: 414.043762, mae: 35.095333, mean_q: -36.246586\n",
            " 1346499/10000000: episode: 6699, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -456.200, mean reward: -2.270 [-228.100, 161.700], mean action: 3.358 [0.000, 9.000], mean observation: 32.827 [0.000, 798.300], loss: 217.098495, mae: 34.655029, mean_q: -36.011608\n",
            " 1346700/10000000: episode: 6700, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -454.800, mean reward: -2.263 [-227.400, 230.400], mean action: 2.940 [0.000, 10.000], mean observation: 34.728 [0.001, 556.200], loss: 267.890106, mae: 34.765770, mean_q: -35.849220\n",
            " 1346901/10000000: episode: 6701, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -604.400, mean reward: -3.007 [-302.200, 161.600], mean action: 3.090 [0.000, 9.000], mean observation: 30.203 [0.000, 481.500], loss: 245.349228, mae: 34.786129, mean_q: -35.957638\n",
            " 1347102/10000000: episode: 6702, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -67.400, mean reward: -0.335 [-33.700, 140.000], mean action: 2.547 [0.000, 10.000], mean observation: 30.774 [0.000, 668.900], loss: 308.880493, mae: 35.169312, mean_q: -36.145390\n",
            " 1347303/10000000: episode: 6703, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: 34.000, mean reward: 0.169 [-8.000, 198.100], mean action: 2.920 [0.000, 8.000], mean observation: 32.362 [0.000, 506.300], loss: 378.446747, mae: 35.166901, mean_q: -36.509762\n",
            " 1347504/10000000: episode: 6704, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -769.600, mean reward: -3.829 [-384.800, 90.400], mean action: 3.383 [0.000, 10.000], mean observation: 36.179 [0.000, 605.900], loss: 401.989502, mae: 35.556244, mean_q: -37.016777\n",
            " 1347705/10000000: episode: 6705, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 774.200, mean reward: 3.852 [-10.000, 394.200], mean action: 2.761 [0.000, 10.000], mean observation: 35.248 [0.000, 650.000], loss: 316.409454, mae: 36.111469, mean_q: -37.589138\n",
            " 1347906/10000000: episode: 6706, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: 577.200, mean reward: 2.872 [-10.000, 720.000], mean action: 3.348 [0.000, 10.000], mean observation: 33.753 [0.000, 574.000], loss: 170.630280, mae: 36.126575, mean_q: -37.621811\n",
            " 1348107/10000000: episode: 6707, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -81.600, mean reward: -0.406 [-40.800, 134.400], mean action: 2.761 [0.000, 10.000], mean observation: 31.908 [0.002, 420.100], loss: 303.590973, mae: 37.102943, mean_q: -38.143391\n",
            " 1348308/10000000: episode: 6708, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: 383.800, mean reward: 1.909 [-10.000, 257.200], mean action: 2.577 [0.000, 10.000], mean observation: 38.313 [0.001, 495.500], loss: 370.650391, mae: 36.726048, mean_q: -37.979908\n",
            " 1348509/10000000: episode: 6709, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -812.000, mean reward: -4.040 [-406.000, 77.400], mean action: 3.274 [0.000, 9.000], mean observation: 33.253 [0.001, 510.400], loss: 362.184052, mae: 36.716183, mean_q: -38.283012\n",
            " 1348710/10000000: episode: 6710, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: 601.800, mean reward: 2.994 [-9.000, 375.400], mean action: 2.721 [0.000, 9.000], mean observation: 36.255 [0.000, 508.900], loss: 293.871796, mae: 37.046024, mean_q: -37.886761\n",
            " 1348911/10000000: episode: 6711, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -434.200, mean reward: -2.160 [-217.100, 64.200], mean action: 2.831 [0.000, 9.000], mean observation: 27.846 [0.001, 618.400], loss: 331.547119, mae: 37.432995, mean_q: -38.463554\n",
            " 1349112/10000000: episode: 6712, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 374.000, mean reward: 1.861 [-10.000, 324.100], mean action: 3.239 [0.000, 10.000], mean observation: 33.580 [0.001, 519.700], loss: 301.897552, mae: 37.565849, mean_q: -38.541367\n",
            " 1349313/10000000: episode: 6713, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 186.000, mean reward: 0.925 [-9.000, 220.800], mean action: 3.124 [0.000, 9.000], mean observation: 32.654 [0.001, 512.000], loss: 349.731110, mae: 38.511345, mean_q: -39.683880\n",
            " 1349514/10000000: episode: 6714, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -628.200, mean reward: -3.125 [-314.100, 48.000], mean action: 3.284 [0.000, 8.000], mean observation: 33.368 [0.000, 586.000], loss: 306.266144, mae: 38.139236, mean_q: -39.305988\n",
            " 1349715/10000000: episode: 6715, duration: 1.502s, episode steps: 201, steps per second: 134, episode reward: 242.800, mean reward: 1.208 [-9.000, 232.500], mean action: 2.995 [0.000, 10.000], mean observation: 29.414 [0.002, 431.000], loss: 395.336060, mae: 38.587696, mean_q: -39.291195\n",
            " 1349916/10000000: episode: 6716, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -514.200, mean reward: -2.558 [-257.100, 123.200], mean action: 2.891 [0.000, 9.000], mean observation: 30.093 [0.001, 459.200], loss: 236.993896, mae: 38.394527, mean_q: -39.314007\n",
            " 1350117/10000000: episode: 6717, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: 436.000, mean reward: 2.169 [-10.000, 686.400], mean action: 3.413 [0.000, 10.000], mean observation: 34.440 [0.003, 422.300], loss: 501.986328, mae: 38.223240, mean_q: -39.812473\n",
            " 1350318/10000000: episode: 6718, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -1168.800, mean reward: -5.815 [-584.400, 109.800], mean action: 3.995 [0.000, 10.000], mean observation: 32.179 [0.001, 497.500], loss: 339.244476, mae: 37.762085, mean_q: -39.621651\n",
            " 1350519/10000000: episode: 6719, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -692.400, mean reward: -3.445 [-346.200, 52.800], mean action: 3.030 [0.000, 10.000], mean observation: 29.365 [0.002, 430.700], loss: 328.852814, mae: 38.851059, mean_q: -40.367344\n",
            " 1350720/10000000: episode: 6720, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -554.600, mean reward: -2.759 [-277.300, 71.900], mean action: 2.562 [0.000, 9.000], mean observation: 34.384 [0.000, 552.400], loss: 357.081085, mae: 39.418186, mean_q: -40.330189\n",
            " 1350921/10000000: episode: 6721, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -732.800, mean reward: -3.646 [-366.400, 38.100], mean action: 2.796 [0.000, 10.000], mean observation: 34.603 [0.000, 720.900], loss: 352.838470, mae: 38.987133, mean_q: -40.023933\n",
            " 1351122/10000000: episode: 6722, duration: 1.527s, episode steps: 201, steps per second: 132, episode reward: -356.800, mean reward: -1.775 [-178.400, 76.400], mean action: 3.149 [0.000, 10.000], mean observation: 29.775 [0.001, 500.800], loss: 350.660614, mae: 38.517761, mean_q: -40.058155\n",
            " 1351323/10000000: episode: 6723, duration: 1.505s, episode steps: 201, steps per second: 134, episode reward: 936.800, mean reward: 4.661 [-10.000, 584.800], mean action: 2.756 [0.000, 10.000], mean observation: 31.907 [0.001, 418.500], loss: 285.179993, mae: 38.601631, mean_q: -39.931393\n",
            " 1351524/10000000: episode: 6724, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: 32.800, mean reward: 0.163 [-9.000, 118.400], mean action: 2.900 [0.000, 9.000], mean observation: 31.980 [0.000, 441.700], loss: 255.149811, mae: 38.424774, mean_q: -39.565090\n",
            " 1351725/10000000: episode: 6725, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -605.600, mean reward: -3.013 [-302.800, 42.000], mean action: 2.418 [0.000, 9.000], mean observation: 29.485 [0.002, 387.100], loss: 347.096863, mae: 38.792221, mean_q: -39.747486\n",
            " 1351926/10000000: episode: 6726, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -1229.200, mean reward: -6.115 [-614.600, 65.400], mean action: 3.682 [0.000, 9.000], mean observation: 35.622 [0.002, 543.500], loss: 325.622772, mae: 37.860844, mean_q: -39.870712\n",
            " 1352127/10000000: episode: 6727, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -521.600, mean reward: -2.595 [-260.800, 52.800], mean action: 2.682 [0.000, 10.000], mean observation: 32.643 [0.001, 562.800], loss: 420.864105, mae: 39.663040, mean_q: -40.825443\n",
            " 1352328/10000000: episode: 6728, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -178.000, mean reward: -0.886 [-89.000, 217.600], mean action: 2.413 [0.000, 9.000], mean observation: 36.195 [0.000, 693.300], loss: 395.193451, mae: 39.419483, mean_q: -40.599125\n",
            " 1352529/10000000: episode: 6729, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -649.400, mean reward: -3.231 [-324.700, 115.500], mean action: 3.035 [0.000, 10.000], mean observation: 36.457 [0.000, 727.600], loss: 383.300598, mae: 39.170788, mean_q: -40.523247\n",
            " 1352730/10000000: episode: 6730, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -736.800, mean reward: -3.666 [-368.400, 24.600], mean action: 2.463 [0.000, 9.000], mean observation: 35.718 [0.002, 479.600], loss: 324.374512, mae: 40.251621, mean_q: -41.060173\n",
            " 1352931/10000000: episode: 6731, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 388.000, mean reward: 1.930 [-9.000, 194.000], mean action: 2.428 [0.000, 9.000], mean observation: 33.843 [0.001, 606.200], loss: 187.439514, mae: 40.794163, mean_q: -41.750759\n",
            " 1353132/10000000: episode: 6732, duration: 1.596s, episode steps: 201, steps per second: 126, episode reward: -674.600, mean reward: -3.356 [-337.300, 133.400], mean action: 3.746 [0.000, 10.000], mean observation: 31.699 [0.001, 597.900], loss: 477.242462, mae: 39.356567, mean_q: -40.993614\n",
            " 1353333/10000000: episode: 6733, duration: 1.756s, episode steps: 201, steps per second: 114, episode reward: -865.400, mean reward: -4.305 [-432.700, 154.400], mean action: 4.184 [0.000, 9.000], mean observation: 32.524 [0.004, 482.900], loss: 386.898041, mae: 39.051334, mean_q: -41.077202\n",
            " 1353534/10000000: episode: 6734, duration: 1.793s, episode steps: 201, steps per second: 112, episode reward: -344.600, mean reward: -1.714 [-172.300, 96.800], mean action: 3.886 [0.000, 10.000], mean observation: 27.609 [0.002, 435.000], loss: 303.618591, mae: 39.713688, mean_q: -41.876717\n",
            " 1353735/10000000: episode: 6735, duration: 1.762s, episode steps: 201, steps per second: 114, episode reward: -926.200, mean reward: -4.608 [-463.100, 45.600], mean action: 3.527 [0.000, 10.000], mean observation: 32.659 [0.001, 697.100], loss: 399.733978, mae: 40.093899, mean_q: -42.009483\n",
            " 1353936/10000000: episode: 6736, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -979.400, mean reward: -4.873 [-489.700, 38.500], mean action: 2.980 [0.000, 10.000], mean observation: 31.166 [0.001, 562.800], loss: 414.919769, mae: 40.298519, mean_q: -42.374310\n",
            " 1354137/10000000: episode: 6737, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -714.800, mean reward: -3.556 [-357.400, 20.400], mean action: 2.095 [0.000, 10.000], mean observation: 33.553 [0.000, 458.900], loss: 354.067078, mae: 41.212948, mean_q: -42.833752\n",
            " 1354338/10000000: episode: 6738, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -866.200, mean reward: -4.309 [-433.100, 19.200], mean action: 2.622 [0.000, 9.000], mean observation: 33.474 [0.002, 522.200], loss: 314.692993, mae: 41.145611, mean_q: -43.056057\n",
            " 1354539/10000000: episode: 6739, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: 1381.400, mean reward: 6.873 [-10.000, 690.700], mean action: 2.652 [0.000, 10.000], mean observation: 29.833 [0.000, 527.200], loss: 309.880310, mae: 40.976337, mean_q: -42.869781\n",
            " 1354740/10000000: episode: 6740, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -487.200, mean reward: -2.424 [-243.600, 138.000], mean action: 2.751 [0.000, 10.000], mean observation: 34.147 [0.000, 426.600], loss: 288.446075, mae: 41.703930, mean_q: -43.806183\n",
            " 1354941/10000000: episode: 6741, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -900.000, mean reward: -4.478 [-450.000, 66.500], mean action: 3.070 [0.000, 10.000], mean observation: 32.769 [0.003, 451.200], loss: 437.256683, mae: 41.977932, mean_q: -44.199783\n",
            " 1355142/10000000: episode: 6742, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: 40.000, mean reward: 0.199 [-10.000, 255.000], mean action: 3.507 [0.000, 10.000], mean observation: 32.016 [0.000, 765.000], loss: 275.123657, mae: 42.486961, mean_q: -44.538692\n",
            " 1355343/10000000: episode: 6743, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 2016.000, mean reward: 10.030 [-10.000, 1008.000], mean action: 3.005 [0.000, 10.000], mean observation: 33.430 [0.002, 439.000], loss: 209.522812, mae: 42.970100, mean_q: -44.615963\n",
            " 1355544/10000000: episode: 6744, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: 52.600, mean reward: 0.262 [-10.000, 261.600], mean action: 1.886 [0.000, 10.000], mean observation: 39.611 [0.000, 793.800], loss: 391.785980, mae: 42.691055, mean_q: -44.041744\n",
            " 1355745/10000000: episode: 6745, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -464.600, mean reward: -2.311 [-232.300, 47.400], mean action: 2.204 [0.000, 10.000], mean observation: 34.064 [0.000, 633.800], loss: 301.012970, mae: 42.191555, mean_q: -43.881023\n",
            " 1355946/10000000: episode: 6746, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -244.200, mean reward: -1.215 [-122.100, 146.700], mean action: 2.010 [0.000, 10.000], mean observation: 40.673 [0.000, 701.500], loss: 222.006256, mae: 41.644695, mean_q: -43.075127\n",
            " 1356147/10000000: episode: 6747, duration: 1.535s, episode steps: 201, steps per second: 131, episode reward: 1117.600, mean reward: 5.560 [-10.000, 558.800], mean action: 2.587 [0.000, 10.000], mean observation: 34.005 [0.002, 496.200], loss: 332.975372, mae: 41.155785, mean_q: -43.022354\n",
            " 1356348/10000000: episode: 6748, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: 968.400, mean reward: 4.818 [-10.000, 628.200], mean action: 3.453 [0.000, 10.000], mean observation: 35.780 [0.000, 783.800], loss: 240.729401, mae: 41.073349, mean_q: -43.444733\n",
            " 1356549/10000000: episode: 6749, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -150.200, mean reward: -0.747 [-75.100, 80.400], mean action: 2.572 [0.000, 10.000], mean observation: 37.516 [0.002, 515.200], loss: 256.178986, mae: 40.970772, mean_q: -42.504532\n",
            " 1356750/10000000: episode: 6750, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -259.800, mean reward: -1.293 [-129.900, 80.400], mean action: 2.194 [0.000, 10.000], mean observation: 34.302 [0.003, 430.300], loss: 392.393341, mae: 41.329586, mean_q: -42.554920\n",
            " 1356951/10000000: episode: 6751, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -209.600, mean reward: -1.043 [-104.800, 205.200], mean action: 2.682 [0.000, 9.000], mean observation: 37.802 [0.000, 673.700], loss: 376.367859, mae: 41.526165, mean_q: -43.003742\n",
            " 1357152/10000000: episode: 6752, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -250.400, mean reward: -1.246 [-125.200, 204.000], mean action: 4.214 [0.000, 10.000], mean observation: 33.430 [0.002, 446.200], loss: 439.155701, mae: 41.177414, mean_q: -42.508312\n",
            " 1357353/10000000: episode: 6753, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -806.400, mean reward: -4.012 [-403.200, 161.400], mean action: 4.229 [0.000, 10.000], mean observation: 36.899 [0.001, 459.000], loss: 527.135681, mae: 39.369251, mean_q: -40.453983\n",
            " 1357554/10000000: episode: 6754, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -398.600, mean reward: -1.983 [-199.300, 564.000], mean action: 4.209 [0.000, 10.000], mean observation: 34.668 [0.001, 558.800], loss: 412.843231, mae: 37.836365, mean_q: -39.170330\n",
            " 1357755/10000000: episode: 6755, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: 775.200, mean reward: 3.857 [-10.000, 522.600], mean action: 4.682 [0.000, 10.000], mean observation: 32.975 [0.000, 933.200], loss: 449.720062, mae: 36.650368, mean_q: -37.985142\n",
            " 1357956/10000000: episode: 6756, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -396.600, mean reward: -1.973 [-198.300, 91.700], mean action: 3.353 [0.000, 10.000], mean observation: 31.648 [0.000, 684.900], loss: 397.405640, mae: 36.839443, mean_q: -38.201763\n",
            " 1358157/10000000: episode: 6757, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -117.200, mean reward: -0.583 [-58.600, 175.200], mean action: 3.050 [0.000, 8.000], mean observation: 31.215 [0.004, 507.800], loss: 323.779785, mae: 37.190990, mean_q: -38.771614\n",
            " 1358358/10000000: episode: 6758, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -163.000, mean reward: -0.811 [-81.500, 143.100], mean action: 2.313 [0.000, 10.000], mean observation: 34.610 [0.000, 556.700], loss: 473.732819, mae: 38.067253, mean_q: -38.786922\n",
            " 1358559/10000000: episode: 6759, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -450.000, mean reward: -2.239 [-225.000, 50.000], mean action: 1.716 [0.000, 9.000], mean observation: 37.064 [0.000, 607.900], loss: 344.903748, mae: 37.886986, mean_q: -38.270435\n",
            " 1358760/10000000: episode: 6760, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -265.600, mean reward: -1.321 [-132.800, 140.500], mean action: 2.249 [0.000, 10.000], mean observation: 32.700 [0.001, 423.500], loss: 261.140717, mae: 36.853516, mean_q: -37.504230\n",
            " 1358961/10000000: episode: 6761, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -479.800, mean reward: -2.387 [-239.900, 42.400], mean action: 2.299 [0.000, 8.000], mean observation: 35.450 [0.000, 528.400], loss: 288.220428, mae: 36.356270, mean_q: -37.268105\n",
            " 1359162/10000000: episode: 6762, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 524.800, mean reward: 2.611 [-10.000, 332.500], mean action: 2.776 [0.000, 10.000], mean observation: 29.932 [0.003, 464.200], loss: 365.900177, mae: 36.085258, mean_q: -37.142971\n",
            " 1359363/10000000: episode: 6763, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -440.000, mean reward: -2.189 [-220.000, 82.800], mean action: 3.517 [0.000, 10.000], mean observation: 31.882 [0.000, 818.500], loss: 493.333557, mae: 35.837822, mean_q: -37.080669\n",
            " 1359564/10000000: episode: 6764, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -1434.200, mean reward: -7.135 [-717.100, 16.200], mean action: 4.015 [0.000, 10.000], mean observation: 30.190 [0.002, 449.500], loss: 440.540649, mae: 35.195534, mean_q: -36.306683\n",
            " 1359765/10000000: episode: 6765, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -346.000, mean reward: -1.721 [-173.000, 225.000], mean action: 3.920 [0.000, 9.000], mean observation: 33.987 [0.001, 619.000], loss: 248.411835, mae: 34.583714, mean_q: -35.598328\n",
            " 1359966/10000000: episode: 6766, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: 1.800, mean reward: 0.009 [-10.000, 287.200], mean action: 3.114 [0.000, 10.000], mean observation: 35.763 [0.002, 402.500], loss: 454.975830, mae: 34.769596, mean_q: -35.979458\n",
            " 1360167/10000000: episode: 6767, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -255.800, mean reward: -1.273 [-127.900, 188.700], mean action: 3.861 [0.000, 10.000], mean observation: 29.653 [0.000, 500.900], loss: 352.537170, mae: 34.357170, mean_q: -35.577324\n",
            " 1360368/10000000: episode: 6768, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -321.600, mean reward: -1.600 [-160.800, 102.000], mean action: 2.811 [0.000, 8.000], mean observation: 40.646 [0.001, 501.600], loss: 410.247253, mae: 35.096691, mean_q: -35.876556\n",
            " 1360569/10000000: episode: 6769, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 827.000, mean reward: 4.114 [-10.000, 787.200], mean action: 3.169 [0.000, 10.000], mean observation: 32.835 [0.000, 590.900], loss: 361.251465, mae: 34.473824, mean_q: -35.248734\n",
            " 1360770/10000000: episode: 6770, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 583.800, mean reward: 2.904 [-10.000, 593.400], mean action: 3.756 [0.000, 10.000], mean observation: 31.559 [0.000, 747.100], loss: 340.662170, mae: 34.066956, mean_q: -34.734390\n",
            " 1360971/10000000: episode: 6771, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -688.000, mean reward: -3.423 [-344.000, 97.800], mean action: 3.005 [0.000, 8.000], mean observation: 35.630 [0.001, 502.600], loss: 265.794891, mae: 34.358486, mean_q: -35.379459\n",
            " 1361172/10000000: episode: 6772, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: 181.800, mean reward: 0.904 [-8.000, 259.200], mean action: 3.010 [0.000, 8.000], mean observation: 28.211 [0.000, 534.800], loss: 342.728638, mae: 34.688370, mean_q: -35.556370\n",
            " 1361373/10000000: episode: 6773, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -496.200, mean reward: -2.469 [-248.100, 82.500], mean action: 2.154 [0.000, 8.000], mean observation: 30.558 [0.000, 580.300], loss: 363.601440, mae: 34.566189, mean_q: -34.942825\n",
            " 1361574/10000000: episode: 6774, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -666.000, mean reward: -3.313 [-333.000, 25.600], mean action: 2.403 [0.000, 10.000], mean observation: 35.725 [0.000, 522.100], loss: 415.808472, mae: 34.344955, mean_q: -35.409443\n",
            " 1361775/10000000: episode: 6775, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -133.000, mean reward: -0.662 [-66.500, 271.000], mean action: 3.861 [0.000, 10.000], mean observation: 29.114 [0.001, 589.600], loss: 434.948456, mae: 33.437195, mean_q: -34.762287\n",
            " 1361976/10000000: episode: 6776, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -531.000, mean reward: -2.642 [-265.500, 172.800], mean action: 4.443 [0.000, 10.000], mean observation: 33.539 [0.000, 813.800], loss: 263.053619, mae: 33.536282, mean_q: -34.723022\n",
            " 1362177/10000000: episode: 6777, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -590.000, mean reward: -2.935 [-295.000, 246.000], mean action: 3.393 [0.000, 10.000], mean observation: 39.666 [0.000, 621.900], loss: 599.624756, mae: 33.699623, mean_q: -34.856079\n",
            " 1362378/10000000: episode: 6778, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -424.400, mean reward: -2.111 [-212.200, 247.200], mean action: 3.338 [0.000, 10.000], mean observation: 28.648 [0.002, 632.400], loss: 258.307220, mae: 33.524399, mean_q: -34.738476\n",
            " 1362579/10000000: episode: 6779, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -923.800, mean reward: -4.596 [-461.900, 44.000], mean action: 2.970 [0.000, 10.000], mean observation: 34.335 [0.002, 519.900], loss: 313.829681, mae: 33.871170, mean_q: -35.166042\n",
            " 1362780/10000000: episode: 6780, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -735.800, mean reward: -3.661 [-367.900, 71.000], mean action: 3.274 [0.000, 10.000], mean observation: 35.762 [0.001, 518.800], loss: 409.554962, mae: 34.338909, mean_q: -35.667706\n",
            " 1362981/10000000: episode: 6781, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -133.600, mean reward: -0.665 [-66.800, 158.400], mean action: 2.662 [0.000, 10.000], mean observation: 43.485 [0.001, 554.400], loss: 405.522827, mae: 35.515884, mean_q: -36.550190\n",
            " 1363182/10000000: episode: 6782, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -7.200, mean reward: -0.036 [-10.000, 114.200], mean action: 2.706 [0.000, 10.000], mean observation: 32.641 [0.001, 627.200], loss: 424.786224, mae: 35.286404, mean_q: -36.656883\n",
            " 1363383/10000000: episode: 6783, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: 289.400, mean reward: 1.440 [-10.000, 185.100], mean action: 2.756 [0.000, 10.000], mean observation: 39.756 [0.001, 501.700], loss: 406.121338, mae: 35.804771, mean_q: -36.899857\n",
            " 1363584/10000000: episode: 6784, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: -568.200, mean reward: -2.827 [-284.100, 68.400], mean action: 2.891 [0.000, 10.000], mean observation: 32.270 [0.000, 604.500], loss: 283.466583, mae: 35.625904, mean_q: -36.607700\n",
            " 1363785/10000000: episode: 6785, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -561.400, mean reward: -2.793 [-280.700, 107.400], mean action: 2.930 [0.000, 10.000], mean observation: 30.795 [0.001, 464.600], loss: 237.436630, mae: 35.355244, mean_q: -35.914936\n",
            " 1363986/10000000: episode: 6786, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -473.000, mean reward: -2.353 [-236.500, 89.000], mean action: 2.851 [0.000, 10.000], mean observation: 27.867 [0.001, 531.100], loss: 403.806976, mae: 34.764042, mean_q: -35.549366\n",
            " 1364187/10000000: episode: 6787, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: 73.600, mean reward: 0.366 [-10.000, 288.000], mean action: 2.149 [0.000, 10.000], mean observation: 36.389 [0.001, 507.400], loss: 339.728088, mae: 34.802746, mean_q: -35.215237\n",
            " 1364388/10000000: episode: 6788, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -602.600, mean reward: -2.998 [-301.300, 42.600], mean action: 2.522 [0.000, 10.000], mean observation: 35.123 [0.000, 784.200], loss: 323.956696, mae: 34.452633, mean_q: -35.049950\n",
            " 1364589/10000000: episode: 6789, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -685.200, mean reward: -3.409 [-342.600, 109.600], mean action: 3.239 [0.000, 10.000], mean observation: 37.628 [0.000, 722.600], loss: 270.804565, mae: 33.760872, mean_q: -34.732525\n",
            " 1364790/10000000: episode: 6790, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 34.000, mean reward: 0.169 [-10.000, 336.500], mean action: 3.373 [0.000, 10.000], mean observation: 32.075 [0.000, 507.700], loss: 292.096527, mae: 33.399254, mean_q: -34.521770\n",
            " 1364991/10000000: episode: 6791, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: -192.800, mean reward: -0.959 [-96.400, 302.400], mean action: 3.289 [0.000, 10.000], mean observation: 33.688 [0.001, 542.200], loss: 227.470963, mae: 33.715847, mean_q: -34.906548\n",
            " 1365192/10000000: episode: 6792, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -410.000, mean reward: -2.040 [-205.000, 71.600], mean action: 3.662 [0.000, 10.000], mean observation: 31.530 [0.000, 527.300], loss: 452.475006, mae: 33.567635, mean_q: -34.616203\n",
            " 1365393/10000000: episode: 6793, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -918.800, mean reward: -4.571 [-459.400, 16.800], mean action: 2.856 [0.000, 10.000], mean observation: 30.529 [0.001, 542.400], loss: 339.101532, mae: 34.100376, mean_q: -35.088001\n",
            " 1365594/10000000: episode: 6794, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -549.000, mean reward: -2.731 [-274.500, 92.400], mean action: 2.662 [0.000, 10.000], mean observation: 38.891 [0.000, 694.400], loss: 444.342987, mae: 34.079460, mean_q: -34.786762\n",
            " 1365795/10000000: episode: 6795, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -957.000, mean reward: -4.761 [-478.500, 88.800], mean action: 3.383 [0.000, 10.000], mean observation: 36.669 [0.000, 598.300], loss: 230.993240, mae: 33.832695, mean_q: -34.867500\n",
            " 1365996/10000000: episode: 6796, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -399.600, mean reward: -1.988 [-199.800, 147.000], mean action: 2.398 [0.000, 10.000], mean observation: 29.828 [0.001, 555.700], loss: 257.173492, mae: 33.923412, mean_q: -34.111458\n",
            " 1366197/10000000: episode: 6797, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -158.600, mean reward: -0.789 [-79.300, 282.600], mean action: 2.721 [0.000, 10.000], mean observation: 31.699 [0.000, 669.900], loss: 303.888794, mae: 33.659557, mean_q: -34.436943\n",
            " 1366398/10000000: episode: 6798, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -536.600, mean reward: -2.670 [-268.300, 122.400], mean action: 2.652 [0.000, 10.000], mean observation: 43.769 [0.001, 662.900], loss: 281.735504, mae: 33.752171, mean_q: -34.509144\n",
            " 1366599/10000000: episode: 6799, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 232.000, mean reward: 1.154 [-10.000, 173.600], mean action: 2.308 [0.000, 10.000], mean observation: 32.399 [0.002, 441.800], loss: 239.203690, mae: 34.125332, mean_q: -35.076538\n",
            " 1366800/10000000: episode: 6800, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -41.400, mean reward: -0.206 [-20.700, 294.400], mean action: 2.433 [0.000, 10.000], mean observation: 37.414 [0.001, 482.600], loss: 388.103180, mae: 34.138817, mean_q: -34.803120\n",
            " 1367001/10000000: episode: 6801, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -161.600, mean reward: -0.804 [-80.800, 222.000], mean action: 3.040 [0.000, 10.000], mean observation: 34.448 [0.000, 654.700], loss: 304.184113, mae: 34.154228, mean_q: -34.793186\n",
            " 1367202/10000000: episode: 6802, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: -415.200, mean reward: -2.066 [-207.600, 81.200], mean action: 2.552 [0.000, 10.000], mean observation: 32.790 [0.000, 706.400], loss: 244.069656, mae: 34.283165, mean_q: -35.055309\n",
            " 1367403/10000000: episode: 6803, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -370.200, mean reward: -1.842 [-185.100, 137.600], mean action: 2.448 [0.000, 10.000], mean observation: 33.138 [0.001, 480.700], loss: 336.266998, mae: 34.497429, mean_q: -35.455452\n",
            " 1367604/10000000: episode: 6804, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -250.200, mean reward: -1.245 [-125.100, 297.000], mean action: 2.826 [0.000, 10.000], mean observation: 31.845 [0.001, 463.400], loss: 208.800659, mae: 34.566643, mean_q: -35.608040\n",
            " 1367805/10000000: episode: 6805, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: 608.400, mean reward: 3.027 [-8.000, 654.400], mean action: 3.020 [0.000, 8.000], mean observation: 31.487 [0.001, 676.900], loss: 293.757507, mae: 34.582260, mean_q: -35.698265\n",
            " 1368006/10000000: episode: 6806, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -669.400, mean reward: -3.330 [-334.700, 79.200], mean action: 3.229 [0.000, 10.000], mean observation: 37.382 [0.001, 714.300], loss: 259.059570, mae: 35.011406, mean_q: -36.321396\n",
            " 1368207/10000000: episode: 6807, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -684.000, mean reward: -3.403 [-342.000, 79.600], mean action: 3.473 [0.000, 10.000], mean observation: 38.025 [0.000, 601.500], loss: 364.789948, mae: 35.494957, mean_q: -36.684219\n",
            " 1368408/10000000: episode: 6808, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -211.400, mean reward: -1.052 [-105.700, 198.600], mean action: 3.791 [0.000, 10.000], mean observation: 33.535 [0.000, 582.800], loss: 380.594421, mae: 35.584843, mean_q: -37.248638\n",
            " 1368609/10000000: episode: 6809, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -374.600, mean reward: -1.864 [-187.300, 87.200], mean action: 3.522 [0.000, 10.000], mean observation: 35.753 [0.001, 498.800], loss: 414.871826, mae: 36.788887, mean_q: -38.279247\n",
            " 1368810/10000000: episode: 6810, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -491.400, mean reward: -2.445 [-245.700, 76.500], mean action: 2.930 [0.000, 10.000], mean observation: 27.634 [0.002, 508.500], loss: 271.978546, mae: 37.064102, mean_q: -38.247669\n",
            " 1369011/10000000: episode: 6811, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -306.600, mean reward: -1.525 [-153.300, 145.600], mean action: 3.920 [0.000, 10.000], mean observation: 30.751 [0.004, 499.200], loss: 346.052246, mae: 36.491661, mean_q: -37.692436\n",
            " 1369212/10000000: episode: 6812, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 179.800, mean reward: 0.895 [-8.000, 558.600], mean action: 3.184 [0.000, 8.000], mean observation: 33.785 [0.000, 376.900], loss: 368.135101, mae: 37.052055, mean_q: -37.872852\n",
            " 1369413/10000000: episode: 6813, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -1082.200, mean reward: -5.384 [-541.100, 50.000], mean action: 3.597 [0.000, 10.000], mean observation: 36.705 [0.001, 630.500], loss: 437.815765, mae: 36.630978, mean_q: -37.712933\n",
            " 1369614/10000000: episode: 6814, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: 678.600, mean reward: 3.376 [-8.000, 360.600], mean action: 3.458 [0.000, 8.000], mean observation: 30.650 [0.001, 518.600], loss: 377.195831, mae: 36.322105, mean_q: -37.124592\n",
            " 1369815/10000000: episode: 6815, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -1149.200, mean reward: -5.717 [-574.600, 59.400], mean action: 3.881 [0.000, 10.000], mean observation: 36.757 [0.000, 562.200], loss: 246.428177, mae: 35.422241, mean_q: -36.121075\n",
            " 1370016/10000000: episode: 6816, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -1.800, mean reward: -0.009 [-10.000, 152.800], mean action: 3.547 [0.000, 10.000], mean observation: 31.967 [0.002, 536.400], loss: 276.819489, mae: 35.651627, mean_q: -36.127293\n",
            " 1370217/10000000: episode: 6817, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -943.200, mean reward: -4.693 [-471.600, 69.600], mean action: 3.577 [0.000, 10.000], mean observation: 32.294 [0.002, 637.200], loss: 276.008789, mae: 35.735508, mean_q: -36.392048\n",
            " 1370418/10000000: episode: 6818, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: 152.400, mean reward: 0.758 [-9.000, 333.600], mean action: 3.199 [0.000, 10.000], mean observation: 28.133 [0.002, 607.700], loss: 442.493561, mae: 35.905685, mean_q: -36.501606\n",
            " 1370619/10000000: episode: 6819, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -424.000, mean reward: -2.109 [-212.000, 152.800], mean action: 3.527 [0.000, 8.000], mean observation: 28.452 [0.001, 497.800], loss: 264.779877, mae: 35.611721, mean_q: -36.459469\n",
            " 1370820/10000000: episode: 6820, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -477.600, mean reward: -2.376 [-238.800, 133.600], mean action: 3.687 [0.000, 10.000], mean observation: 36.244 [0.001, 462.800], loss: 308.219391, mae: 36.197960, mean_q: -37.129753\n",
            " 1371021/10000000: episode: 6821, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -604.000, mean reward: -3.005 [-302.000, 140.000], mean action: 4.985 [0.000, 10.000], mean observation: 38.160 [0.000, 772.000], loss: 472.182526, mae: 35.350147, mean_q: -36.509117\n",
            " 1371222/10000000: episode: 6822, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -403.400, mean reward: -2.007 [-201.700, 304.800], mean action: 5.493 [0.000, 10.000], mean observation: 28.491 [0.000, 471.500], loss: 320.084045, mae: 34.643063, mean_q: -35.382099\n",
            " 1371423/10000000: episode: 6823, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -681.800, mean reward: -3.392 [-340.900, 120.000], mean action: 3.438 [0.000, 10.000], mean observation: 32.392 [0.000, 558.800], loss: 196.981384, mae: 35.134487, mean_q: -36.133801\n",
            " 1371624/10000000: episode: 6824, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: 232.200, mean reward: 1.155 [-10.000, 147.000], mean action: 3.338 [0.000, 10.000], mean observation: 34.449 [0.001, 645.600], loss: 264.705505, mae: 35.297760, mean_q: -36.175419\n",
            " 1371825/10000000: episode: 6825, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -1054.800, mean reward: -5.248 [-527.400, 30.400], mean action: 3.348 [0.000, 10.000], mean observation: 33.356 [0.001, 487.300], loss: 422.009796, mae: 35.649204, mean_q: -36.456715\n",
            " 1372026/10000000: episode: 6826, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: 103.600, mean reward: 0.515 [-10.000, 70.800], mean action: 3.045 [0.000, 10.000], mean observation: 31.017 [0.000, 555.400], loss: 302.902374, mae: 35.500767, mean_q: -36.230049\n",
            " 1372227/10000000: episode: 6827, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -225.600, mean reward: -1.122 [-112.800, 275.200], mean action: 2.517 [0.000, 10.000], mean observation: 36.388 [0.000, 553.100], loss: 259.069153, mae: 35.380844, mean_q: -35.781368\n",
            " 1372428/10000000: episode: 6828, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -698.200, mean reward: -3.474 [-349.100, 106.200], mean action: 3.095 [0.000, 10.000], mean observation: 34.001 [0.000, 668.100], loss: 322.179504, mae: 35.314930, mean_q: -35.721268\n",
            " 1372629/10000000: episode: 6829, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -698.400, mean reward: -3.475 [-349.200, 112.800], mean action: 3.104 [0.000, 10.000], mean observation: 34.585 [0.000, 489.400], loss: 340.701111, mae: 35.673439, mean_q: -36.198795\n",
            " 1372830/10000000: episode: 6830, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -228.000, mean reward: -1.134 [-114.000, 95.400], mean action: 3.338 [0.000, 10.000], mean observation: 28.607 [0.000, 513.300], loss: 236.183502, mae: 35.754528, mean_q: -36.311684\n",
            " 1373031/10000000: episode: 6831, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -403.400, mean reward: -2.007 [-201.700, 158.000], mean action: 3.234 [0.000, 10.000], mean observation: 33.706 [0.000, 587.900], loss: 292.453094, mae: 35.273102, mean_q: -35.800217\n",
            " 1373232/10000000: episode: 6832, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -958.000, mean reward: -4.766 [-479.000, 107.800], mean action: 4.010 [0.000, 10.000], mean observation: 36.450 [0.000, 606.600], loss: 421.870148, mae: 34.868202, mean_q: -35.562447\n",
            " 1373433/10000000: episode: 6833, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -447.000, mean reward: -2.224 [-223.500, 370.200], mean action: 3.866 [0.000, 10.000], mean observation: 34.869 [0.001, 578.200], loss: 358.692932, mae: 34.650707, mean_q: -35.596027\n",
            " 1373634/10000000: episode: 6834, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -814.200, mean reward: -4.051 [-407.100, 186.000], mean action: 3.637 [0.000, 10.000], mean observation: 30.087 [0.003, 461.100], loss: 415.277863, mae: 34.977222, mean_q: -36.032551\n",
            " 1373835/10000000: episode: 6835, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -726.000, mean reward: -3.612 [-363.000, 36.800], mean action: 3.090 [0.000, 10.000], mean observation: 37.210 [0.000, 497.600], loss: 364.470886, mae: 35.112957, mean_q: -35.810825\n",
            " 1374036/10000000: episode: 6836, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -363.600, mean reward: -1.809 [-181.800, 85.000], mean action: 3.075 [0.000, 10.000], mean observation: 34.148 [0.001, 675.300], loss: 347.020050, mae: 35.257393, mean_q: -36.095829\n",
            " 1374237/10000000: episode: 6837, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -227.800, mean reward: -1.133 [-113.900, 122.400], mean action: 2.781 [0.000, 10.000], mean observation: 31.818 [0.001, 605.800], loss: 283.248962, mae: 35.282158, mean_q: -36.131264\n",
            " 1374438/10000000: episode: 6838, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -552.800, mean reward: -2.750 [-276.400, 50.400], mean action: 2.741 [0.000, 10.000], mean observation: 29.533 [0.000, 527.500], loss: 269.729553, mae: 35.357952, mean_q: -36.261772\n",
            " 1374639/10000000: episode: 6839, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 360.800, mean reward: 1.795 [-10.000, 193.200], mean action: 3.687 [0.000, 10.000], mean observation: 30.985 [0.001, 466.900], loss: 343.817169, mae: 35.142315, mean_q: -35.749630\n",
            " 1374840/10000000: episode: 6840, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -139.800, mean reward: -0.696 [-69.900, 168.600], mean action: 3.866 [0.000, 10.000], mean observation: 37.179 [0.000, 544.700], loss: 340.111633, mae: 35.203236, mean_q: -36.006947\n",
            " 1375041/10000000: episode: 6841, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -645.400, mean reward: -3.211 [-322.700, 62.400], mean action: 3.075 [0.000, 10.000], mean observation: 37.413 [0.002, 532.900], loss: 446.866302, mae: 35.186665, mean_q: -35.891323\n",
            " 1375242/10000000: episode: 6842, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -546.600, mean reward: -2.719 [-273.300, 45.000], mean action: 2.597 [0.000, 10.000], mean observation: 32.602 [0.001, 647.400], loss: 359.307526, mae: 35.424099, mean_q: -35.770657\n",
            " 1375443/10000000: episode: 6843, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -665.400, mean reward: -3.310 [-332.700, 103.800], mean action: 3.368 [0.000, 10.000], mean observation: 30.952 [0.000, 602.400], loss: 320.781036, mae: 35.085411, mean_q: -35.471840\n",
            " 1375644/10000000: episode: 6844, duration: 1.655s, episode steps: 201, steps per second: 121, episode reward: -1079.800, mean reward: -5.372 [-539.900, 93.600], mean action: 4.204 [0.000, 10.000], mean observation: 34.161 [0.002, 453.500], loss: 529.208740, mae: 34.594910, mean_q: -35.068920\n",
            " 1375845/10000000: episode: 6845, duration: 1.605s, episode steps: 201, steps per second: 125, episode reward: -203.000, mean reward: -1.010 [-101.500, 177.000], mean action: 3.607 [0.000, 10.000], mean observation: 30.046 [0.001, 458.100], loss: 262.016083, mae: 34.632988, mean_q: -35.697090\n",
            " 1376046/10000000: episode: 6846, duration: 1.631s, episode steps: 201, steps per second: 123, episode reward: -223.200, mean reward: -1.110 [-111.600, 178.400], mean action: 3.224 [0.000, 10.000], mean observation: 31.123 [0.000, 695.400], loss: 271.518494, mae: 34.818958, mean_q: -36.206612\n",
            " 1376247/10000000: episode: 6847, duration: 1.636s, episode steps: 201, steps per second: 123, episode reward: 532.200, mean reward: 2.648 [-10.000, 266.100], mean action: 3.194 [0.000, 10.000], mean observation: 35.091 [0.000, 660.900], loss: 228.063507, mae: 35.431053, mean_q: -36.963081\n",
            " 1376448/10000000: episode: 6848, duration: 1.613s, episode steps: 201, steps per second: 125, episode reward: 470.200, mean reward: 2.339 [-10.000, 499.800], mean action: 3.025 [0.000, 10.000], mean observation: 35.142 [0.001, 472.200], loss: 327.968994, mae: 36.425102, mean_q: -37.701618\n",
            " 1376649/10000000: episode: 6849, duration: 1.629s, episode steps: 201, steps per second: 123, episode reward: -258.000, mean reward: -1.284 [-129.000, 240.000], mean action: 2.900 [0.000, 10.000], mean observation: 32.945 [0.000, 446.900], loss: 376.402588, mae: 36.753662, mean_q: -37.640175\n",
            " 1376850/10000000: episode: 6850, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: 195.200, mean reward: 0.971 [-8.000, 287.400], mean action: 3.015 [0.000, 8.000], mean observation: 34.459 [0.001, 654.600], loss: 257.628662, mae: 36.734531, mean_q: -37.340057\n",
            " 1377051/10000000: episode: 6851, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -930.800, mean reward: -4.631 [-465.400, 59.000], mean action: 3.259 [0.000, 10.000], mean observation: 28.792 [0.000, 466.800], loss: 387.267334, mae: 36.237782, mean_q: -37.130516\n",
            " 1377252/10000000: episode: 6852, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: -647.000, mean reward: -3.219 [-323.500, 216.600], mean action: 3.159 [0.000, 10.000], mean observation: 35.796 [0.000, 467.400], loss: 362.961029, mae: 36.249069, mean_q: -37.434235\n",
            " 1377453/10000000: episode: 6853, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -675.000, mean reward: -3.358 [-337.500, 60.600], mean action: 2.741 [0.000, 10.000], mean observation: 34.590 [0.000, 697.600], loss: 215.125549, mae: 36.026218, mean_q: -37.031040\n",
            " 1377654/10000000: episode: 6854, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -562.200, mean reward: -2.797 [-281.100, 114.500], mean action: 2.458 [0.000, 10.000], mean observation: 33.375 [0.000, 443.500], loss: 290.651306, mae: 36.686100, mean_q: -37.513157\n",
            " 1377855/10000000: episode: 6855, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: 47.600, mean reward: 0.237 [-10.000, 238.800], mean action: 3.030 [0.000, 10.000], mean observation: 30.981 [0.001, 470.800], loss: 392.437164, mae: 36.979294, mean_q: -37.587608\n",
            " 1378056/10000000: episode: 6856, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: 140.800, mean reward: 0.700 [-10.000, 232.000], mean action: 3.060 [0.000, 10.000], mean observation: 31.431 [0.002, 400.000], loss: 382.531250, mae: 36.364342, mean_q: -37.341099\n",
            " 1378257/10000000: episode: 6857, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -649.600, mean reward: -3.232 [-324.800, 86.400], mean action: 3.020 [0.000, 8.000], mean observation: 31.482 [0.002, 515.900], loss: 274.723724, mae: 36.764523, mean_q: -37.776611\n",
            " 1378458/10000000: episode: 6858, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -264.800, mean reward: -1.317 [-132.400, 66.600], mean action: 2.010 [0.000, 10.000], mean observation: 34.638 [0.002, 454.800], loss: 423.854309, mae: 37.173237, mean_q: -37.592880\n",
            " 1378659/10000000: episode: 6859, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: 196.600, mean reward: 0.978 [-10.000, 361.200], mean action: 2.453 [0.000, 10.000], mean observation: 32.047 [0.005, 467.700], loss: 496.572906, mae: 36.479401, mean_q: -37.123547\n",
            " 1378860/10000000: episode: 6860, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -274.800, mean reward: -1.367 [-137.400, 159.000], mean action: 2.711 [0.000, 10.000], mean observation: 25.981 [0.000, 638.600], loss: 355.554779, mae: 36.008888, mean_q: -36.878761\n",
            " 1379061/10000000: episode: 6861, duration: 1.768s, episode steps: 201, steps per second: 114, episode reward: 33.400, mean reward: 0.166 [-10.000, 183.600], mean action: 2.801 [0.000, 10.000], mean observation: 38.412 [0.001, 584.000], loss: 294.490051, mae: 36.070431, mean_q: -36.661999\n",
            " 1379262/10000000: episode: 6862, duration: 2.082s, episode steps: 201, steps per second: 97, episode reward: -367.400, mean reward: -1.828 [-183.700, 166.500], mean action: 2.348 [0.000, 10.000], mean observation: 32.272 [0.000, 530.500], loss: 332.692993, mae: 36.633690, mean_q: -36.917381\n",
            " 1379463/10000000: episode: 6863, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -261.000, mean reward: -1.299 [-130.500, 93.600], mean action: 2.826 [0.000, 10.000], mean observation: 32.627 [0.000, 796.200], loss: 385.965942, mae: 36.244770, mean_q: -36.949863\n",
            " 1379664/10000000: episode: 6864, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -615.000, mean reward: -3.060 [-307.500, 69.600], mean action: 2.572 [0.000, 10.000], mean observation: 38.515 [0.000, 588.400], loss: 261.472870, mae: 35.985535, mean_q: -36.813927\n",
            " 1379865/10000000: episode: 6865, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: 539.600, mean reward: 2.685 [-10.000, 296.000], mean action: 2.398 [0.000, 10.000], mean observation: 31.118 [0.000, 507.900], loss: 329.356689, mae: 36.446785, mean_q: -37.483608\n",
            " 1380066/10000000: episode: 6866, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -192.400, mean reward: -0.957 [-96.200, 294.800], mean action: 2.871 [0.000, 10.000], mean observation: 36.891 [0.001, 466.300], loss: 423.618469, mae: 36.456085, mean_q: -37.490345\n",
            " 1380267/10000000: episode: 6867, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: -439.800, mean reward: -2.188 [-219.900, 132.600], mean action: 2.423 [0.000, 10.000], mean observation: 29.695 [0.003, 438.400], loss: 175.878281, mae: 36.777897, mean_q: -37.590851\n",
            " 1380468/10000000: episode: 6868, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -392.400, mean reward: -1.952 [-196.200, 61.600], mean action: 2.323 [0.000, 10.000], mean observation: 26.047 [0.002, 545.200], loss: 267.322174, mae: 36.863853, mean_q: -36.981640\n",
            " 1380669/10000000: episode: 6869, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -294.600, mean reward: -1.466 [-147.300, 68.400], mean action: 2.692 [0.000, 10.000], mean observation: 37.811 [0.000, 491.000], loss: 308.974884, mae: 36.620476, mean_q: -37.502285\n",
            " 1380870/10000000: episode: 6870, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -235.000, mean reward: -1.169 [-117.500, 134.600], mean action: 2.512 [0.000, 10.000], mean observation: 31.621 [0.001, 481.400], loss: 330.393646, mae: 36.895149, mean_q: -37.848316\n",
            " 1381071/10000000: episode: 6871, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -598.600, mean reward: -2.978 [-299.300, 176.800], mean action: 2.876 [0.000, 10.000], mean observation: 30.325 [0.002, 455.400], loss: 330.981659, mae: 37.447834, mean_q: -38.193836\n",
            " 1381272/10000000: episode: 6872, duration: 1.390s, episode steps: 201, steps per second: 145, episode reward: -1147.800, mean reward: -5.710 [-573.900, 201.600], mean action: 4.512 [0.000, 10.000], mean observation: 31.111 [0.000, 746.900], loss: 437.635468, mae: 36.355972, mean_q: -37.799889\n",
            " 1381473/10000000: episode: 6873, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -441.000, mean reward: -2.194 [-220.500, 307.200], mean action: 4.174 [0.000, 10.000], mean observation: 33.717 [0.001, 583.400], loss: 346.101440, mae: 36.521236, mean_q: -38.071117\n",
            " 1381674/10000000: episode: 6874, duration: 1.390s, episode steps: 201, steps per second: 145, episode reward: -630.000, mean reward: -3.134 [-315.000, 111.000], mean action: 3.597 [0.000, 10.000], mean observation: 32.116 [0.000, 702.100], loss: 412.154663, mae: 36.449570, mean_q: -37.736813\n",
            " 1381875/10000000: episode: 6875, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -312.600, mean reward: -1.555 [-156.300, 102.000], mean action: 3.189 [0.000, 10.000], mean observation: 38.370 [0.000, 653.600], loss: 276.719757, mae: 37.078362, mean_q: -38.078533\n",
            " 1382076/10000000: episode: 6876, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -643.800, mean reward: -3.203 [-321.900, 60.900], mean action: 3.114 [0.000, 10.000], mean observation: 29.351 [0.000, 509.800], loss: 270.982239, mae: 37.412586, mean_q: -38.581890\n",
            " 1382277/10000000: episode: 6877, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -261.800, mean reward: -1.302 [-130.900, 85.200], mean action: 3.234 [0.000, 10.000], mean observation: 35.490 [0.002, 630.900], loss: 264.328583, mae: 37.630943, mean_q: -38.593395\n",
            " 1382478/10000000: episode: 6878, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: 207.800, mean reward: 1.034 [-10.000, 211.200], mean action: 2.711 [0.000, 10.000], mean observation: 38.974 [0.001, 571.000], loss: 364.979980, mae: 38.104282, mean_q: -39.157280\n",
            " 1382679/10000000: episode: 6879, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -787.200, mean reward: -3.916 [-393.600, 31.600], mean action: 2.876 [0.000, 10.000], mean observation: 36.792 [0.001, 591.000], loss: 320.275055, mae: 37.929268, mean_q: -39.061241\n",
            " 1382880/10000000: episode: 6880, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -544.800, mean reward: -2.710 [-272.400, 57.600], mean action: 3.114 [0.000, 10.000], mean observation: 30.813 [0.001, 532.400], loss: 298.915680, mae: 36.855179, mean_q: -38.227203\n",
            " 1383081/10000000: episode: 6881, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: 1283.600, mean reward: 6.386 [-10.000, 641.800], mean action: 3.965 [0.000, 10.000], mean observation: 31.345 [0.003, 595.600], loss: 472.596771, mae: 36.775707, mean_q: -38.292877\n",
            " 1383282/10000000: episode: 6882, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -159.400, mean reward: -0.793 [-79.700, 193.600], mean action: 3.184 [0.000, 10.000], mean observation: 30.184 [0.000, 530.000], loss: 475.658813, mae: 37.134357, mean_q: -38.470722\n",
            " 1383483/10000000: episode: 6883, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -369.800, mean reward: -1.840 [-184.900, 123.800], mean action: 3.249 [0.000, 10.000], mean observation: 30.620 [0.002, 453.300], loss: 315.788116, mae: 37.537891, mean_q: -38.914486\n",
            " 1383684/10000000: episode: 6884, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -586.800, mean reward: -2.919 [-293.400, 48.400], mean action: 3.040 [0.000, 10.000], mean observation: 35.882 [0.001, 591.100], loss: 298.955719, mae: 38.217953, mean_q: -39.470695\n",
            " 1383885/10000000: episode: 6885, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 36.800, mean reward: 0.183 [-10.000, 293.500], mean action: 3.095 [0.000, 10.000], mean observation: 27.626 [0.003, 399.800], loss: 349.112244, mae: 38.924728, mean_q: -40.076439\n",
            " 1384086/10000000: episode: 6886, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -397.600, mean reward: -1.978 [-198.800, 263.200], mean action: 2.910 [0.000, 10.000], mean observation: 39.266 [0.001, 598.700], loss: 332.155243, mae: 38.691959, mean_q: -39.962399\n",
            " 1384287/10000000: episode: 6887, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -676.600, mean reward: -3.366 [-338.300, 30.400], mean action: 2.781 [0.000, 10.000], mean observation: 27.793 [0.002, 369.100], loss: 324.044647, mae: 38.822952, mean_q: -39.585381\n",
            " 1384488/10000000: episode: 6888, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: 316.800, mean reward: 1.576 [-10.000, 460.000], mean action: 2.055 [0.000, 10.000], mean observation: 33.040 [0.000, 461.800], loss: 328.704529, mae: 39.876881, mean_q: -40.530872\n",
            " 1384689/10000000: episode: 6889, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -321.600, mean reward: -1.600 [-160.800, 99.200], mean action: 2.910 [0.000, 10.000], mean observation: 32.627 [0.000, 540.000], loss: 315.524963, mae: 38.804493, mean_q: -39.707504\n",
            " 1384890/10000000: episode: 6890, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -391.200, mean reward: -1.946 [-195.600, 94.400], mean action: 3.473 [0.000, 10.000], mean observation: 28.202 [0.000, 663.800], loss: 286.767212, mae: 39.410339, mean_q: -40.213516\n",
            " 1385091/10000000: episode: 6891, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -415.800, mean reward: -2.069 [-207.900, 85.800], mean action: 2.159 [0.000, 8.000], mean observation: 34.793 [0.000, 463.200], loss: 277.474701, mae: 39.931320, mean_q: -40.645378\n",
            " 1385292/10000000: episode: 6892, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -350.000, mean reward: -1.741 [-175.000, 242.400], mean action: 3.249 [0.000, 8.000], mean observation: 32.326 [0.001, 639.500], loss: 398.776459, mae: 39.432095, mean_q: -40.505939\n",
            " 1385493/10000000: episode: 6893, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -530.200, mean reward: -2.638 [-265.100, 141.400], mean action: 3.179 [0.000, 10.000], mean observation: 32.007 [0.001, 619.200], loss: 546.056274, mae: 39.514919, mean_q: -40.768677\n",
            " 1385694/10000000: episode: 6894, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -61.600, mean reward: -0.306 [-30.800, 232.200], mean action: 2.473 [0.000, 8.000], mean observation: 35.330 [0.001, 651.100], loss: 366.919006, mae: 40.089466, mean_q: -40.988300\n",
            " 1385895/10000000: episode: 6895, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -539.600, mean reward: -2.685 [-269.800, 36.000], mean action: 2.129 [0.000, 9.000], mean observation: 39.394 [0.000, 818.100], loss: 273.756775, mae: 40.477108, mean_q: -41.318649\n",
            " 1386096/10000000: episode: 6896, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -783.000, mean reward: -3.896 [-391.500, 102.500], mean action: 3.468 [0.000, 10.000], mean observation: 28.176 [0.003, 540.500], loss: 562.441284, mae: 39.904152, mean_q: -41.217274\n",
            " 1386297/10000000: episode: 6897, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -718.400, mean reward: -3.574 [-359.200, 56.000], mean action: 2.856 [0.000, 10.000], mean observation: 36.578 [0.001, 565.900], loss: 392.086639, mae: 40.314209, mean_q: -41.690125\n",
            " 1386498/10000000: episode: 6898, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -952.400, mean reward: -4.738 [-476.200, 41.000], mean action: 3.313 [0.000, 10.000], mean observation: 32.922 [0.002, 488.800], loss: 358.851837, mae: 39.567165, mean_q: -41.301201\n",
            " 1386699/10000000: episode: 6899, duration: 1.390s, episode steps: 201, steps per second: 145, episode reward: -345.000, mean reward: -1.716 [-172.500, 104.800], mean action: 3.652 [0.000, 10.000], mean observation: 30.326 [0.000, 720.900], loss: 292.726593, mae: 38.918591, mean_q: -40.857639\n",
            " 1386900/10000000: episode: 6900, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: 612.800, mean reward: 3.049 [-10.000, 535.500], mean action: 4.134 [0.000, 10.000], mean observation: 35.143 [0.002, 624.500], loss: 456.524261, mae: 38.601864, mean_q: -40.252895\n",
            " 1387101/10000000: episode: 6901, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -1086.000, mean reward: -5.403 [-543.000, 88.000], mean action: 4.607 [0.000, 10.000], mean observation: 28.703 [0.001, 488.700], loss: 339.723572, mae: 38.070667, mean_q: -39.670113\n",
            " 1387302/10000000: episode: 6902, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -464.800, mean reward: -2.312 [-232.400, 149.600], mean action: 4.338 [0.000, 10.000], mean observation: 33.107 [0.002, 476.600], loss: 298.793457, mae: 37.741707, mean_q: -39.133713\n",
            " 1387503/10000000: episode: 6903, duration: 1.496s, episode steps: 201, steps per second: 134, episode reward: -695.600, mean reward: -3.461 [-347.800, 111.200], mean action: 4.005 [0.000, 9.000], mean observation: 37.965 [0.000, 615.100], loss: 348.219604, mae: 37.833492, mean_q: -39.016773\n",
            " 1387704/10000000: episode: 6904, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -874.200, mean reward: -4.349 [-437.100, 95.200], mean action: 3.881 [0.000, 10.000], mean observation: 36.411 [0.000, 907.100], loss: 385.900116, mae: 37.439209, mean_q: -38.658291\n",
            " 1387905/10000000: episode: 6905, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: 90.000, mean reward: 0.448 [-10.000, 338.400], mean action: 2.582 [0.000, 10.000], mean observation: 32.142 [0.001, 508.500], loss: 358.824524, mae: 38.519669, mean_q: -39.437698\n",
            " 1388106/10000000: episode: 6906, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -1128.000, mean reward: -5.612 [-564.000, 33.200], mean action: 3.642 [0.000, 10.000], mean observation: 35.611 [0.001, 507.200], loss: 512.174927, mae: 37.919392, mean_q: -39.510109\n",
            " 1388307/10000000: episode: 6907, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: 649.200, mean reward: 3.230 [-10.000, 576.800], mean action: 3.582 [0.000, 10.000], mean observation: 31.529 [0.002, 522.800], loss: 361.315033, mae: 38.206268, mean_q: -39.265118\n",
            " 1388508/10000000: episode: 6908, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -471.600, mean reward: -2.346 [-235.800, 131.000], mean action: 3.985 [0.000, 10.000], mean observation: 28.587 [0.000, 623.900], loss: 370.096893, mae: 38.437267, mean_q: -39.415974\n",
            " 1388709/10000000: episode: 6909, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -865.800, mean reward: -4.307 [-432.900, 42.600], mean action: 3.164 [0.000, 10.000], mean observation: 31.478 [0.001, 402.000], loss: 350.598816, mae: 38.590862, mean_q: -38.967663\n",
            " 1388910/10000000: episode: 6910, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 347.600, mean reward: 1.729 [-10.000, 422.400], mean action: 3.224 [0.000, 10.000], mean observation: 34.328 [0.000, 542.000], loss: 342.338470, mae: 38.984699, mean_q: -40.145882\n",
            " 1389111/10000000: episode: 6911, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -716.400, mean reward: -3.564 [-358.200, 172.800], mean action: 3.841 [0.000, 8.000], mean observation: 34.511 [0.000, 781.200], loss: 421.178406, mae: 38.638195, mean_q: -40.087246\n",
            " 1389312/10000000: episode: 6912, duration: 1.389s, episode steps: 201, steps per second: 145, episode reward: 243.400, mean reward: 1.211 [-8.000, 388.000], mean action: 3.597 [0.000, 8.000], mean observation: 27.864 [0.001, 492.700], loss: 297.041351, mae: 39.531235, mean_q: -40.851574\n",
            " 1389513/10000000: episode: 6913, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -413.800, mean reward: -2.059 [-206.900, 167.000], mean action: 3.000 [0.000, 10.000], mean observation: 34.160 [0.001, 488.900], loss: 247.035309, mae: 40.236557, mean_q: -41.342709\n",
            " 1389714/10000000: episode: 6914, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -230.800, mean reward: -1.148 [-115.400, 135.000], mean action: 3.174 [0.000, 10.000], mean observation: 35.557 [0.001, 511.400], loss: 374.849182, mae: 40.333412, mean_q: -41.391071\n",
            " 1389915/10000000: episode: 6915, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -611.000, mean reward: -3.040 [-305.500, 40.600], mean action: 2.950 [0.000, 10.000], mean observation: 34.132 [0.000, 571.200], loss: 366.179810, mae: 40.386829, mean_q: -41.697121\n",
            " 1390116/10000000: episode: 6916, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -716.200, mean reward: -3.563 [-358.100, 44.700], mean action: 3.204 [0.000, 10.000], mean observation: 34.166 [0.000, 764.600], loss: 390.967743, mae: 40.998840, mean_q: -42.139965\n",
            " 1390317/10000000: episode: 6917, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -109.600, mean reward: -0.545 [-54.800, 188.400], mean action: 3.473 [0.000, 10.000], mean observation: 24.864 [0.001, 516.900], loss: 292.178589, mae: 40.746292, mean_q: -42.060966\n",
            " 1390518/10000000: episode: 6918, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: -376.200, mean reward: -1.872 [-188.100, 74.200], mean action: 2.184 [0.000, 9.000], mean observation: 33.778 [0.000, 598.900], loss: 485.523895, mae: 40.668827, mean_q: -41.441818\n",
            " 1390719/10000000: episode: 6919, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -797.000, mean reward: -3.965 [-398.500, 15.800], mean action: 2.448 [0.000, 10.000], mean observation: 38.577 [0.002, 508.700], loss: 251.248245, mae: 40.412281, mean_q: -41.047695\n",
            " 1390920/10000000: episode: 6920, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -640.000, mean reward: -3.184 [-320.000, 89.400], mean action: 2.587 [0.000, 9.000], mean observation: 34.266 [0.000, 792.800], loss: 300.569336, mae: 40.098663, mean_q: -41.125851\n",
            " 1391121/10000000: episode: 6921, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -603.800, mean reward: -3.004 [-301.900, 42.000], mean action: 2.373 [0.000, 10.000], mean observation: 39.003 [0.000, 746.700], loss: 386.377869, mae: 39.703468, mean_q: -40.366859\n",
            " 1391322/10000000: episode: 6922, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -347.800, mean reward: -1.730 [-173.900, 68.500], mean action: 2.542 [0.000, 9.000], mean observation: 29.911 [0.001, 375.100], loss: 366.295624, mae: 39.754166, mean_q: -40.969723\n",
            " 1391523/10000000: episode: 6923, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -538.000, mean reward: -2.677 [-269.000, 149.400], mean action: 2.796 [0.000, 9.000], mean observation: 34.978 [0.000, 541.500], loss: 371.417297, mae: 40.120956, mean_q: -41.367199\n",
            " 1391724/10000000: episode: 6924, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -113.000, mean reward: -0.562 [-56.500, 195.500], mean action: 3.129 [0.000, 10.000], mean observation: 30.939 [0.004, 519.300], loss: 385.180389, mae: 40.261082, mean_q: -41.539761\n",
            " 1391925/10000000: episode: 6925, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -675.400, mean reward: -3.360 [-337.700, 52.000], mean action: 3.348 [0.000, 9.000], mean observation: 34.465 [0.001, 460.100], loss: 316.695496, mae: 39.828766, mean_q: -40.683697\n",
            " 1392126/10000000: episode: 6926, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -118.800, mean reward: -0.591 [-59.400, 225.800], mean action: 2.920 [0.000, 9.000], mean observation: 30.173 [0.003, 566.800], loss: 399.154236, mae: 40.488476, mean_q: -41.300617\n",
            " 1392327/10000000: episode: 6927, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -558.600, mean reward: -2.779 [-279.300, 41.000], mean action: 2.129 [0.000, 10.000], mean observation: 36.159 [0.000, 579.300], loss: 410.559082, mae: 40.757542, mean_q: -41.515839\n",
            " 1392528/10000000: episode: 6928, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -619.000, mean reward: -3.080 [-309.500, 70.500], mean action: 2.597 [0.000, 10.000], mean observation: 28.004 [0.001, 550.200], loss: 218.475311, mae: 39.794228, mean_q: -40.579407\n",
            " 1392729/10000000: episode: 6929, duration: 1.395s, episode steps: 201, steps per second: 144, episode reward: -568.400, mean reward: -2.828 [-284.200, 89.500], mean action: 2.358 [0.000, 9.000], mean observation: 31.597 [0.001, 548.600], loss: 435.718506, mae: 39.660515, mean_q: -40.305302\n",
            " 1392930/10000000: episode: 6930, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: 268.800, mean reward: 1.337 [-10.000, 148.800], mean action: 2.289 [0.000, 10.000], mean observation: 35.166 [0.002, 494.100], loss: 304.977417, mae: 39.695984, mean_q: -40.300285\n",
            " 1393131/10000000: episode: 6931, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -811.400, mean reward: -4.037 [-405.700, 61.000], mean action: 2.577 [0.000, 9.000], mean observation: 34.170 [0.000, 525.700], loss: 291.685913, mae: 39.096306, mean_q: -39.922359\n",
            " 1393332/10000000: episode: 6932, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -722.200, mean reward: -3.593 [-361.100, 29.600], mean action: 2.284 [0.000, 8.000], mean observation: 38.077 [0.000, 603.200], loss: 264.330597, mae: 39.504211, mean_q: -40.308456\n",
            " 1393533/10000000: episode: 6933, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 233.200, mean reward: 1.160 [-9.000, 279.600], mean action: 2.851 [0.000, 9.000], mean observation: 33.974 [0.000, 803.400], loss: 300.540314, mae: 39.862228, mean_q: -40.759789\n",
            " 1393734/10000000: episode: 6934, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -587.000, mean reward: -2.920 [-293.500, 127.200], mean action: 2.652 [0.000, 8.000], mean observation: 30.557 [0.001, 455.800], loss: 389.170410, mae: 40.065742, mean_q: -41.402321\n",
            " 1393935/10000000: episode: 6935, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -539.600, mean reward: -2.685 [-269.800, 30.900], mean action: 2.746 [0.000, 10.000], mean observation: 35.567 [0.002, 525.700], loss: 336.499298, mae: 41.155415, mean_q: -42.317005\n",
            " 1394136/10000000: episode: 6936, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -558.000, mean reward: -2.776 [-279.000, 51.800], mean action: 2.164 [0.000, 8.000], mean observation: 34.051 [0.000, 545.700], loss: 318.431213, mae: 41.321182, mean_q: -41.940624\n",
            " 1394337/10000000: episode: 6937, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -751.200, mean reward: -3.737 [-375.600, 38.500], mean action: 2.960 [0.000, 10.000], mean observation: 35.030 [0.001, 622.400], loss: 368.855835, mae: 41.228672, mean_q: -42.163654\n",
            " 1394538/10000000: episode: 6938, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -780.800, mean reward: -3.885 [-390.400, 36.900], mean action: 2.498 [0.000, 9.000], mean observation: 36.990 [0.000, 430.400], loss: 594.358459, mae: 40.688835, mean_q: -41.856220\n",
            " 1394739/10000000: episode: 6939, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: 1217.400, mean reward: 6.057 [-10.000, 608.700], mean action: 3.627 [0.000, 10.000], mean observation: 36.079 [0.000, 669.400], loss: 309.455444, mae: 39.965302, mean_q: -41.816624\n",
            " 1394940/10000000: episode: 6940, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -1016.400, mean reward: -5.057 [-508.200, 21.600], mean action: 2.950 [0.000, 10.000], mean observation: 33.535 [0.002, 511.500], loss: 269.100861, mae: 40.787354, mean_q: -42.366707\n",
            " 1395141/10000000: episode: 6941, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: 38.800, mean reward: 0.193 [-10.000, 151.000], mean action: 2.557 [0.000, 10.000], mean observation: 36.689 [0.000, 663.000], loss: 447.168060, mae: 40.739758, mean_q: -42.003658\n",
            " 1395342/10000000: episode: 6942, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -550.200, mean reward: -2.737 [-275.100, 97.200], mean action: 2.577 [0.000, 10.000], mean observation: 38.409 [0.000, 515.300], loss: 426.616302, mae: 41.231800, mean_q: -42.565960\n",
            " 1395543/10000000: episode: 6943, duration: 1.555s, episode steps: 201, steps per second: 129, episode reward: -105.000, mean reward: -0.522 [-52.500, 114.600], mean action: 2.119 [0.000, 10.000], mean observation: 30.998 [0.001, 637.900], loss: 329.185211, mae: 42.017998, mean_q: -42.864952\n",
            " 1395744/10000000: episode: 6944, duration: 1.704s, episode steps: 201, steps per second: 118, episode reward: -327.200, mean reward: -1.628 [-163.600, 58.000], mean action: 2.070 [0.000, 9.000], mean observation: 30.990 [0.001, 539.300], loss: 391.009949, mae: 41.985691, mean_q: -42.484974\n",
            " 1395945/10000000: episode: 6945, duration: 1.695s, episode steps: 201, steps per second: 119, episode reward: -532.600, mean reward: -2.650 [-266.300, 32.700], mean action: 1.925 [0.000, 9.000], mean observation: 38.281 [0.001, 581.100], loss: 310.128845, mae: 41.843746, mean_q: -42.790459\n",
            " 1396146/10000000: episode: 6946, duration: 1.717s, episode steps: 201, steps per second: 117, episode reward: -568.400, mean reward: -2.828 [-284.200, 82.400], mean action: 2.522 [0.000, 9.000], mean observation: 33.334 [0.000, 622.800], loss: 378.002502, mae: 42.104515, mean_q: -43.182400\n",
            " 1396347/10000000: episode: 6947, duration: 1.497s, episode steps: 201, steps per second: 134, episode reward: -735.200, mean reward: -3.658 [-367.600, 31.200], mean action: 2.607 [0.000, 10.000], mean observation: 34.059 [0.001, 570.400], loss: 499.740936, mae: 42.061272, mean_q: -43.231850\n",
            " 1396548/10000000: episode: 6948, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -622.800, mean reward: -3.099 [-311.400, 34.800], mean action: 2.219 [0.000, 9.000], mean observation: 34.176 [0.001, 424.400], loss: 230.730133, mae: 41.728485, mean_q: -43.177910\n",
            " 1396749/10000000: episode: 6949, duration: 1.513s, episode steps: 201, steps per second: 133, episode reward: -621.800, mean reward: -3.094 [-310.900, 37.800], mean action: 2.184 [0.000, 8.000], mean observation: 32.947 [0.000, 749.600], loss: 396.700470, mae: 42.121269, mean_q: -43.382202\n",
            " 1396950/10000000: episode: 6950, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: 109.800, mean reward: 0.546 [-9.000, 231.000], mean action: 2.493 [0.000, 10.000], mean observation: 29.009 [0.001, 587.600], loss: 385.811768, mae: 42.076107, mean_q: -43.149185\n",
            " 1397151/10000000: episode: 6951, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: 465.400, mean reward: 2.315 [-10.000, 575.400], mean action: 3.433 [0.000, 10.000], mean observation: 33.921 [0.002, 516.600], loss: 357.266174, mae: 41.534485, mean_q: -42.778255\n",
            " 1397352/10000000: episode: 6952, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -796.600, mean reward: -3.963 [-398.300, 60.000], mean action: 3.159 [0.000, 10.000], mean observation: 28.042 [0.001, 512.300], loss: 422.724426, mae: 41.416649, mean_q: -42.592258\n",
            " 1397553/10000000: episode: 6953, duration: 1.482s, episode steps: 201, steps per second: 136, episode reward: 144.400, mean reward: 0.718 [-8.000, 273.200], mean action: 2.697 [0.000, 8.000], mean observation: 34.086 [0.005, 377.000], loss: 248.672363, mae: 41.576366, mean_q: -42.892696\n",
            " 1397754/10000000: episode: 6954, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -437.000, mean reward: -2.174 [-218.500, 48.400], mean action: 2.672 [0.000, 10.000], mean observation: 32.935 [0.000, 590.900], loss: 391.468506, mae: 41.886574, mean_q: -43.011684\n",
            " 1397955/10000000: episode: 6955, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -396.600, mean reward: -1.973 [-198.300, 116.100], mean action: 2.647 [0.000, 10.000], mean observation: 34.354 [0.003, 566.300], loss: 387.115601, mae: 41.531929, mean_q: -42.574379\n",
            " 1398156/10000000: episode: 6956, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -755.600, mean reward: -3.759 [-377.800, 82.800], mean action: 2.861 [0.000, 9.000], mean observation: 23.746 [0.000, 446.200], loss: 361.842346, mae: 41.479099, mean_q: -42.456760\n",
            " 1398357/10000000: episode: 6957, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -466.600, mean reward: -2.321 [-233.300, 91.200], mean action: 3.159 [0.000, 10.000], mean observation: 36.127 [0.001, 504.300], loss: 250.719681, mae: 42.000214, mean_q: -43.266266\n",
            " 1398558/10000000: episode: 6958, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: 308.200, mean reward: 1.533 [-8.000, 154.100], mean action: 2.836 [0.000, 10.000], mean observation: 34.049 [0.001, 510.800], loss: 351.003601, mae: 42.356937, mean_q: -43.790092\n",
            " 1398759/10000000: episode: 6959, duration: 1.390s, episode steps: 201, steps per second: 145, episode reward: -216.200, mean reward: -1.076 [-108.100, 82.800], mean action: 2.940 [0.000, 10.000], mean observation: 28.956 [0.002, 487.500], loss: 312.195984, mae: 42.987556, mean_q: -44.453842\n",
            " 1398960/10000000: episode: 6960, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: 210.600, mean reward: 1.048 [-10.000, 236.800], mean action: 3.557 [0.000, 10.000], mean observation: 35.733 [0.000, 637.000], loss: 412.243652, mae: 42.737320, mean_q: -44.540230\n",
            " 1399161/10000000: episode: 6961, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -412.800, mean reward: -2.054 [-206.400, 155.700], mean action: 3.134 [0.000, 10.000], mean observation: 29.634 [0.000, 640.400], loss: 197.731033, mae: 43.326756, mean_q: -44.950344\n",
            " 1399362/10000000: episode: 6962, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -2.600, mean reward: -0.013 [-10.000, 380.400], mean action: 3.308 [0.000, 10.000], mean observation: 28.735 [0.001, 418.000], loss: 226.917038, mae: 43.403900, mean_q: -44.933136\n",
            " 1399563/10000000: episode: 6963, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -142.200, mean reward: -0.707 [-71.100, 140.400], mean action: 3.438 [0.000, 10.000], mean observation: 37.139 [0.002, 512.800], loss: 480.890015, mae: 43.190006, mean_q: -44.846722\n",
            " 1399764/10000000: episode: 6964, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -231.600, mean reward: -1.152 [-115.800, 253.800], mean action: 3.891 [0.000, 10.000], mean observation: 32.383 [0.000, 558.800], loss: 470.645416, mae: 42.182961, mean_q: -43.739529\n",
            " 1399965/10000000: episode: 6965, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -824.600, mean reward: -4.102 [-412.300, 32.900], mean action: 2.910 [0.000, 10.000], mean observation: 38.760 [0.001, 584.600], loss: 294.713440, mae: 42.084442, mean_q: -43.619099\n",
            " 1400166/10000000: episode: 6966, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -980.000, mean reward: -4.876 [-490.000, 86.600], mean action: 3.597 [0.000, 10.000], mean observation: 33.006 [0.000, 455.400], loss: 406.209747, mae: 42.001865, mean_q: -43.935120\n",
            " 1400367/10000000: episode: 6967, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -542.200, mean reward: -2.698 [-271.100, 91.800], mean action: 2.542 [0.000, 10.000], mean observation: 34.023 [0.001, 510.200], loss: 406.472778, mae: 42.860756, mean_q: -44.630810\n",
            " 1400568/10000000: episode: 6968, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -783.200, mean reward: -3.897 [-391.600, 34.400], mean action: 2.856 [0.000, 10.000], mean observation: 32.137 [0.000, 708.200], loss: 332.364899, mae: 42.236931, mean_q: -43.892654\n",
            " 1400769/10000000: episode: 6969, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -118.400, mean reward: -0.589 [-59.200, 212.600], mean action: 3.134 [0.000, 10.000], mean observation: 32.971 [0.002, 436.000], loss: 377.687500, mae: 42.568531, mean_q: -44.275131\n",
            " 1400970/10000000: episode: 6970, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: 153.600, mean reward: 0.764 [-9.000, 134.000], mean action: 3.249 [0.000, 10.000], mean observation: 38.356 [0.000, 500.200], loss: 361.823700, mae: 42.293640, mean_q: -44.324589\n",
            " 1401171/10000000: episode: 6971, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: 211.400, mean reward: 1.052 [-10.000, 351.000], mean action: 3.040 [0.000, 10.000], mean observation: 35.023 [0.000, 425.200], loss: 304.521820, mae: 43.324810, mean_q: -45.190823\n",
            " 1401372/10000000: episode: 6972, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -735.200, mean reward: -3.658 [-367.600, 65.000], mean action: 3.622 [0.000, 10.000], mean observation: 34.474 [0.000, 571.900], loss: 309.971313, mae: 43.231380, mean_q: -45.285351\n",
            " 1401573/10000000: episode: 6973, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -653.400, mean reward: -3.251 [-326.700, 73.200], mean action: 2.975 [0.000, 10.000], mean observation: 31.192 [0.002, 494.100], loss: 311.994415, mae: 43.511349, mean_q: -45.390480\n",
            " 1401774/10000000: episode: 6974, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -579.600, mean reward: -2.884 [-289.800, 141.000], mean action: 3.313 [0.000, 10.000], mean observation: 28.391 [0.000, 490.900], loss: 327.073425, mae: 43.616138, mean_q: -45.123863\n",
            " 1401975/10000000: episode: 6975, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -242.600, mean reward: -1.207 [-121.300, 158.900], mean action: 2.731 [0.000, 10.000], mean observation: 29.466 [0.001, 457.300], loss: 408.249908, mae: 42.953903, mean_q: -44.570023\n",
            " 1402176/10000000: episode: 6976, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -163.400, mean reward: -0.813 [-81.700, 212.800], mean action: 2.980 [0.000, 10.000], mean observation: 33.163 [0.000, 798.300], loss: 390.805267, mae: 42.483101, mean_q: -44.374077\n",
            " 1402377/10000000: episode: 6977, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -354.800, mean reward: -1.765 [-177.400, 86.400], mean action: 2.149 [0.000, 10.000], mean observation: 33.675 [0.001, 556.200], loss: 418.475861, mae: 43.401497, mean_q: -44.644466\n",
            " 1402578/10000000: episode: 6978, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -698.000, mean reward: -3.473 [-349.000, 10.500], mean action: 2.005 [0.000, 10.000], mean observation: 30.231 [0.000, 481.500], loss: 410.367462, mae: 43.201576, mean_q: -43.969570\n",
            " 1402779/10000000: episode: 6979, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: 165.000, mean reward: 0.821 [-10.000, 118.200], mean action: 1.776 [0.000, 10.000], mean observation: 34.757 [0.000, 668.900], loss: 322.835632, mae: 42.759991, mean_q: -43.627544\n",
            " 1402980/10000000: episode: 6980, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -59.000, mean reward: -0.294 [-29.500, 202.400], mean action: 2.438 [0.000, 10.000], mean observation: 29.969 [0.000, 383.900], loss: 257.421844, mae: 42.035385, mean_q: -43.330700\n",
            " 1403181/10000000: episode: 6981, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -461.400, mean reward: -2.296 [-230.700, 80.000], mean action: 2.005 [0.000, 10.000], mean observation: 34.972 [0.000, 605.900], loss: 321.002411, mae: 42.094589, mean_q: -42.751648\n",
            " 1403382/10000000: episode: 6982, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: 247.400, mean reward: 1.231 [-10.000, 394.200], mean action: 2.104 [0.000, 10.000], mean observation: 37.101 [0.000, 650.000], loss: 395.947388, mae: 42.124454, mean_q: -43.291027\n",
            " 1403583/10000000: episode: 6983, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 169.200, mean reward: 0.842 [-10.000, 180.000], mean action: 2.199 [0.000, 10.000], mean observation: 31.147 [0.004, 432.300], loss: 333.383057, mae: 41.174526, mean_q: -42.721985\n",
            " 1403784/10000000: episode: 6984, duration: 1.380s, episode steps: 201, steps per second: 146, episode reward: -501.400, mean reward: -2.495 [-250.700, 31.500], mean action: 2.139 [0.000, 10.000], mean observation: 34.092 [0.002, 420.100], loss: 356.079254, mae: 41.224297, mean_q: -42.461494\n",
            " 1403985/10000000: episode: 6985, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -276.800, mean reward: -1.377 [-138.400, 98.100], mean action: 2.378 [0.000, 9.000], mean observation: 40.933 [0.001, 510.400], loss: 497.870117, mae: 40.381443, mean_q: -41.622498\n",
            " 1404186/10000000: episode: 6986, duration: 1.401s, episode steps: 201, steps per second: 144, episode reward: -558.000, mean reward: -2.776 [-279.000, 37.200], mean action: 2.299 [0.000, 8.000], mean observation: 32.452 [0.002, 508.900], loss: 366.225555, mae: 40.863892, mean_q: -41.639713\n",
            " 1404387/10000000: episode: 6987, duration: 1.383s, episode steps: 201, steps per second: 145, episode reward: 463.400, mean reward: 2.305 [-10.000, 337.500], mean action: 2.000 [0.000, 10.000], mean observation: 33.939 [0.000, 447.500], loss: 259.667511, mae: 41.184139, mean_q: -42.000526\n",
            " 1404588/10000000: episode: 6988, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -589.600, mean reward: -2.933 [-294.800, 64.200], mean action: 2.607 [0.000, 10.000], mean observation: 28.465 [0.001, 618.400], loss: 480.441772, mae: 40.029915, mean_q: -41.173016\n",
            " 1404789/10000000: episode: 6989, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: 160.800, mean reward: 0.800 [-10.000, 174.900], mean action: 2.353 [0.000, 10.000], mean observation: 32.511 [0.001, 519.700], loss: 315.114868, mae: 39.937462, mean_q: -41.385746\n",
            " 1404990/10000000: episode: 6990, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -121.200, mean reward: -0.603 [-60.600, 110.400], mean action: 2.149 [0.000, 10.000], mean observation: 33.106 [0.000, 586.000], loss: 243.760498, mae: 40.257408, mean_q: -41.493168\n",
            " 1405191/10000000: episode: 6991, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -422.000, mean reward: -2.100 [-211.000, 27.400], mean action: 2.065 [0.000, 9.000], mean observation: 31.945 [0.001, 471.000], loss: 391.251312, mae: 40.101067, mean_q: -41.202660\n",
            " 1405392/10000000: episode: 6992, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: 183.000, mean reward: 0.910 [-7.000, 116.900], mean action: 1.632 [0.000, 8.000], mean observation: 31.934 [0.001, 431.000], loss: 221.504593, mae: 40.228840, mean_q: -41.020123\n",
            " 1405593/10000000: episode: 6993, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -448.400, mean reward: -2.231 [-224.200, 129.600], mean action: 2.254 [0.000, 10.000], mean observation: 30.832 [0.001, 459.200], loss: 314.452240, mae: 40.311924, mean_q: -41.395916\n",
            " 1405794/10000000: episode: 6994, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -491.600, mean reward: -2.446 [-245.800, 75.600], mean action: 1.866 [0.000, 9.000], mean observation: 33.375 [0.001, 497.500], loss: 261.165192, mae: 40.096977, mean_q: -40.946274\n",
            " 1405995/10000000: episode: 6995, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -377.800, mean reward: -1.880 [-188.900, 52.800], mean action: 1.910 [0.000, 10.000], mean observation: 31.784 [0.002, 439.900], loss: 254.767563, mae: 39.876469, mean_q: -40.763332\n",
            " 1406196/10000000: episode: 6996, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -288.200, mean reward: -1.434 [-144.100, 71.400], mean action: 2.219 [0.000, 10.000], mean observation: 28.105 [0.002, 430.700], loss: 249.701019, mae: 39.752766, mean_q: -40.828068\n",
            " 1406397/10000000: episode: 6997, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: 73.800, mean reward: 0.367 [-10.000, 143.800], mean action: 2.114 [0.000, 10.000], mean observation: 36.867 [0.000, 552.400], loss: 295.678131, mae: 39.688206, mean_q: -40.784222\n",
            " 1406598/10000000: episode: 6998, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -523.000, mean reward: -2.602 [-261.500, 34.200], mean action: 2.184 [0.000, 10.000], mean observation: 32.563 [0.000, 720.900], loss: 221.020248, mae: 40.719082, mean_q: -41.509373\n",
            " 1406799/10000000: episode: 6999, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -83.000, mean reward: -0.413 [-41.500, 81.800], mean action: 2.154 [0.000, 8.000], mean observation: 30.667 [0.001, 500.800], loss: 275.102081, mae: 40.913044, mean_q: -41.748611\n",
            " 1407000/10000000: episode: 7000, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -58.200, mean reward: -0.290 [-29.100, 191.400], mean action: 2.010 [0.000, 10.000], mean observation: 34.736 [0.001, 441.700], loss: 324.222015, mae: 41.173164, mean_q: -42.074215\n",
            " 1407201/10000000: episode: 7001, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: 1127.200, mean reward: 5.608 [-8.000, 563.600], mean action: 2.483 [0.000, 10.000], mean observation: 27.404 [0.000, 348.700], loss: 273.006653, mae: 41.337727, mean_q: -42.214821\n",
            " 1407402/10000000: episode: 7002, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -574.600, mean reward: -2.859 [-287.300, 32.700], mean action: 2.264 [0.000, 10.000], mean observation: 35.188 [0.002, 537.100], loss: 255.961090, mae: 40.407001, mean_q: -41.496983\n",
            " 1407603/10000000: episode: 7003, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -308.000, mean reward: -1.532 [-154.000, 151.200], mean action: 2.353 [0.000, 10.000], mean observation: 33.319 [0.001, 562.800], loss: 223.726212, mae: 40.505634, mean_q: -41.593555\n",
            " 1407804/10000000: episode: 7004, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -253.800, mean reward: -1.263 [-126.900, 108.800], mean action: 2.229 [0.000, 10.000], mean observation: 30.445 [0.001, 530.600], loss: 305.066650, mae: 40.218094, mean_q: -41.311501\n",
            " 1408005/10000000: episode: 7005, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -486.600, mean reward: -2.421 [-243.300, 35.600], mean action: 1.861 [0.000, 9.000], mean observation: 39.498 [0.000, 693.300], loss: 342.446625, mae: 40.360832, mean_q: -41.176704\n",
            " 1408206/10000000: episode: 7006, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -212.200, mean reward: -1.056 [-106.100, 138.600], mean action: 2.408 [0.000, 8.000], mean observation: 32.867 [0.000, 727.600], loss: 189.710861, mae: 39.923325, mean_q: -41.192726\n",
            " 1408407/10000000: episode: 7007, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -401.000, mean reward: -1.995 [-200.500, 71.400], mean action: 2.483 [0.000, 10.000], mean observation: 35.989 [0.002, 479.600], loss: 322.996277, mae: 39.855392, mean_q: -40.983490\n",
            " 1408608/10000000: episode: 7008, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -340.200, mean reward: -1.693 [-170.100, 65.600], mean action: 2.692 [0.000, 10.000], mean observation: 33.135 [0.001, 606.200], loss: 211.694031, mae: 39.652451, mean_q: -40.655483\n",
            " 1408809/10000000: episode: 7009, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -343.400, mean reward: -1.708 [-171.700, 200.100], mean action: 2.358 [0.000, 10.000], mean observation: 33.871 [0.002, 597.900], loss: 231.660141, mae: 39.705173, mean_q: -40.677834\n",
            " 1409010/10000000: episode: 7010, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -438.000, mean reward: -2.179 [-219.000, 82.600], mean action: 2.622 [0.000, 8.000], mean observation: 31.227 [0.003, 482.900], loss: 300.970367, mae: 39.848045, mean_q: -40.932159\n",
            " 1409211/10000000: episode: 7011, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -351.600, mean reward: -1.749 [-175.800, 72.600], mean action: 2.348 [0.000, 10.000], mean observation: 26.357 [0.002, 365.900], loss: 278.213806, mae: 39.352772, mean_q: -40.482628\n",
            " 1409412/10000000: episode: 7012, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -574.400, mean reward: -2.858 [-287.200, 25.200], mean action: 2.050 [0.000, 8.000], mean observation: 33.425 [0.001, 697.100], loss: 231.243942, mae: 39.823528, mean_q: -40.775272\n",
            " 1409613/10000000: episode: 7013, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -520.000, mean reward: -2.587 [-260.000, 75.600], mean action: 2.706 [0.000, 10.000], mean observation: 31.736 [0.001, 562.800], loss: 305.978180, mae: 40.053566, mean_q: -41.156212\n",
            " 1409814/10000000: episode: 7014, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -794.600, mean reward: -3.953 [-397.300, 64.500], mean action: 2.572 [0.000, 10.000], mean observation: 34.885 [0.000, 458.900], loss: 345.398987, mae: 40.128971, mean_q: -41.321556\n",
            " 1410015/10000000: episode: 7015, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -332.400, mean reward: -1.654 [-166.200, 52.400], mean action: 2.557 [0.000, 10.000], mean observation: 30.045 [0.001, 522.200], loss: 255.139816, mae: 39.972050, mean_q: -41.213905\n",
            " 1410216/10000000: episode: 7016, duration: 1.380s, episode steps: 201, steps per second: 146, episode reward: 570.600, mean reward: 2.839 [-8.000, 285.300], mean action: 2.607 [0.000, 8.000], mean observation: 30.836 [0.000, 527.200], loss: 272.606110, mae: 40.881584, mean_q: -41.794891\n",
            " 1410417/10000000: episode: 7017, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -704.600, mean reward: -3.505 [-352.300, 48.900], mean action: 2.383 [0.000, 10.000], mean observation: 36.034 [0.000, 424.800], loss: 223.832397, mae: 40.859200, mean_q: -41.668682\n",
            " 1410618/10000000: episode: 7018, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -563.000, mean reward: -2.801 [-281.500, 51.600], mean action: 2.338 [0.000, 9.000], mean observation: 33.541 [0.000, 765.000], loss: 293.729126, mae: 41.301315, mean_q: -42.174278\n",
            " 1410819/10000000: episode: 7019, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -552.800, mean reward: -2.750 [-276.400, 73.800], mean action: 2.388 [0.000, 9.000], mean observation: 31.346 [0.000, 638.500], loss: 205.433029, mae: 40.809090, mean_q: -41.793602\n",
            " 1411020/10000000: episode: 7020, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -125.600, mean reward: -0.625 [-62.800, 135.200], mean action: 2.080 [0.000, 8.000], mean observation: 34.679 [0.001, 534.900], loss: 267.794800, mae: 41.115253, mean_q: -41.633015\n",
            " 1411221/10000000: episode: 7021, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: 673.000, mean reward: 3.348 [-10.000, 386.000], mean action: 2.408 [0.000, 10.000], mean observation: 38.013 [0.000, 793.800], loss: 299.334900, mae: 40.583302, mean_q: -41.443340\n",
            " 1411422/10000000: episode: 7022, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -276.400, mean reward: -1.375 [-138.200, 82.800], mean action: 2.706 [0.000, 10.000], mean observation: 36.250 [0.001, 681.600], loss: 214.559631, mae: 41.155422, mean_q: -42.251740\n",
            " 1411623/10000000: episode: 7023, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -802.400, mean reward: -3.992 [-401.200, 31.000], mean action: 2.373 [0.000, 8.000], mean observation: 39.792 [0.000, 701.500], loss: 233.582947, mae: 41.190018, mean_q: -42.348831\n",
            " 1411824/10000000: episode: 7024, duration: 1.389s, episode steps: 201, steps per second: 145, episode reward: 130.000, mean reward: 0.647 [-8.000, 126.400], mean action: 2.050 [0.000, 8.000], mean observation: 33.495 [0.000, 783.800], loss: 281.374573, mae: 41.482159, mean_q: -42.325207\n",
            " 1412025/10000000: episode: 7025, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: 572.600, mean reward: 2.849 [-9.000, 286.300], mean action: 2.030 [0.000, 9.000], mean observation: 36.298 [0.000, 642.000], loss: 254.334457, mae: 41.472988, mean_q: -42.139458\n",
            " 1412226/10000000: episode: 7026, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -302.400, mean reward: -1.504 [-151.200, 26.800], mean action: 1.861 [0.000, 10.000], mean observation: 34.838 [0.002, 515.200], loss: 200.508713, mae: 41.391243, mean_q: -41.995808\n",
            " 1412427/10000000: episode: 7027, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -328.600, mean reward: -1.635 [-164.300, 56.000], mean action: 2.119 [0.000, 10.000], mean observation: 37.263 [0.001, 430.300], loss: 296.225800, mae: 41.756180, mean_q: -42.632790\n",
            " 1412628/10000000: episode: 7028, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -365.600, mean reward: -1.819 [-182.800, 87.900], mean action: 2.577 [0.000, 10.000], mean observation: 36.586 [0.000, 673.700], loss: 241.316925, mae: 41.879566, mean_q: -43.202003\n",
            " 1412829/10000000: episode: 7029, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -271.200, mean reward: -1.349 [-135.600, 68.000], mean action: 2.493 [0.000, 10.000], mean observation: 29.861 [0.001, 446.200], loss: 305.933563, mae: 41.983345, mean_q: -43.292202\n",
            " 1413030/10000000: episode: 7030, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: 935.000, mean reward: 4.652 [-10.000, 564.000], mean action: 2.478 [0.000, 10.000], mean observation: 41.286 [0.001, 558.800], loss: 195.877625, mae: 41.914589, mean_q: -43.297974\n",
            " 1413231/10000000: episode: 7031, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: 5.200, mean reward: 0.026 [-10.000, 274.000], mean action: 2.254 [0.000, 10.000], mean observation: 31.678 [0.002, 533.200], loss: 278.719574, mae: 42.700249, mean_q: -43.959801\n",
            " 1413432/10000000: episode: 7032, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -176.400, mean reward: -0.878 [-88.200, 187.800], mean action: 2.269 [0.000, 10.000], mean observation: 34.601 [0.000, 933.200], loss: 248.007690, mae: 41.992928, mean_q: -43.458420\n",
            " 1413633/10000000: episode: 7033, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -405.000, mean reward: -2.015 [-202.500, 63.000], mean action: 2.567 [0.000, 10.000], mean observation: 32.959 [0.000, 684.900], loss: 327.528839, mae: 41.727669, mean_q: -43.345177\n",
            " 1413834/10000000: episode: 7034, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -517.800, mean reward: -2.576 [-258.900, 64.800], mean action: 2.164 [0.000, 10.000], mean observation: 29.934 [0.001, 556.700], loss: 295.802612, mae: 42.240074, mean_q: -43.241146\n",
            " 1414035/10000000: episode: 7035, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -718.000, mean reward: -3.572 [-359.000, 97.200], mean action: 3.343 [0.000, 10.000], mean observation: 35.179 [0.000, 528.900], loss: 273.247131, mae: 41.410313, mean_q: -43.092503\n",
            " 1414236/10000000: episode: 7036, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -96.000, mean reward: -0.478 [-48.000, 300.000], mean action: 3.090 [0.000, 10.000], mean observation: 36.850 [0.000, 607.900], loss: 351.113190, mae: 41.569210, mean_q: -43.056408\n",
            " 1414437/10000000: episode: 7037, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: -387.600, mean reward: -1.928 [-193.800, 57.400], mean action: 2.363 [0.000, 10.000], mean observation: 35.790 [0.001, 528.400], loss: 219.692413, mae: 42.472069, mean_q: -43.703125\n",
            " 1414638/10000000: episode: 7038, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -689.800, mean reward: -3.432 [-344.900, 58.400], mean action: 2.736 [0.000, 9.000], mean observation: 30.756 [0.000, 455.800], loss: 417.278931, mae: 42.093376, mean_q: -43.581131\n",
            " 1414839/10000000: episode: 7039, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: 137.800, mean reward: 0.686 [-9.000, 332.500], mean action: 3.104 [0.000, 9.000], mean observation: 31.174 [0.000, 818.500], loss: 274.138489, mae: 42.408733, mean_q: -44.016029\n",
            " 1415040/10000000: episode: 7040, duration: 1.479s, episode steps: 201, steps per second: 136, episode reward: 41.600, mean reward: 0.207 [-9.000, 82.800], mean action: 2.388 [0.000, 9.000], mean observation: 34.388 [0.002, 441.900], loss: 230.286697, mae: 43.272816, mean_q: -44.580235\n",
            " 1415241/10000000: episode: 7041, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -678.000, mean reward: -3.373 [-339.000, 20.000], mean action: 2.189 [0.000, 10.000], mean observation: 27.811 [0.001, 619.000], loss: 209.624664, mae: 43.439281, mean_q: -44.357410\n",
            " 1415442/10000000: episode: 7042, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: 129.200, mean reward: 0.643 [-9.000, 150.000], mean action: 2.159 [0.000, 9.000], mean observation: 33.056 [0.001, 461.700], loss: 225.305115, mae: 43.052357, mean_q: -44.122871\n",
            " 1415643/10000000: episode: 7043, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -335.200, mean reward: -1.668 [-167.600, 179.500], mean action: 2.134 [0.000, 10.000], mean observation: 36.769 [0.002, 402.500], loss: 212.940598, mae: 43.116577, mean_q: -44.078228\n",
            " 1415844/10000000: episode: 7044, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -380.400, mean reward: -1.893 [-190.200, 121.200], mean action: 2.577 [0.000, 10.000], mean observation: 33.478 [0.000, 501.600], loss: 266.956512, mae: 42.364441, mean_q: -43.711716\n",
            " 1416045/10000000: episode: 7045, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -190.800, mean reward: -0.949 [-95.400, 82.800], mean action: 1.771 [0.000, 9.000], mean observation: 37.349 [0.001, 492.600], loss: 277.525665, mae: 43.092323, mean_q: -43.513351\n",
            " 1416246/10000000: episode: 7046, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: 905.800, mean reward: 4.506 [-9.000, 688.800], mean action: 2.294 [0.000, 9.000], mean observation: 30.606 [0.000, 590.900], loss: 307.994293, mae: 42.274773, mean_q: -43.470833\n",
            " 1416447/10000000: episode: 7047, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -430.200, mean reward: -2.140 [-215.100, 85.500], mean action: 2.517 [0.000, 9.000], mean observation: 35.172 [0.000, 747.100], loss: 281.860565, mae: 42.500782, mean_q: -43.813122\n",
            " 1416648/10000000: episode: 7048, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -200.200, mean reward: -0.996 [-100.100, 86.400], mean action: 2.194 [0.000, 9.000], mean observation: 34.151 [0.000, 462.000], loss: 268.635406, mae: 42.034458, mean_q: -43.264332\n",
            " 1416849/10000000: episode: 7049, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -153.000, mean reward: -0.761 [-76.500, 88.000], mean action: 2.269 [0.000, 9.000], mean observation: 25.792 [0.001, 534.800], loss: 298.452454, mae: 42.415905, mean_q: -43.464893\n",
            " 1417050/10000000: episode: 7050, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -712.600, mean reward: -3.545 [-356.300, 19.300], mean action: 2.303 [0.000, 10.000], mean observation: 30.378 [0.000, 580.300], loss: 339.605804, mae: 41.847908, mean_q: -43.135078\n",
            " 1417251/10000000: episode: 7051, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -615.800, mean reward: -3.064 [-307.900, 22.400], mean action: 2.493 [0.000, 9.000], mean observation: 37.003 [0.000, 522.100], loss: 191.134430, mae: 41.697510, mean_q: -42.752399\n",
            " 1417452/10000000: episode: 7052, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -95.200, mean reward: -0.474 [-47.600, 93.000], mean action: 2.632 [0.000, 10.000], mean observation: 27.159 [0.001, 590.600], loss: 277.607208, mae: 41.289261, mean_q: -42.361103\n",
            " 1417653/10000000: episode: 7053, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -543.400, mean reward: -2.703 [-271.700, 45.600], mean action: 2.224 [0.000, 9.000], mean observation: 35.804 [0.000, 813.800], loss: 226.339127, mae: 41.447071, mean_q: -42.655861\n",
            " 1417854/10000000: episode: 7054, duration: 1.473s, episode steps: 201, steps per second: 136, episode reward: -736.200, mean reward: -3.663 [-368.100, 48.000], mean action: 2.413 [0.000, 10.000], mean observation: 39.328 [0.001, 632.400], loss: 237.886505, mae: 41.356094, mean_q: -42.786953\n",
            " 1418055/10000000: episode: 7055, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -376.800, mean reward: -1.875 [-188.400, 216.300], mean action: 3.035 [0.000, 10.000], mean observation: 29.725 [0.003, 531.400], loss: 294.067535, mae: 41.381321, mean_q: -43.070652\n",
            " 1418256/10000000: episode: 7056, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -796.800, mean reward: -3.964 [-398.400, 70.000], mean action: 3.129 [0.000, 8.000], mean observation: 35.448 [0.002, 519.900], loss: 319.764252, mae: 40.904453, mean_q: -42.923431\n",
            " 1418457/10000000: episode: 7057, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -526.400, mean reward: -2.619 [-263.200, 72.600], mean action: 2.846 [0.000, 10.000], mean observation: 37.803 [0.001, 518.800], loss: 207.845901, mae: 41.301620, mean_q: -43.107510\n",
            " 1418658/10000000: episode: 7058, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: 242.200, mean reward: 1.205 [-10.000, 342.600], mean action: 2.801 [0.000, 10.000], mean observation: 38.180 [0.001, 554.400], loss: 277.699036, mae: 42.176983, mean_q: -43.365620\n",
            " 1418859/10000000: episode: 7059, duration: 1.599s, episode steps: 201, steps per second: 126, episode reward: -139.200, mean reward: -0.693 [-69.600, 75.800], mean action: 2.055 [0.000, 10.000], mean observation: 39.020 [0.001, 627.200], loss: 242.784637, mae: 42.114113, mean_q: -43.151489\n",
            " 1419060/10000000: episode: 7060, duration: 1.613s, episode steps: 201, steps per second: 125, episode reward: -38.000, mean reward: -0.189 [-19.000, 102.800], mean action: 2.179 [0.000, 10.000], mean observation: 34.487 [0.000, 604.500], loss: 219.903214, mae: 42.710773, mean_q: -43.525974\n",
            " 1419261/10000000: episode: 7061, duration: 1.592s, episode steps: 201, steps per second: 126, episode reward: -530.200, mean reward: -2.638 [-265.100, 84.900], mean action: 2.701 [0.000, 10.000], mean observation: 32.767 [0.001, 500.600], loss: 300.992645, mae: 42.444267, mean_q: -44.017643\n",
            " 1419462/10000000: episode: 7062, duration: 1.590s, episode steps: 201, steps per second: 126, episode reward: -460.800, mean reward: -2.293 [-230.400, 52.000], mean action: 2.368 [0.000, 9.000], mean observation: 28.874 [0.001, 464.600], loss: 232.440491, mae: 42.569408, mean_q: -43.842152\n",
            " 1419663/10000000: episode: 7063, duration: 1.603s, episode steps: 201, steps per second: 125, episode reward: -364.200, mean reward: -1.812 [-182.100, 67.200], mean action: 2.259 [0.000, 10.000], mean observation: 31.193 [0.004, 531.100], loss: 231.489273, mae: 42.557446, mean_q: -43.629440\n",
            " 1419864/10000000: episode: 7064, duration: 1.733s, episode steps: 201, steps per second: 116, episode reward: 14.000, mean reward: 0.070 [-10.000, 120.300], mean action: 2.005 [0.000, 10.000], mean observation: 35.619 [0.001, 507.400], loss: 245.653992, mae: 42.291920, mean_q: -43.151337\n",
            " 1420065/10000000: episode: 7065, duration: 1.580s, episode steps: 201, steps per second: 127, episode reward: -597.800, mean reward: -2.974 [-298.900, 51.100], mean action: 2.174 [0.000, 9.000], mean observation: 34.815 [0.000, 784.200], loss: 230.194656, mae: 42.239330, mean_q: -43.318275\n",
            " 1420266/10000000: episode: 7066, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -23.800, mean reward: -0.118 [-11.900, 164.400], mean action: 1.826 [0.000, 10.000], mean observation: 35.881 [0.000, 722.600], loss: 212.339340, mae: 42.889584, mean_q: -43.600815\n",
            " 1420467/10000000: episode: 7067, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -511.000, mean reward: -2.542 [-255.500, 34.300], mean action: 1.876 [0.000, 9.000], mean observation: 33.250 [0.001, 542.200], loss: 275.675018, mae: 43.438187, mean_q: -44.009781\n",
            " 1420668/10000000: episode: 7068, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: 823.400, mean reward: 4.097 [-9.000, 525.300], mean action: 2.184 [0.000, 9.000], mean observation: 35.737 [0.001, 448.100], loss: 238.017426, mae: 42.961117, mean_q: -44.060104\n",
            " 1420869/10000000: episode: 7069, duration: 1.451s, episode steps: 201, steps per second: 139, episode reward: -923.800, mean reward: -4.596 [-461.900, 19.600], mean action: 2.632 [0.000, 10.000], mean observation: 29.046 [0.000, 527.300], loss: 309.053406, mae: 42.791988, mean_q: -44.347404\n",
            " 1421070/10000000: episode: 7070, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -699.400, mean reward: -3.480 [-349.700, 50.000], mean action: 2.597 [0.000, 10.000], mean observation: 32.537 [0.003, 542.400], loss: 346.111694, mae: 42.898403, mean_q: -44.835632\n",
            " 1421271/10000000: episode: 7071, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -643.600, mean reward: -3.202 [-321.800, 92.400], mean action: 2.463 [0.000, 8.000], mean observation: 38.668 [0.000, 694.400], loss: 223.880539, mae: 43.908096, mean_q: -45.483624\n",
            " 1421472/10000000: episode: 7072, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -581.400, mean reward: -2.893 [-290.700, 44.400], mean action: 1.910 [0.000, 9.000], mean observation: 34.663 [0.000, 598.300], loss: 305.530914, mae: 44.882259, mean_q: -45.448666\n",
            " 1421673/10000000: episode: 7073, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -583.800, mean reward: -2.904 [-291.900, 49.000], mean action: 2.398 [0.000, 10.000], mean observation: 30.409 [0.004, 555.700], loss: 278.513489, mae: 43.655842, mean_q: -45.121922\n",
            " 1421874/10000000: episode: 7074, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -502.000, mean reward: -2.498 [-251.000, 57.600], mean action: 2.781 [0.000, 10.000], mean observation: 34.996 [0.000, 669.900], loss: 242.650787, mae: 43.813995, mean_q: -45.943359\n",
            " 1422075/10000000: episode: 7075, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -414.800, mean reward: -2.064 [-207.400, 65.600], mean action: 2.348 [0.000, 10.000], mean observation: 39.327 [0.001, 662.900], loss: 264.436310, mae: 44.999981, mean_q: -46.561947\n",
            " 1422276/10000000: episode: 7076, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: 208.400, mean reward: 1.037 [-9.000, 153.300], mean action: 1.900 [0.000, 9.000], mean observation: 35.318 [0.002, 482.600], loss: 343.008545, mae: 44.717785, mean_q: -45.662479\n",
            " 1422477/10000000: episode: 7077, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -418.600, mean reward: -2.083 [-209.300, 34.800], mean action: 2.502 [0.000, 10.000], mean observation: 35.062 [0.001, 419.000], loss: 283.547699, mae: 44.148968, mean_q: -46.059151\n",
            " 1422678/10000000: episode: 7078, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -486.600, mean reward: -2.421 [-243.300, 111.000], mean action: 2.502 [0.000, 10.000], mean observation: 35.787 [0.000, 654.700], loss: 269.762054, mae: 44.201084, mean_q: -45.850498\n",
            " 1422879/10000000: episode: 7079, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -45.600, mean reward: -0.227 [-22.800, 203.000], mean action: 2.269 [0.000, 7.000], mean observation: 34.503 [0.000, 706.400], loss: 273.559692, mae: 45.526760, mean_q: -46.603458\n",
            " 1423080/10000000: episode: 7080, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -95.800, mean reward: -0.477 [-47.900, 120.400], mean action: 2.478 [0.000, 10.000], mean observation: 31.183 [0.001, 422.500], loss: 171.272354, mae: 45.493675, mean_q: -46.715424\n",
            " 1423281/10000000: episode: 7081, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -237.800, mean reward: -1.183 [-118.900, 148.500], mean action: 2.418 [0.000, 9.000], mean observation: 33.015 [0.001, 568.100], loss: 229.152512, mae: 45.921185, mean_q: -47.228836\n",
            " 1423482/10000000: episode: 7082, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -470.800, mean reward: -2.342 [-235.400, 63.300], mean action: 2.373 [0.000, 10.000], mean observation: 29.808 [0.001, 676.900], loss: 271.588226, mae: 45.583721, mean_q: -46.690514\n",
            " 1423683/10000000: episode: 7083, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -668.200, mean reward: -3.324 [-334.100, 49.200], mean action: 2.622 [0.000, 10.000], mean observation: 37.643 [0.000, 714.300], loss: 266.999207, mae: 45.038208, mean_q: -46.674046\n",
            " 1423884/10000000: episode: 7084, duration: 1.379s, episode steps: 201, steps per second: 146, episode reward: -767.400, mean reward: -3.818 [-383.700, 24.500], mean action: 2.378 [0.000, 9.000], mean observation: 36.394 [0.000, 601.500], loss: 273.810516, mae: 44.452866, mean_q: -46.056328\n",
            " 1424085/10000000: episode: 7085, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: 92.000, mean reward: 0.458 [-10.000, 99.300], mean action: 2.547 [0.000, 10.000], mean observation: 34.527 [0.000, 582.800], loss: 178.285995, mae: 43.989399, mean_q: -45.700821\n",
            " 1424286/10000000: episode: 7086, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -202.200, mean reward: -1.006 [-101.100, 46.500], mean action: 2.299 [0.000, 9.000], mean observation: 33.560 [0.001, 498.800], loss: 152.871460, mae: 44.450554, mean_q: -45.766285\n",
            " 1424487/10000000: episode: 7087, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -453.200, mean reward: -2.255 [-226.600, 65.100], mean action: 2.925 [0.000, 10.000], mean observation: 28.846 [0.002, 508.500], loss: 324.318451, mae: 43.907398, mean_q: -46.138264\n",
            " 1424688/10000000: episode: 7088, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: 357.600, mean reward: 1.779 [-10.000, 343.800], mean action: 2.527 [0.000, 10.000], mean observation: 31.179 [0.000, 413.800], loss: 211.347641, mae: 44.513180, mean_q: -46.165462\n",
            " 1424889/10000000: episode: 7089, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -721.000, mean reward: -3.587 [-360.500, 30.000], mean action: 2.517 [0.000, 10.000], mean observation: 35.323 [0.000, 367.900], loss: 284.885651, mae: 44.162109, mean_q: -46.054035\n",
            " 1425090/10000000: episode: 7090, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -722.200, mean reward: -3.593 [-361.100, 32.000], mean action: 2.433 [0.000, 9.000], mean observation: 37.051 [0.001, 630.500], loss: 227.561493, mae: 44.973373, mean_q: -46.680912\n",
            " 1425291/10000000: episode: 7091, duration: 1.379s, episode steps: 201, steps per second: 146, episode reward: -53.400, mean reward: -0.266 [-26.700, 132.100], mean action: 2.423 [0.000, 10.000], mean observation: 30.866 [0.001, 518.600], loss: 239.283936, mae: 45.654053, mean_q: -46.867653\n",
            " 1425492/10000000: episode: 7092, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -94.600, mean reward: -0.471 [-47.300, 222.200], mean action: 2.413 [0.000, 9.000], mean observation: 36.471 [0.000, 562.200], loss: 252.832291, mae: 45.128674, mean_q: -45.905830\n",
            " 1425693/10000000: episode: 7093, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: 1030.400, mean reward: 5.126 [-10.000, 515.200], mean action: 2.647 [0.000, 10.000], mean observation: 33.292 [0.002, 637.200], loss: 366.021149, mae: 44.244946, mean_q: -44.905113\n",
            " 1425894/10000000: episode: 7094, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -505.600, mean reward: -2.515 [-252.800, 125.100], mean action: 2.602 [0.000, 8.000], mean observation: 26.522 [0.005, 368.500], loss: 314.476227, mae: 43.323719, mean_q: -44.107922\n",
            " 1426095/10000000: episode: 7095, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -23.600, mean reward: -0.117 [-11.800, 91.000], mean action: 2.806 [0.000, 9.000], mean observation: 33.169 [0.002, 607.700], loss: 317.318176, mae: 43.390656, mean_q: -44.817902\n",
            " 1426296/10000000: episode: 7096, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -713.000, mean reward: -3.547 [-356.500, 30.400], mean action: 2.498 [0.000, 10.000], mean observation: 28.065 [0.001, 497.800], loss: 288.498627, mae: 43.059486, mean_q: -44.539894\n",
            " 1426497/10000000: episode: 7097, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -381.600, mean reward: -1.899 [-190.800, 66.800], mean action: 1.821 [0.000, 10.000], mean observation: 39.440 [0.001, 456.700], loss: 160.565231, mae: 43.385826, mean_q: -44.578388\n",
            " 1426698/10000000: episode: 7098, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -321.400, mean reward: -1.599 [-160.700, 63.200], mean action: 2.229 [0.000, 10.000], mean observation: 32.212 [0.000, 772.000], loss: 173.962051, mae: 43.103619, mean_q: -44.528873\n",
            " 1426899/10000000: episode: 7099, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -187.000, mean reward: -0.930 [-93.500, 114.300], mean action: 2.373 [0.000, 10.000], mean observation: 31.541 [0.001, 558.800], loss: 215.610214, mae: 42.824871, mean_q: -44.099407\n",
            " 1427100/10000000: episode: 7100, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -90.800, mean reward: -0.452 [-45.400, 132.000], mean action: 2.184 [0.000, 10.000], mean observation: 30.776 [0.000, 501.100], loss: 247.280487, mae: 42.919544, mean_q: -44.274963\n",
            " 1427301/10000000: episode: 7101, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: 611.600, mean reward: 3.043 [-10.000, 305.800], mean action: 1.940 [0.000, 10.000], mean observation: 35.217 [0.001, 645.600], loss: 159.516434, mae: 43.027843, mean_q: -44.319092\n",
            " 1427502/10000000: episode: 7102, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -687.400, mean reward: -3.420 [-343.700, 28.800], mean action: 2.184 [0.000, 10.000], mean observation: 32.154 [0.001, 516.000], loss: 223.709854, mae: 42.406914, mean_q: -43.738430\n",
            " 1427703/10000000: episode: 7103, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 249.000, mean reward: 1.239 [-10.000, 172.800], mean action: 3.124 [0.000, 10.000], mean observation: 34.262 [0.000, 555.400], loss: 382.048737, mae: 41.804123, mean_q: -43.716431\n",
            " 1427904/10000000: episode: 7104, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: -667.800, mean reward: -3.322 [-333.900, 137.600], mean action: 3.214 [0.000, 10.000], mean observation: 34.361 [0.000, 553.100], loss: 234.885147, mae: 41.952839, mean_q: -44.002934\n",
            " 1428105/10000000: episode: 7105, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -275.200, mean reward: -1.369 [-137.600, 97.800], mean action: 2.721 [0.000, 10.000], mean observation: 37.264 [0.000, 668.100], loss: 265.000671, mae: 42.229523, mean_q: -44.064743\n",
            " 1428306/10000000: episode: 7106, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -954.800, mean reward: -4.750 [-477.400, 12.000], mean action: 2.741 [0.000, 9.000], mean observation: 31.612 [0.000, 483.800], loss: 164.733246, mae: 42.129082, mean_q: -44.053234\n",
            " 1428507/10000000: episode: 7107, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: 135.800, mean reward: 0.676 [-8.000, 196.200], mean action: 2.100 [0.000, 8.000], mean observation: 28.187 [0.002, 513.300], loss: 215.669434, mae: 42.226303, mean_q: -43.371326\n",
            " 1428708/10000000: episode: 7108, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -284.800, mean reward: -1.417 [-142.400, 63.200], mean action: 2.363 [0.000, 10.000], mean observation: 32.650 [0.000, 606.600], loss: 185.415985, mae: 41.684109, mean_q: -43.204899\n",
            " 1428909/10000000: episode: 7109, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: -557.200, mean reward: -2.772 [-278.600, 61.700], mean action: 2.328 [0.000, 9.000], mean observation: 36.868 [0.001, 493.400], loss: 217.131760, mae: 41.937603, mean_q: -43.456360\n",
            " 1429110/10000000: episode: 7110, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -609.000, mean reward: -3.030 [-304.500, 43.800], mean action: 2.239 [0.000, 10.000], mean observation: 33.485 [0.002, 578.200], loss: 245.314987, mae: 42.215202, mean_q: -43.732780\n",
            " 1429311/10000000: episode: 7111, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -658.600, mean reward: -3.277 [-329.300, 56.000], mean action: 2.731 [0.000, 9.000], mean observation: 32.448 [0.001, 461.100], loss: 197.725449, mae: 42.061848, mean_q: -43.754745\n",
            " 1429512/10000000: episode: 7112, duration: 1.383s, episode steps: 201, steps per second: 145, episode reward: -522.600, mean reward: -2.600 [-261.300, 51.000], mean action: 1.995 [0.000, 8.000], mean observation: 40.175 [0.000, 527.200], loss: 288.508423, mae: 42.800137, mean_q: -44.033306\n",
            " 1429713/10000000: episode: 7113, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -470.200, mean reward: -2.339 [-235.100, 122.400], mean action: 3.144 [0.000, 8.000], mean observation: 30.930 [0.001, 675.300], loss: 224.193726, mae: 42.360844, mean_q: -44.525444\n",
            " 1429914/10000000: episode: 7114, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -388.800, mean reward: -1.934 [-194.400, 165.500], mean action: 3.244 [0.000, 9.000], mean observation: 30.803 [0.001, 472.800], loss: 252.580765, mae: 42.846500, mean_q: -44.531143\n",
            " 1430115/10000000: episode: 7115, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: 592.800, mean reward: 2.949 [-9.000, 392.000], mean action: 2.343 [0.000, 9.000], mean observation: 32.835 [0.000, 527.500], loss: 161.367950, mae: 43.698967, mean_q: -44.745975\n",
            " 1430316/10000000: episode: 7116, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -245.400, mean reward: -1.221 [-122.700, 52.500], mean action: 1.662 [0.000, 8.000], mean observation: 30.900 [0.000, 544.700], loss: 244.782104, mae: 44.258118, mean_q: -44.390854\n",
            " 1430517/10000000: episode: 7117, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -175.200, mean reward: -0.872 [-87.600, 112.000], mean action: 1.572 [0.000, 8.000], mean observation: 39.278 [0.001, 532.900], loss: 311.282135, mae: 43.603680, mean_q: -43.060406\n",
            " 1430718/10000000: episode: 7118, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: -430.800, mean reward: -2.143 [-215.400, 28.000], mean action: 1.493 [0.000, 9.000], mean observation: 34.158 [0.002, 510.800], loss: 204.497635, mae: 41.999561, mean_q: -42.146763\n",
            " 1430919/10000000: episode: 7119, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -475.800, mean reward: -2.367 [-237.900, 40.800], mean action: 1.806 [0.000, 8.000], mean observation: 31.388 [0.001, 647.400], loss: 215.111877, mae: 41.341808, mean_q: -41.464085\n",
            " 1431120/10000000: episode: 7120, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -431.000, mean reward: -2.144 [-215.500, 51.900], mean action: 2.020 [0.000, 10.000], mean observation: 32.894 [0.000, 602.400], loss: 243.894516, mae: 40.850693, mean_q: -41.340843\n",
            " 1431321/10000000: episode: 7121, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -407.800, mean reward: -2.029 [-203.900, 46.800], mean action: 1.756 [0.000, 10.000], mean observation: 32.586 [0.002, 364.300], loss: 284.944214, mae: 40.366753, mean_q: -40.701149\n",
            " 1431522/10000000: episode: 7122, duration: 1.382s, episode steps: 201, steps per second: 145, episode reward: 160.600, mean reward: 0.799 [-9.000, 106.200], mean action: 2.119 [0.000, 9.000], mean observation: 27.527 [0.001, 458.100], loss: 248.117691, mae: 39.979000, mean_q: -40.930180\n",
            " 1431723/10000000: episode: 7123, duration: 1.382s, episode steps: 201, steps per second: 145, episode reward: 123.000, mean reward: 0.612 [-9.000, 287.500], mean action: 2.781 [0.000, 10.000], mean observation: 33.070 [0.000, 695.400], loss: 280.686676, mae: 39.508617, mean_q: -40.928860\n",
            " 1431924/10000000: episode: 7124, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: 612.400, mean reward: 3.047 [-10.000, 353.000], mean action: 2.318 [0.000, 10.000], mean observation: 38.265 [0.000, 660.900], loss: 242.297180, mae: 40.160313, mean_q: -41.152161\n",
            " 1432125/10000000: episode: 7125, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -187.200, mean reward: -0.931 [-93.600, 66.500], mean action: 1.985 [0.000, 8.000], mean observation: 31.605 [0.001, 400.300], loss: 180.495605, mae: 39.757534, mean_q: -40.477871\n",
            " 1432326/10000000: episode: 7126, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -454.800, mean reward: -2.263 [-227.400, 47.900], mean action: 1.796 [0.000, 10.000], mean observation: 33.419 [0.000, 446.900], loss: 222.454498, mae: 40.308807, mean_q: -40.685738\n",
            " 1432527/10000000: episode: 7127, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -423.000, mean reward: -2.104 [-211.500, 133.800], mean action: 1.851 [0.000, 8.000], mean observation: 32.789 [0.001, 654.600], loss: 212.358215, mae: 40.493721, mean_q: -40.972137\n",
            " 1432728/10000000: episode: 7128, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: 717.000, mean reward: 3.567 [-10.000, 358.500], mean action: 2.129 [0.000, 10.000], mean observation: 31.734 [0.000, 467.400], loss: 229.776276, mae: 40.446259, mean_q: -41.310860\n",
            " 1432929/10000000: episode: 7129, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -379.600, mean reward: -1.889 [-189.800, 76.800], mean action: 2.204 [0.000, 10.000], mean observation: 36.116 [0.000, 420.600], loss: 231.251572, mae: 40.318520, mean_q: -41.261684\n",
            " 1433130/10000000: episode: 7130, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -314.200, mean reward: -1.563 [-157.100, 114.500], mean action: 2.522 [0.000, 10.000], mean observation: 33.076 [0.000, 697.600], loss: 307.285797, mae: 39.482231, mean_q: -40.872330\n",
            " 1433331/10000000: episode: 7131, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -673.000, mean reward: -3.348 [-336.500, 62.300], mean action: 2.736 [0.000, 10.000], mean observation: 31.492 [0.000, 443.500], loss: 190.067001, mae: 40.105892, mean_q: -41.648201\n",
            " 1433532/10000000: episode: 7132, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: -488.800, mean reward: -2.432 [-244.400, 79.600], mean action: 2.647 [0.000, 10.000], mean observation: 34.245 [0.001, 470.800], loss: 261.254364, mae: 40.388702, mean_q: -41.762856\n",
            " 1433733/10000000: episode: 7133, duration: 1.389s, episode steps: 201, steps per second: 145, episode reward: -284.200, mean reward: -1.414 [-142.100, 72.200], mean action: 2.075 [0.000, 10.000], mean observation: 28.026 [0.002, 400.000], loss: 206.819183, mae: 40.708832, mean_q: -41.806095\n",
            " 1433934/10000000: episode: 7134, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -707.800, mean reward: -3.521 [-353.900, 21.700], mean action: 2.358 [0.000, 9.000], mean observation: 33.425 [0.002, 515.900], loss: 262.218079, mae: 40.163891, mean_q: -41.456230\n",
            " 1434135/10000000: episode: 7135, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -400.000, mean reward: -1.990 [-200.000, 84.000], mean action: 1.955 [0.000, 8.000], mean observation: 35.206 [0.002, 467.700], loss: 174.161591, mae: 40.481743, mean_q: -41.261127\n",
            " 1434336/10000000: episode: 7136, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -220.400, mean reward: -1.097 [-110.200, 58.200], mean action: 1.682 [0.000, 7.000], mean observation: 27.581 [0.005, 434.600], loss: 281.588898, mae: 40.422234, mean_q: -41.118080\n",
            " 1434537/10000000: episode: 7137, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -115.200, mean reward: -0.573 [-57.600, 60.600], mean action: 1.637 [0.000, 7.000], mean observation: 29.540 [0.000, 638.600], loss: 255.201431, mae: 40.425117, mean_q: -40.537338\n",
            " 1434738/10000000: episode: 7138, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -284.400, mean reward: -1.415 [-142.200, 51.600], mean action: 1.493 [0.000, 9.000], mean observation: 38.839 [0.000, 584.000], loss: 241.801376, mae: 39.496174, mean_q: -39.454060\n",
            " 1434939/10000000: episode: 7139, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -426.800, mean reward: -2.123 [-213.400, 34.800], mean action: 1.871 [0.000, 7.000], mean observation: 29.882 [0.000, 530.500], loss: 207.416687, mae: 39.220676, mean_q: -39.556553\n",
            " 1435140/10000000: episode: 7140, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -315.400, mean reward: -1.569 [-157.700, 17.000], mean action: 1.368 [0.000, 10.000], mean observation: 35.925 [0.000, 796.200], loss: 210.139847, mae: 39.499306, mean_q: -39.333366\n",
            " 1435341/10000000: episode: 7141, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -616.200, mean reward: -3.066 [-308.100, 17.400], mean action: 1.995 [0.000, 8.000], mean observation: 37.800 [0.000, 588.400], loss: 220.903534, mae: 38.266010, mean_q: -39.008297\n",
            " 1435542/10000000: episode: 7142, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: 150.600, mean reward: 0.749 [-10.000, 168.200], mean action: 2.209 [0.000, 10.000], mean observation: 33.417 [0.000, 507.900], loss: 233.754547, mae: 38.082897, mean_q: -39.249413\n",
            " 1435743/10000000: episode: 7143, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -623.400, mean reward: -3.101 [-311.700, 57.200], mean action: 2.343 [0.000, 10.000], mean observation: 33.550 [0.003, 463.800], loss: 243.818130, mae: 38.755600, mean_q: -40.012119\n",
            " 1435944/10000000: episode: 7144, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 60.200, mean reward: 0.300 [-10.000, 222.000], mean action: 2.373 [0.000, 10.000], mean observation: 28.973 [0.002, 545.200], loss: 240.608826, mae: 38.779583, mean_q: -40.228794\n",
            " 1436145/10000000: episode: 7145, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -578.400, mean reward: -2.878 [-289.200, 22.800], mean action: 1.886 [0.000, 10.000], mean observation: 28.491 [0.004, 451.800], loss: 214.484390, mae: 39.638706, mean_q: -40.505573\n",
            " 1436346/10000000: episode: 7146, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: 295.200, mean reward: 1.469 [-9.000, 147.600], mean action: 2.124 [0.000, 9.000], mean observation: 35.936 [0.000, 491.000], loss: 315.452698, mae: 39.609341, mean_q: -40.400677\n",
            " 1436547/10000000: episode: 7147, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -959.600, mean reward: -4.774 [-479.800, 25.200], mean action: 2.642 [0.000, 10.000], mean observation: 33.579 [0.002, 481.400], loss: 260.738831, mae: 39.282513, mean_q: -40.328262\n",
            " 1436748/10000000: episode: 7148, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -478.200, mean reward: -2.379 [-239.100, 68.000], mean action: 2.682 [0.000, 10.000], mean observation: 31.169 [0.000, 746.900], loss: 187.682510, mae: 38.883385, mean_q: -39.793911\n",
            " 1436949/10000000: episode: 7149, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -431.400, mean reward: -2.146 [-215.700, 50.400], mean action: 1.910 [0.000, 10.000], mean observation: 31.440 [0.000, 609.400], loss: 259.558838, mae: 39.156887, mean_q: -39.718559\n",
            " 1437150/10000000: episode: 7150, duration: 1.481s, episode steps: 201, steps per second: 136, episode reward: -536.800, mean reward: -2.671 [-268.400, 46.000], mean action: 2.244 [0.000, 8.000], mean observation: 29.559 [0.001, 583.400], loss: 242.148285, mae: 38.509796, mean_q: -39.341076\n",
            " 1437351/10000000: episode: 7151, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -541.200, mean reward: -2.693 [-270.600, 48.600], mean action: 2.358 [0.000, 8.000], mean observation: 32.689 [0.000, 702.100], loss: 169.211197, mae: 38.240326, mean_q: -39.386395\n",
            " 1437552/10000000: episode: 7152, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -357.400, mean reward: -1.778 [-178.700, 61.200], mean action: 2.194 [0.000, 9.000], mean observation: 42.790 [0.000, 653.600], loss: 281.773224, mae: 38.767670, mean_q: -40.149502\n",
            " 1437753/10000000: episode: 7153, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -263.400, mean reward: -1.310 [-131.700, 55.800], mean action: 2.313 [0.000, 10.000], mean observation: 25.008 [0.000, 509.800], loss: 264.795471, mae: 39.109337, mean_q: -40.691319\n",
            " 1437954/10000000: episode: 7154, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -380.000, mean reward: -1.891 [-190.000, 54.600], mean action: 2.274 [0.000, 9.000], mean observation: 37.563 [0.002, 630.900], loss: 279.427734, mae: 39.258209, mean_q: -40.666012\n",
            " 1438155/10000000: episode: 7155, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: 454.600, mean reward: 2.262 [-9.000, 252.900], mean action: 2.124 [0.000, 9.000], mean observation: 37.483 [0.001, 571.000], loss: 247.993256, mae: 39.542324, mean_q: -40.850262\n",
            " 1438356/10000000: episode: 7156, duration: 1.593s, episode steps: 201, steps per second: 126, episode reward: -363.200, mean reward: -1.807 [-181.600, 41.400], mean action: 1.522 [0.000, 10.000], mean observation: 36.015 [0.001, 591.000], loss: 164.086929, mae: 39.684181, mean_q: -40.483368\n",
            " 1438557/10000000: episode: 7157, duration: 1.651s, episode steps: 201, steps per second: 122, episode reward: -547.200, mean reward: -2.722 [-273.600, 27.300], mean action: 2.139 [0.000, 10.000], mean observation: 29.271 [0.001, 532.400], loss: 321.406616, mae: 39.669922, mean_q: -40.707630\n",
            " 1438758/10000000: episode: 7158, duration: 1.655s, episode steps: 201, steps per second: 121, episode reward: -381.000, mean reward: -1.896 [-190.500, 36.900], mean action: 1.552 [0.000, 10.000], mean observation: 34.952 [0.002, 595.600], loss: 196.889847, mae: 39.980141, mean_q: -40.563900\n",
            " 1438959/10000000: episode: 7159, duration: 1.623s, episode steps: 201, steps per second: 124, episode reward: -130.400, mean reward: -0.649 [-65.200, 116.400], mean action: 1.736 [0.000, 8.000], mean observation: 31.893 [0.000, 530.000], loss: 188.188507, mae: 39.202110, mean_q: -39.974346\n",
            " 1439160/10000000: episode: 7160, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: 478.600, mean reward: 2.381 [-10.000, 247.600], mean action: 1.736 [0.000, 10.000], mean observation: 28.141 [0.002, 504.200], loss: 156.547867, mae: 38.884434, mean_q: -39.580849\n",
            " 1439361/10000000: episode: 7161, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -235.200, mean reward: -1.170 [-117.600, 114.300], mean action: 2.005 [0.000, 10.000], mean observation: 34.030 [0.001, 591.100], loss: 223.049789, mae: 38.971325, mean_q: -39.838425\n",
            " 1439562/10000000: episode: 7162, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -120.200, mean reward: -0.598 [-60.100, 117.400], mean action: 1.766 [0.000, 9.000], mean observation: 30.984 [0.003, 578.100], loss: 217.588364, mae: 39.032009, mean_q: -39.754654\n",
            " 1439763/10000000: episode: 7163, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -136.400, mean reward: -0.679 [-68.200, 263.200], mean action: 2.015 [0.000, 10.000], mean observation: 38.202 [0.001, 598.700], loss: 249.983673, mae: 38.145012, mean_q: -39.283867\n",
            " 1439964/10000000: episode: 7164, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: 399.200, mean reward: 1.986 [-10.000, 199.600], mean action: 2.070 [0.000, 10.000], mean observation: 29.004 [0.002, 308.400], loss: 252.037735, mae: 38.143646, mean_q: -38.804264\n",
            " 1440165/10000000: episode: 7165, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -246.400, mean reward: -1.226 [-123.200, 93.800], mean action: 2.652 [0.000, 10.000], mean observation: 29.982 [0.000, 479.300], loss: 214.804138, mae: 36.908463, mean_q: -38.194248\n",
            " 1440366/10000000: episode: 7166, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -283.200, mean reward: -1.409 [-141.600, 100.800], mean action: 2.716 [0.000, 10.000], mean observation: 33.581 [0.000, 663.800], loss: 224.871307, mae: 35.960613, mean_q: -37.312874\n",
            " 1440567/10000000: episode: 7167, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -431.000, mean reward: -2.144 [-215.500, 70.800], mean action: 2.861 [0.000, 10.000], mean observation: 30.062 [0.001, 477.000], loss: 240.534088, mae: 36.043247, mean_q: -37.269745\n",
            " 1440768/10000000: episode: 7168, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -894.600, mean reward: -4.451 [-447.300, 40.000], mean action: 2.905 [0.000, 9.000], mean observation: 34.236 [0.000, 639.500], loss: 264.594360, mae: 36.177341, mean_q: -37.575722\n",
            " 1440969/10000000: episode: 7169, duration: 1.450s, episode steps: 201, steps per second: 139, episode reward: -589.400, mean reward: -2.932 [-294.700, 123.300], mean action: 2.891 [0.000, 10.000], mean observation: 34.013 [0.001, 619.200], loss: 194.593307, mae: 35.986553, mean_q: -37.404373\n",
            " 1441170/10000000: episode: 7170, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -323.000, mean reward: -1.607 [-161.500, 140.000], mean action: 2.746 [0.000, 9.000], mean observation: 30.380 [0.001, 651.100], loss: 219.183792, mae: 35.781239, mean_q: -37.076084\n",
            " 1441371/10000000: episode: 7171, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -713.400, mean reward: -3.549 [-356.700, 51.000], mean action: 2.687 [0.000, 9.000], mean observation: 35.680 [0.000, 534.000], loss: 242.281631, mae: 34.975609, mean_q: -36.042263\n",
            " 1441572/10000000: episode: 7172, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -633.200, mean reward: -3.150 [-316.600, 76.500], mean action: 2.776 [0.000, 10.000], mean observation: 37.233 [0.000, 818.100], loss: 262.768372, mae: 34.349060, mean_q: -35.346977\n",
            " 1441773/10000000: episode: 7173, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -348.000, mean reward: -1.731 [-174.000, 201.600], mean action: 3.129 [0.000, 10.000], mean observation: 31.191 [0.003, 540.500], loss: 120.082733, mae: 34.135525, mean_q: -35.502258\n",
            " 1441974/10000000: episode: 7174, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -734.200, mean reward: -3.653 [-367.100, 36.500], mean action: 2.826 [0.000, 10.000], mean observation: 33.498 [0.001, 565.900], loss: 191.678833, mae: 34.845638, mean_q: -36.188988\n",
            " 1442175/10000000: episode: 7175, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -618.200, mean reward: -3.076 [-309.100, 143.500], mean action: 2.736 [0.000, 10.000], mean observation: 34.263 [0.000, 720.900], loss: 188.880951, mae: 35.657295, mean_q: -37.193108\n",
            " 1442376/10000000: episode: 7176, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -110.000, mean reward: -0.547 [-55.000, 214.200], mean action: 2.423 [0.000, 10.000], mean observation: 30.926 [0.002, 624.500], loss: 271.729523, mae: 36.562626, mean_q: -37.748711\n",
            " 1442577/10000000: episode: 7177, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -382.200, mean reward: -1.901 [-191.100, 29.400], mean action: 2.085 [0.000, 10.000], mean observation: 35.722 [0.002, 503.100], loss: 189.185226, mae: 37.301357, mean_q: -38.178104\n",
            " 1442778/10000000: episode: 7178, duration: 1.455s, episode steps: 201, steps per second: 138, episode reward: -597.000, mean reward: -2.970 [-298.500, 44.000], mean action: 1.896 [0.000, 10.000], mean observation: 25.926 [0.001, 462.800], loss: 167.856735, mae: 37.616169, mean_q: -38.469009\n",
            " 1442979/10000000: episode: 7179, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: 278.800, mean reward: 1.387 [-8.000, 230.600], mean action: 2.030 [0.000, 8.000], mean observation: 34.953 [0.000, 476.600], loss: 260.135406, mae: 37.136436, mean_q: -37.935543\n",
            " 1443180/10000000: episode: 7180, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -304.200, mean reward: -1.513 [-152.100, 55.600], mean action: 2.124 [0.000, 10.000], mean observation: 39.184 [0.001, 642.700], loss: 203.820007, mae: 37.684467, mean_q: -38.491901\n",
            " 1443381/10000000: episode: 7181, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -65.000, mean reward: -0.323 [-32.500, 71.400], mean action: 1.547 [0.000, 10.000], mean observation: 35.510 [0.000, 907.100], loss: 203.727615, mae: 37.755222, mean_q: -37.862938\n",
            " 1443582/10000000: episode: 7182, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -467.600, mean reward: -2.326 [-233.800, 23.100], mean action: 1.935 [0.000, 7.000], mean observation: 30.228 [0.004, 399.700], loss: 143.734360, mae: 37.516216, mean_q: -38.083027\n",
            " 1443783/10000000: episode: 7183, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -510.000, mean reward: -2.537 [-255.000, 24.900], mean action: 2.030 [0.000, 10.000], mean observation: 33.993 [0.001, 507.200], loss: 236.346954, mae: 37.810993, mean_q: -38.433662\n",
            " 1443984/10000000: episode: 7184, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -67.200, mean reward: -0.334 [-33.600, 216.300], mean action: 2.159 [0.000, 10.000], mean observation: 32.107 [0.000, 522.800], loss: 226.871216, mae: 37.949390, mean_q: -38.683884\n",
            " 1444185/10000000: episode: 7185, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -229.800, mean reward: -1.143 [-114.900, 79.600], mean action: 2.065 [0.000, 10.000], mean observation: 28.371 [0.000, 623.900], loss: 227.667374, mae: 38.366562, mean_q: -38.989082\n",
            " 1444386/10000000: episode: 7186, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -496.000, mean reward: -2.468 [-248.000, 63.900], mean action: 2.488 [0.000, 10.000], mean observation: 34.380 [0.002, 542.000], loss: 205.403244, mae: 37.526161, mean_q: -38.592396\n",
            " 1444587/10000000: episode: 7187, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: 879.000, mean reward: 4.373 [-10.000, 439.500], mean action: 2.612 [0.000, 10.000], mean observation: 33.947 [0.000, 493.100], loss: 225.081619, mae: 37.556568, mean_q: -38.873505\n",
            " 1444788/10000000: episode: 7188, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -239.600, mean reward: -1.192 [-119.800, 86.400], mean action: 2.537 [0.000, 10.000], mean observation: 31.921 [0.000, 781.200], loss: 157.952194, mae: 37.341099, mean_q: -38.560352\n",
            " 1444989/10000000: episode: 7189, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: 175.200, mean reward: 0.872 [-10.000, 233.800], mean action: 2.493 [0.000, 10.000], mean observation: 31.152 [0.001, 492.700], loss: 225.214249, mae: 36.985706, mean_q: -38.096939\n",
            " 1445190/10000000: episode: 7190, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -374.000, mean reward: -1.861 [-187.000, 141.000], mean action: 2.393 [0.000, 10.000], mean observation: 36.549 [0.001, 467.200], loss: 246.356750, mae: 36.440090, mean_q: -37.673397\n",
            " 1445391/10000000: episode: 7191, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: -86.000, mean reward: -0.428 [-43.000, 135.000], mean action: 2.055 [0.000, 10.000], mean observation: 32.987 [0.001, 511.400], loss: 264.168488, mae: 37.074505, mean_q: -37.657356\n",
            " 1445592/10000000: episode: 7192, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -194.400, mean reward: -0.967 [-97.200, 59.600], mean action: 2.692 [0.000, 10.000], mean observation: 34.195 [0.000, 764.600], loss: 328.834381, mae: 36.283722, mean_q: -37.486416\n",
            " 1445793/10000000: episode: 7193, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: -588.600, mean reward: -2.928 [-294.300, 57.200], mean action: 2.771 [0.000, 10.000], mean observation: 30.760 [0.000, 498.200], loss: 170.860306, mae: 35.833370, mean_q: -37.042637\n",
            " 1445994/10000000: episode: 7194, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -645.800, mean reward: -3.213 [-322.900, 32.900], mean action: 2.572 [0.000, 10.000], mean observation: 27.528 [0.001, 516.900], loss: 232.456085, mae: 36.063908, mean_q: -37.085587\n",
            " 1446195/10000000: episode: 7195, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -358.400, mean reward: -1.783 [-179.200, 111.300], mean action: 2.348 [0.000, 10.000], mean observation: 34.951 [0.000, 598.900], loss: 166.955307, mae: 36.363205, mean_q: -37.483028\n",
            " 1446396/10000000: episode: 7196, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -616.600, mean reward: -3.068 [-308.300, 62.600], mean action: 2.284 [0.000, 10.000], mean observation: 40.299 [0.000, 792.800], loss: 221.004318, mae: 36.531921, mean_q: -37.457020\n",
            " 1446597/10000000: episode: 7197, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -784.400, mean reward: -3.902 [-392.200, 63.600], mean action: 2.806 [0.000, 10.000], mean observation: 30.198 [0.001, 455.000], loss: 215.598557, mae: 36.353798, mean_q: -37.465508\n",
            " 1446798/10000000: episode: 7198, duration: 1.500s, episode steps: 201, steps per second: 134, episode reward: -647.400, mean reward: -3.221 [-323.700, 63.600], mean action: 2.746 [0.000, 10.000], mean observation: 39.068 [0.000, 746.700], loss: 252.731674, mae: 36.188725, mean_q: -37.596870\n",
            " 1446999/10000000: episode: 7199, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -86.000, mean reward: -0.428 [-43.000, 175.200], mean action: 1.925 [0.000, 7.000], mean observation: 31.119 [0.001, 375.100], loss: 218.654892, mae: 36.707813, mean_q: -37.232597\n",
            " 1447200/10000000: episode: 7200, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -575.600, mean reward: -2.864 [-287.800, 39.000], mean action: 2.174 [0.000, 10.000], mean observation: 36.064 [0.000, 541.500], loss: 300.286469, mae: 35.936344, mean_q: -36.697693\n",
            " 1447401/10000000: episode: 7201, duration: 1.379s, episode steps: 201, steps per second: 146, episode reward: 235.600, mean reward: 1.172 [-10.000, 117.800], mean action: 1.955 [0.000, 10.000], mean observation: 26.760 [0.001, 519.300], loss: 213.567596, mae: 35.712132, mean_q: -36.146225\n",
            " 1447602/10000000: episode: 7202, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -496.000, mean reward: -2.468 [-248.000, 68.400], mean action: 1.910 [0.000, 10.000], mean observation: 36.686 [0.002, 460.100], loss: 148.294098, mae: 35.738846, mean_q: -36.211124\n",
            " 1447803/10000000: episode: 7203, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 504.000, mean reward: 2.507 [-10.000, 252.000], mean action: 2.040 [0.000, 10.000], mean observation: 33.854 [0.000, 579.300], loss: 182.633591, mae: 35.902073, mean_q: -36.389797\n",
            " 1448004/10000000: episode: 7204, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -343.200, mean reward: -1.707 [-171.600, 115.200], mean action: 1.900 [0.000, 10.000], mean observation: 31.738 [0.001, 576.200], loss: 202.859451, mae: 36.221622, mean_q: -36.661633\n",
            " 1448205/10000000: episode: 7205, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -407.000, mean reward: -2.025 [-203.500, 42.300], mean action: 2.169 [0.000, 10.000], mean observation: 30.010 [0.001, 550.200], loss: 191.869949, mae: 35.923817, mean_q: -36.444668\n",
            " 1448406/10000000: episode: 7206, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -754.800, mean reward: -3.755 [-377.400, 28.000], mean action: 2.413 [0.000, 10.000], mean observation: 31.881 [0.001, 548.600], loss: 227.027054, mae: 35.641247, mean_q: -36.509998\n",
            " 1448607/10000000: episode: 7207, duration: 1.383s, episode steps: 201, steps per second: 145, episode reward: 100.600, mean reward: 0.500 [-10.000, 158.400], mean action: 2.164 [0.000, 10.000], mean observation: 35.351 [0.001, 494.100], loss: 149.378067, mae: 35.661793, mean_q: -36.678154\n",
            " 1448808/10000000: episode: 7208, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -489.400, mean reward: -2.435 [-244.700, 24.400], mean action: 1.652 [0.000, 10.000], mean observation: 33.932 [0.000, 525.700], loss: 165.491837, mae: 35.750526, mean_q: -36.534851\n",
            " 1449009/10000000: episode: 7209, duration: 1.372s, episode steps: 201, steps per second: 147, episode reward: -306.200, mean reward: -1.523 [-153.100, 58.800], mean action: 2.065 [0.000, 10.000], mean observation: 41.708 [0.000, 803.400], loss: 257.869507, mae: 36.030994, mean_q: -36.427498\n",
            " 1449210/10000000: episode: 7210, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -429.800, mean reward: -2.138 [-214.900, 36.000], mean action: 2.219 [0.000, 10.000], mean observation: 28.574 [0.000, 597.200], loss: 150.690933, mae: 35.555573, mean_q: -36.269901\n",
            " 1449411/10000000: episode: 7211, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -447.000, mean reward: -2.224 [-223.500, 42.400], mean action: 1.816 [0.000, 10.000], mean observation: 31.560 [0.002, 455.800], loss: 267.780640, mae: 35.141937, mean_q: -35.537247\n",
            " 1449612/10000000: episode: 7212, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -465.000, mean reward: -2.313 [-232.500, 66.600], mean action: 2.478 [0.000, 10.000], mean observation: 40.409 [0.001, 545.700], loss: 273.673340, mae: 34.546234, mean_q: -35.265469\n",
            " 1449813/10000000: episode: 7213, duration: 1.390s, episode steps: 201, steps per second: 145, episode reward: -444.400, mean reward: -2.211 [-222.200, 53.200], mean action: 2.020 [0.000, 10.000], mean observation: 30.090 [0.000, 581.400], loss: 101.038597, mae: 34.582035, mean_q: -34.954090\n",
            " 1450014/10000000: episode: 7214, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -580.800, mean reward: -2.890 [-290.400, 35.400], mean action: 1.905 [0.000, 7.000], mean observation: 35.637 [0.000, 622.400], loss: 126.359383, mae: 34.768818, mean_q: -35.347549\n",
            " 1450215/10000000: episode: 7215, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -353.200, mean reward: -1.757 [-176.600, 52.400], mean action: 1.925 [0.000, 10.000], mean observation: 34.695 [0.001, 430.400], loss: 214.384720, mae: 34.575089, mean_q: -35.014244\n",
            " 1450416/10000000: episode: 7216, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: 63.800, mean reward: 0.317 [-10.000, 178.500], mean action: 2.149 [0.000, 10.000], mean observation: 36.952 [0.000, 669.400], loss: 153.938187, mae: 35.051865, mean_q: -35.832615\n",
            " 1450617/10000000: episode: 7217, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -490.800, mean reward: -2.442 [-245.400, 41.500], mean action: 2.060 [0.000, 10.000], mean observation: 33.813 [0.002, 511.500], loss: 217.014816, mae: 35.325390, mean_q: -36.116241\n",
            " 1450818/10000000: episode: 7218, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -118.200, mean reward: -0.588 [-59.100, 120.800], mean action: 1.627 [0.000, 10.000], mean observation: 37.287 [0.000, 663.000], loss: 171.778671, mae: 35.299965, mean_q: -35.583477\n",
            " 1451019/10000000: episode: 7219, duration: 1.389s, episode steps: 201, steps per second: 145, episode reward: -178.000, mean reward: -0.886 [-89.000, 151.800], mean action: 1.692 [0.000, 10.000], mean observation: 36.591 [0.000, 637.900], loss: 154.114746, mae: 35.103870, mean_q: -35.827923\n",
            " 1451220/10000000: episode: 7220, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -182.600, mean reward: -0.908 [-91.300, 95.500], mean action: 1.582 [0.000, 7.000], mean observation: 34.900 [0.002, 542.900], loss: 196.708252, mae: 35.357273, mean_q: -35.966278\n",
            " 1451421/10000000: episode: 7221, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -406.000, mean reward: -2.020 [-203.000, 72.600], mean action: 2.020 [0.000, 10.000], mean observation: 32.178 [0.001, 581.100], loss: 244.321747, mae: 35.197292, mean_q: -35.625042\n",
            " 1451622/10000000: episode: 7222, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: -499.800, mean reward: -2.487 [-249.900, 35.800], mean action: 2.114 [0.000, 10.000], mean observation: 32.394 [0.001, 469.000], loss: 210.188141, mae: 34.787025, mean_q: -35.531982\n",
            " 1451823/10000000: episode: 7223, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -268.600, mean reward: -1.336 [-134.300, 113.500], mean action: 1.622 [0.000, 10.000], mean observation: 35.405 [0.000, 622.800], loss: 200.343445, mae: 34.736790, mean_q: -35.337288\n",
            " 1452024/10000000: episode: 7224, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -402.600, mean reward: -2.003 [-201.300, 61.500], mean action: 1.731 [0.000, 10.000], mean observation: 34.333 [0.001, 446.000], loss: 205.216690, mae: 34.524834, mean_q: -34.697605\n",
            " 1452225/10000000: episode: 7225, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -273.200, mean reward: -1.359 [-136.600, 206.000], mean action: 2.154 [0.000, 10.000], mean observation: 33.334 [0.001, 424.400], loss: 227.742920, mae: 34.013691, mean_q: -34.689308\n",
            " 1452426/10000000: episode: 7226, duration: 1.504s, episode steps: 201, steps per second: 134, episode reward: -769.600, mean reward: -3.829 [-384.800, 48.300], mean action: 2.567 [0.000, 8.000], mean observation: 31.319 [0.000, 749.600], loss: 247.170166, mae: 34.152153, mean_q: -34.922215\n",
            " 1452627/10000000: episode: 7227, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -395.200, mean reward: -1.966 [-197.600, 32.400], mean action: 1.940 [0.000, 9.000], mean observation: 31.011 [0.001, 587.600], loss: 193.035324, mae: 34.362160, mean_q: -34.871136\n",
            " 1452828/10000000: episode: 7228, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: 251.400, mean reward: 1.251 [-7.000, 202.100], mean action: 1.861 [0.000, 8.000], mean observation: 35.508 [0.002, 516.600], loss: 213.113190, mae: 33.899311, mean_q: -34.028255\n",
            " 1453029/10000000: episode: 7229, duration: 1.452s, episode steps: 201, steps per second: 138, episode reward: -551.600, mean reward: -2.744 [-275.800, 28.000], mean action: 2.239 [0.000, 9.000], mean observation: 28.172 [0.001, 415.200], loss: 240.074142, mae: 33.641766, mean_q: -33.986706\n",
            " 1453230/10000000: episode: 7230, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -20.400, mean reward: -0.101 [-10.200, 273.200], mean action: 2.279 [0.000, 9.000], mean observation: 35.379 [0.004, 509.100], loss: 315.627563, mae: 33.330303, mean_q: -33.570278\n",
            " 1453431/10000000: episode: 7231, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -519.000, mean reward: -2.582 [-259.500, 60.500], mean action: 2.592 [0.000, 10.000], mean observation: 28.279 [0.000, 590.900], loss: 250.310913, mae: 33.621418, mean_q: -33.924541\n",
            " 1453632/10000000: episode: 7232, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -732.200, mean reward: -3.643 [-366.100, 40.800], mean action: 2.567 [0.000, 9.000], mean observation: 33.186 [0.003, 566.300], loss: 187.992142, mae: 33.103531, mean_q: -33.536449\n",
            " 1453833/10000000: episode: 7233, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -440.600, mean reward: -2.192 [-220.300, 76.800], mean action: 2.642 [0.000, 10.000], mean observation: 28.139 [0.000, 486.700], loss: 164.872986, mae: 32.740734, mean_q: -33.214329\n",
            " 1454034/10000000: episode: 7234, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -231.200, mean reward: -1.150 [-115.600, 98.000], mean action: 2.458 [0.000, 9.000], mean observation: 34.501 [0.001, 510.800], loss: 250.650223, mae: 33.182194, mean_q: -33.765980\n",
            " 1454235/10000000: episode: 7235, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: 107.200, mean reward: 0.533 [-9.000, 117.600], mean action: 2.493 [0.000, 9.000], mean observation: 31.916 [0.001, 423.300], loss: 190.734955, mae: 32.798557, mean_q: -33.326523\n",
            " 1454436/10000000: episode: 7236, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: 261.400, mean reward: 1.300 [-8.000, 131.000], mean action: 2.159 [0.000, 8.000], mean observation: 34.366 [0.000, 637.000], loss: 257.403656, mae: 32.656532, mean_q: -33.196053\n",
            " 1454637/10000000: episode: 7237, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -78.800, mean reward: -0.392 [-39.400, 162.900], mean action: 2.542 [0.000, 10.000], mean observation: 32.259 [0.000, 640.400], loss: 166.743042, mae: 32.632050, mean_q: -33.522881\n",
            " 1454838/10000000: episode: 7238, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -148.200, mean reward: -0.737 [-74.100, 174.300], mean action: 2.303 [0.000, 8.000], mean observation: 29.678 [0.001, 413.700], loss: 109.013603, mae: 33.039867, mean_q: -34.369129\n",
            " 1455039/10000000: episode: 7239, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: 279.000, mean reward: 1.388 [-10.000, 350.500], mean action: 2.398 [0.000, 10.000], mean observation: 33.247 [0.001, 512.800], loss: 139.536667, mae: 33.373913, mean_q: -34.465912\n",
            " 1455240/10000000: episode: 7240, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -418.400, mean reward: -2.082 [-209.200, 97.800], mean action: 2.771 [0.000, 10.000], mean observation: 32.331 [0.000, 558.800], loss: 234.165848, mae: 33.541332, mean_q: -34.739201\n",
            " 1455441/10000000: episode: 7241, duration: 1.495s, episode steps: 201, steps per second: 134, episode reward: 209.600, mean reward: 1.043 [-10.000, 211.500], mean action: 2.552 [0.000, 10.000], mean observation: 34.861 [0.003, 539.800], loss: 151.602829, mae: 33.962566, mean_q: -35.160126\n",
            " 1455642/10000000: episode: 7242, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -457.200, mean reward: -2.275 [-228.600, 82.200], mean action: 2.463 [0.000, 10.000], mean observation: 35.576 [0.001, 584.600], loss: 235.070602, mae: 34.673145, mean_q: -35.323177\n",
            " 1455843/10000000: episode: 7243, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -401.200, mean reward: -1.996 [-200.600, 86.600], mean action: 2.075 [0.000, 10.000], mean observation: 36.506 [0.000, 455.400], loss: 191.679642, mae: 34.760780, mean_q: -35.526577\n",
            " 1456044/10000000: episode: 7244, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -243.600, mean reward: -1.212 [-121.800, 44.500], mean action: 1.915 [0.000, 9.000], mean observation: 31.409 [0.001, 510.200], loss: 191.862137, mae: 35.239716, mean_q: -35.979370\n",
            " 1456245/10000000: episode: 7245, duration: 1.385s, episode steps: 201, steps per second: 145, episode reward: -474.600, mean reward: -2.361 [-237.300, 36.000], mean action: 2.214 [0.000, 10.000], mean observation: 31.683 [0.000, 708.200], loss: 264.162628, mae: 35.099003, mean_q: -35.721096\n",
            " 1456446/10000000: episode: 7246, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -181.200, mean reward: -0.901 [-90.600, 167.500], mean action: 2.343 [0.000, 10.000], mean observation: 38.141 [0.002, 500.200], loss: 191.996750, mae: 35.017803, mean_q: -35.822899\n",
            " 1456647/10000000: episode: 7247, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -369.800, mean reward: -1.840 [-184.900, 57.000], mean action: 2.050 [0.000, 10.000], mean observation: 35.830 [0.000, 440.500], loss: 159.428635, mae: 35.069977, mean_q: -35.545147\n",
            " 1456848/10000000: episode: 7248, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -547.000, mean reward: -2.721 [-273.500, 22.400], mean action: 2.259 [0.000, 7.000], mean observation: 33.279 [0.000, 566.800], loss: 188.553986, mae: 35.080921, mean_q: -35.740593\n",
            " 1457049/10000000: episode: 7249, duration: 1.476s, episode steps: 201, steps per second: 136, episode reward: -536.000, mean reward: -2.667 [-268.000, 31.800], mean action: 2.433 [0.000, 9.000], mean observation: 31.266 [0.001, 571.900], loss: 182.287979, mae: 35.448696, mean_q: -36.352135\n",
            " 1457250/10000000: episode: 7250, duration: 1.384s, episode steps: 201, steps per second: 145, episode reward: -628.000, mean reward: -3.124 [-314.000, 36.600], mean action: 2.294 [0.000, 10.000], mean observation: 33.889 [0.000, 494.100], loss: 209.284241, mae: 35.524834, mean_q: -36.553493\n",
            " 1457451/10000000: episode: 7251, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: -667.600, mean reward: -3.321 [-333.800, 29.600], mean action: 2.552 [0.000, 10.000], mean observation: 28.623 [0.002, 465.200], loss: 279.713043, mae: 35.338417, mean_q: -36.525581\n",
            " 1457652/10000000: episode: 7252, duration: 1.387s, episode steps: 201, steps per second: 145, episode reward: -21.600, mean reward: -0.107 [-10.800, 112.000], mean action: 2.667 [0.000, 10.000], mean observation: 31.169 [0.001, 533.900], loss: 178.591599, mae: 36.180733, mean_q: -37.511208\n",
            " 1457853/10000000: episode: 7253, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -271.200, mean reward: -1.349 [-135.600, 91.200], mean action: 2.224 [0.000, 9.000], mean observation: 32.892 [0.000, 798.300], loss: 206.970963, mae: 36.638481, mean_q: -37.665821\n",
            " 1458054/10000000: episode: 7254, duration: 1.487s, episode steps: 201, steps per second: 135, episode reward: -305.600, mean reward: -1.520 [-152.800, 181.800], mean action: 2.159 [0.000, 10.000], mean observation: 30.532 [0.001, 434.300], loss: 177.520676, mae: 37.194820, mean_q: -37.858315\n",
            " 1458255/10000000: episode: 7255, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -614.600, mean reward: -3.058 [-307.300, 35.600], mean action: 2.303 [0.000, 10.000], mean observation: 32.597 [0.000, 668.900], loss: 172.898911, mae: 37.827400, mean_q: -38.795475\n",
            " 1458456/10000000: episode: 7256, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: 199.000, mean reward: 0.990 [-10.000, 176.800], mean action: 2.005 [0.000, 10.000], mean observation: 34.280 [0.000, 506.300], loss: 178.328918, mae: 38.530586, mean_q: -39.514423\n",
            " 1458657/10000000: episode: 7257, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: -508.400, mean reward: -2.529 [-254.200, 25.300], mean action: 1.995 [0.000, 10.000], mean observation: 29.488 [0.003, 383.900], loss: 231.734421, mae: 38.700672, mean_q: -39.778454\n",
            " 1458858/10000000: episode: 7258, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -269.600, mean reward: -1.341 [-134.800, 160.000], mean action: 2.433 [0.000, 10.000], mean observation: 37.302 [0.000, 605.900], loss: 217.387863, mae: 39.047092, mean_q: -40.164219\n",
            " 1459059/10000000: episode: 7259, duration: 1.379s, episode steps: 201, steps per second: 146, episode reward: 638.800, mean reward: 3.178 [-10.000, 459.900], mean action: 2.582 [0.000, 10.000], mean observation: 35.433 [0.000, 650.000], loss: 189.197388, mae: 39.242947, mean_q: -40.441669\n",
            " 1459260/10000000: episode: 7260, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -319.800, mean reward: -1.591 [-159.900, 180.000], mean action: 2.682 [0.000, 10.000], mean observation: 29.739 [0.002, 432.300], loss: 198.781830, mae: 39.080521, mean_q: -40.772224\n",
            " 1459461/10000000: episode: 7261, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -353.800, mean reward: -1.760 [-176.900, 64.600], mean action: 2.612 [0.000, 9.000], mean observation: 36.250 [0.003, 420.100], loss: 215.763214, mae: 39.791515, mean_q: -41.244766\n",
            " 1459662/10000000: episode: 7262, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -337.400, mean reward: -1.679 [-168.700, 327.500], mean action: 3.433 [0.000, 9.000], mean observation: 39.292 [0.001, 510.400], loss: 307.436951, mae: 39.522579, mean_q: -41.154861\n",
            " 1459863/10000000: episode: 7263, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -372.200, mean reward: -1.852 [-186.100, 99.500], mean action: 3.483 [0.000, 10.000], mean observation: 33.343 [0.000, 508.900], loss: 211.275146, mae: 39.208183, mean_q: -40.952541\n",
            " 1460064/10000000: episode: 7264, duration: 1.379s, episode steps: 201, steps per second: 146, episode reward: 960.600, mean reward: 4.779 [-9.000, 675.000], mean action: 1.871 [0.000, 9.000], mean observation: 32.383 [0.002, 447.500], loss: 150.346985, mae: 39.463436, mean_q: -40.080379\n",
            " 1460265/10000000: episode: 7265, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: 265.400, mean reward: 1.320 [-10.000, 291.500], mean action: 2.393 [0.000, 10.000], mean observation: 30.506 [0.001, 618.400], loss: 171.961365, mae: 38.576229, mean_q: -39.549835\n",
            " 1460466/10000000: episode: 7266, duration: 1.381s, episode steps: 201, steps per second: 146, episode reward: 253.400, mean reward: 1.261 [-10.000, 240.600], mean action: 2.224 [0.000, 10.000], mean observation: 33.458 [0.001, 512.000], loss: 142.581955, mae: 38.030521, mean_q: -39.055996\n",
            " 1460667/10000000: episode: 7267, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -610.800, mean reward: -3.039 [-305.400, 82.200], mean action: 2.692 [0.000, 10.000], mean observation: 32.554 [0.000, 586.000], loss: 180.276840, mae: 38.462132, mean_q: -39.870365\n",
            " 1460868/10000000: episode: 7268, duration: 1.389s, episode steps: 201, steps per second: 145, episode reward: -488.400, mean reward: -2.430 [-244.200, 40.800], mean action: 2.134 [0.000, 10.000], mean observation: 32.071 [0.001, 431.000], loss: 179.296234, mae: 39.093651, mean_q: -40.376347\n",
            " 1461069/10000000: episode: 7269, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: 839.200, mean reward: 4.175 [-9.000, 542.500], mean action: 2.060 [0.000, 9.000], mean observation: 30.803 [0.001, 459.200], loss: 166.894257, mae: 39.134594, mean_q: -40.318073\n",
            " 1461270/10000000: episode: 7270, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -571.600, mean reward: -2.844 [-285.800, 34.800], mean action: 2.458 [0.000, 10.000], mean observation: 31.429 [0.001, 422.300], loss: 191.173309, mae: 38.480503, mean_q: -40.251183\n",
            " 1461471/10000000: episode: 7271, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 626.200, mean reward: 3.115 [-9.000, 572.000], mean action: 1.731 [0.000, 9.000], mean observation: 33.136 [0.001, 497.500], loss: 119.322380, mae: 39.447269, mean_q: -40.487831\n",
            " 1461672/10000000: episode: 7272, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -310.200, mean reward: -1.543 [-155.100, 47.600], mean action: 1.801 [0.000, 10.000], mean observation: 31.302 [0.002, 439.900], loss: 184.588776, mae: 39.896461, mean_q: -40.526955\n",
            " 1461873/10000000: episode: 7273, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -608.600, mean reward: -3.028 [-304.300, 35.200], mean action: 2.393 [0.000, 10.000], mean observation: 29.451 [0.002, 527.400], loss: 229.315079, mae: 38.978180, mean_q: -39.904957\n",
            " 1462074/10000000: episode: 7274, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -353.200, mean reward: -1.757 [-176.600, 71.900], mean action: 2.104 [0.000, 9.000], mean observation: 36.895 [0.000, 720.900], loss: 153.755310, mae: 38.751652, mean_q: -39.819687\n",
            " 1462275/10000000: episode: 7275, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -118.600, mean reward: -0.590 [-59.300, 62.000], mean action: 2.139 [0.000, 9.000], mean observation: 31.905 [0.001, 500.800], loss: 200.799362, mae: 39.267433, mean_q: -39.772812\n",
            " 1462476/10000000: episode: 7276, duration: 1.539s, episode steps: 201, steps per second: 131, episode reward: -152.200, mean reward: -0.757 [-76.100, 127.600], mean action: 1.692 [0.000, 10.000], mean observation: 31.769 [0.001, 424.400], loss: 291.974762, mae: 39.256805, mean_q: -39.790512\n",
            " 1462677/10000000: episode: 7277, duration: 1.575s, episode steps: 201, steps per second: 128, episode reward: -62.400, mean reward: -0.310 [-31.200, 365.500], mean action: 2.522 [0.000, 9.000], mean observation: 32.390 [0.000, 441.700], loss: 198.451843, mae: 38.335976, mean_q: -39.526211\n",
            " 1462878/10000000: episode: 7278, duration: 1.630s, episode steps: 201, steps per second: 123, episode reward: 104.200, mean reward: 0.518 [-9.000, 236.800], mean action: 2.776 [0.000, 10.000], mean observation: 26.804 [0.003, 380.000], loss: 240.867340, mae: 38.063000, mean_q: -39.133587\n",
            " 1463079/10000000: episode: 7279, duration: 1.583s, episode steps: 201, steps per second: 127, episode reward: -624.200, mean reward: -3.105 [-312.100, 76.300], mean action: 2.706 [0.000, 9.000], mean observation: 36.040 [0.002, 537.100], loss: 215.871475, mae: 37.787022, mean_q: -38.672188\n",
            " 1463280/10000000: episode: 7280, duration: 1.558s, episode steps: 201, steps per second: 129, episode reward: -431.400, mean reward: -2.146 [-215.700, 100.800], mean action: 2.378 [0.000, 9.000], mean observation: 34.024 [0.001, 562.800], loss: 156.566360, mae: 37.969887, mean_q: -38.776649\n",
            " 1463481/10000000: episode: 7281, duration: 1.564s, episode steps: 201, steps per second: 128, episode reward: -181.800, mean reward: -0.904 [-90.900, 108.800], mean action: 2.129 [0.000, 10.000], mean observation: 30.153 [0.001, 530.600], loss: 232.171753, mae: 37.985813, mean_q: -38.657070\n",
            " 1463682/10000000: episode: 7282, duration: 1.595s, episode steps: 201, steps per second: 126, episode reward: -110.800, mean reward: -0.551 [-55.400, 89.000], mean action: 1.652 [0.000, 8.000], mean observation: 41.297 [0.000, 693.300], loss: 158.544586, mae: 37.971516, mean_q: -38.540134\n",
            " 1463883/10000000: episode: 7283, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -466.800, mean reward: -2.322 [-233.400, 13.700], mean action: 1.423 [0.000, 9.000], mean observation: 32.756 [0.000, 727.600], loss: 112.880905, mae: 37.702209, mean_q: -37.905937\n",
            " 1464084/10000000: episode: 7284, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: 62.200, mean reward: 0.309 [-9.000, 86.000], mean action: 1.881 [0.000, 9.000], mean observation: 34.159 [0.002, 453.000], loss: 191.081467, mae: 37.715240, mean_q: -38.210110\n",
            " 1464285/10000000: episode: 7285, duration: 1.927s, episode steps: 201, steps per second: 104, episode reward: -309.600, mean reward: -1.540 [-154.800, 51.600], mean action: 1.990 [0.000, 10.000], mean observation: 34.063 [0.001, 606.200], loss: 195.055161, mae: 37.440517, mean_q: -37.600269\n",
            " 1464486/10000000: episode: 7286, duration: 1.885s, episode steps: 201, steps per second: 107, episode reward: -484.600, mean reward: -2.411 [-242.300, 66.700], mean action: 1.697 [0.000, 9.000], mean observation: 35.889 [0.003, 593.100], loss: 194.979309, mae: 36.941437, mean_q: -37.190815\n",
            " 1464687/10000000: episode: 7287, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -255.400, mean reward: -1.271 [-127.700, 57.900], mean action: 1.985 [0.000, 10.000], mean observation: 28.693 [0.002, 435.000], loss: 145.916779, mae: 36.715584, mean_q: -37.202560\n",
            " 1464888/10000000: episode: 7288, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: -432.800, mean reward: -2.153 [-216.400, 64.000], mean action: 2.373 [0.000, 10.000], mean observation: 30.647 [0.001, 697.100], loss: 167.569763, mae: 36.407452, mean_q: -36.768284\n",
            " 1465089/10000000: episode: 7289, duration: 1.387s, episode steps: 201, steps per second: 145, episode reward: -520.600, mean reward: -2.590 [-260.300, 22.800], mean action: 1.677 [0.000, 9.000], mean observation: 29.597 [0.001, 562.800], loss: 211.376862, mae: 36.283848, mean_q: -36.669651\n",
            " 1465290/10000000: episode: 7290, duration: 1.389s, episode steps: 201, steps per second: 145, episode reward: -442.800, mean reward: -2.203 [-221.400, 35.400], mean action: 1.597 [0.000, 10.000], mean observation: 30.003 [0.001, 471.400], loss: 169.125473, mae: 35.829239, mean_q: -35.818249\n",
            " 1465491/10000000: episode: 7291, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -349.800, mean reward: -1.740 [-174.900, 38.700], mean action: 1.264 [0.000, 9.000], mean observation: 35.321 [0.000, 522.200], loss: 199.608170, mae: 35.521320, mean_q: -35.005440\n",
            " 1465692/10000000: episode: 7292, duration: 1.355s, episode steps: 201, steps per second: 148, episode reward: 422.400, mean reward: 2.101 [-7.000, 258.300], mean action: 1.075 [0.000, 7.000], mean observation: 30.212 [0.001, 521.600], loss: 224.261337, mae: 34.483963, mean_q: -33.861050\n",
            " 1465893/10000000: episode: 7293, duration: 1.373s, episode steps: 201, steps per second: 146, episode reward: 18.200, mean reward: 0.091 [-9.000, 78.800], mean action: 1.910 [0.000, 9.000], mean observation: 31.655 [0.000, 527.200], loss: 198.977692, mae: 33.785717, mean_q: -33.780014\n",
            " 1466094/10000000: episode: 7294, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -567.000, mean reward: -2.821 [-283.500, 39.000], mean action: 1.900 [0.000, 7.000], mean observation: 36.069 [0.000, 451.200], loss: 190.227539, mae: 33.655643, mean_q: -33.626389\n",
            " 1466295/10000000: episode: 7295, duration: 1.365s, episode steps: 201, steps per second: 147, episode reward: -91.800, mean reward: -0.457 [-45.900, 136.500], mean action: 1.826 [0.000, 10.000], mean observation: 30.256 [0.000, 765.000], loss: 168.740692, mae: 33.636574, mean_q: -33.484325\n",
            " 1466496/10000000: episode: 7296, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: 4.200, mean reward: 0.021 [-7.000, 145.800], mean action: 1.697 [0.000, 7.000], mean observation: 36.301 [0.000, 638.500], loss: 200.390854, mae: 33.410862, mean_q: -33.170547\n",
            " 1466697/10000000: episode: 7297, duration: 1.366s, episode steps: 201, steps per second: 147, episode reward: 351.000, mean reward: 1.746 [-7.000, 348.800], mean action: 1.761 [0.000, 8.000], mean observation: 33.862 [0.000, 793.800], loss: 175.921005, mae: 32.924561, mean_q: -32.927971\n",
            " 1466898/10000000: episode: 7298, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -557.200, mean reward: -2.772 [-278.600, 58.000], mean action: 2.333 [0.000, 9.000], mean observation: 37.385 [0.000, 633.800], loss: 182.568298, mae: 33.107605, mean_q: -33.470234\n",
            " 1467099/10000000: episode: 7299, duration: 1.380s, episode steps: 201, steps per second: 146, episode reward: -573.200, mean reward: -2.852 [-286.600, 17.600], mean action: 2.015 [0.000, 10.000], mean observation: 36.464 [0.000, 701.500], loss: 252.896378, mae: 33.021358, mean_q: -33.175663\n",
            " 1467300/10000000: episode: 7300, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: -767.200, mean reward: -3.817 [-383.600, 6.500], mean action: 2.095 [0.000, 8.000], mean observation: 40.137 [0.000, 628.000], loss: 172.210449, mae: 32.876827, mean_q: -33.034321\n",
            " 1467501/10000000: episode: 7301, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: 1395.200, mean reward: 6.941 [-10.000, 697.600], mean action: 2.095 [0.000, 10.000], mean observation: 34.615 [0.000, 783.800], loss: 221.238693, mae: 32.882614, mean_q: -33.058361\n",
            " 1467702/10000000: episode: 7302, duration: 1.390s, episode steps: 201, steps per second: 145, episode reward: 73.000, mean reward: 0.363 [-8.000, 207.000], mean action: 2.269 [0.000, 10.000], mean observation: 35.903 [0.000, 620.700], loss: 244.024567, mae: 33.324429, mean_q: -33.627010\n",
            " 1467903/10000000: episode: 7303, duration: 1.400s, episode steps: 201, steps per second: 144, episode reward: -397.000, mean reward: -1.975 [-198.500, 46.800], mean action: 2.174 [0.000, 9.000], mean observation: 33.702 [0.002, 515.200], loss: 165.106247, mae: 33.510357, mean_q: -33.809956\n",
            " 1468104/10000000: episode: 7304, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -315.000, mean reward: -1.567 [-157.500, 84.300], mean action: 2.015 [0.000, 10.000], mean observation: 35.410 [0.001, 430.300], loss: 154.178513, mae: 33.644611, mean_q: -34.126408\n",
            " 1468305/10000000: episode: 7305, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -368.600, mean reward: -1.834 [-184.300, 58.600], mean action: 2.144 [0.000, 10.000], mean observation: 39.334 [0.000, 673.700], loss: 190.781448, mae: 33.732674, mean_q: -34.250069\n",
            " 1468506/10000000: episode: 7306, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -246.600, mean reward: -1.227 [-123.300, 102.000], mean action: 2.249 [0.000, 10.000], mean observation: 32.124 [0.001, 459.000], loss: 138.335602, mae: 33.999111, mean_q: -34.917217\n",
            " 1468707/10000000: episode: 7307, duration: 1.365s, episode steps: 201, steps per second: 147, episode reward: -110.400, mean reward: -0.549 [-55.200, 134.500], mean action: 2.085 [0.000, 7.000], mean observation: 36.319 [0.001, 558.800], loss: 202.011124, mae: 34.768837, mean_q: -35.378147\n",
            " 1468908/10000000: episode: 7308, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: 489.200, mean reward: 2.434 [-10.000, 313.000], mean action: 2.010 [0.000, 10.000], mean observation: 35.310 [0.000, 933.200], loss: 168.818756, mae: 34.855686, mean_q: -35.356052\n",
            " 1469109/10000000: episode: 7309, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -84.600, mean reward: -0.421 [-42.300, 101.000], mean action: 2.542 [0.000, 10.000], mean observation: 31.214 [0.000, 684.900], loss: 232.074203, mae: 34.895191, mean_q: -35.664848\n",
            " 1469310/10000000: episode: 7310, duration: 1.395s, episode steps: 201, steps per second: 144, episode reward: -178.400, mean reward: -0.888 [-89.200, 90.000], mean action: 2.070 [0.000, 10.000], mean observation: 32.115 [0.000, 567.300], loss: 256.108917, mae: 35.194897, mean_q: -35.893673\n",
            " 1469511/10000000: episode: 7311, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -448.800, mean reward: -2.233 [-224.400, 47.700], mean action: 2.055 [0.000, 10.000], mean observation: 31.542 [0.000, 556.700], loss: 251.052429, mae: 35.440128, mean_q: -36.010777\n",
            " 1469712/10000000: episode: 7312, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -575.200, mean reward: -2.862 [-287.600, 28.000], mean action: 1.935 [0.000, 10.000], mean observation: 35.985 [0.002, 444.500], loss: 213.857315, mae: 35.260708, mean_q: -35.686462\n",
            " 1469913/10000000: episode: 7313, duration: 1.358s, episode steps: 201, steps per second: 148, episode reward: -521.600, mean reward: -2.595 [-260.800, 58.800], mean action: 2.010 [0.000, 10.000], mean observation: 35.989 [0.000, 607.900], loss: 286.902863, mae: 35.327454, mean_q: -35.664883\n",
            " 1470114/10000000: episode: 7314, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -291.600, mean reward: -1.451 [-145.800, 39.600], mean action: 1.930 [0.000, 10.000], mean observation: 35.832 [0.000, 528.400], loss: 240.370789, mae: 34.882118, mean_q: -35.186668\n",
            " 1470315/10000000: episode: 7315, duration: 1.374s, episode steps: 201, steps per second: 146, episode reward: -613.800, mean reward: -3.054 [-306.900, 48.800], mean action: 2.000 [0.000, 10.000], mean observation: 30.885 [0.001, 455.800], loss: 170.184631, mae: 34.575439, mean_q: -34.897491\n",
            " 1470516/10000000: episode: 7316, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: 8.800, mean reward: 0.044 [-10.000, 160.000], mean action: 2.025 [0.000, 10.000], mean observation: 30.117 [0.000, 818.500], loss: 306.051941, mae: 34.175529, mean_q: -34.497242\n",
            " 1470717/10000000: episode: 7317, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -443.400, mean reward: -2.206 [-221.700, 102.500], mean action: 2.433 [0.000, 10.000], mean observation: 33.096 [0.002, 441.900], loss: 196.988449, mae: 33.755566, mean_q: -34.204678\n",
            " 1470918/10000000: episode: 7318, duration: 1.379s, episode steps: 201, steps per second: 146, episode reward: -769.600, mean reward: -3.829 [-384.800, 50.000], mean action: 2.383 [0.000, 10.000], mean observation: 28.865 [0.001, 619.000], loss: 134.119461, mae: 33.754276, mean_q: -34.289471\n",
            " 1471119/10000000: episode: 7319, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: 11.400, mean reward: 0.057 [-8.000, 86.000], mean action: 1.846 [0.000, 8.000], mean observation: 34.702 [0.001, 412.100], loss: 206.046021, mae: 33.964935, mean_q: -34.359787\n",
            " 1471320/10000000: episode: 7320, duration: 1.376s, episode steps: 201, steps per second: 146, episode reward: -502.800, mean reward: -2.501 [-251.400, 29.000], mean action: 2.174 [0.000, 8.000], mean observation: 35.146 [0.000, 500.900], loss: 155.077881, mae: 34.087688, mean_q: -34.717327\n",
            " 1471521/10000000: episode: 7321, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -435.000, mean reward: -2.164 [-217.500, 60.600], mean action: 1.925 [0.000, 10.000], mean observation: 36.188 [0.001, 501.600], loss: 210.817535, mae: 34.543743, mean_q: -35.174328\n",
            " 1471722/10000000: episode: 7322, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -386.400, mean reward: -1.922 [-193.200, 81.000], mean action: 2.194 [0.000, 10.000], mean observation: 32.873 [0.001, 492.600], loss: 127.660255, mae: 34.468109, mean_q: -35.314007\n",
            " 1471923/10000000: episode: 7323, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -126.600, mean reward: -0.630 [-63.300, 119.700], mean action: 2.214 [0.000, 8.000], mean observation: 32.390 [0.000, 590.900], loss: 269.210754, mae: 34.579391, mean_q: -35.513767\n",
            " 1472124/10000000: episode: 7324, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: -333.800, mean reward: -1.661 [-166.900, 84.500], mean action: 2.602 [0.000, 10.000], mean observation: 37.254 [0.000, 747.100], loss: 164.869553, mae: 34.640522, mean_q: -35.630630\n",
            " 1472325/10000000: episode: 7325, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: 229.600, mean reward: 1.142 [-10.000, 432.000], mean action: 2.806 [0.000, 10.000], mean observation: 29.511 [0.000, 462.000], loss: 229.750641, mae: 34.707439, mean_q: -35.753433\n",
            " 1472526/10000000: episode: 7326, duration: 1.459s, episode steps: 201, steps per second: 138, episode reward: 208.000, mean reward: 1.035 [-10.000, 150.300], mean action: 2.746 [0.000, 10.000], mean observation: 31.311 [0.000, 534.800], loss: 220.313751, mae: 34.599266, mean_q: -35.722759\n",
            " 1472727/10000000: episode: 7327, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -665.800, mean reward: -3.312 [-332.900, 22.400], mean action: 2.726 [0.000, 10.000], mean observation: 28.708 [0.000, 580.300], loss: 201.477707, mae: 34.646320, mean_q: -35.944187\n",
            " 1472928/10000000: episode: 7328, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -658.200, mean reward: -3.275 [-329.100, 65.000], mean action: 2.741 [0.000, 10.000], mean observation: 38.446 [0.000, 522.100], loss: 174.568451, mae: 34.575855, mean_q: -35.841640\n",
            " 1473129/10000000: episode: 7329, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -793.200, mean reward: -3.946 [-396.600, 31.500], mean action: 2.905 [0.000, 10.000], mean observation: 29.360 [0.000, 813.800], loss: 230.284592, mae: 34.802452, mean_q: -35.807289\n",
            " 1473330/10000000: episode: 7330, duration: 1.380s, episode steps: 201, steps per second: 146, episode reward: -437.800, mean reward: -2.178 [-218.900, 144.000], mean action: 2.264 [0.000, 10.000], mean observation: 34.429 [0.000, 530.600], loss: 247.440933, mae: 34.359451, mean_q: -34.986420\n",
            " 1473531/10000000: episode: 7331, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -482.400, mean reward: -2.400 [-241.200, 205.000], mean action: 2.731 [0.000, 10.000], mean observation: 34.002 [0.001, 632.400], loss: 158.793762, mae: 33.843239, mean_q: -34.500244\n",
            " 1473732/10000000: episode: 7332, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -494.200, mean reward: -2.459 [-247.100, 83.000], mean action: 2.403 [0.000, 10.000], mean observation: 33.406 [0.003, 531.400], loss: 238.033234, mae: 33.415592, mean_q: -33.925793\n",
            " 1473933/10000000: episode: 7333, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -660.400, mean reward: -3.286 [-330.200, 68.000], mean action: 2.532 [0.000, 10.000], mean observation: 34.913 [0.001, 519.900], loss: 258.664459, mae: 32.726620, mean_q: -33.017490\n",
            " 1474134/10000000: episode: 7334, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -521.800, mean reward: -2.596 [-260.900, 71.000], mean action: 2.159 [0.000, 10.000], mean observation: 38.876 [0.001, 518.800], loss: 176.541748, mae: 32.275566, mean_q: -32.473564\n",
            " 1474335/10000000: episode: 7335, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 198.600, mean reward: 0.988 [-10.000, 285.500], mean action: 2.557 [0.000, 10.000], mean observation: 38.332 [0.001, 627.200], loss: 223.769196, mae: 32.203476, mean_q: -32.458595\n",
            " 1474536/10000000: episode: 7336, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -196.200, mean reward: -0.976 [-98.100, 151.600], mean action: 2.100 [0.000, 9.000], mean observation: 38.057 [0.001, 501.700], loss: 140.103470, mae: 32.110939, mean_q: -32.222534\n",
            " 1474737/10000000: episode: 7337, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 141.800, mean reward: 0.705 [-10.000, 308.500], mean action: 2.194 [0.000, 10.000], mean observation: 32.207 [0.000, 604.500], loss: 197.018387, mae: 31.999426, mean_q: -31.981518\n",
            " 1474938/10000000: episode: 7338, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: 23.200, mean reward: 0.115 [-9.000, 74.700], mean action: 1.756 [0.000, 9.000], mean observation: 32.977 [0.001, 437.600], loss: 215.660751, mae: 31.737480, mean_q: -31.836744\n",
            " 1475139/10000000: episode: 7339, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -492.600, mean reward: -2.451 [-246.300, 35.400], mean action: 2.100 [0.000, 9.000], mean observation: 30.311 [0.001, 464.600], loss: 198.098495, mae: 31.338116, mean_q: -31.537243\n",
            " 1475340/10000000: episode: 7340, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -32.000, mean reward: -0.159 [-16.000, 144.000], mean action: 2.358 [0.000, 10.000], mean observation: 31.366 [0.001, 531.100], loss: 203.258118, mae: 31.439491, mean_q: -31.868523\n",
            " 1475541/10000000: episode: 7341, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -541.200, mean reward: -2.693 [-270.600, 44.400], mean action: 2.214 [0.000, 10.000], mean observation: 36.399 [0.002, 439.400], loss: 192.039841, mae: 31.489166, mean_q: -31.895880\n",
            " 1475742/10000000: episode: 7342, duration: 1.444s, episode steps: 201, steps per second: 139, episode reward: -408.200, mean reward: -2.031 [-204.100, 49.000], mean action: 2.159 [0.000, 10.000], mean observation: 33.996 [0.000, 784.200], loss: 183.289825, mae: 31.448778, mean_q: -31.988985\n",
            " 1475943/10000000: episode: 7343, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -305.000, mean reward: -1.517 [-152.500, 109.600], mean action: 2.199 [0.000, 9.000], mean observation: 38.007 [0.000, 722.600], loss: 137.499451, mae: 31.777130, mean_q: -32.379391\n",
            " 1476144/10000000: episode: 7344, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -318.000, mean reward: -1.582 [-159.000, 67.300], mean action: 1.886 [0.000, 10.000], mean observation: 31.460 [0.001, 542.200], loss: 181.930328, mae: 32.039635, mean_q: -32.365520\n",
            " 1476345/10000000: episode: 7345, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: 46.400, mean reward: 0.231 [-9.000, 201.600], mean action: 1.925 [0.000, 9.000], mean observation: 36.645 [0.001, 527.300], loss: 222.904892, mae: 32.022839, mean_q: -32.363197\n",
            " 1476546/10000000: episode: 7346, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -540.000, mean reward: -2.687 [-270.000, 51.500], mean action: 2.104 [0.000, 10.000], mean observation: 28.512 [0.000, 529.100], loss: 182.829559, mae: 31.813705, mean_q: -32.174522\n",
            " 1476747/10000000: episode: 7347, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -455.400, mean reward: -2.266 [-227.700, 77.000], mean action: 2.114 [0.000, 10.000], mean observation: 32.082 [0.000, 694.400], loss: 135.399811, mae: 31.648899, mean_q: -32.018467\n",
            " 1476948/10000000: episode: 7348, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -495.200, mean reward: -2.464 [-247.600, 18.400], mean action: 1.493 [0.000, 10.000], mean observation: 40.955 [0.001, 598.300], loss: 176.147537, mae: 31.687712, mean_q: -31.818913\n",
            " 1477149/10000000: episode: 7349, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -527.400, mean reward: -2.624 [-263.700, 34.900], mean action: 1.935 [0.000, 10.000], mean observation: 33.445 [0.000, 555.700], loss: 206.915619, mae: 31.300747, mean_q: -31.239429\n",
            " 1477350/10000000: episode: 7350, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -522.400, mean reward: -2.599 [-261.200, 60.600], mean action: 1.796 [0.000, 10.000], mean observation: 28.398 [0.003, 492.800], loss: 353.130981, mae: 31.177824, mean_q: -31.034803\n",
            " 1477551/10000000: episode: 7351, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -429.400, mean reward: -2.136 [-214.700, 60.500], mean action: 1.985 [0.000, 10.000], mean observation: 37.281 [0.000, 669.900], loss: 216.143311, mae: 31.050762, mean_q: -31.154282\n",
            " 1477752/10000000: episode: 7352, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -429.000, mean reward: -2.134 [-214.500, 49.000], mean action: 2.090 [0.000, 10.000], mean observation: 37.089 [0.001, 662.900], loss: 140.943268, mae: 31.036434, mean_q: -31.368208\n",
            " 1477953/10000000: episode: 7353, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -35.000, mean reward: -0.174 [-17.500, 153.300], mean action: 2.348 [0.000, 10.000], mean observation: 35.287 [0.002, 482.600], loss: 157.245377, mae: 31.495604, mean_q: -31.997959\n",
            " 1478154/10000000: episode: 7354, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -542.200, mean reward: -2.698 [-271.100, 69.600], mean action: 2.433 [0.000, 10.000], mean observation: 38.699 [0.000, 654.700], loss: 215.206345, mae: 32.301865, mean_q: -32.995014\n",
            " 1478355/10000000: episode: 7355, duration: 1.387s, episode steps: 201, steps per second: 145, episode reward: -760.000, mean reward: -3.781 [-380.000, 22.400], mean action: 2.522 [0.000, 10.000], mean observation: 34.234 [0.000, 706.400], loss: 155.620926, mae: 32.475033, mean_q: -33.255936\n",
            " 1478556/10000000: episode: 7356, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -645.600, mean reward: -3.212 [-322.800, 81.200], mean action: 2.746 [0.000, 10.000], mean observation: 30.903 [0.001, 480.700], loss: 247.853043, mae: 32.726795, mean_q: -33.388321\n",
            " 1478757/10000000: episode: 7357, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -670.400, mean reward: -3.335 [-335.200, 34.400], mean action: 2.811 [0.000, 10.000], mean observation: 35.130 [0.001, 422.500], loss: 236.091843, mae: 33.067513, mean_q: -33.906593\n",
            " 1478958/10000000: episode: 7358, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: 26.400, mean reward: 0.131 [-10.000, 327.200], mean action: 2.403 [0.000, 10.000], mean observation: 31.573 [0.001, 676.900], loss: 217.551056, mae: 33.483082, mean_q: -34.232689\n",
            " 1479159/10000000: episode: 7359, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -405.800, mean reward: -2.019 [-202.900, 59.400], mean action: 2.453 [0.000, 10.000], mean observation: 26.668 [0.007, 497.200], loss: 120.772057, mae: 33.764259, mean_q: -34.824184\n",
            " 1479360/10000000: episode: 7360, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -548.600, mean reward: -2.729 [-274.300, 79.600], mean action: 2.139 [0.000, 9.000], mean observation: 45.503 [0.000, 714.300], loss: 203.175995, mae: 34.208111, mean_q: -35.055683\n",
            " 1479561/10000000: episode: 7361, duration: 1.457s, episode steps: 201, steps per second: 138, episode reward: -416.000, mean reward: -2.070 [-208.000, 69.000], mean action: 2.040 [0.000, 10.000], mean observation: 33.298 [0.000, 582.800], loss: 164.471710, mae: 34.946278, mean_q: -35.894711\n",
            " 1479762/10000000: episode: 7362, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 56.400, mean reward: 0.281 [-10.000, 135.900], mean action: 1.766 [0.000, 10.000], mean observation: 32.905 [0.000, 566.700], loss: 139.962585, mae: 35.676811, mean_q: -36.545025\n",
            " 1479963/10000000: episode: 7363, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -242.600, mean reward: -1.207 [-121.300, 37.200], mean action: 1.940 [0.000, 10.000], mean observation: 29.537 [0.001, 498.800], loss: 134.692108, mae: 35.633205, mean_q: -36.408566\n",
            " 1480164/10000000: episode: 7364, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -501.000, mean reward: -2.493 [-250.500, 54.400], mean action: 2.154 [0.000, 10.000], mean observation: 32.465 [0.002, 508.500], loss: 131.938202, mae: 35.566730, mean_q: -36.400639\n",
            " 1480365/10000000: episode: 7365, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -350.800, mean reward: -1.745 [-175.400, 55.300], mean action: 1.935 [0.000, 10.000], mean observation: 32.012 [0.000, 413.800], loss: 125.580711, mae: 35.508980, mean_q: -36.119263\n",
            " 1480566/10000000: episode: 7366, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -597.400, mean reward: -2.972 [-298.700, 18.600], mean action: 1.910 [0.000, 10.000], mean observation: 38.273 [0.000, 546.900], loss: 149.486191, mae: 35.384575, mean_q: -36.146465\n",
            " 1480767/10000000: episode: 7367, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: 93.400, mean reward: 0.465 [-7.000, 262.000], mean action: 1.995 [0.000, 7.000], mean observation: 34.072 [0.001, 630.500], loss: 174.367462, mae: 35.342335, mean_q: -36.306721\n",
            " 1480968/10000000: episode: 7368, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: 187.200, mean reward: 0.931 [-10.000, 132.100], mean action: 2.423 [0.000, 10.000], mean observation: 31.158 [0.001, 527.300], loss: 144.378876, mae: 35.346416, mean_q: -36.358742\n",
            " 1481169/10000000: episode: 7369, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -204.000, mean reward: -1.015 [-102.000, 111.100], mean action: 2.517 [0.000, 10.000], mean observation: 35.933 [0.000, 562.200], loss: 171.900909, mae: 35.548527, mean_q: -36.588421\n",
            " 1481370/10000000: episode: 7370, duration: 1.597s, episode steps: 201, steps per second: 126, episode reward: 324.600, mean reward: 1.615 [-10.000, 269.200], mean action: 2.502 [0.000, 10.000], mean observation: 32.186 [0.002, 637.200], loss: 175.374451, mae: 35.600040, mean_q: -36.781673\n",
            " 1481571/10000000: episode: 7371, duration: 1.606s, episode steps: 201, steps per second: 125, episode reward: -280.800, mean reward: -1.397 [-140.400, 166.800], mean action: 2.781 [0.000, 10.000], mean observation: 25.947 [0.003, 436.400], loss: 216.712143, mae: 35.961433, mean_q: -37.167843\n",
            " 1481772/10000000: episode: 7372, duration: 1.581s, episode steps: 201, steps per second: 127, episode reward: 85.800, mean reward: 0.427 [-10.000, 83.300], mean action: 2.786 [0.000, 10.000], mean observation: 31.455 [0.001, 607.700], loss: 162.888214, mae: 35.644180, mean_q: -36.857014\n",
            " 1481973/10000000: episode: 7373, duration: 1.519s, episode steps: 201, steps per second: 132, episode reward: -381.000, mean reward: -1.896 [-190.500, 81.000], mean action: 2.244 [0.000, 10.000], mean observation: 33.603 [0.001, 462.800], loss: 183.973846, mae: 35.424625, mean_q: -36.363937\n",
            " 1482174/10000000: episode: 7374, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: -640.800, mean reward: -3.188 [-320.400, 13.600], mean action: 2.119 [0.000, 10.000], mean observation: 37.000 [0.003, 524.500], loss: 171.763977, mae: 35.102337, mean_q: -35.722580\n",
            " 1482375/10000000: episode: 7375, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: -121.400, mean reward: -0.604 [-60.700, 52.200], mean action: 1.816 [0.000, 9.000], mean observation: 28.094 [0.000, 772.000], loss: 175.627716, mae: 34.976761, mean_q: -35.737637\n",
            " 1482576/10000000: episode: 7376, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -569.800, mean reward: -2.835 [-284.900, 46.800], mean action: 2.100 [0.000, 10.000], mean observation: 35.030 [0.000, 558.800], loss: 207.024490, mae: 34.975372, mean_q: -35.919842\n",
            " 1482777/10000000: episode: 7377, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: -292.600, mean reward: -1.456 [-146.300, 76.500], mean action: 2.557 [0.000, 10.000], mean observation: 31.391 [0.001, 501.100], loss: 171.071564, mae: 35.143379, mean_q: -36.510319\n",
            " 1482978/10000000: episode: 7378, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -319.200, mean reward: -1.588 [-159.600, 124.600], mean action: 2.338 [0.000, 10.000], mean observation: 34.191 [0.001, 645.600], loss: 213.984650, mae: 35.630642, mean_q: -36.806023\n",
            " 1483179/10000000: episode: 7379, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -11.400, mean reward: -0.057 [-10.000, 47.200], mean action: 1.891 [0.000, 10.000], mean observation: 31.962 [0.001, 516.000], loss: 161.368240, mae: 35.728687, mean_q: -36.701290\n",
            " 1483380/10000000: episode: 7380, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 3.000, mean reward: 0.015 [-10.000, 88.000], mean action: 2.199 [0.000, 10.000], mean observation: 36.583 [0.000, 555.400], loss: 194.069717, mae: 35.213230, mean_q: -36.226059\n",
            " 1483581/10000000: episode: 7381, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -454.200, mean reward: -2.260 [-227.100, 103.200], mean action: 2.428 [0.000, 10.000], mean observation: 32.462 [0.000, 553.100], loss: 187.214676, mae: 35.617840, mean_q: -36.947483\n",
            " 1483782/10000000: episode: 7382, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -678.000, mean reward: -3.373 [-339.000, 55.500], mean action: 2.642 [0.000, 10.000], mean observation: 36.886 [0.000, 668.100], loss: 212.867661, mae: 35.676121, mean_q: -36.998428\n",
            " 1483983/10000000: episode: 7383, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -747.000, mean reward: -3.716 [-373.500, 45.000], mean action: 2.692 [0.000, 10.000], mean observation: 29.988 [0.000, 483.800], loss: 165.933258, mae: 35.902695, mean_q: -37.143383\n",
            " 1484184/10000000: episode: 7384, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: 97.000, mean reward: 0.483 [-10.000, 142.200], mean action: 1.925 [0.000, 10.000], mean observation: 28.324 [0.002, 513.300], loss: 89.220871, mae: 36.132233, mean_q: -37.097813\n",
            " 1484385/10000000: episode: 7385, duration: 1.387s, episode steps: 201, steps per second: 145, episode reward: -480.800, mean reward: -2.392 [-240.400, 49.000], mean action: 2.025 [0.000, 10.000], mean observation: 37.689 [0.000, 606.600], loss: 192.957825, mae: 36.368923, mean_q: -37.426815\n",
            " 1484586/10000000: episode: 7386, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -335.800, mean reward: -1.671 [-167.900, 107.800], mean action: 1.731 [0.000, 10.000], mean observation: 36.580 [0.001, 440.800], loss: 182.092346, mae: 36.497768, mean_q: -37.475155\n",
            " 1484787/10000000: episode: 7387, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: 12.400, mean reward: 0.062 [-9.000, 279.000], mean action: 2.040 [0.000, 9.000], mean observation: 33.501 [0.002, 578.200], loss: 144.044342, mae: 36.821308, mean_q: -37.778805\n",
            " 1484988/10000000: episode: 7388, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -677.800, mean reward: -3.372 [-338.900, 35.000], mean action: 2.383 [0.000, 10.000], mean observation: 31.025 [0.001, 478.600], loss: 201.485504, mae: 37.016380, mean_q: -38.218792\n",
            " 1485189/10000000: episode: 7389, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -395.200, mean reward: -1.966 [-197.600, 42.800], mean action: 1.995 [0.000, 10.000], mean observation: 37.319 [0.000, 527.200], loss: 156.905975, mae: 37.100315, mean_q: -38.256432\n",
            " 1485390/10000000: episode: 7390, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: 110.200, mean reward: 0.548 [-10.000, 102.000], mean action: 2.020 [0.000, 10.000], mean observation: 32.859 [0.001, 675.300], loss: 216.490784, mae: 37.151470, mean_q: -38.148174\n",
            " 1485591/10000000: episode: 7391, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -578.600, mean reward: -2.879 [-289.300, 24.000], mean action: 1.925 [0.000, 9.000], mean observation: 29.479 [0.004, 432.400], loss: 206.082687, mae: 37.036156, mean_q: -37.937199\n",
            " 1485792/10000000: episode: 7392, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -279.200, mean reward: -1.389 [-139.600, 111.600], mean action: 2.537 [0.000, 10.000], mean observation: 31.556 [0.000, 527.500], loss: 169.004837, mae: 36.727608, mean_q: -37.909340\n",
            " 1485993/10000000: episode: 7393, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -297.800, mean reward: -1.482 [-148.900, 172.000], mean action: 2.791 [0.000, 10.000], mean observation: 36.325 [0.000, 544.700], loss: 186.969528, mae: 37.000732, mean_q: -38.405243\n",
            " 1486194/10000000: episode: 7394, duration: 1.494s, episode steps: 201, steps per second: 135, episode reward: 362.400, mean reward: 1.803 [-10.000, 280.000], mean action: 2.746 [0.000, 10.000], mean observation: 36.463 [0.002, 532.900], loss: 167.732651, mae: 37.110817, mean_q: -38.652451\n",
            " 1486395/10000000: episode: 7395, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -73.400, mean reward: -0.365 [-36.700, 162.000], mean action: 2.602 [0.000, 10.000], mean observation: 33.937 [0.001, 647.400], loss: 174.826843, mae: 37.124573, mean_q: -38.882114\n",
            " 1486596/10000000: episode: 7396, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -547.400, mean reward: -2.723 [-273.700, 63.000], mean action: 2.756 [0.000, 10.000], mean observation: 32.351 [0.001, 602.400], loss: 202.497025, mae: 37.502659, mean_q: -39.211437\n",
            " 1486797/10000000: episode: 7397, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: -794.600, mean reward: -3.953 [-397.300, 51.900], mean action: 2.602 [0.000, 10.000], mean observation: 34.115 [0.000, 453.500], loss: 164.591339, mae: 38.091061, mean_q: -39.715836\n",
            " 1486998/10000000: episode: 7398, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -290.400, mean reward: -1.445 [-145.200, 59.200], mean action: 2.363 [0.000, 10.000], mean observation: 28.822 [0.002, 364.300], loss: 176.617996, mae: 38.126713, mean_q: -39.741329\n",
            " 1487199/10000000: episode: 7399, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: 247.800, mean reward: 1.233 [-10.000, 123.900], mean action: 2.433 [0.000, 10.000], mean observation: 29.020 [0.000, 695.400], loss: 205.411148, mae: 38.332584, mean_q: -40.017490\n",
            " 1487400/10000000: episode: 7400, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -307.800, mean reward: -1.531 [-153.900, 65.600], mean action: 2.189 [0.000, 9.000], mean observation: 34.208 [0.000, 660.900], loss: 277.141754, mae: 38.235298, mean_q: -39.706604\n",
            " 1487601/10000000: episode: 7401, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: 308.000, mean reward: 1.532 [-10.000, 212.000], mean action: 2.398 [0.000, 10.000], mean observation: 34.992 [0.000, 525.900], loss: 140.216919, mae: 37.947109, mean_q: -39.579544\n",
            " 1487802/10000000: episode: 7402, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -531.800, mean reward: -2.646 [-265.900, 29.600], mean action: 2.562 [0.000, 10.000], mean observation: 34.666 [0.001, 446.900], loss: 160.498322, mae: 38.101311, mean_q: -39.575615\n",
            " 1488003/10000000: episode: 7403, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -666.200, mean reward: -3.314 [-333.100, 66.500], mean action: 2.677 [0.000, 10.000], mean observation: 33.216 [0.000, 386.400], loss: 155.713486, mae: 37.947678, mean_q: -39.393944\n",
            " 1488204/10000000: episode: 7404, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: -178.400, mean reward: -0.888 [-89.200, 123.300], mean action: 2.458 [0.000, 10.000], mean observation: 30.218 [0.001, 654.600], loss: 137.287628, mae: 38.337257, mean_q: -39.859848\n",
            " 1488405/10000000: episode: 7405, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -282.200, mean reward: -1.404 [-141.100, 144.000], mean action: 2.741 [0.000, 10.000], mean observation: 36.170 [0.000, 467.400], loss: 149.084000, mae: 37.890610, mean_q: -39.211998\n",
            " 1488606/10000000: episode: 7406, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -479.600, mean reward: -2.386 [-239.800, 87.300], mean action: 2.896 [0.000, 9.000], mean observation: 35.730 [0.000, 697.600], loss: 202.895370, mae: 37.603447, mean_q: -39.250687\n",
            " 1488807/10000000: episode: 7407, duration: 1.416s, episode steps: 201, steps per second: 142, episode reward: -368.400, mean reward: -1.833 [-184.200, 114.500], mean action: 2.846 [0.000, 9.000], mean observation: 30.645 [0.000, 499.800], loss: 209.812012, mae: 37.918129, mean_q: -39.664738\n",
            " 1489008/10000000: episode: 7408, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -452.800, mean reward: -2.253 [-226.400, 60.300], mean action: 2.398 [0.000, 9.000], mean observation: 32.050 [0.001, 443.500], loss: 197.283783, mae: 38.097713, mean_q: -39.674152\n",
            " 1489209/10000000: episode: 7409, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -109.600, mean reward: -0.545 [-54.800, 168.300], mean action: 2.920 [0.000, 10.000], mean observation: 33.310 [0.001, 470.800], loss: 163.138687, mae: 38.241844, mean_q: -39.974731\n",
            " 1489410/10000000: episode: 7410, duration: 1.388s, episode steps: 201, steps per second: 145, episode reward: -185.800, mean reward: -0.924 [-92.900, 156.800], mean action: 2.587 [0.000, 10.000], mean observation: 30.578 [0.002, 447.400], loss: 142.817749, mae: 38.457840, mean_q: -40.110950\n",
            " 1489611/10000000: episode: 7411, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -544.000, mean reward: -2.706 [-272.000, 62.400], mean action: 2.637 [0.000, 10.000], mean observation: 32.212 [0.002, 515.900], loss: 145.930908, mae: 38.773502, mean_q: -40.361732\n",
            " 1489812/10000000: episode: 7412, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: 194.400, mean reward: 0.967 [-10.000, 270.900], mean action: 2.657 [0.000, 10.000], mean observation: 34.614 [0.002, 467.700], loss: 147.933548, mae: 38.796490, mean_q: -40.616901\n",
            " 1490013/10000000: episode: 7413, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: 413.000, mean reward: 2.055 [-9.000, 238.500], mean action: 2.478 [0.000, 9.000], mean observation: 27.836 [0.002, 434.600], loss: 204.123840, mae: 38.852573, mean_q: -40.591743\n",
            " 1490214/10000000: episode: 7414, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -646.200, mean reward: -3.215 [-323.100, 88.900], mean action: 2.915 [0.000, 9.000], mean observation: 33.297 [0.000, 638.600], loss: 197.572571, mae: 39.266617, mean_q: -40.984001\n",
            " 1490415/10000000: episode: 7415, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: -524.800, mean reward: -2.611 [-262.400, 102.600], mean action: 2.716 [0.000, 10.000], mean observation: 33.307 [0.000, 407.100], loss: 176.954926, mae: 39.276974, mean_q: -40.684994\n",
            " 1490616/10000000: episode: 7416, duration: 1.488s, episode steps: 201, steps per second: 135, episode reward: -339.800, mean reward: -1.691 [-169.900, 79.200], mean action: 2.254 [0.000, 9.000], mean observation: 35.758 [0.000, 530.500], loss: 170.544037, mae: 39.287960, mean_q: -40.637184\n",
            " 1490817/10000000: episode: 7417, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -266.000, mean reward: -1.323 [-133.000, 156.600], mean action: 2.318 [0.000, 9.000], mean observation: 34.170 [0.000, 796.200], loss: 110.484955, mae: 38.966393, mean_q: -40.172565\n",
            " 1491018/10000000: episode: 7418, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -616.600, mean reward: -3.068 [-308.300, 36.400], mean action: 1.866 [0.000, 9.000], mean observation: 37.319 [0.000, 588.400], loss: 150.571884, mae: 39.056145, mean_q: -40.438206\n",
            " 1491219/10000000: episode: 7419, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -731.200, mean reward: -3.638 [-365.600, 24.800], mean action: 2.264 [0.000, 10.000], mean observation: 30.649 [0.001, 466.300], loss: 180.547958, mae: 38.825191, mean_q: -40.134624\n",
            " 1491420/10000000: episode: 7420, duration: 1.402s, episode steps: 201, steps per second: 143, episode reward: 88.000, mean reward: 0.438 [-9.000, 294.800], mean action: 2.060 [0.000, 9.000], mean observation: 33.628 [0.003, 438.400], loss: 119.679108, mae: 39.288582, mean_q: -40.590282\n",
            " 1491621/10000000: episode: 7421, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: 44.200, mean reward: 0.220 [-9.000, 132.600], mean action: 1.965 [0.000, 9.000], mean observation: 27.277 [0.002, 545.200], loss: 113.961128, mae: 39.620590, mean_q: -40.757221\n",
            " 1491822/10000000: episode: 7422, duration: 1.428s, episode steps: 201, steps per second: 141, episode reward: -309.000, mean reward: -1.537 [-154.500, 36.000], mean action: 1.811 [0.000, 9.000], mean observation: 35.470 [0.001, 491.000], loss: 183.906876, mae: 39.387218, mean_q: -40.304699\n",
            " 1492023/10000000: episode: 7423, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: 126.200, mean reward: 0.628 [-9.000, 134.600], mean action: 2.806 [0.000, 9.000], mean observation: 30.128 [0.000, 452.700], loss: 146.853607, mae: 38.915119, mean_q: -40.250603\n",
            " 1492224/10000000: episode: 7424, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: -797.400, mean reward: -3.967 [-398.700, 20.400], mean action: 2.249 [0.000, 9.000], mean observation: 35.350 [0.002, 481.400], loss: 246.114777, mae: 38.767330, mean_q: -39.846977\n",
            " 1492425/10000000: episode: 7425, duration: 1.383s, episode steps: 201, steps per second: 145, episode reward: -602.200, mean reward: -2.996 [-301.100, 33.600], mean action: 2.085 [0.000, 9.000], mean observation: 28.290 [0.000, 746.900], loss: 134.257233, mae: 38.752739, mean_q: -40.134640\n",
            " 1492626/10000000: episode: 7426, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -571.200, mean reward: -2.842 [-285.600, 43.500], mean action: 2.403 [0.000, 9.000], mean observation: 33.559 [0.000, 609.400], loss: 268.804504, mae: 38.855221, mean_q: -40.045780\n",
            " 1492827/10000000: episode: 7427, duration: 1.384s, episode steps: 201, steps per second: 145, episode reward: -355.600, mean reward: -1.769 [-177.800, 153.600], mean action: 2.259 [0.000, 9.000], mean observation: 30.465 [0.000, 702.100], loss: 139.854431, mae: 39.192295, mean_q: -40.500275\n",
            " 1493028/10000000: episode: 7428, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -659.000, mean reward: -3.279 [-329.500, 45.000], mean action: 2.398 [0.000, 9.000], mean observation: 34.750 [0.000, 472.400], loss: 123.033180, mae: 39.088242, mean_q: -40.247864\n",
            " 1493229/10000000: episode: 7429, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -144.800, mean reward: -0.720 [-72.400, 182.700], mean action: 2.264 [0.000, 9.000], mean observation: 38.616 [0.000, 653.600], loss: 181.824387, mae: 39.133125, mean_q: -40.473621\n",
            " 1493430/10000000: episode: 7430, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: -279.600, mean reward: -1.391 [-139.800, 122.500], mean action: 2.826 [0.000, 10.000], mean observation: 27.350 [0.000, 509.800], loss: 178.717804, mae: 39.162071, mean_q: -40.743488\n",
            " 1493631/10000000: episode: 7431, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -567.200, mean reward: -2.822 [-283.600, 63.700], mean action: 2.826 [0.000, 10.000], mean observation: 41.450 [0.002, 630.900], loss: 193.050491, mae: 38.718948, mean_q: -40.556030\n",
            " 1493832/10000000: episode: 7432, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 4.000, mean reward: 0.020 [-9.000, 106.500], mean action: 2.940 [0.000, 9.000], mean observation: 31.651 [0.001, 571.000], loss: 172.009995, mae: 39.109108, mean_q: -41.101120\n",
            " 1494033/10000000: episode: 7433, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -671.000, mean reward: -3.338 [-335.500, 55.200], mean action: 2.741 [0.000, 10.000], mean observation: 35.864 [0.001, 591.000], loss: 151.206940, mae: 39.503315, mean_q: -41.365463\n",
            " 1494234/10000000: episode: 7434, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -284.400, mean reward: -1.415 [-142.200, 63.000], mean action: 2.597 [0.000, 9.000], mean observation: 31.938 [0.001, 595.600], loss: 195.811798, mae: 39.701157, mean_q: -41.559643\n",
            " 1494435/10000000: episode: 7435, duration: 1.390s, episode steps: 201, steps per second: 145, episode reward: -146.200, mean reward: -0.727 [-73.100, 74.900], mean action: 1.900 [0.000, 10.000], mean observation: 33.219 [0.000, 530.000], loss: 215.395462, mae: 39.823864, mean_q: -41.025078\n",
            " 1494636/10000000: episode: 7436, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: 1001.800, mean reward: 4.984 [-9.000, 500.900], mean action: 2.582 [0.000, 9.000], mean observation: 31.337 [0.001, 453.300], loss: 127.969933, mae: 39.661686, mean_q: -40.954082\n",
            " 1494837/10000000: episode: 7437, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -188.000, mean reward: -0.935 [-94.000, 84.700], mean action: 1.990 [0.000, 10.000], mean observation: 31.913 [0.006, 504.200], loss: 181.196167, mae: 39.957630, mean_q: -41.195759\n",
            " 1495038/10000000: episode: 7438, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -336.800, mean reward: -1.676 [-168.400, 50.800], mean action: 2.303 [0.000, 9.000], mean observation: 30.858 [0.001, 591.100], loss: 151.336746, mae: 39.949799, mean_q: -41.584831\n",
            " 1495239/10000000: episode: 7439, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -378.200, mean reward: -1.882 [-189.100, 67.800], mean action: 2.090 [0.000, 10.000], mean observation: 35.255 [0.001, 578.100], loss: 149.826996, mae: 40.148178, mean_q: -41.312714\n",
            " 1495440/10000000: episode: 7440, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -421.400, mean reward: -2.097 [-210.700, 131.600], mean action: 2.254 [0.000, 9.000], mean observation: 33.358 [0.001, 598.700], loss: 139.435379, mae: 39.926613, mean_q: -41.099518\n",
            " 1495641/10000000: episode: 7441, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: 315.000, mean reward: 1.567 [-9.000, 172.500], mean action: 2.244 [0.000, 9.000], mean observation: 27.752 [0.003, 305.100], loss: 158.964523, mae: 39.341518, mean_q: -40.760036\n",
            " 1495842/10000000: episode: 7442, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: 138.400, mean reward: 0.689 [-9.000, 124.000], mean action: 2.567 [0.000, 9.000], mean observation: 32.786 [0.000, 540.000], loss: 222.868118, mae: 39.256886, mean_q: -40.670540\n",
            " 1496043/10000000: episode: 7443, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -409.200, mean reward: -2.036 [-204.600, 86.400], mean action: 2.756 [0.000, 10.000], mean observation: 29.997 [0.000, 663.800], loss: 160.646118, mae: 39.500957, mean_q: -41.179035\n",
            " 1496244/10000000: episode: 7444, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: -678.400, mean reward: -3.375 [-339.200, 42.500], mean action: 2.662 [0.000, 9.000], mean observation: 33.032 [0.000, 477.000], loss: 189.916061, mae: 39.364544, mean_q: -40.842083\n",
            " 1496445/10000000: episode: 7445, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -33.000, mean reward: -0.164 [-16.500, 128.700], mean action: 2.876 [0.000, 10.000], mean observation: 33.700 [0.000, 639.500], loss: 214.830597, mae: 39.402771, mean_q: -40.667919\n",
            " 1496646/10000000: episode: 7446, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: 307.600, mean reward: 1.530 [-9.000, 181.800], mean action: 2.657 [0.000, 9.000], mean observation: 35.496 [0.001, 619.200], loss: 224.388397, mae: 39.161804, mean_q: -40.425030\n",
            " 1496847/10000000: episode: 7447, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -619.400, mean reward: -3.082 [-309.700, 78.300], mean action: 3.652 [0.000, 10.000], mean observation: 30.160 [0.001, 651.100], loss: 285.444977, mae: 38.379845, mean_q: -39.543354\n",
            " 1497048/10000000: episode: 7448, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -131.600, mean reward: -0.655 [-65.800, 193.500], mean action: 3.488 [0.000, 9.000], mean observation: 37.994 [0.000, 818.100], loss: 168.146118, mae: 37.636414, mean_q: -38.750877\n",
            " 1497249/10000000: episode: 7449, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: 3.600, mean reward: 0.018 [-10.000, 123.000], mean action: 2.920 [0.000, 10.000], mean observation: 32.755 [0.002, 429.000], loss: 194.178421, mae: 37.502148, mean_q: -38.519642\n",
            " 1497450/10000000: episode: 7450, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -754.400, mean reward: -3.753 [-377.200, 135.900], mean action: 3.353 [0.000, 9.000], mean observation: 33.218 [0.001, 565.900], loss: 156.138321, mae: 37.393280, mean_q: -38.293903\n",
            " 1497651/10000000: episode: 7451, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: -1295.200, mean reward: -6.444 [-647.600, 40.500], mean action: 4.368 [0.000, 9.000], mean observation: 35.469 [0.002, 446.700], loss: 191.216888, mae: 36.957275, mean_q: -37.900448\n",
            " 1497852/10000000: episode: 7452, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -1391.400, mean reward: -6.922 [-695.700, 31.500], mean action: 4.398 [0.000, 9.000], mean observation: 31.779 [0.000, 720.900], loss: 231.972519, mae: 36.823112, mean_q: -37.338135\n",
            " 1498053/10000000: episode: 7453, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -49.800, mean reward: -0.248 [-24.900, 321.300], mean action: 4.886 [0.000, 9.000], mean observation: 30.874 [0.002, 624.500], loss: 111.421898, mae: 35.504948, mean_q: -36.094158\n",
            " 1498254/10000000: episode: 7454, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -989.400, mean reward: -4.922 [-494.700, 79.200], mean action: 4.383 [0.000, 9.000], mean observation: 34.659 [0.003, 497.700], loss: 169.767334, mae: 35.059177, mean_q: -35.753811\n",
            " 1498455/10000000: episode: 7455, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -598.400, mean reward: -2.977 [-299.200, 106.200], mean action: 4.259 [0.000, 9.000], mean observation: 24.996 [0.001, 462.800], loss: 148.126495, mae: 34.720112, mean_q: -35.575439\n",
            " 1498656/10000000: episode: 7456, duration: 1.395s, episode steps: 201, steps per second: 144, episode reward: -1.400, mean reward: -0.007 [-9.000, 132.300], mean action: 3.831 [0.000, 9.000], mean observation: 38.124 [0.000, 476.600], loss: 148.054901, mae: 34.572468, mean_q: -35.253891\n",
            " 1498857/10000000: episode: 7457, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -781.400, mean reward: -3.888 [-390.700, 59.500], mean action: 3.264 [0.000, 9.000], mean observation: 37.550 [0.000, 642.700], loss: 180.861282, mae: 34.039707, mean_q: -34.679226\n",
            " 1499058/10000000: episode: 7458, duration: 1.442s, episode steps: 201, steps per second: 139, episode reward: 33.000, mean reward: 0.164 [-10.000, 380.700], mean action: 4.473 [0.000, 10.000], mean observation: 35.135 [0.000, 907.100], loss: 190.329163, mae: 34.058990, mean_q: -35.216423\n",
            " 1499259/10000000: episode: 7459, duration: 1.407s, episode steps: 201, steps per second: 143, episode reward: 234.600, mean reward: 1.167 [-9.000, 689.500], mean action: 4.338 [0.000, 9.000], mean observation: 31.984 [0.004, 399.700], loss: 202.412430, mae: 34.266319, mean_q: -35.518768\n",
            " 1499460/10000000: episode: 7460, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -710.200, mean reward: -3.533 [-355.100, 191.700], mean action: 3.721 [0.000, 9.000], mean observation: 34.201 [0.001, 522.800], loss: 173.682449, mae: 34.651814, mean_q: -36.181591\n",
            " 1499661/10000000: episode: 7461, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -362.800, mean reward: -1.805 [-181.400, 288.400], mean action: 3.871 [0.000, 9.000], mean observation: 31.147 [0.000, 476.000], loss: 201.615631, mae: 34.885513, mean_q: -35.923592\n",
            " 1499862/10000000: episode: 7462, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 2055.400, mean reward: 10.226 [-10.000, 1027.700], mean action: 3.308 [0.000, 10.000], mean observation: 29.581 [0.001, 623.900], loss: 167.566711, mae: 34.439972, mean_q: -35.682777\n",
            " 1500063/10000000: episode: 7463, duration: 1.392s, episode steps: 201, steps per second: 144, episode reward: -1051.200, mean reward: -5.230 [-525.600, 21.700], mean action: 3.085 [0.000, 10.000], mean observation: 31.660 [0.002, 542.000], loss: 310.955688, mae: 34.589874, mean_q: -35.385815\n",
            " 1500264/10000000: episode: 7464, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 598.800, mean reward: 2.979 [-9.000, 299.400], mean action: 2.781 [0.000, 9.000], mean observation: 38.592 [0.000, 515.500], loss: 254.320816, mae: 34.836685, mean_q: -35.972233\n",
            " 1500465/10000000: episode: 7465, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -92.800, mean reward: -0.462 [-46.400, 131.400], mean action: 2.552 [0.000, 9.000], mean observation: 27.585 [0.000, 781.200], loss: 212.877411, mae: 35.764484, mean_q: -36.904022\n",
            " 1500666/10000000: episode: 7466, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: 606.600, mean reward: 3.018 [-9.000, 303.300], mean action: 2.557 [0.000, 9.000], mean observation: 33.100 [0.001, 492.700], loss: 117.451164, mae: 35.887577, mean_q: -37.167591\n",
            " 1500867/10000000: episode: 7467, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -289.000, mean reward: -1.438 [-144.500, 84.000], mean action: 2.199 [0.000, 9.000], mean observation: 34.421 [0.002, 467.200], loss: 134.237518, mae: 36.088852, mean_q: -37.138542\n",
            " 1501068/10000000: episode: 7468, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: 319.800, mean reward: 1.591 [-10.000, 365.400], mean action: 2.517 [0.000, 10.000], mean observation: 33.480 [0.000, 511.400], loss: 131.078690, mae: 36.240505, mean_q: -37.378284\n",
            " 1501269/10000000: episode: 7469, duration: 1.377s, episode steps: 201, steps per second: 146, episode reward: -495.000, mean reward: -2.463 [-247.500, 61.200], mean action: 3.045 [0.000, 9.000], mean observation: 34.233 [0.000, 764.600], loss: 270.491058, mae: 36.003838, mean_q: -37.469963\n",
            " 1501470/10000000: episode: 7470, duration: 1.396s, episode steps: 201, steps per second: 144, episode reward: -398.800, mean reward: -1.984 [-199.400, 114.400], mean action: 3.418 [0.000, 10.000], mean observation: 28.397 [0.004, 498.200], loss: 226.711639, mae: 36.471050, mean_q: -37.893429\n",
            " 1501671/10000000: episode: 7471, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 19.000, mean reward: 0.095 [-9.000, 185.500], mean action: 2.512 [0.000, 9.000], mean observation: 33.189 [0.000, 598.900], loss: 195.463486, mae: 37.281357, mean_q: -38.333496\n",
            " 1501872/10000000: episode: 7472, duration: 1.439s, episode steps: 201, steps per second: 140, episode reward: -444.400, mean reward: -2.211 [-222.200, 114.300], mean action: 2.363 [0.000, 9.000], mean observation: 34.106 [0.001, 508.700], loss: 158.143555, mae: 37.403866, mean_q: -38.534943\n",
            " 1502073/10000000: episode: 7473, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -318.800, mean reward: -1.586 [-159.400, 100.100], mean action: 2.443 [0.000, 10.000], mean observation: 37.094 [0.000, 792.800], loss: 152.941895, mae: 37.548725, mean_q: -38.582657\n",
            " 1502274/10000000: episode: 7474, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: -44.800, mean reward: -0.223 [-22.400, 190.800], mean action: 2.896 [0.000, 10.000], mean observation: 32.024 [0.002, 455.000], loss: 243.751831, mae: 37.416763, mean_q: -38.609238\n",
            " 1502475/10000000: episode: 7475, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: -119.800, mean reward: -0.596 [-59.900, 131.400], mean action: 3.095 [0.000, 10.000], mean observation: 38.353 [0.000, 746.700], loss: 221.271103, mae: 37.207142, mean_q: -38.165478\n",
            " 1502676/10000000: episode: 7476, duration: 1.429s, episode steps: 201, steps per second: 141, episode reward: -102.200, mean reward: -0.508 [-51.100, 149.400], mean action: 2.736 [0.000, 10.000], mean observation: 33.894 [0.001, 541.500], loss: 321.850189, mae: 37.034309, mean_q: -37.908615\n",
            " 1502877/10000000: episode: 7477, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -640.400, mean reward: -3.186 [-320.200, 117.300], mean action: 2.781 [0.000, 10.000], mean observation: 31.207 [0.000, 516.300], loss: 181.312805, mae: 36.676414, mean_q: -37.657967\n",
            " 1503078/10000000: episode: 7478, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -534.600, mean reward: -2.660 [-267.300, 58.500], mean action: 2.552 [0.000, 10.000], mean observation: 31.690 [0.001, 519.300], loss: 145.719101, mae: 36.692486, mean_q: -37.731823\n",
            " 1503279/10000000: episode: 7479, duration: 1.374s, episode steps: 201, steps per second: 146, episode reward: -61.800, mean reward: -0.307 [-30.900, 225.800], mean action: 2.647 [0.000, 10.000], mean observation: 36.046 [0.003, 566.800], loss: 174.644043, mae: 36.953110, mean_q: -37.891689\n",
            " 1503480/10000000: episode: 7480, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -698.800, mean reward: -3.477 [-349.400, 65.000], mean action: 3.313 [0.000, 10.000], mean observation: 32.372 [0.000, 579.300], loss: 292.846558, mae: 37.113937, mean_q: -38.515640\n",
            " 1503681/10000000: episode: 7481, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -867.400, mean reward: -4.315 [-433.700, 112.700], mean action: 4.100 [0.000, 10.000], mean observation: 29.153 [0.001, 576.200], loss: 180.426163, mae: 37.529083, mean_q: -39.376823\n",
            " 1503882/10000000: episode: 7482, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -440.000, mean reward: -2.189 [-220.000, 118.800], mean action: 3.353 [0.000, 10.000], mean observation: 32.688 [0.001, 550.200], loss: 209.775757, mae: 37.709625, mean_q: -39.583179\n",
            " 1504083/10000000: episode: 7483, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -648.000, mean reward: -3.224 [-324.000, 64.800], mean action: 2.552 [0.000, 9.000], mean observation: 31.046 [0.001, 493.400], loss: 214.329269, mae: 38.453705, mean_q: -39.592289\n",
            " 1504284/10000000: episode: 7484, duration: 1.445s, episode steps: 201, steps per second: 139, episode reward: -297.000, mean reward: -1.478 [-148.500, 106.200], mean action: 2.418 [0.000, 10.000], mean observation: 33.266 [0.001, 494.100], loss: 156.510208, mae: 38.128796, mean_q: -39.643803\n",
            " 1504485/10000000: episode: 7485, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -714.200, mean reward: -3.553 [-357.100, 45.600], mean action: 2.433 [0.000, 9.000], mean observation: 39.374 [0.000, 525.700], loss: 200.819061, mae: 37.854168, mean_q: -39.682014\n",
            " 1504686/10000000: episode: 7486, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: 411.400, mean reward: 2.047 [-10.000, 279.600], mean action: 2.612 [0.000, 10.000], mean observation: 37.808 [0.000, 803.400], loss: 226.392883, mae: 38.302750, mean_q: -40.083309\n",
            " 1504887/10000000: episode: 7487, duration: 1.441s, episode steps: 201, steps per second: 139, episode reward: -626.600, mean reward: -3.117 [-313.300, 30.000], mean action: 2.731 [0.000, 9.000], mean observation: 32.501 [0.001, 442.600], loss: 136.549637, mae: 38.693123, mean_q: -40.420731\n",
            " 1505088/10000000: episode: 7488, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -251.000, mean reward: -1.249 [-125.500, 84.800], mean action: 2.299 [0.000, 10.000], mean observation: 31.046 [0.002, 525.700], loss: 190.868042, mae: 38.872253, mean_q: -40.404129\n",
            " 1505289/10000000: episode: 7489, duration: 1.386s, episode steps: 201, steps per second: 145, episode reward: -267.400, mean reward: -1.330 [-133.700, 66.600], mean action: 2.179 [0.000, 9.000], mean observation: 35.169 [0.001, 545.700], loss: 187.763351, mae: 39.252815, mean_q: -40.625744\n",
            " 1505490/10000000: episode: 7490, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -636.000, mean reward: -3.164 [-318.000, 68.400], mean action: 2.373 [0.000, 10.000], mean observation: 32.763 [0.000, 622.400], loss: 189.827316, mae: 39.101555, mean_q: -40.487026\n",
            " 1505691/10000000: episode: 7491, duration: 1.406s, episode steps: 201, steps per second: 143, episode reward: -863.600, mean reward: -4.297 [-431.800, 88.500], mean action: 3.597 [0.000, 10.000], mean observation: 38.516 [0.000, 592.700], loss: 290.119995, mae: 38.710907, mean_q: -40.445824\n",
            " 1505892/10000000: episode: 7492, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -96.200, mean reward: -0.479 [-48.100, 195.000], mean action: 2.995 [0.000, 10.000], mean observation: 35.193 [0.000, 669.400], loss: 241.042374, mae: 38.702145, mean_q: -40.531021\n",
            " 1506093/10000000: episode: 7493, duration: 1.491s, episode steps: 201, steps per second: 135, episode reward: -343.000, mean reward: -1.706 [-171.500, 214.200], mean action: 3.085 [0.000, 10.000], mean observation: 32.924 [0.000, 556.200], loss: 205.064255, mae: 39.111137, mean_q: -40.994579\n",
            " 1506294/10000000: episode: 7494, duration: 1.620s, episode steps: 201, steps per second: 124, episode reward: 50.000, mean reward: 0.249 [-9.000, 211.400], mean action: 2.642 [0.000, 10.000], mean observation: 35.655 [0.002, 511.500], loss: 199.859955, mae: 39.236153, mean_q: -40.703060\n",
            " 1506495/10000000: episode: 7495, duration: 1.676s, episode steps: 201, steps per second: 120, episode reward: -346.400, mean reward: -1.723 [-173.200, 104.400], mean action: 2.443 [0.000, 10.000], mean observation: 41.410 [0.000, 663.000], loss: 264.766724, mae: 39.103386, mean_q: -40.607368\n",
            " 1506696/10000000: episode: 7496, duration: 1.622s, episode steps: 201, steps per second: 124, episode reward: -319.400, mean reward: -1.589 [-159.700, 97.200], mean action: 3.070 [0.000, 10.000], mean observation: 32.074 [0.000, 637.900], loss: 344.280548, mae: 39.023937, mean_q: -40.677284\n",
            " 1506897/10000000: episode: 7497, duration: 1.636s, episode steps: 201, steps per second: 123, episode reward: -452.200, mean reward: -2.250 [-226.100, 76.400], mean action: 3.239 [0.000, 10.000], mean observation: 32.993 [0.001, 459.000], loss: 267.528809, mae: 39.434540, mean_q: -41.045757\n",
            " 1507098/10000000: episode: 7498, duration: 1.619s, episode steps: 201, steps per second: 124, episode reward: -695.800, mean reward: -3.462 [-347.900, 60.600], mean action: 2.796 [0.000, 9.000], mean observation: 33.645 [0.001, 581.100], loss: 205.379379, mae: 39.896469, mean_q: -41.423538\n",
            " 1507299/10000000: episode: 7499, duration: 1.619s, episode steps: 201, steps per second: 124, episode reward: -521.800, mean reward: -2.596 [-260.900, 158.900], mean action: 2.940 [0.000, 10.000], mean observation: 32.418 [0.001, 622.800], loss: 213.027649, mae: 40.333652, mean_q: -42.110725\n",
            " 1507500/10000000: episode: 7500, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -766.400, mean reward: -3.813 [-383.200, 84.000], mean action: 3.269 [0.000, 10.000], mean observation: 33.251 [0.000, 570.400], loss: 247.290207, mae: 40.640942, mean_q: -42.601475\n",
            " 1507701/10000000: episode: 7501, duration: 1.401s, episode steps: 201, steps per second: 144, episode reward: -760.200, mean reward: -3.782 [-380.100, 38.400], mean action: 3.005 [0.000, 10.000], mean observation: 35.452 [0.001, 446.000], loss: 213.145248, mae: 40.812218, mean_q: -42.690853\n",
            " 1507902/10000000: episode: 7502, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -342.800, mean reward: -1.705 [-171.400, 288.400], mean action: 2.970 [0.000, 9.000], mean observation: 32.882 [0.002, 498.600], loss: 174.803909, mae: 41.360897, mean_q: -42.985443\n",
            " 1508103/10000000: episode: 7503, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -608.600, mean reward: -3.028 [-304.300, 96.600], mean action: 3.264 [0.000, 9.000], mean observation: 29.144 [0.000, 749.600], loss: 268.190613, mae: 41.170345, mean_q: -42.433643\n",
            " 1508304/10000000: episode: 7504, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -697.400, mean reward: -3.470 [-348.700, 102.300], mean action: 3.129 [0.000, 9.000], mean observation: 34.986 [0.001, 587.600], loss: 229.130188, mae: 40.270985, mean_q: -42.068134\n",
            " 1508505/10000000: episode: 7505, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: 398.000, mean reward: 1.980 [-9.000, 479.500], mean action: 2.463 [0.000, 9.000], mean observation: 33.765 [0.001, 516.600], loss: 286.235779, mae: 40.812790, mean_q: -42.635647\n",
            " 1508706/10000000: episode: 7506, duration: 1.408s, episode steps: 201, steps per second: 143, episode reward: -726.200, mean reward: -3.613 [-363.100, 32.200], mean action: 2.980 [0.000, 10.000], mean observation: 28.082 [0.003, 415.200], loss: 205.380951, mae: 40.851368, mean_q: -42.732742\n",
            " 1508907/10000000: episode: 7507, duration: 1.423s, episode steps: 201, steps per second: 141, episode reward: -630.200, mean reward: -3.135 [-315.100, 48.000], mean action: 2.831 [0.000, 9.000], mean observation: 35.166 [0.000, 509.100], loss: 211.455688, mae: 40.519211, mean_q: -42.467663\n",
            " 1509108/10000000: episode: 7508, duration: 1.403s, episode steps: 201, steps per second: 143, episode reward: -535.400, mean reward: -2.664 [-267.700, 68.000], mean action: 2.846 [0.000, 10.000], mean observation: 34.089 [0.001, 590.900], loss: 262.326324, mae: 41.237659, mean_q: -42.824287\n",
            " 1509309/10000000: episode: 7509, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -745.200, mean reward: -3.707 [-372.600, 49.500], mean action: 2.975 [0.000, 10.000], mean observation: 25.643 [0.000, 446.200], loss: 190.525879, mae: 40.736919, mean_q: -42.501476\n",
            " 1509510/10000000: episode: 7510, duration: 1.424s, episode steps: 201, steps per second: 141, episode reward: -351.400, mean reward: -1.748 [-175.700, 96.600], mean action: 2.657 [0.000, 9.000], mean observation: 33.496 [0.001, 504.300], loss: 198.818237, mae: 41.296947, mean_q: -42.812851\n",
            " 1509711/10000000: episode: 7511, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -65.200, mean reward: -0.324 [-32.600, 135.000], mean action: 2.886 [0.000, 9.000], mean observation: 30.475 [0.003, 510.800], loss: 284.683350, mae: 41.234161, mean_q: -43.008884\n",
            " 1509912/10000000: episode: 7512, duration: 1.485s, episode steps: 201, steps per second: 135, episode reward: -305.000, mean reward: -1.517 [-152.500, 58.800], mean action: 3.383 [0.000, 10.000], mean observation: 34.764 [0.001, 487.500], loss: 162.759369, mae: 40.664944, mean_q: -42.533676\n",
            " 1510113/10000000: episode: 7513, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: 1072.800, mean reward: 5.337 [-10.000, 536.400], mean action: 3.045 [0.000, 10.000], mean observation: 34.900 [0.000, 637.000], loss: 228.617538, mae: 41.005608, mean_q: -42.734676\n",
            " 1510314/10000000: episode: 7514, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -805.400, mean reward: -4.007 [-402.700, 35.700], mean action: 3.144 [0.000, 10.000], mean observation: 29.552 [0.000, 640.400], loss: 216.261276, mae: 40.990330, mean_q: -42.844391\n",
            " 1510515/10000000: episode: 7515, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -293.400, mean reward: -1.460 [-146.700, 103.800], mean action: 2.876 [0.000, 10.000], mean observation: 30.047 [0.001, 413.700], loss: 310.065216, mae: 41.101601, mean_q: -43.070583\n",
            " 1510716/10000000: episode: 7516, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -722.200, mean reward: -3.593 [-361.100, 36.000], mean action: 3.085 [0.000, 10.000], mean observation: 34.049 [0.001, 512.800], loss: 168.182907, mae: 41.279598, mean_q: -42.970333\n",
            " 1510917/10000000: episode: 7517, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: 73.400, mean reward: 0.365 [-9.000, 107.100], mean action: 2.706 [0.000, 9.000], mean observation: 29.090 [0.000, 558.800], loss: 184.670456, mae: 41.320324, mean_q: -42.841267\n",
            " 1511118/10000000: episode: 7518, duration: 1.391s, episode steps: 201, steps per second: 145, episode reward: -618.400, mean reward: -3.077 [-309.200, 126.900], mean action: 2.866 [0.000, 9.000], mean observation: 38.517 [0.001, 539.800], loss: 179.663422, mae: 41.646553, mean_q: -43.257969\n",
            " 1511319/10000000: episode: 7519, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -811.400, mean reward: -4.037 [-405.700, 27.400], mean action: 2.726 [0.000, 9.000], mean observation: 35.308 [0.000, 584.600], loss: 337.326324, mae: 41.663860, mean_q: -43.355206\n",
            " 1511520/10000000: episode: 7520, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: 117.000, mean reward: 0.582 [-10.000, 259.800], mean action: 2.637 [0.000, 10.000], mean observation: 37.681 [0.002, 510.200], loss: 281.950897, mae: 42.223347, mean_q: -44.029568\n",
            " 1511721/10000000: episode: 7521, duration: 1.431s, episode steps: 201, steps per second: 140, episode reward: -713.800, mean reward: -3.551 [-356.900, 36.600], mean action: 3.149 [0.000, 10.000], mean observation: 29.039 [0.001, 486.800], loss: 230.542557, mae: 42.198383, mean_q: -44.056698\n",
            " 1511922/10000000: episode: 7522, duration: 1.492s, episode steps: 201, steps per second: 135, episode reward: -264.800, mean reward: -1.317 [-132.400, 79.500], mean action: 2.244 [0.000, 7.000], mean observation: 31.868 [0.000, 708.200], loss: 319.027008, mae: 42.890053, mean_q: -44.565628\n",
            " 1512123/10000000: episode: 7523, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -414.400, mean reward: -2.062 [-207.200, 70.000], mean action: 2.249 [0.000, 9.000], mean observation: 39.237 [0.002, 500.200], loss: 146.798218, mae: 42.967522, mean_q: -44.620441\n",
            " 1512324/10000000: episode: 7524, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: 1016.200, mean reward: 5.056 [-10.000, 649.000], mean action: 2.433 [0.000, 10.000], mean observation: 35.328 [0.000, 440.500], loss: 260.067810, mae: 42.626865, mean_q: -44.532475\n",
            " 1512525/10000000: episode: 7525, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: 122.000, mean reward: 0.607 [-9.000, 195.000], mean action: 2.378 [0.000, 9.000], mean observation: 34.403 [0.000, 566.800], loss: 152.337723, mae: 42.972111, mean_q: -45.174206\n",
            " 1512726/10000000: episode: 7526, duration: 1.409s, episode steps: 201, steps per second: 143, episode reward: -392.200, mean reward: -1.951 [-196.100, 76.500], mean action: 2.448 [0.000, 10.000], mean observation: 30.646 [0.001, 571.900], loss: 147.341049, mae: 43.559574, mean_q: -45.626953\n",
            " 1512927/10000000: episode: 7527, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: 1885.600, mean reward: 9.381 [-10.000, 1269.000], mean action: 2.627 [0.000, 10.000], mean observation: 30.584 [0.000, 494.100], loss: 374.808289, mae: 43.350018, mean_q: -45.402149\n",
            " 1513128/10000000: episode: 7528, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -32.800, mean reward: -0.163 [-16.400, 484.200], mean action: 3.388 [0.000, 10.000], mean observation: 31.023 [0.001, 465.200], loss: 165.366776, mae: 42.663410, mean_q: -45.241108\n",
            " 1513329/10000000: episode: 7529, duration: 1.435s, episode steps: 201, steps per second: 140, episode reward: 985.000, mean reward: 4.900 [-10.000, 492.500], mean action: 3.134 [0.000, 10.000], mean observation: 32.075 [0.000, 798.300], loss: 397.093048, mae: 43.182087, mean_q: -45.361660\n",
            " 1513530/10000000: episode: 7530, duration: 1.438s, episode steps: 201, steps per second: 140, episode reward: -593.600, mean reward: -2.953 [-296.800, 44.800], mean action: 2.512 [0.000, 10.000], mean observation: 34.332 [0.001, 556.200], loss: 312.306580, mae: 43.108582, mean_q: -44.814220\n",
            " 1513731/10000000: episode: 7531, duration: 1.454s, episode steps: 201, steps per second: 138, episode reward: -267.400, mean reward: -1.330 [-133.700, 181.800], mean action: 2.806 [0.000, 10.000], mean observation: 26.566 [0.002, 434.300], loss: 236.236084, mae: 43.204941, mean_q: -45.078423\n",
            " 1513932/10000000: episode: 7532, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -615.400, mean reward: -3.062 [-307.700, 24.900], mean action: 2.617 [0.000, 10.000], mean observation: 34.963 [0.000, 668.900], loss: 306.740662, mae: 43.279984, mean_q: -45.286369\n",
            " 1514133/10000000: episode: 7533, duration: 1.503s, episode steps: 201, steps per second: 134, episode reward: 537.600, mean reward: 2.675 [-9.000, 483.300], mean action: 3.368 [0.000, 9.000], mean observation: 33.185 [0.000, 506.300], loss: 264.877380, mae: 42.959946, mean_q: -45.088276\n",
            " 1514334/10000000: episode: 7534, duration: 1.465s, episode steps: 201, steps per second: 137, episode reward: -562.000, mean reward: -2.796 [-281.000, 51.100], mean action: 2.721 [0.000, 10.000], mean observation: 32.899 [0.000, 531.100], loss: 265.095337, mae: 43.716709, mean_q: -45.996258\n",
            " 1514535/10000000: episode: 7535, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: -69.800, mean reward: -0.347 [-34.900, 200.000], mean action: 2.751 [0.000, 9.000], mean observation: 35.834 [0.000, 650.000], loss: 190.608505, mae: 44.165108, mean_q: -46.129993\n",
            " 1514736/10000000: episode: 7536, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -935.400, mean reward: -4.654 [-467.700, 50.000], mean action: 2.816 [0.000, 9.000], mean observation: 36.287 [0.000, 574.000], loss: 431.260040, mae: 43.304592, mean_q: -45.189224\n",
            " 1514937/10000000: episode: 7537, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -427.200, mean reward: -2.125 [-213.600, 105.900], mean action: 3.090 [0.000, 9.000], mean observation: 27.338 [0.002, 420.100], loss: 270.412323, mae: 43.515175, mean_q: -45.539360\n",
            " 1515138/10000000: episode: 7538, duration: 1.430s, episode steps: 201, steps per second: 141, episode reward: 1379.200, mean reward: 6.862 [-10.000, 689.600], mean action: 2.119 [0.000, 10.000], mean observation: 39.717 [0.002, 470.300], loss: 218.132736, mae: 43.437626, mean_q: -44.893063\n",
            " 1515339/10000000: episode: 7539, duration: 1.447s, episode steps: 201, steps per second: 139, episode reward: -567.000, mean reward: -2.821 [-283.500, 58.000], mean action: 2.697 [0.000, 9.000], mean observation: 35.411 [0.001, 510.400], loss: 438.856384, mae: 43.281338, mean_q: -45.051327\n",
            " 1515540/10000000: episode: 7540, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: 5355.600, mean reward: 26.645 [-9.000, 3037.500], mean action: 3.413 [0.000, 9.000], mean observation: 34.755 [0.000, 508.900], loss: 327.804810, mae: 43.677483, mean_q: -45.188339\n",
            " 1515741/10000000: episode: 7541, duration: 1.422s, episode steps: 201, steps per second: 141, episode reward: -1223.400, mean reward: -6.087 [-611.700, 67.500], mean action: 4.338 [0.000, 9.000], mean observation: 30.534 [0.002, 472.800], loss: 430.674713, mae: 42.799946, mean_q: -44.442902\n",
            " 1515942/10000000: episode: 7542, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -928.000, mean reward: -4.617 [-464.000, 171.900], mean action: 4.592 [0.000, 9.000], mean observation: 30.047 [0.001, 618.400], loss: 277.909058, mae: 42.454334, mean_q: -43.406467\n",
            " 1516143/10000000: episode: 7543, duration: 1.448s, episode steps: 201, steps per second: 139, episode reward: 207.400, mean reward: 1.032 [-9.000, 252.000], mean action: 4.209 [0.000, 9.000], mean observation: 35.323 [0.001, 512.000], loss: 1047.978882, mae: 42.027752, mean_q: -42.632618\n",
            " 1516344/10000000: episode: 7544, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -29.200, mean reward: -0.145 [-14.600, 246.600], mean action: 3.701 [0.000, 9.000], mean observation: 31.422 [0.000, 586.000], loss: 153.597641, mae: 41.532124, mean_q: -42.094196\n",
            " 1516545/10000000: episode: 7545, duration: 1.493s, episode steps: 201, steps per second: 135, episode reward: -1002.800, mean reward: -4.989 [-501.400, 49.500], mean action: 3.378 [0.000, 9.000], mean observation: 32.913 [0.001, 431.000], loss: 230.953827, mae: 41.275795, mean_q: -42.054222\n",
            " 1516746/10000000: episode: 7546, duration: 1.477s, episode steps: 201, steps per second: 136, episode reward: 635.000, mean reward: 3.159 [-10.000, 317.500], mean action: 2.876 [0.000, 10.000], mean observation: 29.366 [0.001, 459.200], loss: 330.656677, mae: 41.357773, mean_q: -42.424267\n",
            " 1516947/10000000: episode: 7547, duration: 1.458s, episode steps: 201, steps per second: 138, episode reward: -1043.200, mean reward: -5.190 [-521.600, 57.600], mean action: 3.746 [0.000, 9.000], mean observation: 32.626 [0.001, 422.300], loss: 351.838196, mae: 40.985584, mean_q: -42.135292\n",
            " 1517148/10000000: episode: 7548, duration: 1.456s, episode steps: 201, steps per second: 138, episode reward: 1414.400, mean reward: 7.037 [-10.000, 1029.600], mean action: 3.806 [0.000, 10.000], mean observation: 33.459 [0.001, 497.500], loss: 893.923340, mae: 40.842381, mean_q: -41.843952\n",
            " 1517349/10000000: episode: 7549, duration: 1.410s, episode steps: 201, steps per second: 143, episode reward: -737.600, mean reward: -3.670 [-368.800, 91.800], mean action: 4.353 [0.000, 9.000], mean observation: 29.375 [0.002, 430.700], loss: 909.925659, mae: 39.979561, mean_q: -40.546524\n",
            " 1517550/10000000: episode: 7550, duration: 1.467s, episode steps: 201, steps per second: 137, episode reward: -653.200, mean reward: -3.250 [-326.600, 243.900], mean action: 4.313 [0.000, 9.000], mean observation: 31.655 [0.002, 527.400], loss: 283.775604, mae: 39.398903, mean_q: -39.851715\n",
            " 1517751/10000000: episode: 7551, duration: 1.462s, episode steps: 201, steps per second: 137, episode reward: -690.200, mean reward: -3.434 [-345.100, 143.800], mean action: 3.353 [0.000, 9.000], mean observation: 36.420 [0.000, 720.900], loss: 388.831482, mae: 38.661999, mean_q: -38.970081\n",
            " 1517952/10000000: episode: 7552, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: -67.800, mean reward: -0.337 [-33.900, 204.400], mean action: 3.692 [0.000, 9.000], mean observation: 30.478 [0.001, 500.800], loss: 298.848633, mae: 38.357796, mean_q: -38.671677\n",
            " 1518153/10000000: episode: 7553, duration: 1.418s, episode steps: 201, steps per second: 142, episode reward: -403.200, mean reward: -2.006 [-201.600, 127.600], mean action: 2.547 [0.000, 9.000], mean observation: 33.295 [0.001, 424.400], loss: 424.802307, mae: 38.691917, mean_q: -39.201832\n",
            " 1518354/10000000: episode: 7554, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: 164.400, mean reward: 0.818 [-9.000, 146.200], mean action: 2.945 [0.000, 9.000], mean observation: 32.085 [0.000, 441.700], loss: 394.137054, mae: 37.924206, mean_q: -38.274097\n",
            " 1518555/10000000: episode: 7555, duration: 1.462s, episode steps: 201, steps per second: 138, episode reward: -118.600, mean reward: -0.590 [-59.300, 208.400], mean action: 2.900 [0.000, 9.000], mean observation: 28.400 [0.002, 387.100], loss: 281.114136, mae: 37.359406, mean_q: -37.697250\n",
            " 1518756/10000000: episode: 7556, duration: 1.419s, episode steps: 201, steps per second: 142, episode reward: -777.400, mean reward: -3.868 [-388.700, 55.800], mean action: 2.826 [0.000, 9.000], mean observation: 32.944 [0.002, 537.100], loss: 1099.762573, mae: 37.035282, mean_q: -36.975918\n",
            " 1518957/10000000: episode: 7557, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -406.400, mean reward: -2.022 [-203.200, 126.900], mean action: 3.607 [0.000, 9.000], mean observation: 36.531 [0.001, 562.800], loss: 361.516632, mae: 36.683861, mean_q: -36.197216\n",
            " 1519158/10000000: episode: 7558, duration: 1.453s, episode steps: 201, steps per second: 138, episode reward: -29.800, mean reward: -0.148 [-14.900, 207.000], mean action: 4.070 [0.000, 9.000], mean observation: 30.886 [0.000, 693.300], loss: 260.759094, mae: 35.745415, mean_q: -35.380764\n",
            " 1519359/10000000: episode: 7559, duration: 1.421s, episode steps: 201, steps per second: 141, episode reward: -931.200, mean reward: -4.633 [-465.600, 72.900], mean action: 3.323 [0.000, 9.000], mean observation: 38.944 [0.001, 600.600], loss: 363.303162, mae: 35.367237, mean_q: -35.163212\n",
            " 1519560/10000000: episode: 7560, duration: 1.464s, episode steps: 201, steps per second: 137, episode reward: -900.000, mean reward: -4.478 [-450.000, 207.900], mean action: 3.796 [0.000, 9.000], mean observation: 35.272 [0.000, 727.600], loss: 498.595856, mae: 34.878586, mean_q: -34.792004\n",
            " 1519761/10000000: episode: 7561, duration: 1.512s, episode steps: 201, steps per second: 133, episode reward: 431.600, mean reward: 2.147 [-9.000, 282.600], mean action: 3.896 [0.000, 9.000], mean observation: 35.387 [0.001, 606.200], loss: 1086.436401, mae: 34.976578, mean_q: -35.180065\n",
            " 1519962/10000000: episode: 7562, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -758.400, mean reward: -3.773 [-379.200, 154.800], mean action: 3.796 [0.000, 9.000], mean observation: 31.214 [0.001, 597.900], loss: 278.509552, mae: 34.660019, mean_q: -34.439491\n",
            " 1520163/10000000: episode: 7563, duration: 1.425s, episode steps: 201, steps per second: 141, episode reward: -817.800, mean reward: -4.069 [-408.900, 106.200], mean action: 3.816 [0.000, 9.000], mean observation: 33.954 [0.003, 593.100], loss: 223.783157, mae: 34.480324, mean_q: -34.364487\n",
            " 1520364/10000000: episode: 7564, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -756.600, mean reward: -3.764 [-378.300, 72.900], mean action: 3.229 [0.000, 9.000], mean observation: 28.054 [0.002, 435.000], loss: 178.156311, mae: 34.323772, mean_q: -34.246304\n",
            " 1520565/10000000: episode: 7565, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -645.600, mean reward: -3.212 [-322.800, 93.600], mean action: 3.000 [0.000, 9.000], mean observation: 32.817 [0.001, 697.100], loss: 399.093781, mae: 34.247868, mean_q: -34.152187\n",
            " 1520766/10000000: episode: 7566, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -682.000, mean reward: -3.393 [-341.000, 97.200], mean action: 3.587 [0.000, 9.000], mean observation: 29.946 [0.001, 562.800], loss: 428.886810, mae: 33.960190, mean_q: -33.913990\n",
            " 1520967/10000000: episode: 7567, duration: 1.436s, episode steps: 201, steps per second: 140, episode reward: -983.600, mean reward: -4.894 [-491.800, 70.800], mean action: 3.333 [0.000, 9.000], mean observation: 31.896 [0.000, 458.900], loss: 399.194702, mae: 33.792088, mean_q: -33.959393\n",
            " 1521168/10000000: episode: 7568, duration: 1.471s, episode steps: 201, steps per second: 137, episode reward: -1172.800, mean reward: -5.835 [-586.400, 52.400], mean action: 3.726 [0.000, 9.000], mean observation: 33.999 [0.002, 522.200], loss: 265.915863, mae: 33.542770, mean_q: -33.720150\n",
            " 1521369/10000000: episode: 7569, duration: 1.486s, episode steps: 201, steps per second: 135, episode reward: -66.600, mean reward: -0.331 [-33.300, 172.200], mean action: 3.055 [0.000, 9.000], mean observation: 28.335 [0.001, 521.600], loss: 241.539627, mae: 33.782120, mean_q: -34.063854\n",
            " 1521570/10000000: episode: 7570, duration: 1.417s, episode steps: 201, steps per second: 142, episode reward: 407.400, mean reward: 2.027 [-9.000, 370.800], mean action: 2.816 [0.000, 9.000], mean observation: 34.690 [0.000, 527.200], loss: 419.680511, mae: 33.854366, mean_q: -33.935375\n",
            " 1521771/10000000: episode: 7571, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -623.200, mean reward: -3.100 [-311.600, 39.000], mean action: 2.269 [0.000, 9.000], mean observation: 33.965 [0.003, 451.200], loss: 345.249756, mae: 34.004925, mean_q: -34.090725\n",
            " 1521972/10000000: episode: 7572, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -515.400, mean reward: -2.564 [-257.700, 85.000], mean action: 2.443 [0.000, 9.000], mean observation: 31.514 [0.000, 765.000], loss: 1186.522583, mae: 33.751633, mean_q: -33.697762\n",
            " 1522173/10000000: episode: 7573, duration: 1.466s, episode steps: 201, steps per second: 137, episode reward: 118.800, mean reward: 0.591 [-9.000, 145.800], mean action: 2.368 [0.000, 9.000], mean observation: 37.007 [0.002, 439.000], loss: 400.991455, mae: 33.335712, mean_q: -33.298397\n",
            " 1522374/10000000: episode: 7574, duration: 1.489s, episode steps: 201, steps per second: 135, episode reward: -422.200, mean reward: -2.100 [-211.100, 58.800], mean action: 2.373 [0.000, 9.000], mean observation: 36.010 [0.000, 793.800], loss: 216.778915, mae: 33.447075, mean_q: -33.468624\n",
            " 1522575/10000000: episode: 7575, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -293.000, mean reward: -1.458 [-146.500, 55.200], mean action: 2.433 [0.000, 9.000], mean observation: 34.358 [0.000, 633.800], loss: 960.559265, mae: 33.878555, mean_q: -33.655090\n",
            " 1522776/10000000: episode: 7576, duration: 1.478s, episode steps: 201, steps per second: 136, episode reward: -400.600, mean reward: -1.993 [-200.300, 146.700], mean action: 2.343 [0.000, 9.000], mean observation: 36.438 [0.000, 701.500], loss: 1540.850464, mae: 32.798912, mean_q: -31.644590\n",
            " 1522977/10000000: episode: 7577, duration: 1.461s, episode steps: 201, steps per second: 138, episode reward: -390.200, mean reward: -1.941 [-195.100, 60.300], mean action: 1.970 [0.000, 9.000], mean observation: 40.016 [0.002, 542.400], loss: 220.670242, mae: 32.061947, mean_q: -31.065565\n",
            " 1523178/10000000: episode: 7578, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: 1010.000, mean reward: 5.025 [-9.000, 523.500], mean action: 1.761 [0.000, 9.000], mean observation: 32.554 [0.000, 783.800], loss: 868.490723, mae: 31.643162, mean_q: -30.423405\n",
            " 1523379/10000000: episode: 7579, duration: 1.475s, episode steps: 201, steps per second: 136, episode reward: -194.200, mean reward: -0.966 [-97.100, 67.000], mean action: 2.075 [0.000, 9.000], mean observation: 37.921 [0.000, 620.700], loss: 401.344635, mae: 31.128567, mean_q: -30.097143\n",
            " 1523580/10000000: episode: 7580, duration: 1.432s, episode steps: 201, steps per second: 140, episode reward: -357.800, mean reward: -1.780 [-178.900, 58.500], mean action: 2.035 [0.000, 9.000], mean observation: 36.527 [0.002, 515.200], loss: 282.300537, mae: 31.056429, mean_q: -30.386648\n",
            " 1523781/10000000: episode: 7581, duration: 1.437s, episode steps: 201, steps per second: 140, episode reward: -181.800, mean reward: -0.904 [-90.900, 122.400], mean action: 2.428 [0.000, 9.000], mean observation: 34.092 [0.000, 673.700], loss: 987.884766, mae: 30.986099, mean_q: -30.496576\n",
            " 1523982/10000000: episode: 7582, duration: 1.499s, episode steps: 201, steps per second: 134, episode reward: -574.200, mean reward: -2.857 [-287.100, 146.500], mean action: 2.602 [0.000, 9.000], mean observation: 36.921 [0.002, 446.200], loss: 328.500092, mae: 30.589331, mean_q: -30.255548\n",
            " 1524183/10000000: episode: 7583, duration: 1.552s, episode steps: 201, steps per second: 130, episode reward: -170.200, mean reward: -0.847 [-85.100, 68.000], mean action: 1.965 [0.000, 9.000], mean observation: 33.242 [0.001, 459.000], loss: 413.810486, mae: 30.715242, mean_q: -30.032917\n",
            " 1524384/10000000: episode: 7584, duration: 1.584s, episode steps: 201, steps per second: 127, episode reward: -231.000, mean reward: -1.149 [-115.500, 154.800], mean action: 2.234 [0.000, 9.000], mean observation: 37.004 [0.001, 558.800], loss: 281.272888, mae: 30.765791, mean_q: -30.184750\n",
            " 1524585/10000000: episode: 7585, duration: 1.585s, episode steps: 201, steps per second: 127, episode reward: -54.200, mean reward: -0.270 [-27.100, 155.700], mean action: 2.468 [0.000, 9.000], mean observation: 33.629 [0.000, 933.200], loss: 421.514862, mae: 30.850161, mean_q: -30.446514\n",
            " 1524786/10000000: episode: 7586, duration: 1.480s, episode steps: 201, steps per second: 136, episode reward: 189.600, mean reward: 0.943 [-9.000, 126.000], mean action: 2.682 [0.000, 9.000], mean observation: 32.509 [0.000, 684.900], loss: 1074.114746, mae: 30.517525, mean_q: -30.092026\n",
            " 1524987/10000000: episode: 7587, duration: 1.440s, episode steps: 201, steps per second: 140, episode reward: -570.400, mean reward: -2.838 [-285.200, 64.800], mean action: 2.532 [0.000, 9.000], mean observation: 30.794 [0.002, 507.800], loss: 299.906799, mae: 30.476912, mean_q: -30.347462\n",
            " 1525188/10000000: episode: 7588, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: 24.000, mean reward: 0.119 [-9.000, 146.000], mean action: 2.378 [0.000, 9.000], mean observation: 33.300 [0.000, 556.700], loss: 312.809143, mae: 30.924662, mean_q: -30.670870\n",
            " 1525389/10000000: episode: 7589, duration: 1.426s, episode steps: 201, steps per second: 141, episode reward: -453.200, mean reward: -2.255 [-226.600, 25.200], mean action: 1.776 [0.000, 9.000], mean observation: 34.532 [0.001, 444.500], loss: 276.150269, mae: 30.824753, mean_q: -30.416901\n",
            " 1525590/10000000: episode: 7590, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: -5.800, mean reward: -0.029 [-9.000, 252.900], mean action: 2.433 [0.000, 9.000], mean observation: 36.869 [0.000, 607.900], loss: 370.921478, mae: 30.662092, mean_q: -30.575178\n",
            " 1525791/10000000: episode: 7591, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: 155.600, mean reward: 0.774 [-9.000, 207.000], mean action: 2.677 [0.000, 9.000], mean observation: 35.056 [0.000, 528.400], loss: 322.080139, mae: 30.956020, mean_q: -31.043371\n",
            " 1525992/10000000: episode: 7592, duration: 1.368s, episode steps: 201, steps per second: 147, episode reward: 130.600, mean reward: 0.650 [-9.000, 332.500], mean action: 2.562 [0.000, 9.000], mean observation: 29.290 [0.001, 455.800], loss: 311.787201, mae: 30.801476, mean_q: -30.798004\n",
            " 1526193/10000000: episode: 7593, duration: 1.414s, episode steps: 201, steps per second: 142, episode reward: -253.000, mean reward: -1.259 [-126.500, 64.000], mean action: 2.214 [0.000, 9.000], mean observation: 31.458 [0.000, 818.500], loss: 1025.374878, mae: 30.939123, mean_q: -30.755714\n",
            " 1526394/10000000: episode: 7594, duration: 1.412s, episode steps: 201, steps per second: 142, episode reward: -537.400, mean reward: -2.674 [-268.700, 41.000], mean action: 2.294 [0.000, 9.000], mean observation: 32.095 [0.002, 441.900], loss: 1101.772583, mae: 30.576920, mean_q: -30.417650\n",
            " 1526595/10000000: episode: 7595, duration: 1.398s, episode steps: 201, steps per second: 144, episode reward: -740.200, mean reward: -3.683 [-370.100, 40.400], mean action: 2.637 [0.000, 9.000], mean observation: 32.583 [0.001, 619.000], loss: 272.242615, mae: 30.314381, mean_q: -30.480978\n",
            " 1526796/10000000: episode: 7596, duration: 1.490s, episode steps: 201, steps per second: 135, episode reward: -538.200, mean reward: -2.678 [-269.100, 32.400], mean action: 2.199 [0.000, 9.000], mean observation: 34.148 [0.001, 412.100], loss: 281.211182, mae: 30.465179, mean_q: -30.595345\n",
            " 1526997/10000000: episode: 7597, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -619.200, mean reward: -3.081 [-309.600, 62.900], mean action: 2.552 [0.000, 9.000], mean observation: 33.256 [0.000, 500.900], loss: 559.582825, mae: 30.452839, mean_q: -30.497080\n",
            " 1527198/10000000: episode: 7598, duration: 1.427s, episode steps: 201, steps per second: 141, episode reward: -24.600, mean reward: -0.122 [-12.300, 181.800], mean action: 2.746 [0.000, 9.000], mean observation: 38.177 [0.001, 501.600], loss: 274.565094, mae: 30.353292, mean_q: -30.460571\n",
            " 1527399/10000000: episode: 7599, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: 147.800, mean reward: 0.735 [-10.000, 295.200], mean action: 2.697 [0.000, 10.000], mean observation: 31.951 [0.001, 590.900], loss: 418.687958, mae: 30.353302, mean_q: -30.477448\n",
            " 1527600/10000000: episode: 7600, duration: 1.397s, episode steps: 201, steps per second: 144, episode reward: 20.800, mean reward: 0.103 [-9.000, 197.800], mean action: 3.264 [0.000, 9.000], mean observation: 30.391 [0.000, 567.300], loss: 327.445160, mae: 30.469080, mean_q: -30.615128\n",
            " 1527801/10000000: episode: 7601, duration: 1.401s, episode steps: 201, steps per second: 143, episode reward: -141.000, mean reward: -0.701 [-70.500, 169.200], mean action: 2.697 [0.000, 9.000], mean observation: 37.573 [0.000, 747.100], loss: 265.431702, mae: 30.917017, mean_q: -31.235088\n",
            " 1528002/10000000: episode: 7602, duration: 1.443s, episode steps: 201, steps per second: 139, episode reward: -162.000, mean reward: -0.806 [-81.000, 86.400], mean action: 2.682 [0.000, 9.000], mean observation: 29.925 [0.000, 534.800], loss: 281.261047, mae: 31.053585, mean_q: -31.398705\n",
            " 1528203/10000000: episode: 7603, duration: 1.404s, episode steps: 201, steps per second: 143, episode reward: -79.400, mean reward: -0.395 [-39.700, 247.500], mean action: 3.453 [0.000, 9.000], mean observation: 29.630 [0.000, 478.800], loss: 1053.868652, mae: 30.852753, mean_q: -30.958218\n",
            " 1528404/10000000: episode: 7604, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -761.800, mean reward: -3.790 [-380.900, 96.500], mean action: 3.378 [0.000, 9.000], mean observation: 34.986 [0.000, 580.300], loss: 178.875916, mae: 30.733665, mean_q: -31.072680\n",
            " 1528605/10000000: episode: 7605, duration: 1.413s, episode steps: 201, steps per second: 142, episode reward: 122.400, mean reward: 0.609 [-9.000, 162.600], mean action: 2.970 [0.000, 9.000], mean observation: 32.479 [0.000, 589.600], loss: 279.406097, mae: 31.018520, mean_q: -31.315109\n",
            " 1528806/10000000: episode: 7606, duration: 1.449s, episode steps: 201, steps per second: 139, episode reward: -640.000, mean reward: -3.184 [-320.000, 93.000], mean action: 3.348 [0.000, 9.000], mean observation: 30.840 [0.000, 813.800], loss: 466.697388, mae: 31.168308, mean_q: -31.659657\n",
            " 1529007/10000000: episode: 7607, duration: 1.441s, episode steps: 201, steps per second: 140, episode reward: -747.800, mean reward: -3.720 [-373.900, 19.200], mean action: 2.418 [0.000, 9.000], mean observation: 37.472 [0.000, 621.900], loss: 240.852509, mae: 31.406162, mean_q: -32.002075\n",
            " 1529208/10000000: episode: 7608, duration: 1.434s, episode steps: 201, steps per second: 140, episode reward: -357.800, mean reward: -1.780 [-178.900, 216.300], mean action: 2.915 [0.000, 9.000], mean observation: 30.131 [0.002, 632.400], loss: 337.902863, mae: 31.700043, mean_q: -32.491001\n",
            " 1529409/10000000: episode: 7609, duration: 1.470s, episode steps: 201, steps per second: 137, episode reward: -697.000, mean reward: -3.468 [-348.500, 90.000], mean action: 3.428 [0.000, 9.000], mean observation: 34.293 [0.003, 531.400], loss: 268.262909, mae: 31.607716, mean_q: -32.382496\n",
            " 1529610/10000000: episode: 7610, duration: 1.463s, episode steps: 201, steps per second: 137, episode reward: -638.400, mean reward: -3.176 [-319.200, 122.400], mean action: 3.323 [0.000, 9.000], mean observation: 35.425 [0.001, 519.900], loss: 550.738647, mae: 31.780655, mean_q: -32.509396\n",
            " 1529811/10000000: episode: 7611, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: -654.800, mean reward: -3.258 [-327.400, 38.400], mean action: 2.886 [0.000, 9.000], mean observation: 37.437 [0.001, 447.800], loss: 979.930664, mae: 31.797987, mean_q: -32.334831\n",
            " 1530012/10000000: episode: 7612, duration: 1.460s, episode steps: 201, steps per second: 138, episode reward: 450.400, mean reward: 2.241 [-9.000, 285.500], mean action: 2.995 [0.000, 9.000], mean observation: 40.411 [0.001, 627.200], loss: 306.498901, mae: 31.417673, mean_q: -31.782333\n",
            " 1530213/10000000: episode: 7613, duration: 1.532s, episode steps: 201, steps per second: 131, episode reward: -496.600, mean reward: -2.471 [-248.300, 63.000], mean action: 2.507 [0.000, 9.000], mean observation: 38.836 [0.003, 501.700], loss: 383.641510, mae: 31.691114, mean_q: -32.162273\n",
            " 1530414/10000000: episode: 7614, duration: 1.411s, episode steps: 201, steps per second: 142, episode reward: -133.400, mean reward: -0.664 [-66.700, 99.200], mean action: 2.493 [0.000, 9.000], mean observation: 30.026 [0.000, 604.500], loss: 291.690765, mae: 31.744089, mean_q: -32.082870\n",
            " 1530615/10000000: episode: 7615, duration: 2.169s, episode steps: 201, steps per second: 93, episode reward: -279.600, mean reward: -1.391 [-139.800, 84.900], mean action: 2.294 [0.000, 9.000], mean observation: 31.141 [0.001, 437.600], loss: 368.590759, mae: 32.166767, mean_q: -32.606129\n",
            " 1530816/10000000: episode: 7616, duration: 2.005s, episode steps: 201, steps per second: 100, episode reward: -299.000, mean reward: -1.488 [-149.500, 85.200], mean action: 2.512 [0.000, 9.000], mean observation: 29.578 [0.001, 464.600], loss: 471.946899, mae: 31.993393, mean_q: -32.449074\n",
            " 1531017/10000000: episode: 7617, duration: 1.405s, episode steps: 201, steps per second: 143, episode reward: 74.400, mean reward: 0.370 [-9.000, 172.800], mean action: 2.562 [0.000, 9.000], mean observation: 36.398 [0.001, 531.100], loss: 411.946930, mae: 32.225307, mean_q: -32.710976\n",
            " 1531218/10000000: episode: 7618, duration: 1.399s, episode steps: 201, steps per second: 144, episode reward: -266.600, mean reward: -1.326 [-133.300, 94.800], mean action: 2.448 [0.000, 9.000], mean observation: 36.139 [0.000, 467.700], loss: 917.696655, mae: 32.174721, mean_q: -32.413849\n",
            " 1531419/10000000: episode: 7619, duration: 1.415s, episode steps: 201, steps per second: 142, episode reward: -60.200, mean reward: -0.300 [-30.100, 274.000], mean action: 2.607 [0.000, 10.000], mean observation: 37.161 [0.000, 784.200], loss: 711.224670, mae: 31.854856, mean_q: -32.201717\n",
            " 1531620/10000000: episode: 7620, duration: 1.394s, episode steps: 201, steps per second: 144, episode reward: -376.200, mean reward: -1.872 [-188.100, 52.500], mean action: 2.070 [0.000, 9.000], mean observation: 32.208 [0.000, 507.700], loss: 437.078583, mae: 31.399479, mean_q: -31.171215\n",
            " 1531821/10000000: episode: 7621, duration: 1.420s, episode steps: 201, steps per second: 142, episode reward: -685.200, mean reward: -3.409 [-342.600, 26.800], mean action: 2.303 [0.000, 9.000], mean observation: 31.930 [0.001, 542.200], loss: 307.525146, mae: 31.150579, mean_q: -30.927486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAzrMyOTHmxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習後のモデルの重みの出力\n",
        "dqn.save_weights(\"h5f/my_dqn_weights_try_modify_reward.h5f\", overwrite=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58y45XXrVcZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952bcfa0-d2a3-4d18-a2ca-d7a488472e1e"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for obs in cb_train.rewards.values():\n",
        "    plt.plot([o for o in obs])\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"pos\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-70ae0065d17b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcb_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cb_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uorHBKYVHaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dqn.load_weights(\"h5f/my_dqn_weights_try_modify_reward.h5f\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QATBnmB8awC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e95d09-c5a8-470e-d67f-7241706e3bbd"
      },
      "source": [
        "cb_ep = EpisodeLogger()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-943a0e005c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcb_ep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpisodeLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'EpisodeLogger' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHT0OfELHqrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習済モデルに対して、テストを実行 （必要に応じて適切なCallbackも定義、設定可能）\n",
        "dqn.test(env, nb_episodes=20, visualize=False, callbacks=[cb_ep])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-A5ApZnHvx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for obs in cb_ep.observations.values():\n",
        "    plt.plot([o for o in obs])\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"pos\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6kBmjN9TvYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(cb_ep.actions[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5l3xRXuTzjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reward_amount = 0\n",
        "for obs in cb_ep.rewards.values():\n",
        "  print(np.array(obs).sum())\n",
        "    # for o in obs:\n",
        "      # print(o)\n",
        "      # reward_amount += o\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AybG9q1OtZst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テスト結果の報酬合計\n",
        "np.concatenate(list(cb_ep.rewards.values())).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUdALFu1aCCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list(cb_ep.rewards.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob9biREnf6lU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}